

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="NSX">
  <meta name="keywords" content="">
  
    <meta name="description" content="Lightgbm介绍LightGBM 是Light Gradient Boosted Machine的缩写，是Microsoft开发的用于机器学习的免费开源分布式梯度提升框架。它基于决策树算法，用于排名，分类和其他机器学习任务。开发重点是性能和可伸缩性。该框架支持不同的算法，包括GBT，GBDT，GBRT，GBM和MART。 本文介绍Lightgbm的常用参数以及调参方法。最后给出一个实用超参数优">
<meta property="og:type" content="article">
<meta property="og:title" content="Lightgbm使用指南">
<meta property="og:url" content="http://example.com/2020/08/24/2020-08-24-Lightgbm%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/index.html">
<meta property="og:site_name" content="神的个人博客">
<meta property="og:description" content="Lightgbm介绍LightGBM 是Light Gradient Boosted Machine的缩写，是Microsoft开发的用于机器学习的免费开源分布式梯度提升框架。它基于决策树算法，用于排名，分类和其他机器学习任务。开发重点是性能和可伸缩性。该框架支持不同的算法，包括GBT，GBDT，GBRT，GBM和MART。 本文介绍Lightgbm的常用参数以及调参方法。最后给出一个实用超参数优">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/13876065-d31ee3257cba9977.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/600/format/webp">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/13876065-6ef4ba7cff5dca51.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/557/format/webp">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/13876065-1eed3540256cb9d3.png?imageMogr2/auto-orient/strip|imageView2/2/w/596/format/webp">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/13876065-4e4e2cd1bff18db7.png?imageMogr2/auto-orient/strip|imageView2/2/w/769/format/webp">
<meta property="og:image" content="https://img-blog.csdn.net/20180719163927276?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180719172833390?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/2018071916292255?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180804213102717?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180804213248677?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180804213500786?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://static.bookstack.cn/projects/huaxiaozhuan-ai/7fbff4847407186f47048e68c9f3567f.svg">
<meta property="og:image" content="https://static.bookstack.cn/projects/huaxiaozhuan-ai/bcee7d20347ab929a0ad08ef43059880.svg">
<meta property="og:image" content="https://static.bookstack.cn/projects/huaxiaozhuan-ai/7fbff4847407186f47048e68c9f3567f.svg">
<meta property="article:published_time" content="2020-08-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-04-23T10:28:32.053Z">
<meta property="article:author" content="Ning Shixian">
<meta property="article:tag" content="技术">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/13876065-d31ee3257cba9977.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/600/format/webp">
  
  
  
  <title>Lightgbm使用指南 - 神的个人博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>神的个人博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Lightgbm调参方法以及optuna超参数优化"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2020-08-24 00:00" pubdate>
          2020年8月24日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          154 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Lightgbm调参方法以及optuna超参数优化</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Lightgbm介绍"><a href="#Lightgbm介绍" class="headerlink" title="Lightgbm介绍"></a>Lightgbm介绍</h1><p><a target="_blank" rel="noopener" href="https://github.com/Microsoft/LightGBM">LightGBM</a> 是Light Gradient Boosted Machine的缩写，是Microsoft开发的用于机器学习的免费开源分布式梯度提升框架。它基于决策树算法，用于排名，分类和其他机器学习任务。开发重点是性能和可伸缩性。该框架支持不同的算法，包括GBT，GBDT，GBRT，GBM和MART。</p>
<p>本文介绍Lightgbm的常用参数以及调参方法。最后给出一个实用超参数优化库optuna来帮助实现参数的随机搜索！</p>
<ul>
<li>LightGBM的介绍及优势</li>
<li>LightGBM的使用（代码）</li>
<li>LightGBM的调参指导</li>
<li>LightGBM的API接口方法</li>
<li><p>附录：optuna超参数优化</p>
<span id="more"></span>
</li>
</ul>
<h1 id="LightGBM在哪些地方进行了优化-区别XGBoost-？"><a href="#LightGBM在哪些地方进行了优化-区别XGBoost-？" class="headerlink" title="LightGBM在哪些地方进行了优化    (区别XGBoost)？"></a>LightGBM在哪些地方进行了优化    (区别XGBoost)？</h1><p>参考《<a target="_blank" rel="noopener" href="https://blog.csdn.net/huacha__/article/details/81057150">LightGBM——提升机器算法（图解+理论）</a>》</p>
<ul>
<li>基于Histogram的决策树算法（直方图优化）</li>
<li>LightGBM的生长策略（基于最大深度的Leaf-wise）</li>
<li>直方图做差加速</li>
<li>支持类别特征</li>
<li>Cache命中率优化</li>
<li>支持并行学习</li>
</ul>
<p><img src="https:////upload-images.jianshu.io/upload_images/13876065-d31ee3257cba9977.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/600/format/webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>image</p>
<h2 id="1-1-直方图优化"><a href="#1-1-直方图优化" class="headerlink" title="1.1 直方图优化"></a>1.1 直方图优化</h2><p>直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数（其实又是分桶的思想，而这些桶称为bin，比如[0,0.1)→0, [0.1,0.3)→1），同时构造一个宽度为k的直方图。</p>
<p>在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13876065-6ef4ba7cff5dca51.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/557/format/webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>使用直方图算法有很多优点。首先，最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。然后在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data<em>#feature)优化到O(k</em>#features)。</p>
<h2 id="1-2-带深度限制的Leaf-wise的叶子生长策略"><a href="#1-2-带深度限制的Leaf-wise的叶子生长策略" class="headerlink" title="1.2 带深度限制的Leaf-wise的叶子生长策略"></a>1.2 带深度限制的Leaf-wise的叶子生长策略</h2><p>在XGBoost中，树是按层生长的，称为Level-wise tree growth，同一层的所有节点都做分裂，最后剪枝，如下图所示：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13876065-1eed3540256cb9d3.png?imageMogr2/auto-orient/strip|imageView2/2/w/596/format/webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
<p>在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise)<br> 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise)算法。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13876065-4e4e2cd1bff18db7.png?imageMogr2/auto-orient/strip|imageView2/2/w/769/format/webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<h2 id="1-3-直方图做差优化"><a href="#1-3-直方图做差优化" class="headerlink" title="1.3 直方图做差优化"></a>1.3 直方图做差优化</h2><p>LightGBM另一个优化是Histogram（直方图）做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。</p>
<p>利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
<p><img src="https://img-blog.csdn.net/20180719163927276?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" srcset="/img/loading.gif" lazyload alt="img"></p>
<h2 id="1-4-直接支持类别特征"><a href="#1-4-直接支持类别特征" class="headerlink" title="1.4 直接支持类别特征"></a>1.4 直接支持类别特征</h2><p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的0/1特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。</p>
<p>基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。在Expo数据集上的实验，相比0/1展开的方法，训练速度可以加速8倍，并且精度一致。<strong>LightGBM是第一个直接支持类别特征的GBDT工具</strong>。</p>
<p><img src="https://img-blog.csdn.net/20180719172833390?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" srcset="/img/loading.gif" lazyload alt="img"></p>
<h2 id="1-5-Cache命中率优化"><a href="#1-5-Cache命中率优化" class="headerlink" title="1.5 Cache命中率优化"></a>1.5 Cache命中率优化</h2><p>当我们用数据的bin描述数据特征的时候带来的变化：首先是不需要像预排序算法那样去存储每一个排序后数据的序列，也就是下图灰色的表，在LightGBM中，这部分的计算代价是0；第二个，一般bin会控制在一个比较小的范围，所以我们可以用更小的内存来存储</p>
<p><img src="https://img-blog.csdn.net/2018071916292255?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" srcset="/img/loading.gif" lazyload alt="img"></p>
<h2 id="1-6-支持并行学习"><a href="#1-6-支持并行学习" class="headerlink" title="1.6 支持并行学习"></a>1.6 支持并行学习</h2><p>LightGBM原生支持并行学习，目前支持<strong>特征并行(Featrue Parallelization)</strong>和<strong>数据并行(Data Parallelization)</strong>两种，还有一种是<strong>基于投票的数据并行(Voting Parallelization)</strong></p>
<ul>
<li><strong>特征并行</strong>的主要思想是在不同机器、在<strong>不同的特征集合</strong>上分别寻找最优的分割点，然后在机器间同步最优的分割点。</li>
<li><strong>数据并行</strong>则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后<strong>在合并的直方图上面</strong>寻找最优分割点。</li>
</ul>
<p>LightGBM针对这两种并行方法都做了优化。</p>
<ul>
<li><strong>特征并行</strong>算法中，通过在本地保存全部数据避免对数据切分结果的通信。</li>
<li><strong>数据并行</strong>中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。</li>
<li><strong>基于投票的数据并行(Voting Parallelization)</strong>则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。</li>
</ul>
<p>下图更好的说明了以上这三种并行学习的整体流程：</p>
<p><img src="https://img-blog.csdn.net/20180804213102717?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" srcset="/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://img-blog.csdn.net/20180804213248677?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>在直方图合并的时候，通信代价比较大，基于投票的数据并行能够很好的解决这一点。</p>
<p><img src="https://img-blog.csdn.net/20180804213500786?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" srcset="/img/loading.gif" lazyload alt="img"></p>
<h2 id="1-7-LightGBM-vs-XGBoost"><a href="#1-7-LightGBM-vs-XGBoost" class="headerlink" title="1.7 LightGBM vs. XGBoost"></a>1.7 LightGBM vs. XGBoost</h2><p>下面这个表格给出了XGBoost和<a target="_blank" rel="noopener" href="https://github.com/Microsoft/LightGBM">LightGBM</a> （Light Gradient Boosting Machine）之间更加细致的性能对比，包括了树的生长方式，LightGBM是直接去选择获得最大收益的结点来展开，而XGBoost是通过按层增长的方式来做，这样呢LightGBM能够在更小的计算代价上建立我们需要的决策树。当然在这样的算法中我们也需要控制树的深度和每个叶子结点的最小数据量，从而减少过拟合。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><strong>XGBoost</strong></th>
<th><strong>LightGBM</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>树木生长算法</td>
<td><strong>按层生长的方式</strong>有利于工程优化，但对学习模型效率不高</td>
<td>直接<strong>选择最大收益的节点</strong>来展开，在更小的计算代价上去选择我们需要的决策树控制树的深度和每个叶子节点的数据量，能减少过拟合</td>
</tr>
<tr>
<td>划分点搜索算 法</td>
<td>对特征预排序的方法</td>
<td>直方图算法：将特征值分成许多小筒，进而在筒上搜索分裂点，减少了计算代价和存储代价，得到更好的性能。另外数据结构的变化使得在细节处的变化理上效率会不同</td>
</tr>
<tr>
<td>内存开销</td>
<td>8个字节</td>
<td>1个字节</td>
</tr>
<tr>
<td>划分的计算增益</td>
<td>数据特征</td>
<td>容器特征</td>
</tr>
<tr>
<td>高速缓存优化</td>
<td>无</td>
<td>在Higgs数据集上加速40%</td>
</tr>
<tr>
<td>类别特征处理</td>
<td>无</td>
<td>在Expo数据集上速度快了8倍</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Lightgbm的实战应用（代码）"><a href="#Lightgbm的实战应用（代码）" class="headerlink" title="Lightgbm的实战应用（代码）"></a>Lightgbm的实战应用（代码）</h1><blockquote>
<p>参考: </p>
<p>1、用lightgbm算法实现鸢尾花种类的分类任务，GitHub：<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FNLP-LOVE%2FML-NLP%2Fblob%2Fmaster%2FMachine%20Learning%2F3.4%20LightGBM%2F3.4%20LightGBM.ipynb">点击进入</a></p>
<p>2、<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247539023&amp;idx=1&amp;sn=b105f6b08d3b886f62e16b367eabfd02&amp;chksm=e8738802df0401146dfdd1a9052f7848d20c6439e39299d065d89f77e6604a97609430f86923&amp;scene=126&amp;sessionid=1604042119&amp;key=c3402f98b9ff36463480cd8ac280815c46524dc60b794c1a66aef363ca2df3e0af7ad5b74aa091fdf8bd3b1142bc96d10c3b6be0d9be874a1554e7300b8368731a5610c2dc0ccf10d3b5492c95c08b1ca2b216e355bedb99cf9f36ab5892b229c0e9875addd229e8354b3fedba81cf192d0c5b3fcfc3b9f7c64124742c31c959&amp;ascene=1&amp;uin=MjM2MDA1NjcyMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=AyhYM9JTEQwg%2Fzwv1Q%2F9PJQ%3D&amp;pass_ticket=r5dszQk2lRNNteX%2BZmX6%2B1wF2g3D57o1FKmEWEXG6IdPb9qLFiUO6rf4TKQjjajJ&amp;wx_header=0">比赛杀器LightGBM常用操作总结！</a></p>
<p><a href="LightGBM小课.ipynb">3、《机器学习⼩课堂之初探 LightGBM》，强烈建议逐行阅读学习！</a></p>
</blockquote>
<p>lightgbm的使用起来也很简单。大致步骤可以分为下面几个</p>
<ul>
<li>首先用lgb包的DataSet类包装一下需要测试的数据；</li>
<li>将lightgbm的参数构成一个dict字典格式的变量</li>
<li>将参数字典，训练样本，测试样本，评价指标一股脑的塞进lgb.train()方法的参数中去</li>
<li>上一步的方法会自觉地得到最佳参数和最佳的模型，保存模型</li>
<li>使用模型进行测试集的预测</li>
</ul>
<p>其中比较重要的是第二步也就是设置参数。有很多很重要的参数，在下面的第二部分（参数字典）中，我大概介绍一下使用的比较多的比较有意义的参数。</p>
<h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install lightgbm<br>pip install --no-binary :<span class="hljs-built_in">all</span>: lightgbm <span class="hljs-comment">#从源码编译安装</span><br>pip install lightgbm --install-option=--mpi <span class="hljs-comment">#从源码编译安装 MPI 版本</span><br>pip install lightgbm --install-option=--gpu <span class="hljs-comment">#从源码编译安装 GPU 版本</span><br></code></pre></td></tr></table></figure>
<h2 id="2-定义数据集"><a href="#2-定义数据集" class="headerlink" title="2. 定义数据集"></a>2. 定义数据集</h2><p>lightgbm的一些特点：</p>
<ul>
<li>LightGBM 支持 <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FComma-separated_values">CSV</a>, <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTab-separated_values">TSV</a> 和 <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.csie.ntu.edu.tw%2F~cjlin%2Flibsvm%2F">LibSVM</a> 格式的输入数据文件。</li>
<li><strong>LightGBM 可以直接使用 categorical feature（类别特征）（不需要单独编码）</strong>。 <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=http%3A%2F%2Fstat-computing.org%2Fdataexpo%2F2009%2F">Expo data</a> 实验显示，与 one-hot 编码相比，其速度提高了 8 倍。可以在包装数据的时候指定哪些属性是类别特征（也可以使用pd.DataFrame存放特征X, 每一列表示1个特征, 将类别特征设置为X[cat_cols].astype(‘category’). 这样模型在fit时会自动识别类别特征 <a target="_blank" rel="noopener" href="https://blog.csdn.net/u013385018/article/details/104167969">参考</a>）</li>
<li>LightGBM 也支持加权训练，可以在包装数据的时候指定每条记录的权重</li>
</ul>
<p>LightGBM 中的 Dataset 对象由于只需要保存 discrete bins（离散的数据块）, 因此它具有很好的内存效率. 然而, Numpy/Array/Pandas 对象的内存开销较大. 如果你关心你的内存消耗. 您可以根据以下方式来节省内存:</p>
<ul>
<li>在构造 Dataset 时设置 free_raw_data=True （默认为 True）</li>
<li>在 Dataset 被构造完之后手动设置 raw_data=None</li>
<li>调用 gc</li>
</ul>
<p>LightGBM Python 模块能够使用以下几种方式来加载数据:</p>
<ul>
<li>libsvm/tsv/csv txt format file（libsvm/tsv/csv 文本文件格式）</li>
<li>Numpy 2D array, pandas object（Numpy 2维数组, pandas 对象）</li>
<li>LightGBM binary file（LightGBM 二进制文件）<br> 加载后的数据存在 Dataset 对象中.</li>
</ul>
<p>要加载 numpy 数组到 Dataset 中:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># data = np.arange(0, 5000).reshape((500, 10))</span><br>data = np.random.rand(500, 10)  <span class="hljs-comment"># 500 个样本, 每一个包含 10 个特征</span><br>label = np.random.randint(2, size=500)  <span class="hljs-comment"># 二元目标变量,  0 和 1</span><br>train_data = lgb.Dataset(data, label=label)<br></code></pre></td></tr></table></figure>
<p>在现实情况下，我们可能之前使用的是pandas的dataFrame格式在训练数据，那也没有关系，可以先使用sklearn包对训练集和测试集进行划分，然后再使用DataSet类包装。DataSet第一个参数是训练特征，第二个参数是标签</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jsx"><span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">model_selection</span> <span class="hljs-keyword">import</span> train_test_split<br>X_train,X_val,y_train,y_val = <span class="hljs-title function_">train_test_split</span>(X,Y,test_size=<span class="hljs-number">0.2</span>)<br>xgtrain = lgb.<span class="hljs-title class_">Dataset</span>(X_train, y_train)<br>xgvalid = lgb.<span class="hljs-title class_">Dataset</span>(X_val, y_val, reference=xgtrain)<br></code></pre></td></tr></table></figure>
<p>在 LightGBM 中, 验证数据应该与训练数据一致（格式一致）.<br> 保存 Dataset 到 LightGBM 二进制文件将会使得加载更快速:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">train_data = lgb.Dataset(<span class="hljs-string">&#x27;train.svm.txt&#x27;</span>)<br>train_data.save_binary(<span class="hljs-string">&#x27;train.bin&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p><strong>指定 feature names（特征名称）和 categorical features（分类特征）,注意在你构造 Dataset 之前, 你应该将分类特征转换为 int 类型的非负整数。还可以指定每条数据的权重（比如在样本规模不均衡的时候希望少样本的标签对应的记录可以拥有较大的权重）</strong></p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs kotlin">w = np.random.rand(<span class="hljs-number">500</span>, )<br>train_data = lgb.Dataset(<span class="hljs-keyword">data</span>, label=label, feature_name=[<span class="hljs-string">&#x27;c1&#x27;</span>, <span class="hljs-string">&#x27;c2&#x27;</span>, <span class="hljs-string">&#x27;c3&#x27;</span>], categorical_feature=[<span class="hljs-string">&#x27;c3&#x27;</span>],weight=w)<br></code></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs kotlin">train_data = lgb.Dataset(<span class="hljs-keyword">data</span>, label=label, group=group_x)<br>w = np.random.rand(<span class="hljs-number">500</span>, )<br>train_data.set_weight(w)<br></code></pre></td></tr></table></figure>
<h2 id="3-设置参数"><a href="#3-设置参数" class="headerlink" title="3. 设置参数"></a>3. 设置参数</h2><ol>
<li><p>参数字典</p>
<p>每个参数的含义后面介绍（使用<code>max_position</code> 设置 <code>NDCG</code> 优化的位置）</p>
</li>
</ol>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs clean"># 将参数写成字典下形式<br>lgb_params = &#123;<br>    <span class="hljs-string">&quot;task&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>,  # task type, support train and predict<br>    <span class="hljs-string">&quot;objective&quot;</span>: <span class="hljs-string">&quot;lambdarank&quot;</span>,  # 排序任务(目标函数)<br>    <span class="hljs-string">&quot;boosting_type&quot;</span>: <span class="hljs-string">&quot;gbdt&quot;</span>,  # 基学习器 gbrt dart<br>    <span class="hljs-string">&quot;metric&quot;</span>: &#123;<span class="hljs-string">&#x27;ndcg&#x27;</span>, <span class="hljs-string">&#x27;map&#x27;</span>&#125;,  # 评估函数<br>    # <span class="hljs-string">&quot;ndcg_at&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>],<br>    # <span class="hljs-string">&quot;max_position&quot;</span>: <span class="hljs-number">5</span>,  # @NDCG 位置优化 <span class="hljs-number">5</span><br>    <span class="hljs-string">&quot;train_metric&quot;</span>: <span class="hljs-literal">True</span>,  # 训练时就输出度量结果 <span class="hljs-literal">True</span><br>    <span class="hljs-string">&quot;tree_learner&quot;</span>: <span class="hljs-string">&quot;serial&quot;</span>,  # 用于并行学习<br>    <span class="hljs-string">&quot;num_threads&quot;</span>: <span class="hljs-number">1</span>,  # 线程数，可以限制模型训练时CPU的占用率！！！<br>    <span class="hljs-string">&quot;verbose&quot;</span>: <span class="hljs-number">-1</span>,  # &lt;<span class="hljs-number">0</span> 显示致命的, =<span class="hljs-number">0</span> 显示错误 (警告), &gt;<span class="hljs-number">0</span> 显示信息<br>    <br>    <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">0.05</span>,  # 学习速率<br>    <span class="hljs-string">&#x27;max_depth&#x27;</span>: <span class="hljs-number">-1</span>,<br>    <span class="hljs-string">&#x27;num_leaves&#x27;</span>: <span class="hljs-number">31</span>,  # 叶子节点数，一般设为少于<span class="hljs-number">2</span>^(max_depth)<br>    <span class="hljs-string">&#x27;max_bin&#x27;</span>: <span class="hljs-number">256</span>,  # 设置连续特征或大量类型的离散特征的bins的数量<br>    <span class="hljs-string">&#x27;feature_fraction&#x27;</span>: <span class="hljs-number">0.8</span>,  # 特征采样<br>    <span class="hljs-string">&#x27;bagging_fraction&#x27;</span>: <span class="hljs-number">0.8</span>,  # 数据采样<br>    <span class="hljs-string">&#x27;bagging_freq&#x27;</span>: <span class="hljs-number">5</span>,  # k 意味着每 k 次迭代执行bagging<br>    <span class="hljs-string">&#x27;lambda_l1&#x27;</span>: <span class="hljs-number">0.1</span>,<br>    <span class="hljs-string">&#x27;lambda_l2&#x27;</span>: <span class="hljs-number">0.1</span>,<br>    <span class="hljs-string">&#x27;min_split_gain&#x27;</span>: <span class="hljs-number">0.8</span>,<br>    <br>    <span class="hljs-string">&#x27;is_unbalance&#x27;</span>: <span class="hljs-string">&#x27;true&#x27;</span>,  #当训练数据是不平衡的，正负样本相差悬殊的时候，可以将这个属性设为true,此时会自动给少的样本赋予更高的权重<br>&#125;<br></code></pre></td></tr></table></figure>
<ol>
<li><p>自定义评价函数</p>
<p>评价函数可以是自定义的，也可以是sklearn中使用的。这里是一个自定义的评价函数写法：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">feval_spec</span>(<span class="hljs-params">preds, train_data</span>):<br>    <span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve<br>    fpr, tpr, threshold = roc_curve(train_data.get_label(), preds)<br>    tpr0001 = tpr[fpr &lt;= <span class="hljs-number">0.0005</span>].<span class="hljs-built_in">max</span>()<br>    tpr001 = tpr[fpr &lt;= <span class="hljs-number">0.001</span>].<span class="hljs-built_in">max</span>()<br>    tpr005 = tpr[fpr &lt;= <span class="hljs-number">0.005</span>].<span class="hljs-built_in">max</span>()<br>    <span class="hljs-comment">#tpr01 = tpr[fpr.values &lt;= 0.01].max()</span><br>    tprcal = <span class="hljs-number">0.4</span> * tpr0001 + <span class="hljs-number">0.3</span> * tpr001 + <span class="hljs-number">0.3</span> * tpr005<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;spec_cal&#x27;</span>,tprcal,<span class="hljs-literal">True</span><br><br>lgb.train(feval=feval_spec)<br></code></pre></td></tr></table></figure>
<p>如果是自定义的评价函数，那么需要函数的输入是预测值、输入数据。返回参数有三个，第一个是评价指标名称、第二个是评价值、第三个是True表示成功。</p>
<h2 id="4-模型训练"><a href="#4-模型训练" class="headerlink" title="4. 模型训练"></a>4. 模型训练</h2><h3 id="4-1基础版"><a href="#4-1基础版" class="headerlink" title="4.1基础版"></a>4.1基础版</h3><p>训练一个模型时, 需要一个 parameter list（参数列表、字典）和 data set（数据集）这里使用上面定义的param参数字典和上面提到的训练数据:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">num_round = 10<br>bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])<br></code></pre></td></tr></table></figure>
<h3 id="4-2-交叉验证"><a href="#4-2-交叉验证" class="headerlink" title="4.2 交叉验证"></a>4.2 交叉验证</h3><p>时间充足的时候，应该使用交叉验证来选择最好的训练模型，使用 5-折 方式的交叉验证来进行训练（4 个训练集, 1 个测试集）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">num_round = 10<br>lgb.cv(param, train_data, num_round, nfold=5)<br></code></pre></td></tr></table></figure>
<h3 id="4-3-提前停止"><a href="#4-3-提前停止" class="headerlink" title="4.3 提前停止"></a>4.3 提前停止</h3><p>如果您有一个验证集, 你可以使用提前停止找到最佳数量的 boosting rounds（梯度次数）. 提前停止需要在 valid_sets 中至少有一个集合. 如果有多个，它们都会被使用:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">bst = lgb.train(param, train_data, num_round, valid_sets=valid_sets, <br>      early_stopping_rounds=10)<br>bst.save_model(<span class="hljs-string">&#x27;model.txt&#x27;</span>, num_iteration=bst.best_iteration)<br></code></pre></td></tr></table></figure>
<p>该模型将开始训练, 直到验证得分停止提高为止. 验证错误需要至少每个 early_stopping_rounds 减少以继续训练.</p>
<p>如果提前停止, 模型将有 1 个额外的字段: bst.best_iteration. 请注意 train() 将从最后一次迭代中返回一个模型, 而不是最好的一个.. 请注意, 如果您指定多个评估指标, 则它们都会用于提前停止.</p>
<p>提前停止可以节约训练的时间。</p>
<h3 id="4-4-查看特征重要性"><a href="#4-4-查看特征重要性" class="headerlink" title="4.4 查看特征重要性"></a>4.4 查看特征重要性</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># feature names</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Feature names:&#x27;</span>, gbm.feature_name())<br><br><span class="hljs-comment"># feature importances</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Feature importances:&#x27;</span>, list(gbm.feature_importance()))<br></code></pre></td></tr></table></figure>
<h3 id="4-5-动态调整模型超参数"><a href="#4-5-动态调整模型超参数" class="headerlink" title="4.5 动态调整模型超参数"></a>4.5 动态调整模型超参数</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># decay learning rates</span><br><span class="hljs-comment"># learning_rates accepts:</span><br><span class="hljs-comment"># 1. list/tuple with length = num_boost_round</span><br><span class="hljs-comment"># 2. function(curr_iter)</span><br>gbm = lgb.train(params,<br>                lgb_train,<br>                <span class="hljs-attribute">num_boost_round</span>=10,<br>                <span class="hljs-attribute">init_model</span>=gbm,<br>                <span class="hljs-attribute">learning_rates</span>=lambda iter: 0.05 * (0.99 ** iter),<br>                <span class="hljs-attribute">valid_sets</span>=lgb_eval)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished 20 - 30 rounds with decay learning rates...&#x27;</span>)<br><br><span class="hljs-comment"># change other parameters during training</span><br>gbm = lgb.train(params,<br>                lgb_train,<br>                <span class="hljs-attribute">num_boost_round</span>=10,<br>                <span class="hljs-attribute">init_model</span>=gbm,<br>                <span class="hljs-attribute">valid_sets</span>=lgb_eval,<br>                callbacks=[lgb.reset_parameter(bagging_fraction=[0.7] * 5 + [0.6] * 5)])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished 30 - 40 rounds with changing bagging_fraction...&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h2 id="5-模型保存-amp-加载"><a href="#5-模型保存-amp-加载" class="headerlink" title="5. 模型保存&amp;加载"></a>5. 模型保存&amp;加载</h2><p>在训练完成后, 可以使用如下方式来存储模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">bst.save_model(<span class="hljs-string">&#x27;model.txt&#x27;</span>)<br><span class="hljs-comment"># with open(model_path, &quot;wb&quot;) as f:</span><br><span class="hljs-comment">#     pkl.dump(bst, f)</span><br><span class="hljs-comment"># with open(&#x27;model.json&#x27;, &#x27;w+&#x27;) as f:</span><br><span class="hljs-comment">#     json.dump(bst, f, indent=4)</span><br></code></pre></td></tr></table></figure>
<p>模型的重新载入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">bst = lgb.Booster(model_file=<span class="hljs-string">&#x27;model.txt&#x27;</span>)<br><span class="hljs-comment"># with open(model_path, &quot;rb&quot;) as fin:</span><br><span class="hljs-comment">#     bst = pkl.load(fin)</span><br></code></pre></td></tr></table></figure>
<p>已经训练或加载的模型都可以对数据集进行预测:</p>
<h2 id="6-预测"><a href="#6-预测" class="headerlink" title="6. 预测"></a>6. 预测</h2><p>7 个样本, 每一个包含 10 个特征</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs kotlin"><span class="hljs-keyword">data</span> = np.random.rand(<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)<br>ypred = bst.predict(<span class="hljs-keyword">data</span>)<br></code></pre></td></tr></table></figure>
<p>如果在训练过程中启用了提前停止, 可以用 bst.best_iteration 从最佳迭代中获得预测结果:</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs kotlin">ypred = bst.predict(<span class="hljs-keyword">data</span>, num_iteration=bst.best_iteration)<br></code></pre></td></tr></table></figure>
<h2 id="7-自定义损失函数"><a href="#7-自定义损失函数" class="headerlink" title="7. 自定义损失函数"></a>7. 自定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 类似在xgboost中的形式</span><br><span class="hljs-comment"># 自定义损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loglikelood</span>(<span class="hljs-params">preds, train_data</span>):<br>    labels = train_data.get_label()<br>    preds = <span class="hljs-number">1.</span> / (<span class="hljs-number">1.</span> + np.exp(-preds))<br>    grad = preds - labels<br>    hess = preds * (<span class="hljs-number">1.</span> - preds)<br>    <span class="hljs-keyword">return</span> grad, hess<br><br>lgb.train(fobj=loglikelood)<br></code></pre></td></tr></table></figure>
<h1 id="Lightgbm调参指导★"><a href="#Lightgbm调参指导★" class="headerlink" title="Lightgbm调参指导★"></a>Lightgbm调参指导★</h1><p>参考《<a target="_blank" rel="noopener" href="https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.2.spilt.2.8d95f7184b045e7a.md">AI算法工程师手册—lightgbm使用指南—调参</a>》</p>
<p>参考《<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI4ODY2NjYzMQ==&amp;mid=2247487494&amp;idx=2&amp;sn=bbe940e4748e6ac5cf6db0340fe0b014&amp;chksm=ec3bb56edb4c3c78718ab413f70e5786116acdf18923d0b17368f919c8ceb7537743ab262afb&amp;scene=27#wechat_redirect">深入理解LightGBM—Lightgbm调参</a>》√</p>
<ol>
<li><p>针对 <code>leaf-wise</code> 树的参数优化：</p>
<ul>
<li><p><code>num_leaves</code>：控制了叶节点的数目。它是控制树模型复杂度的主要参数。</p>
<p>如果是<code>level-wise</code>， 则该参数为 <img src="https://static.bookstack.cn/projects/huaxiaozhuan-ai/7fbff4847407186f47048e68c9f3567f.svg" srcset="/img/loading.gif" lazyload alt="2.1 调参指导 - 图1">，其中 <img src="https://static.bookstack.cn/projects/huaxiaozhuan-ai/bcee7d20347ab929a0ad08ef43059880.svg" srcset="/img/loading.gif" lazyload alt="2.1 调参指导 - 图2"> 为树的深度。</p>
<p>但是当叶子数量相同时，<code>leaf-wise</code> 的树要远远深过<code>level-wise</code> 树，非常容易导致过拟合。因此应该让 <code>num_leaves</code> 小于 <img src="https://static.bookstack.cn/projects/huaxiaozhuan-ai/7fbff4847407186f47048e68c9f3567f.svg" srcset="/img/loading.gif" lazyload alt="2.1 调参指导 - 图3"></p>
<blockquote>
<p>在<code>leaf-wise</code> 树中，并不存在<code>depth</code> 的概念。因为不存在一个从<code>leaves</code> 到 <code>depth</code> 的合理映射</p>
</blockquote>
</li>
<li><p><code>min_data_in_leaf</code>： 每个叶节点的最少样本数量。它是处理<code>leaf-wise</code> 树的过拟合的重要参数。</p>
<p>将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合。</p>
</li>
<li><p><code>max_depth</code>： 控制了树的最大深度。</p>
<p>该参数可以显式的限制树的深度。</p>
</li>
</ul>
</li>
<li><p>针对更快的训练速度：</p>
<ul>
<li>通过设置 <code>bagging_fraction</code> 和 <code>bagging_freq</code> 参数来使用 bagging 方法</li>
<li>通过设置 <code>feature_fraction</code> 参数来使用特征的子抽样</li>
<li>使用较小的 <code>max_bin</code></li>
<li>使用 <code>save_binary</code> 在未来的学习过程对数据加载进行加速</li>
</ul>
</li>
<li><p><strong>获取更好的准确率：</strong></p>
<ul>
<li>使用较大的 <code>max_bin</code> （学习速度可能变慢）</li>
<li>使用较小的 <code>learning_rate</code> 和较大的 <code>num_iterations</code></li>
<li>使用较大的 <code>num_leaves</code> （可能导致过拟合）</li>
<li>使用更大的训练数据</li>
<li>尝试 <code>dart</code></li>
</ul>
</li>
<li><p>缓解过拟合：</p>
<ul>
<li>使用较小的 <code>max_bin</code></li>
<li>使用较小的 <code>num_leaves</code></li>
<li>使用 <code>min_data_in_leaf</code> 和 <code>min_sum_hessian_in_leaf</code></li>
<li>通过设置 <code>bagging_fraction</code> 和 <code>bagging_freq</code> 来使用 <code>bagging</code></li>
<li>通过设置 <code>feature_fraction</code> 来使用特征子抽样</li>
<li>使用更大的训练数据</li>
<li>使用 <code>lambda_l1</code>, <code>lambda_l2</code> 和 <code>min_gain_to_split</code> 来使用正则</li>
<li>尝试 <code>max_depth</code> 来避免生成过深的树</li>
</ul>
</li>
</ol>
<h2 id="核心参数"><a href="#核心参数" class="headerlink" title="核心参数"></a>核心参数</h2><div class="table-container">
<table>
<thead>
<tr>
<th>核心参数</th>
<th>含义</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>task</td>
<td>要执行的任务</td>
<td>train/predict/convert_model</td>
</tr>
<tr>
<td>application 或者objective 或者 app</td>
<td>任务类型</td>
<td>default = <code>regression</code>, options: <code>regression</code>, <code>binary</code>, <code>multiclass</code>, <code>cross_entropy</code>, <code>cross_entropy_lambda</code>, <code>lambdarank</code>, <code>rank_xendcg</code>, …</td>
</tr>
<tr>
<td>boosting或者boost或者boosting_type</td>
<td>基学习器模型算法</td>
<td>gbdt/rf/dart/goss</td>
</tr>
<tr>
<td>num_iteration或者num_tree或者 num_round或者 num_boost_round</td>
<td>迭代次数</td>
<td>默认100</td>
</tr>
<tr>
<td>learning_rate</td>
<td>学习率</td>
<td>默认为 0.1</td>
</tr>
<tr>
<td>num_leaves或者num_leaf</td>
<td>一棵树上的叶子数</td>
<td>默认为 31</td>
</tr>
</tbody>
</table>
</div>
<h2 id="学习控制参数"><a href="#学习控制参数" class="headerlink" title="学习控制参数"></a>学习控制参数</h2><div class="table-container">
<table>
<thead>
<tr>
<th>学习控制参数</th>
<th>含义</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>max_depth</td>
<td>树模型的最大深度</td>
<td>默认值为-1</td>
</tr>
<tr>
<td>min_data_in_leaf</td>
<td>一个叶子节点上包含的最少样本数量。</td>
<td>默认值为 20。将其设置的较大可以避免生成一个过深的树, 但有可能导致欠拟合</td>
</tr>
<tr>
<td>feature_fraction</td>
<td>如0.8 表示：在每棵树训练之前选择80% 的特征来训练</td>
<td>取值范围为[0.0,1.0]， 默认值为1.0。降低过拟合</td>
</tr>
<tr>
<td>bagging_fraction 或者 subsample</td>
<td>如0.8 表示：在每棵树训练之前选择80% 的样本（非重复采样）来训练</td>
<td>取值范围为[0.0,1.0]， 默认值为1.0。降低过拟合</td>
</tr>
<tr>
<td>early_stopping_round或者early_stopping</td>
<td>如果一个验证集的度量在early_stopping_round 循环中没有提升，则停止训练</td>
<td>-</td>
</tr>
<tr>
<td>lambda_l1 或者reg_alpha</td>
<td>表示L1正则化系数。</td>
<td>默认为0。降低过拟合</td>
</tr>
<tr>
<td>lambda_l2 或者reg_lambda</td>
<td>表示L2正则化系数。</td>
<td>默认为0。降低过拟合</td>
</tr>
<tr>
<td>min_split_gain 或者min_gain_to_split</td>
<td>一个浮点数，表示执行切分的最小增益</td>
<td>默认为0</td>
</tr>
<tr>
<td>min_data_per_group</td>
<td>表示每个分类组的最小数据量 用于排序任务</td>
<td>默认值为100</td>
</tr>
<tr>
<td>cat_smooth</td>
<td>用于category 特征的概率平滑，降低噪声在category 特征中的影响，尤其是对于数据很少的类。</td>
<td>默认值为 10</td>
</tr>
</tbody>
</table>
</div>
<h2 id="度量参数"><a href="#度量参数" class="headerlink" title="度量参数"></a>度量参数</h2><div class="table-container">
<table>
<thead>
<tr>
<th>度量参数</th>
<th>含义</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>metric</td>
<td>度量的指标</td>
<td>对于回归问题，使用l2 ； 对于二分类问题，使用binary_logloss；对于lambdarank 问题，使用ndcg</td>
</tr>
<tr>
<td>metric_freq或者’output_freq</td>
<td>一个正式，表示每隔多少次输出一次度量结果</td>
<td>默认为1</td>
</tr>
<tr>
<td>train_metric 或者training_metric</td>
<td>如果为True，则在训练时就输出度量结果</td>
<td>默认值为 False</td>
</tr>
<tr>
<td>ndcg_at 或者 ndcg_eval_at 或者eval_at</td>
<td>指定了NDCG 评估点的位置。</td>
<td>默认为1,2,3,4,5</td>
</tr>
</tbody>
</table>
</div>
<h2 id="并行学习"><a href="#并行学习" class="headerlink" title="并行学习"></a>并行学习</h2><ol>
<li><p><code>lightgbm</code> 已经提供了以下并行学习算法：</p>
<p>| 并行算法 | 开启方式               |<br>| :———- | :——————————- |<br>| 数据并行 | tree_learner=’data’    |<br>| 特征并行 | tree_learner=’feature’ |<br>| 投票并行 | tree_learner=’voting’  |</p>
<blockquote>
<p><code>tree_learner</code> 默认为 <code>&#39;serial&#39;</code>。 表示串行学习。</p>
</blockquote>
</li>
</ol>
<h2 id="调参示例"><a href="#调参示例" class="headerlink" title="调参示例"></a>调参示例</h2><ol>
<li><p><strong>第一步：学习率和迭代次数</strong>我们先把学习率先定一个较高的值，这里取 learning_rate = 0.1，其次确定估计器boosting/boost/boosting_type的类型，不过默认都会选gbdt。</p>
<p>迭代的次数，也可以说是残差树的数目，参数名为n_estimators/num_iterations/num_round/num_boost_round。我们可以先将该参数设成一个较大的数</p>
</li>
<li><p><strong>第二步：确定max_depth和num_leaves</strong>这是提高精确度的最重要的参数。这里我们引入sklearn里的GridSearchCV()函数进行搜索</p>
</li>
<li><strong>第三步：确定min_data_in_leaf和max_bin</strong></li>
<li><strong>第四步：确定feature_fraction、bagging_fraction、bagging_freq</strong></li>
<li><strong>第五步：确定lambda_l1和lambda_l2</strong></li>
<li><strong>第六步：确定 min_split_gain</strong></li>
<li><strong>第七步：降低学习率，增加迭代次数，验证模型</strong></li>
</ol>
<h1 id="Lightgbm-API接口方法"><a href="#Lightgbm-API接口方法" class="headerlink" title="Lightgbm API接口方法"></a>Lightgbm API接口方法</h1><h2 id="数据接口-Dataset"><a href="#数据接口-Dataset" class="headerlink" title="数据接口 Dataset"></a>数据接口 Dataset</h2><ol>
<li><p><code>Dataset</code>： 由<code>lightgbm</code> 内部使用的数据结构，它存储了数据集。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">class lightgbm.Dataset(data, <span class="hljs-attribute">label</span>=None, <span class="hljs-attribute">max_bin</span>=None, <span class="hljs-attribute">reference</span>=None, <span class="hljs-attribute">weight</span>=None,    <span class="hljs-attribute">group</span>=None, <span class="hljs-attribute">init_score</span>=None, <span class="hljs-attribute">silent</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">feature_name</span>=<span class="hljs-string">&#x27;auto&#x27;</span>,    <span class="hljs-attribute">categorical_feature</span>=<span class="hljs-string">&#x27;auto&#x27;</span>, <span class="hljs-attribute">params</span>=None, <span class="hljs-attribute">free_raw_data</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<ul>
<li><p>参数：</p>
<ul>
<li><p><code>data</code>： 一个字符串、<code>numpy array</code> 或者 <code>scipy.parse</code>， 它指定了数据源。</p>
<p>如果是字符串，则表示数据源文件的文件名。</p>
</li>
<li><p><code>label</code>： 一个列表、1维的<code>numpy array</code> 或者<code>None</code>， 它指定了样本标记。默认为<code>None</code>。</p>
</li>
<li><p><code>max_bin</code>： 一个整数或者<code>None</code>， 指定每个特征的最大分桶数量。默认为<code>None</code>。</p>
<p>如果为<code>None</code>，则从配置文件中读取。</p>
</li>
<li><p><code>reference</code>： 一个<code>Dataset</code> 或者 <code>None</code>。 默认为<code>None</code>。</p>
<p>如果当前构建的数据集用于验证集，则<code>reference</code> 必须传入训练集。否则会报告<code>has different bin mappers</code>。</p>
</li>
<li><p><code>weight</code>： 一个列表、1维的<code>numpy array</code> 或者<code>None</code>， 它指定了样本的权重。默认为<code>None</code>。</p>
</li>
<li><p><code>group</code>： 一个列表、1维的<code>numpy array</code> 或者<code>None</code>， 它指定了数据集的<code>group/query size</code>。默认为<code>None</code>。</p>
</li>
<li><p><code>init_score</code>： 一个列表、1维的<code>numpy array</code> 或者<code>None</code>， 它指定了<code>Booster</code>的初始<code>score</code> 。默认为<code>None</code>。</p>
</li>
<li><p><code>silent</code>： 一个布尔值，指示是否在构建过程中输出信息。默认为<code>False</code></p>
</li>
<li><p><code>feature_name</code>： 一个字符串列表或者<code>&#39;auto&#39;</code>，它指定了特征的名字。默认为<code>&#39;auto&#39;</code></p>
<ul>
<li>如果数据源为<code>pandas DataFrame</code> 并且<code>feature_name=&#39;auto&#39;</code>，则使用<code>DataFrame</code> 的 <code>column names</code></li>
</ul>
</li>
<li><p><code>categorical_feature</code>： 一个字符串列表、整数列表、或者<code>&#39;auto&#39;</code>。它指定了<code>categorical</code> 特征。默认为<code>&#39;auto&#39;</code></p>
<ul>
<li>如果是整数列表，则给定了<code>categorical</code> 特征的下标</li>
<li>如果是字符串列表，在给定了<code>categorical</code> 特征的名字。此时必须设定<code>feature_name</code> 参数。</li>
<li>如果是<code>&#39;auto&#39;</code> 并且数据源为<code>pandas DataFrame</code>，则<code>DataFrame</code> 的 <code>categorical</code> 列将作为<code>categorical</code> 特征</li>
</ul>
</li>
<li><p><code>params</code>： 一个字典或者<code>None</code>，指定了其它的参数。默认为<code>None</code></p>
</li>
<li><p><code>free_raw_data</code>： 一个布尔值，指定是否在创建完<code>Dataset</code> 之后释放原始的数据。默认为<code>True</code></p>
<p>调用<code>Dataset()</code> 之后，并没有构建完<code>Dataset</code>。 构建完需要等到构造一个<code>Booster</code> 的时候。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>方法：</p>
<ul>
<li><p><code>.get_group()</code>： 获取当前<code>Dataset</code> 的<code>group</code></p>
<blockquote>
<p><code>get_xxx()</code> 等方法，都是调用的 <code>get_field()</code> 方法来实现的</p>
</blockquote>
<ul>
<li>返回值：一个<code>numpy array</code>，表示每个分组的<code>size</code> 。</li>
</ul>
</li>
<li><p><code>.set_group(group)</code>： 设置当前<code>Dataset</code> 的<code>group</code></p>
<ul>
<li>参数：<code>group</code>： 一个列表、<code>numpy array</code> 或者<code>None</code>，表示每个分组的<code>size</code> 。</li>
</ul>
</li>
<li><p><code>.num_data()</code>： 返回<code>Dataset</code> 中的样本数量</p>
</li>
<li><code>.num_feature()</code>： 返回<code>Dataset</code> 中的特征数量</li>
</ul>
</li>
<li><p>示例：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DatasetTest</span>:<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    self._matrix1 = lgb.Dataset(<span class="hljs-string">&#x27;data/train.svm.txt&#x27;</span>)<br>    self._matrix2 = lgb.Dataset(data=np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">12</span>).reshape((<span class="hljs-number">4</span>, <span class="hljs-number">3</span>)), <br>                                label=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>], weight=[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.2</span>],<br>                                silent=<span class="hljs-literal">False</span>, feature_name=[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>])<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">print</span>(<span class="hljs-params">self,matrix</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Matrix 构建尚未完成时的属性</span><br><span class="hljs-string">    :param matrix:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;data: %s&#x27;</span> % matrix.data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;label: %s&#x27;</span> % matrix.label)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;weight: %s&#x27;</span> % matrix.weight)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;init_score: %s&#x27;</span> % matrix.init_score)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;group: %s&#x27;</span> % matrix.group)<br>​<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_method</span>(<span class="hljs-params">self,matrix</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    测试一些 方法</span><br><span class="hljs-string">    :param matrix:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;get_ref_chain():&#x27;</span>, matrix.get_ref_chain(ref_limit=<span class="hljs-number">10</span>))<br>    <span class="hljs-comment"># get_ref_chain(): &#123;&lt;lightgbm.basic.Dataset object at 0x7f29cd762f28&gt;&#125;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;subset():&#x27;</span>, matrix.subset(used_indices=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]))<br>    <span class="hljs-comment"># subset(): &lt;lightgbm.basic.Dataset object at 0x7f29a4aeb518&gt;</span><br>  <br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">self</span>):<br>    self.<span class="hljs-built_in">print</span>(self._matrix1)<br>    <span class="hljs-comment"># data: data/train.svm.txt</span><br>    <span class="hljs-comment"># label: None</span><br>    <span class="hljs-comment"># weight: None</span><br>    <span class="hljs-comment"># init_score: None</span><br>    <span class="hljs-comment"># group: None</span><br>    self.<span class="hljs-built_in">print</span>(self._matrix2)<br>    <span class="hljs-comment"># data: [[ 0  1  2]</span><br>    <span class="hljs-comment">#  [ 3  4  5]</span><br>    <span class="hljs-comment">#  [ 6  7  8]</span><br>    <span class="hljs-comment">#  [ 9 10 11]]</span><br>    <span class="hljs-comment"># label: [1, 2, 3, 4]</span><br>    <span class="hljs-comment"># weight: [0.5, 0.4, 0.3, 0.2]</span><br>    <span class="hljs-comment"># init_score: No</span><br>    self.run_method(self._matrix2)<br></code></pre></td></tr></table></figure>
<h2 id="模型接口-lightgbm-train"><a href="#模型接口-lightgbm-train" class="headerlink" title="模型接口 lightgbm.train"></a>模型接口 lightgbm.train</h2><ol>
<li><p><code>lightgbm.train()</code> 函数执行直接训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">lightgbm.train(params, train_set, num_boost_round=<span class="hljs-number">100</span>, valid_sets=<span class="hljs-literal">None</span>,<br>  valid_names=<span class="hljs-literal">None</span>, fobj=<span class="hljs-literal">None</span>, feval=<span class="hljs-literal">None</span>, init_model=<span class="hljs-literal">None</span>, feature_name=<span class="hljs-string">&#x27;auto&#x27;</span>,<br>  categorical_feature=<span class="hljs-string">&#x27;auto&#x27;</span>, early_stopping_rounds=<span class="hljs-literal">None</span>, evals_result=<span class="hljs-literal">None</span>, <br>  verbose_eval=<span class="hljs-literal">True</span>, learning_rates=<span class="hljs-literal">None</span>, keep_training_booster=<span class="hljs-literal">False</span>, callbacks=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure>
<p>参数：</p>
<ul>
<li><p><code>params</code>： 一个字典，给出了训练参数</p>
</li>
<li><p><code>train_set</code>： 一个<code>Dataset</code>对象，给出了训练集</p>
</li>
<li><p><code>num_boost_round</code>： 一个整数，给出了<code>boosting iteration</code> 的次数。默认为<code>100</code></p>
</li>
<li><p><code>valid_sets</code>：一个<code>Dataset</code> 的列表或者<code>None</code>，给出了训练期间用于<code>evaluate</code>的数据集。默认为<code>None</code></p>
</li>
<li><p><code>valid_names</code>：一个字符串列表或者<code>None</code>， 给出了<code>valid_sets</code> 中每个数据集的名字。默认为<code>None</code></p>
</li>
<li><p><code>fobj</code>：一个可调用对象或者<code>None</code>，表示自定义的目标函数。默认为<code>None</code></p>
</li>
<li><p><code>feval</code>：一个可调用对象或者<code>None</code>， 它表示自定义的<code>evaluation</code> 函数。默认为<code>None</code>。它的输入为<code>(y_true, y_pred)</code>、或者<code>( y_true, y_pred, weight)</code> 、或者<code>(y_true, y_pred, weight, group)</code>， 返回一个元组：<code>(eval_name,eval_result,is_higher_better)</code> 。或者返回该元组的列表。</p>
</li>
<li><p><code>init_model</code>：一个字符串或者<code>None</code>，它给出了<code>lightgbm model</code> 保存的文件名，或者<code>Booster</code>实例的名字。后续的训练在该<code>model</code> 或者<code>Booster</code> 实例的基础上继续训练。默认为<code>None</code></p>
</li>
<li><p><code>feature_name</code>： 一个字符串列表或者<code>&#39;auto&#39;</code>，它指定了特征的名字。默认为<code>&#39;auto&#39;</code></p>
<ul>
<li>如果数据源为<code>pandas DataFrame</code> 并且<code>feature_name=&#39;auto&#39;</code>，则使用<code>DataFrame</code> 的 <code>column names</code></li>
</ul>
</li>
<li><p><code>categorical_feature</code>：一个字符串列表、整数列表、或者<code>&#39;auto&#39;</code>。它指定了<code>categorical</code> 特征。默认为<code>&#39;auto&#39;</code></p>
<ul>
<li>如果是整数列表，则给定了<code>categorical</code> 特征的下标</li>
<li>如果是字符串列表，在给定了<code>categorical</code> 特征的名字。此时必须设定<code>feature_name</code> 参数。</li>
<li>如果是<code>&#39;auto&#39;</code> 并且数据源为<code>pandas DataFrame</code>，则<code>DataFrame</code> 的 <code>categorical</code> 列将作为<code>categorical</code> 特征</li>
</ul>
</li>
<li><p><code>early_stopping_rounds</code>：一个整数或者<code>None</code>，表示验证集的<code>score</code> 在连续多少轮未改善之后就早停。默认为<code>None</code></p>
<p>该参数要求至少有一个验证集以及一个<code>metric</code>。</p>
<p>如果由多个验证集或者多个<code>metric</code>，则对所有的验证集和所有的<code>metric</code> 执行。</p>
<p>如果发生了早停，则模型会添加一个<code>best_iteration</code>字段。该字段持有了最佳的迭代步。</p>
</li>
<li><p><code>evals_result</code>：一个字典或者<code>None</code>，这个字典用于存储在<code>valid_sets</code> 中指定的所有验证集的所有验证结果。默认为<code>None</code></p>
</li>
<li><p><code>verbose_eval</code>：一个布尔值或者整数。默认为<code>True</code></p>
<ul>
<li>如果是<code>True</code>，则在验证集上每个<code>boosting stage</code> 打印对验证集评估的<code>metric</code>。</li>
<li>如果是整数，则每隔<code>verbose_eval</code> 个 <code>boosting stage</code> 打印对验证集评估的<code>metric</code>。</li>
<li>否则，不打印这些</li>
</ul>
<p>该参数要求至少由一个验证集。</p>
</li>
<li><p><code>learning_rates</code>：一个列表、<code>None</code>、 可调用对象。它指定了学习率。默认为<code>None</code></p>
<ul>
<li>如果为列表，则它给出了每一个<code>boosting</code> 步的学习率</li>
<li>如果为一个可调用对象，则在每个<code>boosting</code> 步都调用它，从而生成一个学习率</li>
<li>如果为一个数值，则学习率在学习期间都固定为它。</li>
</ul>
<p>你可以使用学习率衰减从而生成一个更好的学习率序列。</p>
</li>
<li><p><code>keep_training_booster</code>：一个布尔值，指示训练得到的<code>Booster</code>对象是否还会继续训练。默认为<code>False</code></p>
<ul>
<li><p>如果为<code>False</code>，则返回的<code>booster</code> 对象在返回之前将被转换为<code>_InnerPredictor</code> 。</p>
<p>当然你也可以将<code>_InnerPredictor</code> 传递给<code>init_model</code> 参数从而继续训练。</p>
</li>
</ul>
</li>
<li><p><code>callbacks</code>：一个可调用对象的列表，或者<code>None</code>。 它给出了在每个迭代步之后需要执行的函数。默认为<code>None</code></p>
</li>
</ul>
<p>返回：一个<code>Booster</code> 实例</p>
</li>
<li><p><code>lightgbm.cv()</code> 函数执行交叉验证训练。</p>
</li>
</ol>
<h2 id="绘图接口"><a href="#绘图接口" class="headerlink" title="绘图接口"></a>绘图接口</h2><p><a target="_blank" rel="noopener" href="https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.3.spilt.4.8d95f7184b045e7a.md">https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.3.spilt.4.8d95f7184b045e7a.md</a></p>
<ol>
<li><code>lightgbm.plot_importance()</code>： 绘制特征的重要性。</li>
<li><code>lightgbm.plot_metric()</code>： 在训练过程中绘制一个<code>metric</code></li>
<li><code>lightgbm.plot_tree()</code>：绘制指定的树模型。</li>
<li><code>lightgbm.create_tree_digraph()</code>： 绘制指定的树模型，但是返回一个<code>digraph</code>，而不是直接绘制。</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a target="_blank" rel="noopener" href="https://www.bookstack.cn/read/huaxiaozhuan-ai/8d95f7184b045e7a.md">AI算法工程师手册 - lightgbm使用指南</a></p>
<p>【<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FNLP-LOVE%2FML-NLP">机器学习通俗易懂系列文章</a>】</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/ba9ab1adbfe1">https://www.jianshu.com/p/ba9ab1adbfe1</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/d07f0b0726da">关于lightgbm处理category特征的理解</a></p>
<h1 id="附录：自动超参数优化"><a href="#附录：自动超参数优化" class="headerlink" title="附录：自动超参数优化"></a>附录：自动超参数优化</h1><p><strong>网格搜索</strong></p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs routeros">lg = lgb.LGBMClassifier(<span class="hljs-attribute">silent</span>=<span class="hljs-literal">False</span>)<br>param_dist = &#123;<span class="hljs-string">&quot;max_depth&quot;</span>: [4,5, 7],<br>              <span class="hljs-string">&quot;learning_rate&quot;</span> : [0.01,0.05,0.1],<br>              <span class="hljs-string">&quot;num_leaves&quot;</span>: [300,900,1200],<br>              <span class="hljs-string">&quot;n_estimators&quot;</span>: [50, 100, 150]<br>             &#125;<br><br>grid_search = GridSearchCV(lg, <span class="hljs-attribute">n_jobs</span>=-1, <span class="hljs-attribute">param_grid</span>=param_dist, cv = 5, <span class="hljs-attribute">scoring</span>=<span class="hljs-string">&quot;roc_auc&quot;</span>, <span class="hljs-attribute">verbose</span>=5)<br>grid_search.fit(train,y_train)<br>grid_search.best_estimator_, grid_search.best_score_<br></code></pre></td></tr></table></figure>
<p><strong>贝叶斯优化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br><span class="hljs-keyword">import</span> time<br>warnings.filterwarnings(<span class="hljs-string">&quot;ignore&quot;</span>)<br><span class="hljs-keyword">from</span> bayes_opt <span class="hljs-keyword">import</span> BayesianOptimization<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lgb_eval</span>(<span class="hljs-params">max_depth, learning_rate, num_leaves, n_estimators</span>):<br>    params = &#123;<br>             <span class="hljs-string">&quot;metric&quot;</span> : <span class="hljs-string">&#x27;auc&#x27;</span><br>        &#125;<br>    params[<span class="hljs-string">&#x27;max_depth&#x27;</span>] = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(max_depth, <span class="hljs-number">1</span>))<br>    params[<span class="hljs-string">&#x27;learning_rate&#x27;</span>] = np.clip(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, learning_rate)<br>    params[<span class="hljs-string">&#x27;num_leaves&#x27;</span>] = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(num_leaves, <span class="hljs-number">1</span>))<br>    params[<span class="hljs-string">&#x27;n_estimators&#x27;</span>] = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(n_estimators, <span class="hljs-number">1</span>))<br>    cv_result = lgb.cv(params, d_train, nfold=<span class="hljs-number">5</span>, seed=<span class="hljs-number">0</span>, verbose_eval =<span class="hljs-number">200</span>,stratified=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> * np.array(cv_result[<span class="hljs-string">&#x27;auc-mean&#x27;</span>]).<span class="hljs-built_in">max</span>()<br><br>lgbBO = BayesianOptimization(lgb_eval, &#123;<span class="hljs-string">&#x27;max_depth&#x27;</span>: (<span class="hljs-number">4</span>, <span class="hljs-number">8</span>),<br>                                            <span class="hljs-string">&#x27;learning_rate&#x27;</span>: (<span class="hljs-number">0.05</span>, <span class="hljs-number">0.2</span>),<br>                                            <span class="hljs-string">&#x27;num_leaves&#x27;</span> : (<span class="hljs-number">20</span>,<span class="hljs-number">1500</span>),<br>                                            <span class="hljs-string">&#x27;n_estimators&#x27;</span>: (<span class="hljs-number">5</span>, <span class="hljs-number">200</span>)&#125;, random_state=<span class="hljs-number">0</span>)<br><br>lgbBO.maximize(init_points=<span class="hljs-number">5</span>, n_iter=<span class="hljs-number">50</span>,acq=<span class="hljs-string">&#x27;ei&#x27;</span>)<br><span class="hljs-built_in">print</span>(lgbBO.<span class="hljs-built_in">max</span>)<br></code></pre></td></tr></table></figure>
<p><strong>optuna超参数优化，参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/138521995">《GIVE OPTUNA A SHOT!》</a></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> optuna<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">objective</span>(<span class="hljs-params">trial</span>):<br>    x = trial.suggest_uniform(<span class="hljs-string">&#x27;x&#x27;</span>, -<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>    <span class="hljs-keyword">return</span> (x - <span class="hljs-number">2</span>) ** <span class="hljs-number">2</span><br><br>study = optuna.create_study()<br>study.optimize(objective, n_trials=<span class="hljs-number">100</span>)<br><br>study.best_params  <span class="hljs-comment"># E.g. &#123;&#x27;x&#x27;: 2.002108042&#125;</span><br></code></pre></td></tr></table></figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%8A%80%E6%9C%AF/">#技术</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Lightgbm使用指南</div>
      <div>http://example.com/2020/08/24/2020-08-24-Lightgbm使用指南/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>NSX</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2020年8月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2020/09/07/2019-01-03-%E4%B8%80%E4%BD%8D%E6%B5%99%E5%A4%A7CS%E5%A4%A7%E4%BD%AC%E7%9A%84%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95%E5%BF%83%E5%BE%97/" title="一位浙大CS大佬的校招面试心得">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">一位浙大CS大佬的校招面试心得</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/08/24/2020-08-24-sklearn%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/" title="sklearn分类评价指标介绍">
                        <span class="hidden-mobile">sklearn分类评价指标介绍</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
