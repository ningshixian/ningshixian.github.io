

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="NSX">
  <meta name="keywords" content="">
  
    <meta name="description" content="常见调参技巧  Learning Rate 优化器选择 Dropout Batch Size 激活函数 BN …  问题和解决方法  关于过拟合问题的讨论 Loss为NAN Loss为负数 Loss不下降 模型训练加速 OOM  keras相关经验  训练集，验证集和测试集  查看模型的评价指标  保存keras输出的loss，val  绘制精度和损失曲线  将整型 label 转换成 one-h">
<meta property="og:type" content="article">
<meta property="og:title" content="NN调参炼丹上分手册">
<meta property="og:url" content="http://example.com/2020/04/24/2020-04-24-%E8%B0%83%E5%8F%82%E7%82%BC%E4%B8%B9%E6%89%8B%E5%86%8C/index.html">
<meta property="og:site_name" content="神的个人博客">
<meta property="og:description" content="常见调参技巧  Learning Rate 优化器选择 Dropout Batch Size 激活函数 BN …  问题和解决方法  关于过拟合问题的讨论 Loss为NAN Loss为负数 Loss不下降 模型训练加速 OOM  keras相关经验  训练集，验证集和测试集  查看模型的评价指标  保存keras输出的loss，val  绘制精度和损失曲线  将整型 label 转换成 one-h">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/6671823-ad7303e0dd58d6a3.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/353/format/webp">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/6671823-d0014bc7e8a172b2.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/257/format/webp">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=i%5Ctimes+L+%2F+m">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cgamma">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cgamma">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbeta">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y%3D%5Cgamma+x%5E+%2B%5Cbeta">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cgamma">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=0">
<meta property="article:published_time" content="2020-04-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-04-23T10:28:32.046Z">
<meta property="article:author" content="Ning Shixian">
<meta property="article:tag" content="Keras">
<meta property="article:tag" content="炼丹">
<meta property="article:tag" content="调参">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/6671823-ad7303e0dd58d6a3.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/353/format/webp">
  
  
  
  <title>NN调参炼丹上分手册 - 神的个人博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>神的个人博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="神经网络调参trick和keras相关经验"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2020-04-24 00:00" pubdate>
          2020年4月24日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          17k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          139 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">神经网络调参trick和keras相关经验</h1>
            
            
              <div class="markdown-body">
                
                <p><strong>常见调参技巧</strong></p>
<ul>
<li>Learning Rate</li>
<li>优化器选择</li>
<li>Dropout</li>
<li>Batch Size</li>
<li>激活函数</li>
<li>BN</li>
<li>…</li>
</ul>
<p><strong>问题和解决方法</strong></p>
<ul>
<li>关于过拟合问题的讨论</li>
<li>Loss为NAN</li>
<li>Loss为负数</li>
<li>Loss不下降</li>
<li>模型训练加速</li>
<li>OOM</li>
</ul>
<p><strong>keras相关经验</strong></p>
<ol>
<li><p>训练集，验证集和测试集</p>
</li>
<li><p>查看模型的评价指标</p>
</li>
<li><p>保存keras输出的loss，val</p>
</li>
<li><p>绘制精度和损失曲线</p>
</li>
<li><p>将整型 label 转换成 one-hot 形式</p>
</li>
<li><p>自制回调函数 callback</p>
</li>
<li><p>网格超参数搜索</p>
</li>
<li><p>编写自己的层</p>
</li>
<li><p>keras保存和加载自定义损失模型</p>
</li>
<li><p>PRF 值计算</p>
</li>
<li><p>keras 获取中间层的输出</p>
</li>
<li><p>categorical_crossentropy vs. sparse_categorical_crossentropy</p>
</li>
<li><p>通过生成器的方式训练模型，节省内存</p>
</li>
<li><p>多类别预测概率转换</p>
</li>
<li><p>CNN+LSTM的思考</p>
</li>
<li><p>使用预训练模型的权重</p>
</li>
</ol>
<span id="more"></span>
<h1 id="常见调参技巧"><a href="#常见调参技巧" class="headerlink" title="常见调参技巧"></a>常见调参技巧</h1><blockquote>
<p>超参上，learning rate 最重要，推荐了解 cosine learning rate 和 cyclic learning rate，其次是 batchsize 和 weight decay。当你的模型还不错的时候，可以试着做数据增广和改损失函数锦上添花了。</p>
</blockquote>
<h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><blockquote>
<p>推荐一篇fastai首席设计师「Sylvain Gugger」的一篇博客：How Do You Find A Good Learning Rate[1]</p>
<p>以及相关的论文Cyclical Learning Rates for Training Neural Networks[2]。</p>
</blockquote>
<p><strong>一般来说，越大的batch-size使用越大的学习率（一般来说Batch Size变成原始几倍，学习率就增加几倍）。</strong>原理很简单，越大的batch-size意味着我们学习的时候，收敛方向的confidence越大，我们前进的方向更加坚定，而小的batch-size则显得比较杂乱，毫无规律性，因为相比批次大的时候，批次小的情况下无法照顾到更多的情况，所以需要小的学习率来保证不至于出错。</p>
<p>在显存足够的条件下，最好采用较大的batch-size进行训练，找到合适的学习率后，可以加快收敛速度。</p>
<p>另外，较大的batch-size可以避免batch normalization出现的一些小问题</p>
<ul>
<li>采用较小的学习率，则<strong>收敛缓慢</strong>，也有可能收敛到<strong>局部最优</strong>解，导致loss没变化；</li>
<li>采用较大的学习率，会使得 loss 上下波动较大，导致loss爆炸=Nan或者无法收敛；</li>
</ul>
<p>调参技巧：</p>
<ul>
<li>优化器推荐使用<strong>AdamW</strong>或者<strong>SGD with Momentum</strong><ul>
<li>初始值建议 3e-4 (SGD可以选0.1)</li>
</ul>
</li>
<li><p>学习率衰减策略采用<strong>cosine learning rate 和 cyclic learning rate</strong></p>
<ul>
<li>参考 <a href="TF/Keras Learning Rate &amp; Schedulers">TF/Keras Learning Rate &amp; Schedulers</a>、 <a target="_blank" rel="noopener" href="https://blog.csdn.net/Light2077/article/details/106629697">Tensorflow2.0学习率衰减详细汇总</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/qxqsunshine/article/details/107486709?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-8.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-8.control">Cosine decay with warmup和 周期性学习率（CLR）</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/zaf0516/article/details/90720759">Tensorflow 中 learning rate decay 的奇技淫巧</a></li>
</ul>
</li>
<li><p>使用动态的学习率：<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/103379602">CLR、余弦退火、SGDR、switch Adam to SGD等</a>(含代码)</p>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/114869268">Warm up</a></strong>：用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，假如一开始就采用较大的学习率容易出现数值不稳定，这也是为什么要使用Warm up。然后等到训练过程基本上稳定了就可以使用原始的初始学习率进行训练了</li>
<li>训练策略：使用<strong>cosine learning rate+warmup</strong>的方法（最终结果差不太多）</li>
</ul>
<h3 id="优化器选择"><a href="#优化器选择" class="headerlink" title="优化器选择"></a>优化器选择</h3><blockquote>
<p> “Optimization” refers to the process of adjusting a model to get the best performance possible on the training data (the “learning” in “machine learning”), </p>
</blockquote>
<ul>
<li><p>优化方法使用</p>
<ul>
<li>Adam，Adade，RMSprop结果都差不多，Nadam因为是adam的动量添加的版本，在收敛效果上会更出色。</li>
<li>优化器公用参数 clipnorm 和 clipvalue<ul>
<li>参数一：clipnorm 对梯度进行裁剪，最大值为1</li>
<li>参数二：clipvalue 对梯度范围进行裁剪，范围（-x，x）</li>
</ul>
</li>
<li><strong>一般的起手式: Adam（<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/25097993">推荐使用3e-4</a>）</strong></li>
<li>Keras 推薦 RNN 使用 RMSProp<ul>
<li>在訓練 RNN 需要注意 explosive gradient 的問題 =&gt; clip gradient 的暴力美學</li>
<li><code>opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)</code></li>
</ul>
</li>
<li>SGD+monmentum：</li>
</ul>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sgd = SGD(<span class="hljs-attribute">lr</span>=learning_rate, <span class="hljs-attribute">decay</span>=learning_rate/nb_epoch, <span class="hljs-attribute">momentum</span>=0.9, <span class="hljs-attribute">nesterov</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><blockquote>
<p>随机丢弃，抑制过拟合，提高模型鲁棒性。</p>
</blockquote>
<p>dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是「暂时」，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。</p>
<p>通常我们在全连接层部分使用dropout，在卷积层则不使用。在全连接层部分，采用较大概率的dropout而在卷积层采用低概率或者不采用dropout。<strong>dropout对小数据防止过拟合有很好的效果,值一般设为0.2-0.5</strong></p>
<p><strong>Dropout作用</strong></p>
<ul>
<li>一方面缓解过拟合，另一方面引入的随机性，可以平缓训练过程，加速训练过程，处理outliers</li>
<li>Dropout可以看做ensemble，特征采样，相当于bagging很多子网络；训练过程中动态扩展拥有类似variation的输入数据集。（在单层网络中，类似折中Naiive bayes(所有特征权重独立)和logistic regression(所有特征之间有关系)；</li>
<li>一般对于越复杂的大规模网络，Dropout效果越好，是一个强regularizer！</li>
<li>最好的防止over-fitting就是有大量不重复数据</li>
</ul>
<h3 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a><strong>Batch Size</strong></h3><ul>
<li>太小可能导致不收敛（梯度更新方向变化太大）</li>
<li>太大可能收敛慢</li>
<li>一般选择<strong>64</strong></li>
</ul>
<p>如果可以容忍训练时间过长<strong>，最好开始使用尽量小的batch size(16,8,1)</strong>。<strong>batch size=1是一个很不错的选择</strong>, 起码在某些task上,这也有可能是很多人无法复现alex graves实验结果的原因之一，因为他总是把batch size设成1。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><blockquote>
<p>RELU用极简的方式实现非线性激活，缓解梯度消失</p>
</blockquote>
<ul>
<li>尽量不要用sigmoid，可以用relu之类的激活函数.</li>
<li>最后一层不要用relu，例如分类问题最后一层用softmax，回归问题可以不用。</li>
<li><strong>PReLU</strong>是一个不错的选择</li>
</ul>
<h3 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h3><blockquote>
<p>BatchNormalization可以加快收敛速度。</p>
</blockquote>
<ul>
<li><p>有BN的全连接层没必要加Dropout</p>
</li>
<li><p>Batchnormalization层的放置问题</p>
<figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs llvm"><span class="hljs-keyword">x</span> <span class="hljs-operator">=</span> (<span class="hljs-keyword">x</span> - <span class="hljs-keyword">x</span>.mean()) / <span class="hljs-keyword">x</span>.std()<br></code></pre></td></tr></table></figure>
<p>BN层针对数据分布进行优化，对于BN来说其不但可以防止过拟合，还可以防止梯度消失等问题，并且可以加快模型的收敛速度，但是加了BN，模型训练往往会变得慢些。具体放置位置试！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">BatchNormalization(mode=<span class="hljs-number">0</span>, axis=<span class="hljs-number">1</span>)	<span class="hljs-comment"># 输入是形如（samples，channels，rows，cols）的4D图像张量，需要设置axis=1</span><br>Dense()<br>BatchNormalization(mode=<span class="hljs-number">1</span>)	<span class="hljs-comment"># 按样本规范化，该模式默认输入为2D</span><br></code></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="加深网络"><a href="#加深网络" class="headerlink" title="加深网络"></a>加深网络</h3><p>都说深度网络精度更高，但深度不是盲目堆起来的，<strong>一定要在浅层网络有一定效果的基础上，增加深度。深度增加是为了增加模型的准确率</strong>，如果浅层都学不到东西，深了也没效果。开始一般用3-8层，当效果不错时，为了得到更高的准确率，再尝试加深网络</p>
<h3 id="隐层神经元的数量"><a href="#隐层神经元的数量" class="headerlink" title="隐层神经元的数量"></a>隐层神经元的数量</h3><blockquote>
<p>太多：训练慢，难去除噪声(over-fitting)</p>
<p>太少：拟合能力下降</p>
<p>一般：256-1024</p>
</blockquote>
<p>调参技巧：</p>
<ul>
<li>分类任务：初始尝试5-10倍类别个数</li>
<li>回归任务：初始尝试2-3倍输入/输出特征数</li>
<li>这里直觉很重要</li>
<li>最终影响其实不大，只是训练过程比较慢，多尝试</li>
</ul>
<h3 id="CNN的trick"><a href="#CNN的trick" class="headerlink" title="CNN的trick"></a>CNN的trick</h3><ul>
<li>pooling或卷积尺寸和步长不一样，增加数据多样性</li>
<li>data augumentation，避免过拟合，提高泛化，加噪声扰动</li>
<li>weight regularization</li>
<li>SGD使用decay的训练方法</li>
<li>最后使用pooling（avgpooling）代替全连接，减少参数量</li>
<li>maxpooling代替avgpooling，避免avgpooling带来的模糊化效果</li>
<li>2个3x3代替一个5x5等，减少参数，增加非线性映射，使CNN对特征学习能力强</li>
<li>3x3,2x2窗口</li>
<li>预训练方法等</li>
<li>数据预处理后(PCA,ZCA)喂给模型</li>
<li>输出结果窗口ensemble</li>
<li>中间节点作为辅助输出节点，相当于模型融合，同时增加反向传播的梯度信号，提供了额外的正则化</li>
<li>1x1卷积，夸通道组织信息，提高网络表达，可对输出降维，低成本，性价比高，增加非线性映射，符合Hebbian原理</li>
<li>NIN增加网络对不同尺度的适应性，类似Multi-Scale思想</li>
<li>Factorization into small convolution，7x7用1x7和7x1代替，节约参数，增加非线性映射</li>
<li>BN减少Internal Covariance Shift问题，提高学习速度，减少过拟合，可以取消dropout，增大学习率，减轻正则，减少光学畸变的数据增强</li>
<li>模型遇到退化问题考虑shortcut结构，增加深度</li>
<li>等等</li>
</ul>
<h3 id="RNN的trick"><a href="#RNN的trick" class="headerlink" title="RNN的trick"></a>RNN的trick</h3><p>小的细节和其他很像，简单说两句个人感觉的其他方面吧，其实RNN也是shortcut结构</p>
<ul>
<li>一般用LSTM结构防止BPTT的梯度消失，GRU拥有更少的参数，可以优先考虑</li>
<li>预处理细节，padding，序列长度设定，罕见词语处理等</li>
<li>一般语言模型的数据量一定要非常大</li>
<li>Gradient Clipping</li>
<li>Seq2Seq结构考虑attention，前提数据量大</li>
<li>序列模型考率性能优良的CNN+gate结构</li>
<li>一般生成模型可以参考GAN，VAE，产生随机变量</li>
<li>RL的框架结合</li>
<li>数据量少考虑简单的MLP</li>
<li>预测采用层级结构降低训练复杂度</li>
<li>设计采样方法，增加模型收敛速度</li>
<li>增加多级shortcut结构</li>
</ul>
<h1 id="问题和解决方法"><a href="#问题和解决方法" class="headerlink" title="问题和解决方法"></a>问题和解决方法</h1><h3 id="关于过拟合问题的讨论"><a href="#关于过拟合问题的讨论" class="headerlink" title="关于过拟合问题的讨论"></a>关于过拟合问题的讨论</h3><blockquote>
<p>当数据集较小而模型较大时会出现过拟合现象，作者指出了为避免过拟合的经验规律，也即<strong>当我们将模型大小扩大8倍时需要将数据集大小扩大5倍。</strong></p>
</blockquote>
<ul>
<li>防止过拟合的方法<ul>
<li>第一种就是添加dropout层，dropout可以放在很多类层的后面，用来抑制过拟合现象，常见的可以直接放在Dense层后面，一般在Dropout设置0.5。Dropout相当于Ensemble，dropout过大相当于多个模型的结合，一些差模型会拉低训练集的精度。<ul>
<li>通常只加在 hidden layer，不會加在 output layer，因為影響太大了，除非 output layer 的 dimension 很大。</li>
<li>Dropout 會讓 training performance 變差</li>
<li>參數少時，regularization</li>
</ul>
</li>
<li>第二种是使用参数正则化，也就是在一些层的声明中加入L1或L2正则化系数，在一定程度上提升了模型的泛化能力。<code>kernel_regularizer=regularizers.l2(0.001)</code></li>
<li>Reducing the network’s size： The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model</li>
<li>Early Stopping<ul>
<li>希望在 Model overfitting 之前就停止 training</li>
<li>Early Stopping in Keras<ul>
<li><code>from keras.callbacks import EarlyStopping</code></li>
<li><code>early_stopping=EarlyStopping(monitor=&#39;val_loss&#39;, patience=3)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="loss的值为NAN"><a href="#loss的值为NAN" class="headerlink" title="loss的值为NAN"></a>loss的值为NAN</h3><ul>
<li>学习率太高: loss爆炸, 或者nan</li>
<li>学习率太小: 半天loss没反映</li>
<li>relu作为激活函数?</li>
<li><strong>training sample中出现了脏数据&amp;异常值(nan, inf等)</strong>！措施：重整你的数据集</li>
<li>如果是自己定义的损失函数，这时候可能是你设计的损失函数有问题</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89588946">https://zhuanlan.zhihu.com/p/89588946</a></li>
</ul>
<h3 id="loss为负数"><a href="#loss为负数" class="headerlink" title="loss为负数"></a>loss为负数</h3><ul>
<li>如果出现loss为负，是因为之前多分类的标签哪些设置不对，现在是5分类的，写成了2分类之后导致了Loss为负数</li>
<li>也可能是损失函数选择错误导致</li>
</ul>
<h3 id="Loss不下降"><a href="#Loss不下降" class="headerlink" title="Loss不下降"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40267472/article/details/82216668">Loss不下降</a></h3><ul>
<li><p>train loss 不断下降，test loss不断下降，说明网络仍在学习;</p>
</li>
<li><p>train loss 不断下降，test loss趋于不变，说明网络过拟合;</p>
</li>
<li><p>train loss 趋于不变，test loss不断下降，说明数据集100%有问题;</p>
</li>
<li><p>train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;</p>
</li>
<li><p>train loss 不断上升，test loss不断上升，说明网络结构设计或超参数设置不当，数据集经过清洗等问题。</p>
</li>
<li><p>train loss下降一点后不再下降，学习率过大过小都不收敛</p>
</li>
</ul>
<h3 id="Loss维持在0-69附近（二分类）"><a href="#Loss维持在0-69附近（二分类）" class="headerlink" title="Loss维持在0.69附近（二分类）"></a><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/45c2180cab17">Loss维持在0.69附近（二分类）</a></h3><p>loss下降到0.69附近就不下降了，一直停在那里，acc在0.5左右？</p>
<ul>
<li><p>0.69是个什么数？</p>
<p>一般采用的都是cross entropy loss value,定义如下：</p>
</li>
</ul>
<p><img src="https:////upload-images.jianshu.io/upload_images/6671823-ad7303e0dd58d6a3.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/353/format/webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>​    发现就是网络预测给出的二类概率向量为[0.5,0.5]，即a和1-a都是0.5，不管y取值0/1，整个的平均loss就是-ln(0.5)=0.69. <strong>也就是训练过程中，无论如何调节网络都不收敛。</strong></p>
<ul>
<li><p>为啥预测的a老是为0.5呢？</p>
<p>a的值是softmax的输出，在二分类的情况下为0.5，表明输入softmax的值x是(近似)相等的。</p>
</li>
</ul>
<p><img src="https:////upload-images.jianshu.io/upload_images/6671823-d0014bc7e8a172b2.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/257/format/webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>​    进一步观察发现，x几乎都很小，随着训练的进行，还有进一步变小的趋势，可怕！</p>
<ul>
<li><p>解决办法</p>
<ol>
<li><p>调整初始化和激活函数无法间接保证与调节数据分布，那就强上BN层</p>
<ul>
<li>即在网络的每一层都加上Batch Normalization层操作，归一化强力保证其分布，果然彻底解决了0.69问题。</li>
</ul>
</li>
<li><p>改了Dense层的初始化方式×</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = Dense(<span class="hljs-number">1</span>,kernel_initializer=<span class="hljs-string">&#x27;he_normal&#x27;</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>,kernel_regularizer=l2(<span class="hljs-number">0.00001</span>))(x)<br></code></pre></td></tr></table></figure>
</li>
<li><p>可能是激活层的激活方式与损失函数不匹配。</p>
<p>一般使用sigmoid，损失函数使用binary_crossentropy ；使用softmax，损失函数使用categorical_crossentropy</p>
<ul>
<li><p><strong>改为softmax loss + sparse_categorical_crossentropy！</strong>×</p>
</li>
<li><p>contrastive_loss ×</p>
</li>
<li>取消relu激活 ×</li>
</ul>
</li>
<li><p>训练数据需要打乱，要检查每此batch是否都是一个类别，如果是，则没有办法优化；×</p>
</li>
<li>检查网络是不是没有回传梯度，而是只做了前向运算；×</li>
<li>二分类问题中 0.5 的 acc 接近随机猜测的值，可以检查下标签是否标错；×</li>
<li><strong>尝试不同的 Learning Rate (1e-6、2e-5、3e-4)；</strong>×</li>
<li>检查是否在 logit 那层加了激活函数，导致 logits 有问题，例如全为 0，经过 softmax 后就是 0.5了<ul>
<li>修改欧式距离为cos距离×</li>
<li>改为差和乘积的拼接；×</li>
</ul>
</li>
<li><strong>过拟合？尝试Dropout(0.5)</strong>×</li>
<li>BERT模型无法共享参数使用？</li>
<li>数据集本身的问题<ol>
<li>数据本身以及label是否有异常</li>
<li>数据是否过于脏乱，没有经过清洗</li>
<li>数据输入是否有问题，比如图片与label是否一致</li>
<li>数据经过预处理后，是否丢失特征或者因预处理而出现别的问题</li>
<li>数据量是否过少，网络出现过拟合的现象</li>
</ol>
</li>
</ol>
</li>
</ul>
<h3 id="Bad-Gradient-Dead-Neurons"><a href="#Bad-Gradient-Dead-Neurons" class="headerlink" title="Bad Gradient(Dead Neurons)"></a><strong>Bad Gradient(Dead Neurons)</strong></h3><p>使用ReLU激活函数，由于其在小于零范围梯度为0，可能会影响模型性能，甚至模型不会在更新<br><strong>当发现模型随着epoch进行，训练error不变化，可能所以神经元都“死”了。这时尝试更换激活函数如leaky ReLU，ELU，再看训练error变化</strong></p>
<ul>
<li>使用ReLU时需要给参数加一点噪声，打破完全对称避免0梯度，甚至给biases加噪声</li>
<li>相对而言对于sigmoid，因为其在0值附近最敏感，梯度最大，初始化全为0就可以啦</li>
<li>任何关于梯度的操作，比如clipping, rounding, max/min都可能产生类似的问题</li>
<li>ReLU相对Sigmoid优点：单侧抑制；宽阔的兴奋边界；稀疏激活性；解决梯度消失</li>
</ul>
<h3 id="模型训练加速"><a href="#模型训练加速" class="headerlink" title="模型训练加速"></a>模型训练加速</h3><p>关于模型训练加速，论文提到了2点，一是<strong>使用更大的Batch Size</strong>，二是<strong>使用低精度(如FP16)进行训练</strong>（也是我们常说的混合精度训练）。关于使用更大的Batch Size进行训练加速，作者指出一般只增加Batch Size的话，效果不会太理想，需要结合如下调参方案：</p>
<ul>
<li><strong>增大学习率</strong>。因为更大的Batch Size意味着每个Batch数据计算得到的梯度更加贴近整个数据集，从数学上来说就是方差更小，因此当更新方向更加准确之后，迈的步子也可以更大，一般来说Batch Size变成原始几倍，学习率就增加几倍。</li>
<li><strong>Warm up</strong>。Warm up指的是用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，假如一开始就采用较大的学习率容易出现数值不稳定，这也是为什么要使用Warm up。然后等到训练过程基本上稳定了就可以使用原始的初始学习率进行训练了。作者在使用Warm up的过程中使用线性增加的策略。举个例子假如Warm up阶段的初始学习率是0，warmup阶段共需要训练m个batch的数据（论文实现中m个batch共5个<code>epoch</code>），假设训练阶段的初始学习率是L，那么在第<img src="https://www.zhihu.com/equation?tex=i" srcset="/img/loading.gif" lazyload alt="[公式]">个batch的学习率就设置为<img src="https://www.zhihu.com/equation?tex=i%5Ctimes+L+%2F+m" srcset="/img/loading.gif" lazyload alt="[公式]">。</li>
<li><strong>每一个残差块后的最后一个BN层的<img src="https://www.zhihu.com/equation?tex=%5Cgamma" srcset="/img/loading.gif" lazyload alt="[公式]">参数初始化为0</strong>。我们知道BN层的<img src="https://www.zhihu.com/equation?tex=%5Cgamma" srcset="/img/loading.gif" lazyload alt="[公式]">，<img src="https://www.zhihu.com/equation?tex=%5Cbeta" srcset="/img/loading.gif" lazyload alt="[公式]">参数是用来对标注化后的数据做线性变换的，公式表示为：<img src="https://www.zhihu.com/equation?tex=y%3D%5Cgamma+x%5E+%2B%5Cbeta" srcset="/img/loading.gif" lazyload alt="[公式]">，其中我们一般会把<img src="https://www.zhihu.com/equation?tex=%5Cgamma" srcset="/img/loading.gif" lazyload alt="[公式]">设为1，而这篇论文提出初始化为<img src="https://www.zhihu.com/equation?tex=0" srcset="/img/loading.gif" lazyload alt="[公式]">则更容易训练。</li>
<li><strong>不对Bias参数做权重惩罚</strong>。但是对权重还是要做的。。</li>
</ul>
<h3 id="ResourceExhaustedError-OOM"><a href="#ResourceExhaustedError-OOM" class="headerlink" title="ResourceExhaustedError: OOM"></a>ResourceExhaustedError: OOM</h3><ul>
<li>意思就是GPU的内存不够了</li>
<li>解决：检查下是否有其他程序占用，不行就重启下IDE，或kill 进程ID</li>
</ul>
<h1 id="Keras相关经验"><a href="#Keras相关经验" class="headerlink" title="Keras相关经验"></a>Keras相关经验</h1><h2 id="1-训练集，验证集和测试集"><a href="#1-训练集，验证集和测试集" class="headerlink" title="1. 训练集，验证集和测试集"></a>1. 训练集，验证集和测试集</h2><ul>
<li>验证集是从训练集中抽取出来用于调参的，在validation_split中设置<ul>
<li>用 Keras 的 <code>validation_split</code> 之前要記得把資料先弄亂，因為它會從資料的最尾端開始取，如果沒有弄亂的話切出來的資料 bias 會很大。可以使用 <code>np.shuffle</code> 來弄亂</li>
</ul>
</li>
<li>测试集是和训练集无交集的，用于测试所选参数用于该模型的效果的。在evaluate函数里设置</li>
<li>尽量对数据做shuffle</li>
</ul>
<h2 id="2-查看模型的评价指标"><a href="#2-查看模型的评价指标" class="headerlink" title="2. 查看模型的评价指标"></a>2. 查看模型的评价指标</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">history_dict = history.history<br>history_dict.keys()<br>dict_keys([<span class="hljs-string">&#x27;val_acc&#x27;</span>, <span class="hljs-string">&#x27;acc&#x27;</span>, <span class="hljs-string">&#x27;val_loss&#x27;</span>, <span class="hljs-string">&#x27;loss’])</span><br></code></pre></td></tr></table></figure>
<h2 id="3-保存keras输出的loss，val"><a href="#3-保存keras输出的loss，val" class="headerlink" title="3. 保存keras输出的loss，val"></a>3. 保存keras输出的loss，val</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">hist=model.fit(train_set_x,train_set_y,batch_size=<span class="hljs-number">256</span>,shuffle=<span class="hljs-literal">True</span>,nb_epoch=nb_epoch,validation_split=<span class="hljs-number">0.1</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;log_sgd_big_32.txt&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(<span class="hljs-built_in">str</span>(hist.history))<br></code></pre></td></tr></table></figure>
<h2 id="4-绘制精度和损失曲线"><a href="#4-绘制精度和损失曲线" class="headerlink" title="4. 绘制精度和损失曲线"></a>4. 绘制精度和损失曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot</span>(<span class="hljs-params">history</span>):<br>   plt.figure(figsize=(<span class="hljs-number">16</span>,<span class="hljs-number">7</span>))<br>   plt.subplot(<span class="hljs-number">121</span>)<br>   plt.xlabel(<span class="hljs-string">&#x27;epoch&#x27;</span>)<br>   plt.ylabel(<span class="hljs-string">&#x27;acc&#x27;</span>)<br>   plt.plot(history.epoch, history.history[<span class="hljs-string">&#x27;acc&#x27;</span>], <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&quot;acc&quot;</span>)<br>   plt.plot(history.epoch, history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>], <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&quot;val_acc&quot;</span>)<br>   plt.scatter(history.epoch, history.history[<span class="hljs-string">&#x27;acc&#x27;</span>], marker=<span class="hljs-string">&#x27;*&#x27;</span>)<br>   plt.scatter(history.epoch, history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>])<br>   plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br><br>   plt.subplot(<span class="hljs-number">122</span>)<br>   plt.xlabel(<span class="hljs-string">&#x27;epoch&#x27;</span>)<br>   plt.ylabel(<span class="hljs-string">&#x27;loss&#x27;</span>)<br>   plt.plot(history.epoch, history.history[<span class="hljs-string">&#x27;loss&#x27;</span>], <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&quot;loss&quot;</span>)<br>   plt.plot(history.epoch, history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>], <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&quot;val_loss&quot;</span>)<br>   plt.scatter(history.epoch, history.history[<span class="hljs-string">&#x27;loss&#x27;</span>], marker=<span class="hljs-string">&#x27;*&#x27;</span>)<br>   plt.scatter(history.epoch, history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>], marker=<span class="hljs-string">&#x27;*&#x27;</span>)<br>   plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>   plt.show()<br><br><span class="hljs-comment"># 或者</span><br>history.loss_plot(<span class="hljs-string">&#x27;epoch&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h2 id="5-将整型-label-转换成-one-hot-形式"><a href="#5-将整型-label-转换成-one-hot-形式" class="headerlink" title="5. 将整型 label 转换成 one-hot 形式"></a>5. 将整型 label 转换成 one-hot 形式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">to_one_hot</span>(<span class="hljs-params">labels, dimension=<span class="hljs-number">46</span></span>):<br>    results = np.zeros((<span class="hljs-built_in">len</span>(labels), dimension))<br>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):<br>        results[i, label] = <span class="hljs-number">1.</span><br>    <span class="hljs-keyword">return</span> results<br></code></pre></td></tr></table></figure>
<h2 id="6-自制回调函数-callback"><a href="#6-自制回调函数-callback" class="headerlink" title="6. 自制回调函数 callback"></a>6. 自制回调函数 callback</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 该回调函数将在每个epoch后保存概率文件</span><br><span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> Callback<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WritePRF</span>(<span class="hljs-title class_ inherited__">Callback</span>):<br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data</span>):<br>      <span class="hljs-built_in">super</span>(WritePRF, self).__init__()<br>      self.data = data<br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">on_epoch_end</span>(<span class="hljs-params">self, epoch, logs=<span class="hljs-literal">None</span></span>):<br>      <br><span class="hljs-comment"># 该回调函数将在每个迭代后保存的最好模型</span><br><span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> ModelCheckpoint  <br><br>checkpoint = ModelCheckpoint(  <br>    <span class="hljs-string">&#x27;model.h5&#x27;</span>,  <br>    monitor = <span class="hljs-string">&#x27;val_loss&#x27;</span>,  <br>    verbose = <span class="hljs-number">1</span>,  <br>    save_best_only = <span class="hljs-literal">True</span>,  <br>    mode = <span class="hljs-string">&#x27;min&#x27;</span>,  <br>) <br></code></pre></td></tr></table></figure>
<h2 id="7-网格超参数搜索"><a href="#7-网格超参数搜索" class="headerlink" title="7. 网格超参数搜索"></a>7. 网格超参数搜索</h2><p><a target="_blank" rel="noopener" href="http://geek.csdn.net/news/detail/95494">Keras/Python深度学习中的网格搜索超参数调优（附源码）</a></p>
<p>先grid search，再random search（由粗到细）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_model</span>():<br>	<span class="hljs-comment"># create model</span><br>	model = Sequential()<br>	model.add(Dense(<span class="hljs-number">12</span>, input_dim=<span class="hljs-number">8</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>	model.add(Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br>	<span class="hljs-comment"># Compile model</span><br>	model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>, optimizer=optimizer, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>	<span class="hljs-keyword">return</span> model<br> <br><span class="hljs-comment"># create model</span><br>model = KerasClassifier(build_fn=create_model, verbose=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># define the grid search parameters</span><br>batch_size = [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">40</span>, <span class="hljs-number">60</span>, <span class="hljs-number">80</span>, <span class="hljs-number">100</span>]<br>epochs = [<span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>]<br><span class="hljs-comment"># define the grid search parameters</span><br>optimizer = [<span class="hljs-string">&#x27;SGD&#x27;</span>, <span class="hljs-string">&#x27;RMSprop&#x27;</span>, <span class="hljs-string">&#x27;Adagrad&#x27;</span>, <span class="hljs-string">&#x27;Adadelta&#x27;</span>, <span class="hljs-string">&#x27;Adam&#x27;</span>, <span class="hljs-string">&#x27;Adamax&#x27;</span>, <span class="hljs-string">&#x27;Nadam&#x27;</span>]<br><span class="hljs-comment"># define the grid search parameters</span><br>learn_rate = [<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>]<br>dropout_rate = [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>]<br>neurons = [<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30</span>]<br><br>param_grid = <span class="hljs-built_in">dict</span>(batch_size=batch_size, epochs=epochs, optimizer=optimizer)<br>grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=<span class="hljs-number">1</span>)<br>grid_result = grid.fit(X, Y)<br><span class="hljs-comment"># summarize results</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Best: %f using %s&quot;</span> % (grid_result.best_score_, grid_result.best_params_))<br>means = grid_result.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]<br>stds = grid_result.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]<br>params = grid_result.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]<br><span class="hljs-keyword">for</span> mean, stdev, param <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(means, stds, params):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;%f (%f) with: %r&quot;</span> % (mean, stdev, param))<br></code></pre></td></tr></table></figure>
<h2 id="8-编写自己的层"><a href="#8-编写自己的层" class="headerlink" title="8. 编写自己的层"></a>8. 编写自己的层</h2><p>对于简单的定制操作，我们或许可以通过使用layers.core.Lambda层来完成。要定制自己的层，你需要实现下面三个方法:</p>
<ul>
<li>build(input_shape)：这是定义权重的方法</li>
<li>call(x)：这是定义层功能的方法，除非你希望你写的层支持masking，否则你只需要关心call的第一个参数：输入张量</li>
<li>compute_output_shape(input_shape)：如果你的层修改了输入数据的shape，你应该在这里指定shape变化的方法，这个函数使得Keras可以做自动shape推断</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-keyword">from</span> keras.engine.topology <span class="hljs-keyword">import</span> Layer<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyLayer</span>(<span class="hljs-title class_ inherited__">Layer</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, output_dim, **kwargs</span>):<br>        self.output_dim = output_dim<br>        <span class="hljs-built_in">super</span>(MyLayer, self).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self, input_shape</span>):<br>        <span class="hljs-comment"># Create a trainable weight variable for this layer.</span><br>        self.kernel = self.add_weight(name=<span class="hljs-string">&#x27;kernel&#x27;</span>, <br>                                      shape=(input_shape[<span class="hljs-number">1</span>], self.output_dim),<br>                                      initializer=<span class="hljs-string">&#x27;uniform&#x27;</span>,<br>                                      trainable=<span class="hljs-literal">True</span>)<br>        <span class="hljs-built_in">super</span>(MyLayer, self).build(input_shape)  <span class="hljs-comment"># Be sure to call this somewhere!</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> K.dot(x, self.kernel)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_output_shape</span>(<span class="hljs-params">self, input_shape</span>):<br>        <span class="hljs-keyword">return</span> (input_shape[<span class="hljs-number">0</span>], self.output_dim)<br></code></pre></td></tr></table></figure>
<h2 id="9-keras保存和加载自定义损失模型"><a href="#9-keras保存和加载自定义损失模型" class="headerlink" title="9. keras保存和加载自定义损失模型"></a>9. keras保存和加载自定义损失模型</h2><p>如果使用了自定义的loss函数， 则需要在加载模型的时候，指定load_model函数提供的一个custom_objects参数：在custom_objects参数词典里加入keras的未知参数，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">custom_objects=&#123;<span class="hljs-string">&#x27;ChainCRF&#x27;</span>: ClassWrapper, <span class="hljs-string">&#x27;loss&#x27;</span>: loss, <span class="hljs-string">&#x27;sparse_loss&#x27;</span>: sparse_loss&#125;<br>model = load_model(<span class="hljs-string">&#x27;model/tmpModel.h5&#x27;</span>, custom_objects=create_custom_objects())<br></code></pre></td></tr></table></figure>
<h2 id="10-PRF-值计算"><a href="#10-PRF-值计算" class="headerlink" title="10. PRF 值计算"></a>10. PRF 值计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate</span>(<span class="hljs-params">predictions, test_label, RESULT_FILE</span>):<br>   num = <span class="hljs-built_in">len</span>(predictions)<br>   <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(RESULT_FILE, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>      <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num):<br>         <span class="hljs-keyword">if</span> predictions[i][<span class="hljs-number">1</span>] &gt; predictions[i][<span class="hljs-number">0</span>]:<br>            predict = +<span class="hljs-number">1</span><br>         <span class="hljs-keyword">else</span>:<br>            predict = -<span class="hljs-number">1</span><br>         f.write(<span class="hljs-built_in">str</span>(predictions[i][<span class="hljs-number">0</span>]) + <span class="hljs-string">&#x27; &#x27;</span> + <span class="hljs-built_in">str</span>(predictions[i][<span class="hljs-number">1</span>]) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>      <span class="hljs-comment"># f.write(str(predict) + str(predictions[i]) + &#x27;\n&#x27;)</span><br><br>   TP = <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span><br>           predictions[i][<span class="hljs-number">1</span>] &gt; predictions[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> (test_label[i] == np.asarray([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])).<span class="hljs-built_in">all</span>()])<br>   FP = <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span><br>           predictions[i][<span class="hljs-number">1</span>] &gt; predictions[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> (test_label[i] == np.asarray([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>])).<span class="hljs-built_in">all</span>()])<br>   FN = <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span><br>           predictions[i][<span class="hljs-number">1</span>] &lt; predictions[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> (test_label[i] == np.asarray([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])).<span class="hljs-built_in">all</span>()])<br>   TN = <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span><br>           predictions[i][<span class="hljs-number">1</span>] &lt; predictions[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> (test_label[i] == np.asarray([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>])).<span class="hljs-built_in">all</span>()])<br><br>   precision = recall = Fscore = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>   <span class="hljs-keyword">try</span>:<br>      precision = TP / (<span class="hljs-built_in">float</span>)(TP + FP)  <span class="hljs-comment"># ZeroDivisionError: float division by zero</span><br>      recall = TP / (<span class="hljs-built_in">float</span>)(TP + FN)<br>      Fscore = (<span class="hljs-number">2</span> * precision * recall) / (precision + recall)<br>   <span class="hljs-keyword">except</span> ZeroDivisionError <span class="hljs-keyword">as</span> exc:<br>      <span class="hljs-built_in">print</span>(exc.message)<br><br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt;&gt; Report the result ...&quot;</span>)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-1 --&gt; &quot;</span>, <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span> predictions[i][<span class="hljs-number">1</span>] &lt; predictions[i][<span class="hljs-number">0</span>]]))<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;+1 --&gt; &quot;</span>, <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span> predictions[i][<span class="hljs-number">1</span>] &gt; predictions[i][<span class="hljs-number">0</span>]]))<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;TP=&quot;</span>, TP, <span class="hljs-string">&quot;  FP=&quot;</span>, FP, <span class="hljs-string">&quot; FN=&quot;</span>, FN, <span class="hljs-string">&quot; TN=&quot;</span>, TN)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;precision= &quot;</span>, precision)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;recall= &quot;</span>, recall)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Fscore= &quot;</span>, Fscore)<br></code></pre></td></tr></table></figure>
<h2 id="11-keras-获取中间层的输出"><a href="#11-keras-获取中间层的输出" class="headerlink" title="11. keras 获取中间层的输出"></a>11. keras 获取中间层的输出</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载权重到当前模型</span><br>model = load_model(model_path)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;获取中间层的输出&#x27;&#x27;&#x27;</span><br>layer_name = <span class="hljs-string">&#x27;my_layer&#x27;</span><br>intermediate_layer_model = Model(<span class="hljs-built_in">input</span>=model.<span class="hljs-built_in">input</span>,<br>                         output=model.get_layer(layer_name).output)<br><br>intermediate_output = intermediate_layer_model.predict(X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(intermediate_output))<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;intermediate_output.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> intermediate_output:<br>      f.write(i)<br></code></pre></td></tr></table></figure>
<h2 id="12-keras指定显卡且限制显存用量"><a href="#12-keras指定显卡且限制显存用量" class="headerlink" title="12. keras指定显卡且限制显存用量"></a>12. keras指定显卡且限制显存用量</h2><p>keras在使用GPU的时候有个特点，就是默认全部占满显存。需要修改后端代码：</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-comment"># 方法1:显存占用会随着epoch的增长而增长</span><br><span class="hljs-comment"># 也就是后面的epoch会去申请新的显存,前面已完成的并不会释放,为了防止碎片化</span><br><br><span class="hljs-built_in">config</span> = tf.ConfigProto()<br><span class="hljs-built_in">config</span>.gpu_options.allow_growth = True  <span class="hljs-comment"># 按需求增长</span><br>sess = tf.Session(<span class="hljs-built_in">config</span>=<span class="hljs-built_in">config</span>)<br>set_session(sess)<br></code></pre></td></tr></table></figure>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># # 方法2:只允许使用x%的显存,其余的放着不动</span><br><br><span class="hljs-attr">config</span> = tf.ConfigProto()<br><span class="hljs-attr">config.gpu_options.per_process_gpu_memory_fraction</span> = <span class="hljs-number">0.5</span>    <span class="hljs-comment"># 按比例</span><br><span class="hljs-attr">sess</span> = tf.Session(config=config)<br></code></pre></td></tr></table></figure>
<p>PS: 需要注意的是，虽然代码或配置层面设置了对显存占用百分比阈值，但在实际运行中如果达到了这个阈值，程序有需要的话还是会突破这个阈值。换而言之如果跑在一个大数据集上还是会用到更多的显存。以上的显存限制仅仅为了在跑小数据集时避免对显存的浪费而已。</p>
<h2 id="13-Keras-切换后端（Theano和TensorFlow）"><a href="#13-Keras-切换后端（Theano和TensorFlow）" class="headerlink" title="13. Keras 切换后端（Theano和TensorFlow）"></a>13. Keras 切换后端（Theano和TensorFlow）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">vi ~/.keras/keras.json<br><br>&#123;<br>    <span class="hljs-string">&quot;image_dim_ordering&quot;</span>: <span class="hljs-string">&quot;tf&quot;</span>, <br>    <span class="hljs-string">&quot;epsilon&quot;</span>: <span class="hljs-number">1e-07</span>, <br>    <span class="hljs-string">&quot;floatx&quot;</span>: <span class="hljs-string">&quot;float32&quot;</span>, <br>    <span class="hljs-string">&quot;backend&quot;</span>: <span class="hljs-string">&quot;tensorflow&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="14-categorical-crossentropy-vs-sparse-categorical-crossentropy"><a href="#14-categorical-crossentropy-vs-sparse-categorical-crossentropy" class="headerlink" title="14. categorical_crossentropy vs. sparse_categorical_crossentropy"></a>14. categorical_crossentropy vs. sparse_categorical_crossentropy</h2><p>There are two ways to handle labels in multi-class classification: Encoding the labels via “categorical encoding” (also known as “one-hot encoding”) and using <code>categorical_crossentropy</code> as your loss function. Encoding the labels as integers and using the <code>sparse_categorical_crossentropy</code> loss function.</p>
<h2 id="15-通过生成器的方式训练模型，节省内存"><a href="#15-通过生成器的方式训练模型，节省内存" class="headerlink" title="15. 通过生成器的方式训练模型，节省内存"></a>15. 通过生成器的方式训练模型，节省内存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#从节省内存的角度，通过生成器的方式来训练</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generator</span>(<span class="hljs-params">data, chars, targets, data_a, chars_a, targets_a, batch_size</span>): <br>        idx = np.arange(<span class="hljs-built_in">len</span>(data))<br>        np.random.shuffle(idx)<br>        batches = [idx[<span class="hljs-built_in">range</span>(batch_size*i, <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(data), batch_size*(i+<span class="hljs-number">1</span>)))] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data)//batch_size)]<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> batches:<br>                xx, yy = np.array(data[i]), np.array(targets[i])<br>                char = np.array(chars[i])<br>                xx_a, yy_a = np.array(data_a[i]), np.array(targets_a[i])<br>                char_a = np.array(chars_a[i])<br>                <span class="hljs-keyword">yield</span> ([xx, char, xx_a, char_a], [yy, yy_a])<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Build model...&#x27;</span>)<br>    model = buildModel(max_word)<br><br>    generator = data_generator(train_x, train_char, train_y, aux_train_x, aux_train_char, aux_train_y, batch_size)<br>    samples_per_epoch = <span class="hljs-built_in">len</span>(train_x)<br>    steps_per_epoch = samples_per_epoch // batch_size<br>    <span class="hljs-comment"># StopIteration: dataset fully readed before fit end</span><br>    history = model.fit_generator(generator, steps_per_epoch=steps_per_epoch, epochs=epochs)<br></code></pre></td></tr></table></figure>
<h2 id="16-CNN-LSTM的思考"><a href="#16-CNN-LSTM的思考" class="headerlink" title="16. CNN+LSTM的思考"></a>16. CNN+LSTM的思考</h2><p>Because RNNs are extremely expensive for processing very long sequences, but 1D convnets are cheap, it can be a good idea to use a 1D convnet as a preprocessing step before a RNN, shortening the sequence and extracting useful representations for the RNN to process.</p>
<h2 id="17-使用预训练模型的权重"><a href="#17-使用预训练模型的权重" class="headerlink" title="17. 使用预训练模型的权重"></a>17. 使用预训练模型的权重</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">WEIGHTS_PATH = <span class="hljs-string">&#x27;bottleneck_fc_model.h5&#x27;</span><br>model1.save_weights(WEIGHTS_PATH)<br>model2.load_weights(WEIGHTS_PATH)<br><span class="hljs-comment"># layer.trainable = False</span><br>model2.fit()<br></code></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247493509&amp;idx=3&amp;sn=032d0e4e3739bc311b09c6cd388cdf14&amp;chksm=ebb7df51dcc05647ab13b837ec0619d36b970225586c74e0ed8819fddd7bee538e91add623e0&amp;scene=0&amp;xtrack=1#rd">【神经网络训练】trick总结</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247513519&amp;idx=3&amp;sn=10192952888d4a6fb6d71518c788aae1&amp;chksm=ebb78d7bdcc0046da4155b767fec826816fb2f9ddd6f1b5d8799b6227f8ab6fdf1060951287c&amp;scene=126&amp;sessionid=1604041737&amp;key=e4f3c199ec5f11ddce50d5ee0a0958370d1a089556ec124a47f5cb97dd2078bd1269fb9f9878c30ea81c70a9355cc5cbe41593ba0bf0523fde58563694078048dd9b838bdb16eaf0646915443f66b6c9afda97d05025da77c1b7b47bb632a621292b67e4aa2b848e56e842ca91fd421517e467857cbe0d80839c1b2e42c77a72&amp;ascene=14&amp;uin=MjM2MDA1NjcyMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A8QWRj5C0ouaywX0FtXc4WQ%3D&amp;pass_ticket=943CnHv8LXB1GZ0a9Q4gwHdZ0KtBvMEBhT4bPNqTZNi64VF8LD95Wvjqw5jVQfHP&amp;wx_header=0">深度学习调参tricks总结！</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/352971645">大道至简：算法工程师炼丹Trick手册</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Keras/">#Keras</a>
      
        <a href="/tags/%E7%82%BC%E4%B8%B9/">#炼丹</a>
      
        <a href="/tags/%E8%B0%83%E5%8F%82/">#调参</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>NN调参炼丹上分手册</div>
      <div>http://example.com/2020/04/24/2020-04-24-调参炼丹手册/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>NSX</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2020年4月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2020/04/28/2020-04-28-python%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E5%8A%A0%E5%AF%86/" title="python敏感信息加密">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">python敏感信息加密</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/04/21/2020-04-21-%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/" title="自动混合精度训练">
                        <span class="hidden-mobile">自动混合精度训练</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
