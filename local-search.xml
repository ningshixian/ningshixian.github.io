<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>My New Post</title>
    <link href="/2023/04/24/My-New-Post/"/>
    <url>/2023/04/24/My-New-Post/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/04/24/hello-world/"/>
    <url>/2023/04/24/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LoRa 学习笔记</title>
    <link href="/2023/04/23/LoRa%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2023/04/23/LoRa%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="LoRa是什么？"><a href="#LoRa是什么？" class="headerlink" title="LoRa是什么？"></a>LoRa是什么？</h2><p>LoRA，出自论文<a href="https://arxiv.org/abs/2106.09685">《LoRA: Low-Rank Adaptation of Large Language Models》</a>，是Microsoft 于 2021 年推出的一项新技术，用于微调大型语言模型 (LLM)。<br>比如，GPT-3有1750亿参数，为了让它能干特定领域的活儿，需要做微调，但是如果直接对GPT-3做微调，成本太高太麻烦了。<br>LoRA的做法是，冻结预训练好的模型权重参数，然后在每个Transformer（Transforme就是GPT的那个T）块里注入可训练的层，由于不需要对模型的权重参数重新计算梯度，所以，大大减少了需要训练的计算量。<br>研究发现，LoRA的微调质量与全模型微调相当，我愿称之为神器。<br>要做个比喻的话，就好比是大模型的一个小模型，或者说是一个插件。<br>LoRA本来是给大语言模型准备的，但把它用在cross-attention layers（交叉关注层）也能影响用文字生成图片的效果。</p><h2 id="LoRa方法简介"><a href="#LoRa方法简介" class="headerlink" title="LoRa方法简介"></a>LoRa方法简介</h2><blockquote><p><a href="https://kexue.fm/archives/9590">梯度视角下的LoRA：简介、分析、猜测及推广</a></p></blockquote><p>LoRA借鉴了上述结果，提出对于预训练的参数矩阵$W_0∈ℝ^{m×n}$，我们不去直接微调$W_0$，而是对增量做低秩分解假设：</p><img src="/2023/04/23/LoRa%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png" class="" title="image.png"><p>其中$U,V$之一用全零初始化，$W_0$固定不变，优化器只优化$U,V$。由于本征维度很小的结论，所以$r$我们可以取得很小，很多时候我们甚至可以直接取$1$。所以说，LoRA是一种参数高效的微调方法，至少被优化的参数量大大降低了。</p><img src="/2023/04/23/LoRa%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.png" class="" title="image.png"><h2 id="PEFT对LORA的实现"><a href="#PEFT对LORA的实现" class="headerlink" title="PEFT对LORA的实现"></a>PEFT对LORA的实现</h2><p><a href="https://blog.csdn.net/weixin_44826203/article/details/129733930">https://blog.csdn.net/weixin_44826203/article/details/129733930</a></p><p><a href="https://mp.weixin.qq.com/s/bWl9Ke9dHmHPQEad-XZsyQ">https://mp.weixin.qq.com/s/bWl9Ke9dHmHPQEad-XZsyQ</a></p>]]></content>
    
    
    <categories>
      
      <category>ChatGPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LoRa</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GitHub Pages + Hexo搭建个人博客网站</title>
    <link href="/2023/04/23/2023-04-23-GitHub%20Pages%20+%20Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/"/>
    <url>/2023/04/23/2023-04-23-GitHub%20Pages%20+%20Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<h3 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h3><h4 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h4><h4 id="安装NodeJS"><a href="#安装NodeJS" class="headerlink" title="安装NodeJS"></a>安装NodeJS</h4><ol><li>访问<a href="https://nodejs.org/en/">nodejs官网</a>，点击稳定版，并下载node.js</li><li>双击刚下载的文件，按步骤默认安装就行</li><li>验证 <code>npm -v</code>、<code>node -v</code><h3 id="二、创建仓库"><a href="#二、创建仓库" class="headerlink" title="二、创建仓库"></a>二、创建仓库</h3>使用个人 GitHub 创建仓库，并配置 GitHub Pages<blockquote><p>注意: 此仓库用于存放个人博客页面，仓库名必须使用 <GitHub用户名>.github.io 格式。</p></blockquote></li></ol><h3 id="三、安装Hexo"><a href="#三、安装Hexo" class="headerlink" title="三、安装Hexo"></a>三、安装Hexo</h3><p>Hexo 是一个基于NodeJS的静态博客网站生成器，使用Hexo不需开发，只要进行一些必要的配置即可生成一个个性化的博客网站，非常方便。点击进入 官网。</p><ol><li>安装 Hexo <code>npm install -g hexo-cli</code></li><li>查看版本 <code>hexo -v</code></li><li><p>创建一个项目 hexo-blog 并初始化</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">sudo hexo init hexo-<span class="hljs-keyword">blog</span><br><span class="hljs-keyword"></span>sudo cd hexo-<span class="hljs-keyword">blog</span><br><span class="hljs-keyword"></span>sudo npm <span class="hljs-keyword">install</span><br></code></pre></td></tr></table></figure></li><li><p>本地启动</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs verilog">sudo hexo <span class="hljs-keyword">generate</span> # 生成页面，此命令可以简写为 `hexo g`<br>sudo hexo server # 本地启动，可简写为 `hexo s`<br></code></pre></td></tr></table></figure></li><li><p>浏览器访问 <a href="http://localhost:4000">http://localhost:4000</a></p><h3 id="四、更换主题"><a href="#四、更换主题" class="headerlink" title="四、更换主题"></a>四、更换主题</h3><p>Hexo 默认的主题不太好看，不过官方提供了数百种主题供用户选择，可以根据个人喜好更换，官网主题点 <a href="https://hexo.io/themes/">这里</a> 查看。这里介绍两个主题的使用方法，Next 和 Fluid，个人比较喜欢Fluid，后面章节的功能也是以 Fluid 为基础进行讲解的。</p></li><li><p>安装主题</p></li></ol><p>下载 <a href="https://github.com/fluid-dev/hexo-theme-fluid/releases">最新 release 版本</a> 解压到 themes 目录，并将解压出的文件夹重命名为 fluid。</p><ol><li>指定主题</li></ol><p>如下修改 Hexo 博客目录中的 _config.yml：<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">theme:</span> fluid  <span class="hljs-meta"># 指定主题</span><br><span class="hljs-symbol">language:</span> <span class="hljs-built_in">zh</span>-CN  <span class="hljs-meta"># 指定语言，会影响主题显示的语言，按需修改</span><br></code></pre></td></tr></table></figure></p><ol><li>创建「关于页」</li></ol><p>首次使用主题的「关于页」需要手动创建：<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haxe">hexo <span class="hljs-keyword">new</span> <span class="hljs-type">page</span> about<br></code></pre></td></tr></table></figure><br>创建成功后，编辑博客目录下 /source/about/index.md，添加 layout 属性。<br>修改后的文件示例如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">about</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2020-02-23 19:20:33</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">about</span><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-string">这里写关于页的正文，支持</span> <span class="hljs-string">Markdown,</span> <span class="hljs-string">HTML</span><br></code></pre></td></tr></table></figure><br>本地启动<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">sudo</span> hexo g -d  <span class="hljs-comment"># Generate static files &amp; Deploy to remote sites</span><br>sudo hexo s<span class="hljs-comment"># Run server</span><br></code></pre></td></tr></table></figure><br>浏览器访问 <a href="http://localhost:4000">http://localhost:4000</a></p><h3 id="五、创建文章"><a href="#五、创建文章" class="headerlink" title="五、创建文章"></a>五、创建文章</h3><p>如下修改 Hexo 博客目录中的 _config.yml，打开这个配置是为了在生成文章的时候生成一个同名的资源目录用于存放图片文件。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">post_asset_folder:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><br>执行如下命令创建一篇新文章，名为《测试文章》<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haxe">hexo <span class="hljs-keyword">new</span> <span class="hljs-type">post</span> <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">XXX</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2023-04-23 15:24:20</span><br><span class="hljs-attr">tags:</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">原创</span><br><span class="hljs-attr">categories:</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">ChatGPT</span><br><span class="hljs-attr">math:</span> <span class="hljs-literal">true</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><br><strong>本地启动</strong><br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">sudo</span> hexo g -d  <span class="hljs-comment"># Generate static files &amp; Deploy to remote sites</span><br>sudo hexo s<span class="hljs-comment"># Run server</span><br></code></pre></td></tr></table></figure></p><h3 id="六、个性化页面展示"><a href="#六、个性化页面展示" class="headerlink" title="六、个性化页面展示"></a>六、个性化页面展示</h3><p>修改根目录下 _config.yml 以及 themes\fluid 下 _config.yml 文件<br>略……</p><p><a href="https://blog.csdn.net/yexiaohhjk/article/details/82526604"><strong>hexo 主题解决无法显示数学公式？</strong></a><br>基于Hexo搭建的个人博客，默认使用 hexo-renderer-marked 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线_代表斜体，会被渲染引擎处理为<em>标签。<br>因为类 Latex 格式书写的数学公式下划线_表示下标，有特殊的含义，如果被强制转换为<em>标签，那么 MathJax 引擎在渲染数学公式的时候就会出错。<br><strong>解决方法：</strong><br>步骤一、更换 Hexo 的 markdown 渲染引擎，卸载 hexo-renderer-marked，安装 hexo-renderer-kramed 引擎（<a href="https://github.com/fluid-dev/hexo-theme-fluid/issues/301">要保证只留其中一个！</a>）<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm uninstall hexo-renderer-marked --save<br>npm install hexo-renderer-kramed --save<br></code></pre></td></tr></table></figure><br>步骤二、找到node_modules\kramed\lib\rules\inline.js，做如下修改：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">把第11行的 escape 变量的值做·相应的修改：</span><br>//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,<br>  escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">同时把第20行的em变量也要做相应的修改。</span><br>  //em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,<br>  em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,<br></code></pre></td></tr></table></figure><br>步骤三、在 Fluid 主题中开启 MathJax 开关<br>进入到主题目录，找到 _config.yml 配置，把 math 默认的 false 修改为true，具体如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Math Equations Render Support</span><br>math:<br>  enable: true<br>pecific: true<br>engine: mathjax<br></code></pre></td></tr></table></figure><br>之后，在文章的Front-matter里打开mathjax开关，如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">---<br>title: index.html<br>date: 2018-07-05 12:01:30<br>categories:<br>- 博客搭建<br>tags:<br>- 博客搭建<br>- node<br>math: true<br>--<br></code></pre></td></tr></table></figure><br>步骤四、重新启动hexo, 问题完美解决<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo hexo clean<br>sudo hexo g -d<br></code></pre></td></tr></table></figure></p><p><a href="https://blog.csdn.net/z952957407/article/details/111642548"><strong>hexo+github本地和线上图片不显示问题？</strong></a></p><p><a href="https://blog.csdn.net/z952957407/article/details/111642548">https://blog.csdn.net/z952957407/article/details/111642548</a></p><h3 id="七、添加阅读量统计"><a href="#七、添加阅读量统计" class="headerlink" title="七、添加阅读量统计"></a>七、添加阅读量统计</h3><h3 id="八、添加评论功能"><a href="#八、添加评论功能" class="headerlink" title="八、添加评论功能"></a>八、添加评论功能</h3><h3 id="九、发布到GitHub-Pages"><a href="#九、发布到GitHub-Pages" class="headerlink" title="九、发布到GitHub Pages"></a>九、发布到GitHub Pages</h3><p>安装hexo-deployer-git<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">npm install hexo-deployer-git <span class="hljs-comment">--save</span><br></code></pre></td></tr></table></figure><br>修改根目录下的 _config.yml，配置 GitHub 相关信息<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">deploy:</span><br><span class="hljs-symbol">  type:</span> git<br><span class="hljs-symbol">  repo:</span> https:<span class="hljs-comment">//github.com/xxx/xxx.github.io.git</span><br><span class="hljs-symbol">  branch:</span> main<br><span class="hljs-symbol">  token:</span> ghp_3KakcaPHerunNRyMerofcFd9pblU282FSbsY<br></code></pre></td></tr></table></figure><br>其中 token 为 GitHub 的 Personal access tokens，获取方式如下图<br><img src="GitHub Pages + Hexo搭建个人博客网站/1682252610548-d80ad359-a3d6-4de4-a247-5d8beca28249.png" alt="image.png"></p><p>部署到GitHub</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">sudo hexo g -d</span><br></code></pre></td></tr></table></figure><p>浏览器访问 <a href="https://yaorongke.github.io/">https://xxx.github.io/</a>，部署成功</p><h3 id="十、发布到自己服务器，Nginx代理"><a href="#十、发布到自己服务器，Nginx代理" class="headerlink" title="十、发布到自己服务器，Nginx代理"></a>十、发布到自己服务器，<a href="https://so.csdn.net/so/search?q=Nginx&amp;spm=1001.2101.3001.7020">Nginx</a>代理</h3><p>略</p>]]></content>
    
    
    
    <tags>
      
      <tag>个人博客搭建</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大白话 ChatGPT 及大模型</title>
    <link href="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    <url>/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="基础知识介绍"><a href="#基础知识介绍" class="headerlink" title="基础知识介绍"></a>基础知识介绍</h2><ol><li><strong>指令学习（</strong><a href="https://www.yuque.com/ningshixian/pz10h0/nwnib06bpxikneg9"><strong>Instruct Learning</strong></a><strong>）：</strong>Instruct是激发语言模型的理解能力，它通过给出更明显的指令，让模型去做出正确的行动。比如“判断这句话的情感：带女朋友去了一家餐厅，她吃的很开心。选项：A=好，B=一般，C=差”。Instruction Finetuning 经过多任务精调后，也能够在其他任务上做zero-shot！！</li><li><strong>提示学习（</strong><a href="https://www.yuque.com/ningshixian/pz10h0/nwnib06bpxikneg9"><strong>Prompt Learning</strong></a><strong>）：</strong>Prompt是激发语言模型的补全能力，例如根据上半句生成下半句，或是完形填空等。<strong>Prompting 都是针对一个任务的</strong>，比如做个情感分析任务的prompt tuning，精调完的模型只能用于情感分析任务。</li><li><a href="https://huggingface.co/blog/rlhf?continueFlag=cbeef78ac33939a46add3f56bc58a83b">Illustrating Reinforcement Learning from Human Feedback (RLHF)</a><strong>：</strong>RL 是通过奖励（Reward）机制来指导模型训练，而 RLHF 利用人工反馈训一个Reward Model，并将这个模型作为强化学习的奖励，代替人类去指导大模型进行微调。通过不断地反馈和调整，将模型和人类偏好进行对齐。<br>微调：采用的是 RL 经典算法 <a href="https://www.yuque.com/ningshixian/pz10h0/uegpsbdxnxm4zmsl">PPO（Proximal Policy Optimization）</a>：它是OpenAI提出的一个针对策略梯度算法进行改进的算法，可以在多个训练步骤实现小批量的更新，解决了Policy Gradient算法中步长难以确定的问题。</li></ol><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1673576390621-9ef1e60c-725d-442a-b068-c7b199e77df6.jpeg" class=""><p>图3：人工反馈的强化学习的基本原理</p><h2 id="ChatGPT的本质"><a href="#ChatGPT的本质" class="headerlink" title="ChatGPT的本质"></a>ChatGPT的本质</h2><p>ChatGPT（或者更准确地说，它所基于的GPT-3）实际上是在做什么呢？请记住，它的总体目标是基于其从训练中看到的东西（其中包括查看了来自网络等数十亿个页面的文本），“合理地”续写文本。因此，在任何给定的时刻，它都有一定量的文本，并且其目标是为下一个token pick一个适当的选择。</p><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1682065361784-afe190e2-cedd-410d-869e-4e8b3a3dee56.png" class="" title="image.png"><p>原理总结：</p><ul><li>ChatGPT的实质功能是单字接龙</li><li>长文由单字接龙的自回归所生成</li><li>通过提前训练才能让它生成人们想要的问答</li><li>训练方式是让它按照问答范例来做单字接龙</li><li>这样训练是为了让它学会「能举一反三的规律」</li><li>缺点是可能混淆记忆，无法直接查看和更新所学，且高度依赖学习材料。</li></ul><h2 id="ChatGPT的三个训练过程"><a href="#ChatGPT的三个训练过程" class="headerlink" title="ChatGPT的三个训练过程"></a>ChatGPT的三个训练过程</h2><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1682066197164-6b7294e7-6f0d-430c-b252-bdcee26c479c.png" class="" title="image.png"><p>ChatGPT 这个模型建立在 GPT3.5 版本之上，使用基于人工反馈的强化学习 RLHF 进行对齐。ChatGPT的训练过程分为如下几步：</p><ol><li><strong>用监督数据基于GPT3.5训练一个对话模型SFT，训练数据是标注人员手把手写出来的</strong></li><li><strong>人工标注对模型生成的多个结果排序，训练一个给对话回复打分的模型（RM）</strong></li><li><strong>由RM提供reward，利用强化学习的手段（PPO）来训练之前微调过的SFT。</strong></li></ol><p><strong>阶段2与阶段3其实是递交进行的。</strong></p><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1675751265148-3c7f0b42-cd5e-4f94-b0d0-6fc124fe1265.png" class="" title="image.png"><h3 id="数据集采集"><a href="#数据集采集" class="headerlink" title="数据集采集"></a>数据集采集</h3><p>如上图所示，InstructGPT/ChatGPT的训练分成3步，每一步需要的数据也有些许差异，下面我们分别介绍它们。</p><p><strong>① SFT数据集</strong></p><p>SFT数据集是用来训练第1步有监督的模型，即使用采集的新数据，按照GPT-3的训练方式对GPT-3进行微调。因为GPT-3是一个基于提示学习的生成模型，因此SFT数据集也是由提示-答复对组成的样本。SFT数据一部分来自使用OpenAI的PlayGround的用户，另一部分来自OpenAI雇佣的40名标注工（labeler）。并且他们对labeler进行了培训。在这个数据集中，标注工的工作是根据内容自己编写指示，并且要求编写的指示满足下面三点：</p><ul><li><strong>简单任务</strong>：labeler给出任意一个简单的任务，同时要确保任务的多样性；</li><li><strong>Few-shot任务</strong>：labeler给出一个指示，以及该指示的多个查询-相应对；</li><li><strong>用户相关的</strong>：从接口中获取用例，然后让labeler根据这些用例编写指示。</li></ul><p><strong>② RM数据集</strong></p><p>RM数据集用来训练第2步的奖励模型，我们也需要为InstructGPT/ChatGPT的训练设置一个奖励目标。这个奖励目标不必可导，但是一定要尽可能全面且真实的对齐我们需要模型生成的内容。很自然的，我们可以通过人工标注的方式来提供这个奖励，通过人工对可以给那些涉及偏见的生成内容更低的分从而鼓励模型不去生成这些人类不喜欢的内容。</p><p>具体而言，随机抽样一批用户提交的prompt(大部分和第一阶段的相同)，使用第一阶段Fine-tune好的冷启动模型，针对每个 prompt 生成K个不同的回答（$4≤K≤9$），于是模型产生出了<prompt,answer1>,<prompt,answer2>….<prompt,answerK>数据。之后，标注人员对K个结果按照很多标准综合考虑进行排序，给出K个结果的排名顺序，这就是此阶段人工标注的数据。</p><p><strong>③ PPO数据集</strong></p><p>InstructGPT的PPO数据没有进行标注，它均来自GPT-3的API的用户。既又不同用户提供的不同种类的生成任务，其中占比最高的包括生成任务（45.6%），QA（12.4%），头脑风暴（11.2%），对话（8.4%）等。</p><h3 id="ChatGPT训练-一阶段"><a href="#ChatGPT训练-一阶段" class="headerlink" title="ChatGPT训练: 一阶段"></a>ChatGPT训练: 一阶段</h3><p><strong>第一阶段：冷启动阶段的监督策略模型</strong> SFT</p><blockquote><p>The “pre-training on a general task + fine-tuning on a specific task” strategy is called <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>.<br>Most state-of-the-art large language models also go through an additional <a href="https://arxiv.org/abs/2203.02155">instruction fine-tuning</a> step after being pre-trained. In this step, the model is shown thousands of prompt + completion pairs that were <strong>human labeled</strong>. Why? While language modeling on Wikipedia pages makes the model good at continuing sentences, but it doesn’t make it particularly good at following instructions, or having a conversation, or summarizing a document (all the things we would like a GPT to do). Fine-tuning them on human labelled instruction + completion pairs is a way to teach the model how it can be more useful, and make them easier to interact with.</p></blockquote><p>尽管 GPT 3.5本身很强，但是它很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果。为了让GPT 3.5初步具备理解指令中蕴含的意图，首先会从测试用户提交的 prompt (就是指令或问题)中随机抽取一批，交由专业的标注人员，给出高质量答案，然后用这些人工标注好的<prompt,answer>数据来<strong>微调 GPT 3.5 模型</strong>（获得SFT模型, Supervised Fine-Tuning）。</p><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1679302969913-09de361f-4d92-4fc6-bc27-32ea226a0f6c.png" class="" title="image.png"><p>经过这个过程，我们可以认为GPT 3.5初步具备了理解人类prompt中所包含意图，并根据这个意图给出相对高质量回答的能力，但不一定符合人类偏好，并且通常会出现不一致问题。为解决该问题，ChatGPT 的做法是让人工标注者对 SFT 模型的不同输出进行排序，构建一个<strong>排序数据集</strong>，训练奖励模型 RM，并让训好的 RM 来对 SFT 进行打分，通过RLHF来调整 SFT。</p><h3 id="ChatGPT训练-二阶段"><a href="#ChatGPT训练-二阶段" class="headerlink" title="ChatGPT训练: 二阶段"></a>ChatGPT训练: 二阶段</h3><p><strong>第二阶段：训练回报模型 RM（Reward Model）</strong></p><p>这一步的目标是从数据中学习人类的偏好，目的是为 SFT 输出进行打分。它的工作过程为：</p><ul><li>给定 prompt，SFT 为每个 prompt 生成多个输出（4~9 个）</li><li>标注者对这些输出进行排序，得到一个新的<strong>RM排序数据集；</strong></li><li>基于此数据集，训练Reward Model。简单做法是，利用大模型做一个回归任务，即在模型的输出层[CLS]位置添加一个MLP，最后得到的数值即为Reward Model给出的分数。</li></ul><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1679287654270-36317103-d54c-4a4c-a04e-86d85fc39f17-20230424222049583.png" class="" title="未命名绘图.drawio.png"><p>**回归任务该如何训练呢？</p><p>如下图所示，在训练Reward Model的时候采用 pair-wise learning to rank，即针对同一个上文，利用一阶段训好的 SFT 输出$K=4$个回复结果，这4个回复之间两两组合可以形成$C_4^2=6$个数据对 pair，优化目标为：每对之中 与上文关系大的分数 高于与上文关系小的。</p><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1678697434094-6be50ddd-cf86-465a-92a9-8fe0b7e987b0.png" class="" title="image.png"><p><strong>Reward Model </strong>的损失函数为 pairwise loss：</p><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1676520360806-99aca1fb-3e78-4c47-9627-6fe75385f886.png" class=""><p>其中$r_θ(x,y)$是奖励值，$y_i$是排名相对靠前的结果，$y_j$是排名相对靠后的结果。这个损失函数的目标是最大化好的回复结果和不好的回复结果之间的差值。</p><p><strong>归纳下：在这个阶段里，首先由冷启动后的监督策略模型 SFT 为每个prompt产生K个结果，人工根据结果质量由高到低排序，以此作为训练数据，通过pair-wise learning to rank模式来训练回报模型。对于学好的RM模型来说，输入<prompt,answer>，输出结果的质量得分，得分越高说明产生的回答质量越高。</strong></p><h3 id="chatGPT训练-三阶段"><a href="#chatGPT训练-三阶段" class="headerlink" title="chatGPT训练: 三阶段"></a>chatGPT训练: 三阶段</h3><blockquote><p><a href="https://huggingface.co/blog">图解人工反馈强化学习(RLHF) - Hugging Face</a></p></blockquote><p><strong>第三阶段：采用 PPO 强化学习，来增强PLM的能力。</strong></p><p>本阶段无需人工标注数据，而是利用第二阶段训练好的奖励模型，靠奖励打分来更新预训练模型参数，将此微调任务表述为 RL 问题。</p><p>在这一步中，首先随机抽取一批新的 prompt，PPO 模型由SFT模型初始化，价值函数由 RM 模型初始化。期望对 prompt 做出响应。对于给定的 promot和响应，它会产生相应的回报（由 RM模型决定），SFT 模型会对每个 token 添加KL惩罚因子，以尽量避免 RM 模型的过度优化。</p><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1679481256881-cf7d4942-21cb-4304-a5b6-70878ac513dc.png" class="" title="image.png"><ol><li><p>First, the policy is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The action space of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the observation space is the distribution of possible input token sequences. The reward function is a combination of the preference model and a constraint on policy shift. </p><p>首先，Policy Model 就是语言模型（即一阶段<strong>冷启动后的监督策略模型 SFT</strong>），它接收一个提示并返回一个文本序列（或者只是文本的概率分布）。该策略的动作空间是所有对应于语言模型词汇表的 tokens（通常在50k个tokens左右），观察空间是可能的输入token序列的分布。奖励函数是偏好模型（上一阶段训练好的RM模型）和策略转移约束的组合。</p></li><li><p>The reward function is where the system combines all of the models we have discussed into one RLHF process. Given a prompt, $x$,  from the dataset, two texts, $y1, y2$, are generated – one from the initial language model and one from the current iteration of the fine-tuned policy. The text from the current policy is passed to the preference model, which returns a scalar notion of “preferability”, $r_\theta$ . This text is compared to the text from the initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback–Leibler (KL) divergence between these sequences of distributions over tokens, $r_\text{KL}$. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via sampling from both distributions (explained by John Schulman here). The final reward sent to the RL update rule is $r = r_\theta - \lambda r_\text{KL}$</p><p>奖励函数是系统将上述讨论过的所有模型合并为一个RLHF过程的地方。给定数据集中的提示$x$，生成两个文本$y1$和$y2$，一个来自固定参数不微调的初始语言模型 (initial SFT)，另一个来自参数可调的策略模型 (fine-tuned policy SFT)。①当前策略模型的输出文本会输入给偏好模型，得到一个标量的回报“preferability”$r_\theta$。②将这个文本与初始模型的文本进行比较，计算它们之间差异的惩罚。在OpenAI、Anthropic和DeepMind的多篇论文中，这个惩罚被设计为这些tokens分布序列之间的Kullback-Leibler（KL）散度的缩放版本$r_\text{KL}$。在每个 batch，KL散度项会惩罚RL策略，当其向远离初始预训练模型移动，这对于确保模型输出合理的连贯文本片段非常有用。如果没有这个惩罚，优化可能会开始生成无意义的文本，但欺骗奖励模型给出高奖励。在实践中，KL散度是通过从两个分布中进行采样来近似的，至此RL更新规则的最终奖励是$r=r_\theta-\lambda r_\text{KL}$</p></li><li><p>Some RLHF systems have added additional terms to the reward function. For example, OpenAI experimented successfully on InstructGPT by mixing in additional pre-training gradients (from the human annotation set) into the update rule for PPO. It is likely as RLHF is further investigated, the formulation of this reward function will continue to evolve.</p><p>一些RLHF系统在奖励函数中添加了一些额外的项。例如，OpenAI在InstructGPT上成功地进行了实验，将额外的预训练梯度混合到PPO的更新规则中。随着对RLHF系统的进一步研究，这个奖励函数的构建很可能会不断演变。</p></li></ol><p>补充：只用PPO模型进行训练的话，会导致模型在通用NLP任务上性能的大幅下降，OpenAI的解决方案是在训练目标中加入了通用的语言模型目标 $γE_x∼D_{pretrain} [log⁡(π_ϕ^{RL}(x))]$，这个变量在论文中被叫做PPO-ptx。</p><ol><li><p>Finally, the update rule is the parameter update from PPO that maximizes the reward metrics in the current batch of data (PPO is on-policy, which means the parameters are only updated with the current batch of prompt-generation pairs). PPO is a trust region optimization algorithm that uses constraints on the gradient to ensure the update step does not destabilize the learning process. DeepMind used a similar reward setup for Gopher but used synchronous advantage actor-critic (A2C) to optimize the gradients, which is notably different but has not been reproduced externally.</p><p>最后，更新规则是从当前数据批次中最大化奖励值的PPO参数更新（PPO是在线策略，这意味着参数只能通过当前的’提示-生成’对进行更新）。PPO是一种信任区域优化算法，它使用梯度约束来确保更新步骤不会破坏学习过程。DeepMind在Gopher中使用了类似的奖励设置，但使用同步 advantage actor-critic (A2C) 来优化梯度，这明显不同，但尚未在外部得到复制。</p></li></ol><img src="/2023/04/21/2023-04-21-%E5%A4%A7%E7%99%BD%E8%AF%9D%20ChatGPT%20%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B/1678722972193-bc7f626f-3aad-4dd3-a082-472bbe0b792b.png" class="" title="image.png"><p>Optionally, RLHF can continue from this point by iteratively updating the reward model and the policy together. As the RL policy updates, users can continue ranking these outputs versus the model’s earlier versions. </p><p>如果我们不断重复第二和第三阶段，通过<strong>迭代</strong>，会训练出更高质量的ChatGPT模型。</p><blockquote><p>PPO 算法的详细介绍请移步 <a href="https://www.yuque.com/ningshixian/pz10h0/uegpsbdxnxm4zmsl?view=doc_embed">PPO：Proximal Policy Optimization</a></p></blockquote><h2 id="ps-为什么是RLHF？"><a href="#ps-为什么是RLHF？" class="headerlink" title="ps:为什么是RLHF？"></a>ps:为什么是RLHF？</h2><p>这里我们举一个具体的场景，当我们想要训练一个能够对话的机器人时，</p><ul><li>强化学习（RL）的reward 需要人来衡量机器人每句对话的好坏（reward），但这显然非常折磨人。</li><li>模仿学习（IL）不需要人类对机器人的对话做评价，而是机器人反过来模仿人类的对话方式。具体来说，可以从网上或者其它渠道收集大量历史对话数据来训练一个奖赏模型（RM），但是带来了如何收集高质量数据训练RM的问题（如医疗对话等场景）</li><li>而 RLHF 并不提供直接的监督信号。但通过学习简单的排序，RM可以学到人类的偏好。那怎么去理解这里的“偏好”呢？打个比方，有一家冰箱工厂生产了好几种类型的冰箱，虽然这些客户中没有一个懂得如何造冰箱的（或者说他们不需要懂），但他们可以通过消费行为，让厂商明白消费者对冰箱类型的“偏好”，从而引导冰箱厂商生产销量更好的冰箱。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://mp.weixin.qq.com/s/V0BTumv8Ax5ToMn8V2DdAA">张俊林：ChatGPT会成为下一代搜索引擎吗</a></li><li><a href="https://mp.weixin.qq.com/s/cMoR-9tkBvp2eudgcj4LPg">全网唯一，不忽悠的ChatGPT</a></li><li>视频：《【渐构】万字科普GPT4为何会颠覆现有工作流；为何你要关注微软Copilot、文心一言等大模型》，适合零基础小白 <a href="https://www.bilibili.com/video/BV1MY4y1R7EN/">www.bilibili.com</a></li><li>视频：《手把手从头实现GPT by Andrej Karpathy》，适合有基本编程概念的初学者 <a href="https://www.bilibili.com/video/BV1E14y1M75n/">www.bilibili.com</a></li><li>课程：《李宏毅2023春机器学习课程》，适合有简单线代和编程基础、想要系统学习Machine Learning的用户 <a href="https://www.bilibili.com/video/BV1TD4y137mP/">www.bilibili.com</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>ChatGPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LangChain 实战</title>
    <link href="/2023/04/18/2023-04-18-LangChain%20%E5%AE%9E%E6%88%98/"/>
    <url>/2023/04/18/2023-04-18-LangChain%20%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<blockquote><p>参考<a href="https://python.langchain.com/en/latest/getting_started/getting_started.html">官方文档</a>、<a href="https://liaokong.gitbook.io/llm-kai-fa-jiao-cheng/">LangChain 的中文入门教程</a>、<a href="https://github.com/Tor101/LangChain-CheatSheet">LangChain CheatSheet</a>、<a href="https://github.com/gkamradt">gkamradt</a>/<a href="https://github.com/gkamradt/langchain-tutorials">langchain-tutorials</a></p></blockquote><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>众所周知 OpenAI 的 API 无法联网的，所以如果只使用自己的功能实现联网搜索并给出回答、总结 PDF 文档、基于某个 Youtube 视频进行问答等等的功能肯定是无法实现的。所以，我们来介绍一个非常强大的第三方开源库：LangChain 。</p><blockquote><p>文档地址：<a href="https://python.langchain.com/en/latest/">https://python.langchain.com/en/latest/</a></p></blockquote><p>LangChain 是由<a href="https://github.com/hwchase17">Harrison Chase</a>创建的一个 Python 库，可帮助您在几分钟内构建 GPT 驱动的应用程序。他主要拥有 2 个能力：</p><ol><li>可以将 LLM 模型与外部数据源进行连接</li><li>允许与 LLM 模型进行交互</li></ol><p>您可以使用 LangChain 构建的有趣应用程序包括（但不限于）：</p><ul><li>聊天机器人</li><li>特定领域的总结和问答</li><li>查询数据库以获取信息然后处理它们的应用程序</li><li>解决特定问题的代理，例如数学和推理难题</li></ul><p>在本指南中，我们将探讨什么是 LangChain 以及您可以使用它构建什么。我们还将尝试使用 LangChain 构建一个简单的问答应用程序。</p><h2 id="LangChain-模块概述"><a href="#LangChain-模块概述" class="headerlink" title="LangChain 模块概述"></a>LangChain 模块概述</h2><p>接下来我们看一下LangChain中的一些模块：</p><img src="/2023/04/18/2023-04-18-LangChain%20%E5%AE%9E%E6%88%98/1681801175911-824f69b5-f3fd-4857-877e-bbb2d996c22c.png" class="" title="image.png"><h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><p>LangChain 为许多不同的 LLM 提供了通用接口，可以直接通过 API 工作，但也可以运行本地模型。</p><ul><li>大型语言模型 LLM：将文本字符串作为输入，并返回文本字符串作为输出</li><li><strong>Chat Models：</strong>将聊天消息列表作为输入，并返回聊天消息。</li><li><strong>Text Embedding Models：</strong>将文本作为输入并返回一个浮点数组成的向量</li></ul><h4 id="LLMs"><a href="#LLMs" class="headerlink" title="LLMs"></a>LLMs</h4><p>LLMs 是 LangChain 的基础组成部分。它本质上是一个大型语言模型的包装器，可以通过该接口与各种大模型进行交互。LLMs 类的功能如下：</p><ul><li>支持多种模型接口，如 <a href="https://python.langchain.com/en/latest/modules/models/llms/integrations/openai.html">OpenAI</a>、<a href="https://python.langchain.com/en/latest/modules/models/llms/integrations/huggingface_hub.html">Hugging Face Hub</a>、<a href="https://python.langchain.com/en/latest/modules/models/llms/integrations/anthropic_example.html">Anthropic</a>、<a href="https://python.langchain.com/en/latest/modules/models/llms/integrations/azure_openai_example.html">Azure OpenAI</a>、<a href="https://python.langchain.com/en/latest/modules/models/llms/integrations/gpt4all.html">GPT4All</a>、<a href="https://python.langchain.com/en/latest/modules/models/llms/integrations/llamacpp.html">Llama-cpp</a>…</li><li>Fake LLM，用于测试</li><li>缓存的支持，比如 in-mem（内存）、SQLite、Redis、SQL</li><li><p>用量记录</p></li><li><p>支持流模式（就是一个字一个字的返回，类似打字效果）</p></li></ul><details class="lake-collapse"><summary id="u649a5278"><span class="ne-text">langchain.llms 使用示例</span></summary><pre data-language="python" id="DcWr3" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">from langchain.llms import OpenAIllm = OpenAI(model_name="text-davinci-003", n=2, best_of=2)llm("Tell me a joke")llm_result = llm.generate(["Tell me a joke", "Tell me a poem"])llm_result.llm_output    # 返回 tokens 使用量</pre></details><details class="lake-collapse"><summary id="u4cf70b23"><span class="ne-text" style="color: var(--pst-color-muted)">How to cache LLM calls</span></summary><pre data-language="python" id="vkq28" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">from langchain.llms import OpenAIimport langchainfrom langchain.cache import InMemoryCache,SQLiteCache,RedisCachelangchain.llm_cache = InMemoryCache()llm = OpenAI(model_name="text-davinci-002", n=2, best_of=2)llm("Tell me a joke")    # time: 1.1 sllm("Tell me a joke")    # time: 175 µsfrom redis import Redisfrom langchain.cache import RedisCachelangchain.llm_cache = RedisCache(redis_=Redis())llm("Tell me a joke")    # time: 825 msllm("Tell me a joke")    # time: 2.67 ms# 基于语义相似性缓存结果import gptcachefrom gptcache.processor.pre import get_promptfrom gptcache.manager.factory import get_data_managerfrom langchain.cache import GPTCachefrom gptcache.manager import get_data_manager, CacheBase, VectorBasefrom gptcache import Cachefrom gptcache.embedding import Onnxfrom gptcache.similarity_evaluation.distance import SearchDistanceEvaluation# Avoid multiple caches using the same file, causing different llm model caches to affect each otheri = 0file_prefix = "data_map"llm_cache = Cache()def init_gptcache_map(cache_obj: gptcache.Cache):    global i    cache_path = f'{file_prefix}_{i}.txt'    onnx = Onnx()    cache_base = CacheBase('sqlite')    vector_base = VectorBase('faiss', dimension=onnx.dimension)    data_manager = get_data_manager(cache_base, vector_base, max_size=10, clean_size=2)    cache_obj.init(        pre_embedding_func=get_prompt,        embedding_func=onnx.to_embeddings,        data_manager=data_manager,        similarity_evaluation=SearchDistanceEvaluation(),    )    i += 1langchain.llm_cache = GPTCache(init_gptcache_map)llm("Tell me a joke")    # time: 2.49 sllm("Tell me a joke")    # time: 136 ms# This is not an exact match, but semantically within distance so it hits!llm("Tell me joke")        # time: 135 ms</pre></details><details class="lake-collapse"><summary id="u56bebd99"><span class="ne-text" style="color: rgb(50, 50, 50)">在磁盘上写入和读取 LLM 配置</span></summary><pre data-language="python" id="STiq6" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">llm.save("llm.json")llm = load_llm("llm.json")# {#     "model_name": "text-davinci-003",#     "temperature": 0.7,#     "max_tokens": 256,#     "top_p": 1.0,#     "frequency_penalty": 0.0,#     "presence_penalty": 0.0,#     "n": 1,#     "best_of": 1,#     "request_timeout": null,#     "_type": "openai"# }</pre></details><details class="lake-collapse"><summary id="ubaba97df"><span class="ne-text">如何流式传输 LLM 和聊天模型响应</span></summary><pre data-language="python" id="COmI2" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">from langchain.llms import OpenAI, Anthropicfrom langchain.chat_models import ChatOpenAIfrom langchain.callbacks.base import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.schema import HumanMessagellm = OpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)resp = llm("请给我解释 Langchain 是什么")chat = ChatOpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)resp = chat([HumanMessage(content="请给我解释 Langchain 是什么")])# Anthropic的claude模型llm = Anthropic(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)llm("Write me a song about sparkling water.")</pre></details><h4 id="Chat-Models"><a href="#Chat-Models" class="headerlink" title="Chat Models"></a>Chat Models</h4><details class="lake-collapse"><summary id="ube9cbe78"><span class="ne-text">langchain.chat_models 使用示例</span></summary><pre data-language="python" id="GOjMY" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">from langchain.chat_models import ChatOpenAIfrom langchain import PromptTemplate, LLMChainfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import (    AIMessage,    HumanMessage,    SystemMessage)chat = ChatOpenAI(temperature=0)chat([HumanMessage(content="Translate this sentence from English to Chinese. I love programming.")])# 支持多条消息作为输入batch_messages = [    [        SystemMessage(content="You are a helpful assistant that translates English to French."),        HumanMessage(content="Translate this sentence from English to French. I love programming.")    ],    [        SystemMessage(content="You are a helpful assistant that translates English to French."),        HumanMessage(content="Translate this sentence from English to French. I love artificial intelligence.")    ],]result = chat.generate(batch_messages)print(result)print(result.llm_output)</pre></details><details class="lake-collapse"><summary id="uff19a5bd"><span class="ne-text">LLM Chain 使用示例</span></summary><pre data-language="python" id="GNeit" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">from langchain.chat_models import ChatOpenAIfrom langchain import PromptTemplate, LLMChainfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import (    AIMessage,    HumanMessage,    SystemMessage)llm = ChatOpenAI(temperature=0)template = """Please use the following context to answer questions.Context: {context}---Question: {question}Answer: Let's think step by step."""system_message_prompt = SystemMessagePromptTemplate.from_template(template)example_human = HumanMessagePromptTemplate.from_template("Hi")example_ai = AIMessagePromptTemplate.from_template("Argh me mateys")human_message_prompt = HumanMessagePromptTemplate.from_template("{text}")chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])chat_prompt.format_prompt(context=context, question="")chain = LLMChain(llm=llm, prompt=chat_prompt)chain.run()</pre></details><details class="lake-collapse"><summary id="u6f4c356f"><span class="ne-text">如何流式传输</span></summary><pre data-language="python" id="VaC3r" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">from langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessagefrom langchain.callbacks.base import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerchat = ChatOpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)resp = chat([HumanMessage(content="请给我解释 Langchain 是什么")])</pre></details><h4 id="HuggingFace-Models"><a href="#HuggingFace-Models" class="headerlink" title="HuggingFace Models"></a>HuggingFace Models</h4><p>使用 HuggingFace 模型之前，需要先设置环境变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&#x27;HUGGINGFACEHUB_API_TOKEN&#x27;</span>]=<span class="hljs-string">&#x27;&#x27;</span><br><br><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> PromptTemplate, HuggingFaceHub, LLMChain<br><br>template = <span class="hljs-string">&quot;&quot;&quot;Question: &#123;question&#125;</span><br><span class="hljs-string">Answer: Let&#x27;s think step by step.&quot;&quot;&quot;</span><br><br>prompt = PromptTemplate(template=template, input_variables=[<span class="hljs-string">&quot;question&quot;</span>])<br>llm = HuggingFaceHub(repo_id=<span class="hljs-string">&quot;google/flan-t5-xl&quot;</span>, model_kwargs=&#123;<span class="hljs-string">&quot;temperature&quot;</span>:<span class="hljs-number">0</span>, <span class="hljs-string">&quot;max_length&quot;</span>:<span class="hljs-number">64</span>&#125;)<br>llm_chain = LLMChain(prompt=prompt, llm=llm)<br><br>question = <span class="hljs-string">&quot;What NFL team won the Super Bowl in the year Justin Beiber was born?&quot;</span><br><span class="hljs-built_in">print</span>(llm_chain.run(question))<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> HuggingFacePipeline<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM<br><br>model_id = <span class="hljs-string">&#x27;google/flan-t5-large&#x27;</span><br>tokenizer = AutoTokenizer.from_pretrained(model_id)<br>model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=<span class="hljs-literal">True</span>)<br><br>pipe = pipeline(<br>    <span class="hljs-string">&quot;text2text-generation&quot;</span>,<br>    model=model, <br>    tokenizer=tokenizer, <br>    max_length=<span class="hljs-number">100</span><br>)<br><br>local_llm = HuggingFacePipeline(pipeline=pipe)<br><span class="hljs-built_in">print</span>(local_llm(<span class="hljs-string">&#x27;What is the capital of France? &#x27;</span>))<br><br><br>llm_chain = LLMChain(prompt=prompt,  llm=local_llm)<br>question = <span class="hljs-string">&quot;What is the capital of England?&quot;</span><br><span class="hljs-built_in">print</span>(llm_chain.run(question))<br></code></pre></td></tr></table></figure><p>将模型拉到本地使用的好处：</p><ul><li>训练模型</li><li>可以使用本地的 GPU</li><li>有些模型无法在 HuggingFace 运行</li></ul><h4 id="Text-Embedding-Models"><a href="#Text-Embedding-Models" class="headerlink" title="Text Embedding Models"></a>Text Embedding Models</h4><p>LangChain 中的基础 Embedding 类是设计用于与嵌入交互的类，它提供了两个方法：embed_documents和embed_query。最大的区别在于这两种方法具有不同的接口：一种处理多个文档，而另一种处理单个文档。</p><p>Embedding 类能够帮助我们实现基于知识库的问答和semantic search，相比 fine-tuning 最大的优势就是，不用进行训练，并且可以实时添加新的内容，而不用加一次新的内容就训练一次，并且各方面成本要比 fine-tuning 低很多。</p><p>文本嵌入模型集成了如下的源：<a href="https://python.langchain.com/en/latest/modules/models/text_embedding/examples/azureopenai.html">AzureOpenAI</a>、<a href="https://python.langchain.com/en/latest/modules/models/text_embedding/examples/huggingfacehub.html">Hugging Face Hub</a>、<a href="https://python.langchain.com/en/latest/modules/models/text_embedding/examples/instruct_embeddings.html">InstructEmbeddings</a>、<a href="https://python.langchain.com/en/latest/modules/models/text_embedding/examples/llamacpp.html">Llama-cpp</a>、<a href="https://python.langchain.com/en/latest/modules/models/text_embedding/examples/openai.html">OpenAI</a> 等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> HuggingFaceEmbeddings <br>embeddings = HuggingFaceEmbeddings() <br>text = <span class="hljs-string">&quot;This is a test document.&quot;</span> <br>query_result = embeddings.embed_query(text) <br>doc_result = embeddings.embed_documents([text])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">!pip install llama-cpp-python<br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> LlamaCppEmbeddings<br>llama = LlamaCppEmbeddings(model_path=<span class="hljs-string">&quot;/path/to/model/ggml-model-q4_0.bin&quot;</span>)<br>text = <span class="hljs-string">&quot;This is a test document.&quot;</span><br>query_result = llama.embed_query(text)<br>doc_result = llama.embed_documents([text])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings<br>embeddings = OpenAIEmbeddings()<br>text = <span class="hljs-string">&quot;This is a test document.&quot;</span><br>query_result = embeddings.embed_query(text)<br>doc_result = embeddings.embed_documents([text])<br></code></pre></td></tr></table></figure><h3 id="Prompts"><a href="#Prompts" class="headerlink" title="Prompts"></a>Prompts</h3><p>“提示”是指模型的输入。PromptTemplate 负责构建此输入，LangChain 提供了可用于格式化输入和许多其他实用程序的提示模板。 </p><ul><li>LLM Prompt Templates</li><li>Chat Prompt Templates</li><li>Example Selectors</li><li>Output Parsers</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> PromptTemplate<br><br>template = <span class="hljs-string">&quot;&quot;&quot;Question: &#123;question&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">Let&#x27;s think step by step.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Answer: &quot;&quot;&quot;</span><br><br>prompt = PromptTemplate(template=template, input_variables=[<span class="hljs-string">&quot;question&quot;</span>])<br>prompt.<span class="hljs-built_in">format</span>(question=<span class="hljs-string">&quot;Can Barack Obama have a conversation with George Washington?&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="Indexes"><a href="#Indexes" class="headerlink" title="Indexes"></a>Indexes</h3><p>索引指的是构建文档的方式，以便 LLM 可以最好地与它们交互。Indexes主要有 4 个主要组件：</p><ul><li><strong>文档加载器：</strong>如何从各种来源加载文档</li><li><strong>文本拆分器：</strong>拆分文本成多个chunk</li><li>VectorStores<strong>：</strong>对接向量存储与搜索，比如 <a href="https://docs.trychroma.com/">Chroma</a>、Pinecone、Qdrand。默认情况下，LangChain 使用Chroma作为向量存储来索引和搜索嵌入</li><li>检索器<strong>：</strong></li></ul><h4 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h4><p>此示例展示了对文档的问答。文档问答包括四个步骤：</p><ol><li>创建索引</li><li>从该索引创建一个检索器</li><li>创建问答链</li><li>问问题！<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pip install chromadb</span><br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-comment"># 指定要使用的文档加载器</span><br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> TextLoader<br>documents = TextLoader(<span class="hljs-string">&#x27;../state_of_the_union.txt&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf8&#x27;</span>)<br><span class="hljs-comment"># 接下来，我们将文档拆分成块。</span><br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br>text_splitter = CharacterTextSplitter(chunk_size=<span class="hljs-number">1000</span>, chunk_overlap=<span class="hljs-number">0</span>)<br>texts = text_splitter.split_documents(documents)<br><span class="hljs-comment"># 然后我们将选择我们想要使用的嵌入。</span><br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings<br>embeddings = OpenAIEmbeddings()<br><span class="hljs-comment"># 我们现在创建 vectorstore 用作索引。</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br>db = Chroma.from_documents(texts, embeddings)<br><span class="hljs-comment"># 这就是创建索引。然后，我们在检索器接口中公开该索引。</span><br>retriever = db.as_retriever()<br><span class="hljs-comment"># 创建一个链并用它来回答问题！</span><br>qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=<span class="hljs-string">&quot;stuff&quot;</span>, retriever=retriever)<br>query = <span class="hljs-string">&quot;What did the president say about Ketanji Brown Jackson&quot;</span><br>qa.run(query)<br></code></pre></td></tr></table></figure></li></ol><h4 id="Document-Loaders-and-Utils"><a href="#Document-Loaders-and-Utils" class="headerlink" title="Document Loaders and Utils"></a>Document Loaders and Utils</h4><p>LangChain 的<strong>Document Loaders</strong>和<strong>Utils</strong>模块分别用于连接到数据源和计算源。</p><p>当使用loader加载器读取到数据源后，数据源需要转换成 <strong>Document</strong> 对象后，后续才能进行使用。</p><p>假设你有大量的经济学文本语料库，你想在其上构建 NLP 应用程序。您的语料库可能是文本文件、PDF 文档、HTML 网页、图像等的混合体。那么，通过 Document Loaders 的Unstructured 可以将这些原始数据源转换为可处理的文本。</p><p>The following document loaders are provided:</p><ul><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/csv.html">CSV Loader</a> CSV文件</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/dataframe.html">DataFrame Loader</a> 从 pandas 数据帧加载数据</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/diffbot.html">Diffbot</a> 从 URL 列表中提取 HTML 文档，并将其转换为我们可以在下游使用的文档格式</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/directory_loader.html">Directory Loader</a> 加载目录中的所有文档</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/evernote.html">EverNote</a> 印象笔记</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/git.html">Git</a> 从 Git 存储库加载文本文件</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/googledrive.html">Google Drive</a> Google网盘</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/html.html">HTML</a> HTML 文档</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/markdown.html">Markdown</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/notebook.html">Notebook</a> 将 .ipynb 笔记本中的数据加载为适合 LangChain 的格式</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/notion.html">Notion</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html">PDF</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/powerpoint.html">PowerPoint</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html">Unstructured File Loader</a> 使用Unstructured加载多种类型的文件，目前支持加载文本文件、powerpoints、html、pdf、图像等</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/url.html">URL</a> 加载 URL 列表中的 HTML 文档内容</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/word_document.html">Word Documents</a></li></ul><p>更多的加载器可以看看<a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders.html">链接</a>。</p><h4 id="Text-Spltters"><a href="#Text-Spltters" class="headerlink" title="Text Spltters"></a>Text Spltters</h4><p>顾名思义，文本分割就是用来分割文本的。为什么需要分割文本？因为我们每次不管是做把文本当作 prompt 发给 openai api ，还是还是使用 embedding 功能都是有字符限制的。</p><p>比如我们将一份300页的 pdf 发给 openai api，让他进行总结，他肯定会报超过最大 Token 错。所以这里就需要使用文本分割器去分割我们 loader 进来的 Document。</p><ul><li>默认推荐的文本拆分器是 RecursiveCharacterTextSplitter。默认情况以 [“\n\n”, “\n”, “ “, “”] 字符进行拆分。其它参数说明：<code>length_function</code>如何计算块的长度。默认只计算字符数，但在这里传递令牌计数器是很常见的。<code>chunk_size</code>：块的最大大小（由长度函数测量）。<code>chunk_overlap</code>：块之间的最大重叠。有一些重叠可以很好地保持块之间的一些连续性（例如，做一个滑动窗口）</li><li>CharacterTextSplitter 默认情况下以 <code>separator=&quot;\n\n&quot;</code>进行拆分</li><li><a href="https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/tiktoken_splitter.html">TiktokenText Splitter</a> 使用OpenAI 的开源分词器包来估计使用的令牌</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># This is a long document we can split up.</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;../../../state_of_the_union.txt&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    state_of_the_union = f.read()<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br>text_splitter = CharacterTextSplitter(        <br>    separator = <span class="hljs-string">&quot;\n\n&quot;</span>,<br>    chunk_size = <span class="hljs-number">1000</span>,<br>    chunk_overlap  = <span class="hljs-number">200</span>,<br>    length_function = <span class="hljs-built_in">len</span>,<br>)<br>metadatas = [&#123;<span class="hljs-string">&quot;document&quot;</span>: <span class="hljs-number">1</span>&#125;, &#123;<span class="hljs-string">&quot;document&quot;</span>: <span class="hljs-number">2</span>&#125;]<br>documents = text_splitter.create_documents([state_of_the_union, state_of_the_union], metadatas=metadatas)<br><span class="hljs-built_in">print</span>(texts[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># This is a long document we can split up.</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;../../../state_of_the_union.txt&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    state_of_the_union = f.read()<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> TokenTextSplitter<br>text_splitter = TokenTextSplitter(chunk_size=<span class="hljs-number">10</span>, chunk_overlap=<span class="hljs-number">0</span>)<br>texts = text_splitter.split_text(state_of_the_union)<br><span class="hljs-built_in">print</span>(texts[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><h4 id="Vectorstores"><a href="#Vectorstores" class="headerlink" title="Vectorstores"></a>Vectorstores</h4><p>因为数据相关性搜索其实是向量运算。所以，不管我们是使用 openai api embedding 功能还是直接通过 向量数据库 直接查询，都需要将我们加载进来的数据 <code>Document</code> 进行向量化，才能进行向量运算搜索。转换成向量也很简单，只需要我们把数据存储到对应的向量数据库中即可完成向量的转换。</p><p>官方也提供了很多的向量数据库供我们使用，包括：</p><ul><li><a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/annoy.html">Annoy</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html">Chroma</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/elasticsearch.html">ElasticSearch</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html">FAISS</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/milvus.html">Milvus</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pgvector.html">PGVector</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html">Pinecone</a></li><li><a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/redis.html">Redis</a></li></ul><p>更多支持的向量数据库使用方法，可转至<a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores.html">链接</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pip install chromadb</span><br><span class="hljs-comment"># 选择我们想要使用的嵌入。</span><br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings<br>embeddings = OpenAIEmbeddings()<br><span class="hljs-comment"># 加载索引</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br>vectordb = Chroma(persist_directory=<span class="hljs-string">&quot;./vector_store&quot;</span>, embedding_function=embeddings)<br><span class="hljs-comment"># 向量相似度计算</span><br>query = <span class="hljs-string">&quot;未入职同事可以出差吗&quot;</span><br>docs = vectordb.similarity_search(query)<br>docs2 = vectordb.similarity_search_with_score(query)<br><span class="hljs-built_in">print</span>(docs[<span class="hljs-number">0</span>].page_content)<br><span class="hljs-comment"># 在检索器接口中公开该索引</span><br>retriever = vectordb.as_retriever(search_type=<span class="hljs-string">&quot;mmr&quot;</span>)<br>docs = retriever.get_relevant_documents(query)[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(docs.page_content)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">vs_path = <span class="hljs-string">&quot;./vector_store&quot;</span><br><span class="hljs-keyword">if</span> vs_path <span class="hljs-keyword">and</span> os.path.isdir(vs_path):<br>    vector_store = FAISS.load_local(vs_path, embeddings)<br>    vector_store.add_documents(docs)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> vs_path:<br>        vs_path = <span class="hljs-string">f&quot;&quot;&quot;<span class="hljs-subst">&#123;VS_ROOT_PATH&#125;</span><span class="hljs-subst">&#123;os.path.splitext(file)[<span class="hljs-number">0</span>]&#125;</span>_FAISS_<span class="hljs-subst">&#123;datetime.datetime.now().strftime(<span class="hljs-string">&quot;%Y%m%d_%H%M%S&quot;</span>)&#125;</span>&quot;&quot;&quot;</span><br>    vector_store = FAISS.from_documents(docs, embeddings)<br><br>vector_store.save_local(vs_path)<br>docs = vector_store.similarity_search(query)<br>docs_and_scores = vector_store.similarity_search_with_score(query)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">vector_db = Milvus.from_documents(<br>    docs,<br>    embeddings,<br>    connection_args=&#123;<span class="hljs-string">&quot;host&quot;</span>: <span class="hljs-string">&quot;127.0.0.1&quot;</span>, <span class="hljs-string">&quot;port&quot;</span>: <span class="hljs-string">&quot;19530&quot;</span>&#125;,<br>)<br>docs = vector_db.similarity_search(query)<br>docs[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.vectorstores.pgvector <span class="hljs-keyword">import</span> PGVector<br><span class="hljs-keyword">import</span> os<br>CONNECTION_STRING = PGVector.connection_string_from_db_params(<br>    driver=os.environ.get(<span class="hljs-string">&quot;PGVECTOR_DRIVER&quot;</span>, <span class="hljs-string">&quot;psycopg2&quot;</span>),<br>    host=os.environ.get(<span class="hljs-string">&quot;PGVECTOR_HOST&quot;</span>, <span class="hljs-string">&quot;localhost&quot;</span>),<br>    port=<span class="hljs-built_in">int</span>(os.environ.get(<span class="hljs-string">&quot;PGVECTOR_PORT&quot;</span>, <span class="hljs-string">&quot;5432&quot;</span>)),<br>    database=os.environ.get(<span class="hljs-string">&quot;PGVECTOR_DATABASE&quot;</span>, <span class="hljs-string">&quot;postgres&quot;</span>),<br>    user=os.environ.get(<span class="hljs-string">&quot;PGVECTOR_USER&quot;</span>, <span class="hljs-string">&quot;postgres&quot;</span>),<br>    password=os.environ.get(<span class="hljs-string">&quot;PGVECTOR_PASSWORD&quot;</span>, <span class="hljs-string">&quot;postgres&quot;</span>),<br>)<br>db = PGVector.from_documents(<br>    embedding=embeddings,<br>    documents=docs,<br>    collection_name=<span class="hljs-string">&quot;state_of_the_union&quot;</span>,<br>    connection_string=CONNECTION_STRING,<br>)<br><br>query = <span class="hljs-string">&quot;What did the president say about Ketanji Brown Jackson&quot;</span><br>docs_with_score: <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[Document, <span class="hljs-built_in">float</span>]] = db.similarity_search_with_score(query)<br><span class="hljs-keyword">for</span> doc, score <span class="hljs-keyword">in</span> docs_with_score:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Score: &quot;</span>, score)<br>    <span class="hljs-built_in">print</span>(doc.page_content)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pinecone <br><br><span class="hljs-comment"># Connecting to Pinecone</span><br>pinecone.deinit()<br>pinecone.init(<br>    api_key=<span class="hljs-string">&quot;YOUR_API_KEY&quot;</span>,  <span class="hljs-comment"># find at app.pinecone.io</span><br>    environment=<span class="hljs-string">&quot;YOUR_ENV&quot;</span>  <span class="hljs-comment"># next to api key in console</span><br>)<br><br><span class="hljs-comment"># similarity_search</span><br>docsearch = Pinecone.from_documents(docs, embeddings, index_name=<span class="hljs-string">&quot;langchain-demo&quot;</span>)<br>docs = docsearch.similarity_search(query)<br><span class="hljs-built_in">print</span>(docs[<span class="hljs-number">0</span>].page_content)<br><br><span class="hljs-comment"># Create a Pinecone Service</span><br>pinecone_service = pinecone.Service()<br><br><span class="hljs-comment"># Create an Embedding Model</span><br><span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings<br>embeddings = OpenAIEmbeddings()<br><br><span class="hljs-comment"># Create a Vectorstore</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br>vectorstore = Chroma(embeddings, pinecone_service)<br><br><span class="hljs-comment"># Upload documents to Pinecone Vectorstore</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br>docsearch = Chroma.from_documents(texts, embeddings, collection_name=<span class="hljs-string">&quot;collection_name&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="Retrievers"><a href="#Retrievers" class="headerlink" title="Retrievers"></a>Retrievers</h4><p>检索器接口是一个通用接口，可以轻松地将文档与语言模型结合起来。此接口公开了一个 get_relevant_documents 方法，该方法接受一个查询（一个字符串）并返回一个文档列表。</p><p>一般来说，用的都是 <a href="https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/vectorstore-retriever.html">VectorStore Retriever</a>。顾名思义，此检索器由 VectorStore 大力支持。一旦你构造了一个 VectorStore，构造一个检索器就很容易了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># # pip install faiss-cpu</span><br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> DirectoryLoader<br><span class="hljs-comment"># 加载文件夹中的所有txt类型的文件，并转成 document 对象</span><br>loader = DirectoryLoader(<span class="hljs-string">&#x27;./data/&#x27;</span>, glob=<span class="hljs-string">&#x27;**/*.txt&#x27;</span>)<br>documents = loader.load()<br><span class="hljs-comment"># 接下来，我们将文档拆分成块。</span><br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br>text_splitter = CharacterTextSplitter(chunk_size=<span class="hljs-number">1000</span>, chunk_overlap=<span class="hljs-number">0</span>)<br>texts = text_splitter.split_documents(documents)<br><span class="hljs-comment"># 然后我们将选择我们想要使用的嵌入。</span><br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings<br>embeddings = OpenAIEmbeddings()<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> FAISS<br>db = FAISS.from_documents(texts, embeddings)<br>query = <span class="hljs-string">&quot;未入职同事可以出差吗&quot;</span><br>docs = db.similarity_search(query)<br>docs_and_scores = db.similarity_search_with_score(query)<br><span class="hljs-built_in">print</span>(docs)<br><br>retriever = db.as_retriever()<span class="hljs-comment"># 最大边际相关性搜索 mmr</span><br><span class="hljs-comment"># retriever = db.as_retriever(search_kwargs=&#123;&quot;k&quot;: 1&#125;)# 搜索关键字</span><br>docs = retriever.get_relevant_documents(<span class="hljs-string">&quot;未入职同事可以出差吗&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(docs))<br><br><span class="hljs-comment"># db.save_local(&quot;faiss_index&quot;)</span><br><span class="hljs-comment"># new_db = FAISS.load_local(&quot;faiss_index&quot;, embeddings)</span><br><span class="hljs-comment"># docs = new_db.similarity_search(query)</span><br><span class="hljs-comment"># docs[0]</span><br></code></pre></td></tr></table></figure><h3 id="Chains"><a href="#Chains" class="headerlink" title="Chains"></a>Chains</h3><p>Chain 可以根据特定任务将 LLM 调用链接在一起。例如，您可能需要从特定 URL 获取数据，总结返回的文本，并使用生成的摘要回答问题。链允许我们将多个组件组合在一起以创建一个单一的、连贯的应用程序。例如，我们可以创建一个接受用户输入的链，使用 PromptTemplate 对其进行格式化，然后将格式化后的响应传递给 LLM。我们可以通过将多个链组合在一起，或者通过将链与其他组件组合来构建更复杂的链。</p><ul><li>LLMChain</li><li>各种工具Chain</li><li>LangChainHub</li></ul><h4 id="快速入门-1"><a href="#快速入门-1" class="headerlink" title="快速入门"></a>快速入门</h4><p><strong>LLMChain是一个简单的链，它接受一个提示模板，用用户输入格式化它并返回来自 LLM 的响应。</strong>在多步骤工作流程中结合 LLM 和提示。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> LLMChain<br><br>llm_chain = LLMChain(prompt=prompt, llm=llm)<br><br>question = <span class="hljs-string">&quot;Can Barack Obama have a conversation with George Washington?&quot;</span><br><br><span class="hljs-built_in">print</span>(llm_chain.run(question))<br></code></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.docstore.document <span class="hljs-keyword">import</span> Document<br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><span class="hljs-keyword">import</span> pathlib<br><span class="hljs-keyword">import</span> subprocess<br><span class="hljs-keyword">import</span> tempfile<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">生成对以前撰写的博客文章有理解的博客文章，或者可以参考产品文档的产品教程</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>source_chunks = <span class="hljs-string">&quot;&quot;</span><br>search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())<br><br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> LLMChain<br>prompt_template = <span class="hljs-string">&quot;&quot;&quot;Use the context below to write a 400 word blog post about the topic below:</span><br><span class="hljs-string">    Context: &#123;context&#125;</span><br><span class="hljs-string">    Topic: &#123;topic&#125;</span><br><span class="hljs-string">    Blog post:&quot;&quot;&quot;</span><br><br>PROMPT = PromptTemplate(<br>    template=prompt_template, input_variables=[<span class="hljs-string">&quot;context&quot;</span>, <span class="hljs-string">&quot;topic&quot;</span>]<br>)<br><br>llm = OpenAI(temperature=<span class="hljs-number">0</span>)<br><br>chain = LLMChain(llm=llm, prompt=PROMPT)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_blog_post</span>(<span class="hljs-params">topic</span>):<br>    docs = search_index.similarity_search(topic, k=<span class="hljs-number">4</span>)<br>    inputs = [&#123;<span class="hljs-string">&quot;context&quot;</span>: doc.page_content, <span class="hljs-string">&quot;topic&quot;</span>: topic&#125; <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> docs]<br>    <span class="hljs-built_in">print</span>(chain.apply(inputs))<br><br>generate_blog_post(<span class="hljs-string">&quot;environment variables&quot;</span>)<br></code></pre></td></tr></table></figure></p><h4 id="执行多个chain"><a href="#执行多个chain" class="headerlink" title="执行多个chain"></a>执行多个chain</h4><p>顺序链是按预定义顺序执行其链接的链。具体来说，我们使用SimpleSequentialChain，其中每个步骤都有一个输入/输出，一个步骤的输出是下一个步骤的输入。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> LLMChain<br><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> SimpleSequentialChain<br><br><span class="hljs-comment"># location 链</span><br>llm = OpenAI(temperature=<span class="hljs-number">1</span>)<br>template = <span class="hljs-string">&quot;&quot;&quot;Your job is to come up with a classic dish from the area that the users suggests.</span><br><span class="hljs-string">% USER LOCATION</span><br><span class="hljs-string">&#123;user_location&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">YOUR RESPONSE:</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>prompt_template = PromptTemplate(input_variables=[<span class="hljs-string">&quot;user_location&quot;</span>], template=template)<br>location_chain = LLMChain(llm=llm, prompt=prompt_template)<br><br><span class="hljs-comment"># meal 链</span><br>template = <span class="hljs-string">&quot;&quot;&quot;Given a meal, give a short and simple recipe on how to make that dish at home.</span><br><span class="hljs-string">% MEAL</span><br><span class="hljs-string">&#123;user_meal&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">YOUR RESPONSE:</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>prompt_template = PromptTemplate(input_variables=[<span class="hljs-string">&quot;user_meal&quot;</span>], template=template)<br>meal_chain = LLMChain(llm=llm, prompt=prompt_template)<br><br><span class="hljs-comment"># 通过 SimpleSequentialChain 串联起来，第一个答案会被替换第二个中的user_meal，然后再进行询问</span><br>overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=<span class="hljs-literal">True</span>)<br>review = overall_chain.run(<span class="hljs-string">&quot;Rome&quot;</span>)<br></code></pre></td></tr></table></figure></p><h4 id="load-qa-chain"><a href="#load-qa-chain" class="headerlink" title="load_qa_chain"></a>load_qa_chain</h4><p>Load question answering chain.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_qa_chain</span>(<span class="hljs-params"></span><br><span class="hljs-params">    llm: BaseLanguageModel,</span><br><span class="hljs-params">    chain_type: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;stuff&quot;</span>,</span><br><span class="hljs-params">    verbose: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    callback_manager: <span class="hljs-type">Optional</span>[BaseCallbackManager] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    **kwargs: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; BaseCombineDocumentsChain:<br></code></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.docstore.document <span class="hljs-keyword">import</span> Document<br><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><span class="hljs-keyword">from</span> langchain.indexes.vectorstore <span class="hljs-keyword">import</span> VectorstoreIndexCreator<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;../../state_of_the_union.txt&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    state_of_the_union = f.read()<br>text_splitter = CharacterTextSplitter(chunk_size=<span class="hljs-number">1000</span>, chunk_overlap=<span class="hljs-number">0</span>)<br>texts = text_splitter.split_text(state_of_the_union)<br><br>embeddings = OpenAIEmbeddings()<br>docsearch = Chroma.from_texts(texts, embeddings, metadatas=[&#123;<span class="hljs-string">&quot;source&quot;</span>: <span class="hljs-built_in">str</span>(i)&#125; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(texts))]).as_retriever()<br><span class="hljs-comment"># retriever=docsearch.as_retriever()</span><br><br>query = <span class="hljs-string">&quot;What did the president say about Justice Breyer&quot;</span><br>docs = docsearch.get_relevant_documents(query)<br><br><span class="hljs-keyword">from</span> langchain.chains.question_answering <span class="hljs-keyword">import</span> load_qa_chain<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><br>chain = load_qa_chain(OpenAI(temperature=<span class="hljs-number">0</span>), chain_type=<span class="hljs-string">&quot;stuff&quot;</span>)<br>query = <span class="hljs-string">&quot;What did the president say about Justice Breyer&quot;</span><br>chain.run(input_documents=docs, question=query)<br></code></pre></td></tr></table></figure></p><h4 id="RetrievalQA"><a href="#RetrievalQA" class="headerlink" title="RetrievalQA"></a>RetrievalQA</h4><p>Chain for question-answering against an index.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">此示例展示了对文档的问答。文档问答包括四个步骤：</span><br><span class="hljs-string">1. 创建索引</span><br><span class="hljs-string">2. 从该索引创建一个检索器</span><br><span class="hljs-string">3. 创建问答链</span><br><span class="hljs-string">4. 问问题！</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># pip install chromadb</span><br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQAWithSourcesChain<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> DirectoryLoader<br><span class="hljs-comment"># 加载文件夹中的所有txt类型的文件，并转成 document 对象</span><br>loader = DirectoryLoader(<span class="hljs-string">&#x27;./data/&#x27;</span>, glob=<span class="hljs-string">&#x27;**/*.txt&#x27;</span>)<br>documents = loader.load()<br><span class="hljs-comment"># 接下来，我们将文档拆分成块。</span><br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br>text_splitter = CharacterTextSplitter(chunk_size=<span class="hljs-number">1000</span>, chunk_overlap=<span class="hljs-number">0</span>)<br>texts = text_splitter.split_documents(documents)<br><span class="hljs-comment"># 然后我们将选择我们想要使用的嵌入。</span><br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings<br>embeddings = OpenAIEmbeddings()<br><span class="hljs-comment"># 我们现在创建 vectorstore 用作索引，并进行持久化</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-comment"># vector_store = Chroma.from_documents(texts, embeddings, persist_directory=&quot;./vector_store&quot;)</span><br><span class="hljs-comment"># vector_store.persist()</span><br>vector_store = Chroma(persist_directory=<span class="hljs-string">&quot;./vector_store&quot;</span>, embedding_function=embeddings)<br><span class="hljs-comment"># 这就是创建索引。然后，我们在检索器接口中公开该索引。</span><br>retriever = vector_store.as_retriever()<br><span class="hljs-comment"># 创建一个链并用它来回答问题！</span><br>qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=<span class="hljs-string">&quot;stuff&quot;</span>, retriever=retriever)<br><span class="hljs-built_in">print</span>(qa.input_keys, qa.output_keys)<br>query = <span class="hljs-string">&quot;出差申请单修改&quot;</span><br><span class="hljs-built_in">print</span>(qa.run(query=query))<br><br>chain = RetrievalQAWithSourcesChain.from_chain_type(llm=OpenAI(temperature=<span class="hljs-number">0</span>), chain_type=<span class="hljs-string">&quot;stuff&quot;</span>, retriever=retriever)<br><span class="hljs-built_in">print</span>(chain.input_keys, chain.output_keys)<br><span class="hljs-built_in">print</span>(chain(&#123;<span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;出差申请单修改&quot;</span>&#125;, return_only_outputs=<span class="hljs-literal">True</span>))<br></code></pre></td></tr></table></figure></p><h4 id="VectorDBQA"><a href="#VectorDBQA" class="headerlink" title="VectorDBQA"></a>VectorDBQA</h4><p>Chain for question-answering against a vector database.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">vector_store = Chroma.from_documents(split_docs, embeddings)<br>qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=<span class="hljs-string">&quot;stuff&quot;</span>, vectorstore=vector_store,return_source_documents=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 进行问答</span><br>res = qa(&#123;<span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;未入职同事可以出差吗&quot;</span>&#125;)<br><span class="hljs-built_in">print</span>(res)<br></code></pre></td></tr></table></figure></p><h4 id="LLMRequestsChain"><a href="#LLMRequestsChain" class="headerlink" title="LLMRequestsChain"></a>LLMRequestsChain</h4><p>使用请求库从 URL 获取 HTML 结果，然后使用 LLM 解析结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> LLMRequestsChain, LLMChain<br><br><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><br>template = <span class="hljs-string">&quot;&quot;&quot;Between &gt;&gt;&gt; and &lt;&lt;&lt; are the raw search result text from google.</span><br><span class="hljs-string">Extract the answer to the question &#x27;&#123;query&#125;&#x27; or say &quot;not found&quot; if the information is not contained.</span><br><span class="hljs-string">Use the format</span><br><span class="hljs-string">Extracted:&lt;answer or &quot;not found&quot;&gt;</span><br><span class="hljs-string"><span class="hljs-meta">&gt;&gt;&gt; </span>&#123;requests_result&#125; &lt;&lt;&lt;</span><br><span class="hljs-string">Extracted:&quot;&quot;&quot;</span><br><br>PROMPT = PromptTemplate(<br>    input_variables=[<span class="hljs-string">&quot;query&quot;</span>, <span class="hljs-string">&quot;requests_result&quot;</span>],<br>    template=template,<br>)<br>chain = LLMRequestsChain(llm_chain = LLMChain(llm=OpenAI(temperature=<span class="hljs-number">0</span>), prompt=PROMPT))<br>question = <span class="hljs-string">&quot;What are the Three (3) biggest countries, and their respective sizes?&quot;</span><br>inputs = &#123;<br>    <span class="hljs-string">&quot;query&quot;</span>: question,<br>    <span class="hljs-string">&quot;url&quot;</span>: <span class="hljs-string">&quot;https://www.google.com/search?q=&quot;</span> + question.replace(<span class="hljs-string">&quot; &quot;</span>, <span class="hljs-string">&quot;+&quot;</span>)<br>&#125;<br>chain(inputs)<br></code></pre></td></tr></table></figure></p><h4 id="SQLDatabaseChain"><a href="#SQLDatabaseChain" class="headerlink" title="SQLDatabaseChain"></a>SQLDatabaseChain</h4><p>如何使用SQLDatabaseChain来回答数据库中的问题。</p><p>在底层，LangChain 使用 <code>SQLAlchemy</code> 连接到 SQL 数据库。因此，<code>SQLDatabaseChain</code>可以与 SQLAlchemy 支持的任何 SQL 方言一起使用，例如 MS SQL、<strong>MySQL</strong>、MariaDB、PostgreSQL、Oracle SQL 和 SQLite。有关连接到数据库的要求的更多信息，请参阅 SQLAlchemy 文档。例如，与 MySQL 的连接需要适当的连接器，例如 PyMySQL。MySQL 连接的 URI 可能类似于：</p><p><code>mysql+pymysql://user:pass@some_mysql_db_address/db_name</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> SQLDatabase <br>db = SQLDatabase.from_uri(<br>    <span class="hljs-string">&quot;mysql+pymysql://user:pass@some_mysql_db_address:3306/db_name&quot;</span>,<br>    include_tables=[<span class="hljs-string">&#x27;table1&#x27;</span>, <span class="hljs-string">&#x27;table2&#x27;</span>],<br>    sample_rows_in_table_info=<span class="hljs-number">2</span><br>) <br><span class="hljs-built_in">print</span>(db.table_info)<br></code></pre></td></tr></table></figure><p><strong>注意：</strong>对于数据敏感的项目，可以return_direct=True在SQLDatabaseChain初始化时指定直接返回SQL查询的输出，不做任何额外的格式化。这可以防止 LLM 查看数据库中的任何内容。但是请注意，默认情况下，LLM 仍然可以访问数据库方案（即方言、表和键名）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> OpenAI, SQLDatabaseChain <br>llm = OpenAI(temperature=<span class="hljs-number">0</span>) <br><br>db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=<span class="hljs-literal">True</span>)<br>db_chain.run(<span class="hljs-string">&quot;获取 flow=LXHITDH 且跟发票相关的知识&quot;</span>) <br><br><br><span class="hljs-comment"># 自定义提示</span><br><span class="hljs-keyword">from</span> langchain.prompts.prompt <span class="hljs-keyword">import</span> PromptTemplate<br>_DEFAULT_TEMP = <span class="hljs-string">&quot;&quot;&quot;Given an input question, first create a syntactically correct &#123;dialect&#125; query to run, then look at the results of the query and return the answer.</span><br><span class="hljs-string">Use the following format:</span><br><span class="hljs-string"></span><br><span class="hljs-string">Question: &quot;Question here&quot;</span><br><span class="hljs-string">SQLQuery: &quot;SQL Query to run&quot;</span><br><span class="hljs-string">SQLResult: &quot;Result of the SQLQuery&quot;</span><br><span class="hljs-string">Answer: &quot;Final answer here&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">Only use the following tables:</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#123;table_info&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">If someone asks for the table foobar, they really mean the employee table.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Question: &#123;input&#125;&quot;&quot;&quot;</span><br>PROMPT = PromptTemplate(<br>    input_variables=[<span class="hljs-string">&quot;input&quot;</span>, <span class="hljs-string">&quot;table_info&quot;</span>, <span class="hljs-string">&quot;dialect&quot;</span>], template=_DEFAULT_TEMP)<br><br><span class="hljs-comment"># 返回中间步骤</span><br><span class="hljs-comment"># top_k 返回最大结果数，相当于 limit</span><br>db_chain = SQLDatabaseChain(llm=llm, database=db, prompt=PROMPT, verbose=<span class="hljs-literal">True</span>, return_intermediate_steps=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">3</span>)<br>result = db_chain(<span class="hljs-string">&quot;获取跟发票相关的知识&quot;</span>)<br>result[<span class="hljs-string">&quot;intermediate_steps&quot;</span>]<br></code></pre></td></tr></table></figure></p><h3 id="Agents"><a href="#Agents" class="headerlink" title="Agents"></a>Agents</h3><p>我们提到“链”可以帮助将一系列 LLM 调用链接在一起。然而，在某些任务中，调用顺序通常是不确定的。下一步可能取决于用户输入和前面步骤中的响应。 </p><p>对于此类应用程序，LangChain 库提供了代理“Agents”，它们可以根据未知的输入而不是硬编码来决定下一步采取的行动。 执行过程可以参考下面这张图:</p><img src="/2023/04/18/2023-04-18-LangChain%20%E5%AE%9E%E6%88%98/1681457282828-768cbe71-f5ba-45a5-b6cc-d0ac93a0e1c0.png" class="" title="image.png"><p>agent 使用LLM来确定要采取哪些行动以及按什么顺序采取的行动。操作可以使用工具并观察其输出，也可以返回用户。创建agent时的参数：</p><ul><li>工具：执行特定职责的功能。比如：Google搜索，数据库查找，Python Repl。工具的接口当前是一个函数，将字符串作为输入，字符串作为输出。</li><li>LLM：为代理提供动力的语言模型。</li><li>代理：highest level API、custom agent</li></ul><h4 id="快速入门-2"><a href="#快速入门-2" class="headerlink" title="快速入门"></a>快速入门</h4><p>代理使用 LLM 来确定采取哪些行动以及采取何种顺序。动作可以是使用工具并观察其输出，也可以是返回给用户。为了加载代理，您应该了解以下概念：</p><ul><li>工具：执行特定任务的功能。这可以是：Google 搜索、数据库查找、Python REPL、其他链。工具的接口目前是一个函数，期望将字符串作为输入，将字符串作为输出。</li><li>LLM：为代理提供支持的语言模型。</li><li>代理：要使用的代理。这应该是一个引用支持代理类的字符串。由于本笔记本侧重于最简单、最高级别的 API，因此仅涵盖使用标准支持的代理。如果您想实施自定义代理，请参阅自定义代理的文档（即将推出）。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Create RetrievalQA Chain</span><br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br>retrieval_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=<span class="hljs-string">&quot;stuff&quot;</span>, retriever=docsearch.as_retriever())<br><br><span class="hljs-comment"># Create an Agent</span><br><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> initialize_agent, Tool<br>tools = [<br>    Tool(<br>        name=<span class="hljs-string">&quot;Example QA System&quot;</span>,<br>        func=retrieval_qa.run,<br>        description=<span class="hljs-string">&quot;Example description of the tool.&quot;</span><br>    ),<br>]<br>agent = initialize_agent(tools, llm, agent=<span class="hljs-string">&quot;zero-shot-react-description&quot;</span>, verbose=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Use Agent</span><br>agent.run(<span class="hljs-string">&quot;Ask a question related to the documents&quot;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> load_tools<br><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> initialize_agent<br><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> AgentType<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><br>llm = OpenAI(temperature=<span class="hljs-number">0</span>)<br><br>tools = load_tools([<span class="hljs-string">&quot;serpapi&quot;</span>, <span class="hljs-string">&quot;llm-math&quot;</span>], llm=llm)<br><br>agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=<span class="hljs-literal">True</span>)<br><br>agent.run(<span class="hljs-string">&quot;Who is Leo DiCaprio&#x27;s girlfriend? What is her current age raised to the 0.43 power?&quot;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> Tool, initialize_agent<br><span class="hljs-keyword">from</span> langchain.memory <span class="hljs-keyword">import</span> ConversationBufferMemory<br><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain.utilities <span class="hljs-keyword">import</span> SerpAPIWrapper, LLMMathChain<br><br><span class="hljs-comment"># 初始化搜索链和计算链</span><br>search = SerpAPIWrapper()<br>llm_math_chain = LLMMathChain(llm=llm, verbose=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 创建一个功能列表，指明这个 agent 里面都有哪些可用工具，agent 执行过程可以看必知概念里的 Agent 那张图</span><br>tools = [<br>    Tool(<br>        name = <span class="hljs-string">&quot;Search&quot;</span>,<br>        func=search.run,<br>        description=<span class="hljs-string">&quot;useful for when you need to answer questions about current events or the current state of the world.&quot;</span><br>    ),<br>    Tool(<br>        name=<span class="hljs-string">&quot;Calculator&quot;</span>,<br>        func=llm_math_chain.run,<br>        description=<span class="hljs-string">&quot;useful for when you need to answer questions about math&quot;</span><br>    )<br>]<br><br>memory = ConversationBufferMemory(memory_key=<span class="hljs-string">&quot;chat_history&quot;</span>, return_messages=<span class="hljs-literal">True</span>)<br>llm = ChatOpenAI(temperature=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 初始化 agent</span><br>agent = initialize_agent(tools, llm, agent=<span class="hljs-string">&quot;chat-conversational-react-description&quot;</span>, verbose=<span class="hljs-literal">True</span>, memory=memory)<br><br><span class="hljs-comment"># 执行 agent</span><br>agent.run(<span class="hljs-string">&quot;Who is Leo DiCaprio&#x27;s girlfriend? What is her current age raised to the 0.43 power?&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h4><p>工具是代理可以用来与世界交互的功能。这些工具可以是通用实用程序（例如搜索）、其他链，比如：</p><ul><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/bash.html">Bash</a> 在终端中执行命令。输入应该是有效的命令，输出将是运行该命令的任何输出。</li><li><a href="https://apify.com/">Apify</a> 一个用于网络抓取和数据提取的云平台</li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/bing_search.html">Bing Search</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/google_search.html">Google Search</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/human_tools.html">Human as a tool</a> 人类是 AGI，当 AI 代理感到困惑时会进行询问</li><li>LLM-MATH 对于您需要回答数学问题时有用。</li><li>Open-Meteo-Api 对于您想从OpenMeteo API获取天气信息时很有用。</li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/python.html"><strong>Python REPL</strong></a><strong> 使用它来执行 python 命令。输入应该是有效的 python 命令</strong></li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/requests.html"><strong>Requests</strong></a><strong> 当您需要从站点获取特定内容时使用它。输入应该是一个特定的 url，输出将是该页面上的所有文本。</strong></li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/serpapi.html"><strong>SerpAPI</strong></a><strong> 一个搜索引擎。当您需要回答有关时事的问题时很有用。输入应该是搜索查询。</strong></li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/wikipedia.html">Wikipedia API</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/wolfram_alpha.html">Wolfram Alpha</a> 当您需要回答有关数学、科学、技术、文化、社会和日常生活的问题时很有用。输入应该是搜索查询。</li><li><a href="https://python.langchain.com/en/latest/modules/agents/tools/examples/zapier.html">Zapier Natural Language Actions API</a> 可以通过自然语言 API 接口访问 Zapier 平台上的 5k+ 应用程序和 20k+ 操作</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> tool<br><br><span class="hljs-meta">@tool</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">search_api</span>(<span class="hljs-params">query: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Searches the API for the query.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Results&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> load_tools<br>tools = load_tools([<span class="hljs-string">&quot;serpapi&quot;</span>, <span class="hljs-string">&quot;llm-math&quot;</span>], llm=llm)<br>tools[<span class="hljs-number">0</span>].name = <span class="hljs-string">&quot;Google Search&quot;</span><br>agent = initialize_agent(tools, llm, agent=<span class="hljs-string">&quot;zero-shot-react-description&quot;</span>, verbose=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">tools = [<br>    Tool(<br>        name=<span class="hljs-string">&quot;Music Search&quot;</span>,<br>        func=<span class="hljs-keyword">lambda</span> x: <span class="hljs-string">&quot;&#x27;All I Want For Christmas Is You&#x27; by Mariah Carey.&quot;</span>,<br>        description=<span class="hljs-string">&quot;A Music search engine. Use this more than the normal search if the question is about Music, like &#x27;who is the singer of yesterday?&#x27; or &#x27;what is the most popular song in 2022?&#x27;&quot;</span>,<br>    )<br>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> initialize_agent, Tool<br><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> AgentType<br><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> tool<br><span class="hljs-keyword">from</span> langchain.tools <span class="hljs-keyword">import</span> BaseTool<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> LLMMathChain, SerpAPIWrapper<br><br>llm = OpenAI(temperature=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 初始化搜索链和计算链</span><br>search = SerpAPIWrapper()<br>llm_math_chain = LLMMathChain(llm=llm, verbose=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Creating custom tools with the tool decorator:</span><br><span class="hljs-meta">@tool</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">search_api</span>(<span class="hljs-params">query: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Searches the API for the query.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Results&quot;</span><br><br><br><span class="hljs-comment"># 创建一个功能列表，指明这个 agent 里面都有哪些可用工具，agent 执行过程可以看必知概念里的 Agent 那张图</span><br>tools = [<br>    Tool(<br>        name = <span class="hljs-string">&quot;Search&quot;</span>,<br>        func=search.run,<br>        description=<span class="hljs-string">&quot;useful for when you need to answer questions about current events&quot;</span><br>    ),<br>    Tool(<br>        name=<span class="hljs-string">&quot;Calculator&quot;</span>,<br>        func=llm_math_chain.run,<br>        description=<span class="hljs-string">&quot;useful for when you need to answer questions about math&quot;</span><br>    )<br>]<br><br><span class="hljs-comment"># 初始化 agent</span><br>agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 执行 agent</span><br>agent.run(<span class="hljs-string">&quot;Who is Leo DiCaprio&#x27;s girlfriend? What is her current age raised to the 0.43 power?&quot;</span>)<br><br></code></pre></td></tr></table></figure><h4 id="Custom-Agent"><a href="#Custom-Agent" class="headerlink" title="Custom Agent"></a>Custom Agent</h4><ul><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/custom_agent.html">Custom Agent</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/custom_llm_agent.html">Custom LLM Agent</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/custom_llm_chat_agent.html">Custom LLM Agent (with a ChatModel)</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/custom_mrkl_agent.html">Custom MRKL Agent</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/custom_multi_action_agent.html">Custom MultiAction Agent</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/custom_agent_with_tool_retrieval.html">Custom Agent with Tool Retrieval</a></li></ul><p>We also have documentation for an in-depth dive into each agent type.</p><ul><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/examples/chat_conversation_agent.html">Conversation Agent (for Chat Models)</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/examples/conversational_agent.html">Conversation Agent</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/examples/mrkl.html">MRKL</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/examples/mrkl_chat.html">MRKL Chat</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/examples/react.html">ReAct</a></li><li><a href="https://python.langchain.com/en/latest/modules/agents/agents/examples/self_ask_with_search.html">Self Ask With Search</a></li></ul><h4 id="Toolkits"><a href="#Toolkits" class="headerlink" title="Toolkits"></a>Toolkits</h4><p>带有工具包的代理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> create_csv_agent<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br>agent = create_csv_agent(OpenAI(temperature=<span class="hljs-number">0</span>), <span class="hljs-string">&#x27;titanic.csv&#x27;</span>, verbose=<span class="hljs-literal">True</span>)<br>agent.run(<span class="hljs-string">&quot;how many rows are there?&quot;</span>)<br></code></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.agents.agent_toolkits <span class="hljs-keyword">import</span> create_python_agent<br><span class="hljs-keyword">from</span> langchain.tools.python.tool <span class="hljs-keyword">import</span> PythonREPLTool<br><span class="hljs-keyword">from</span> langchain.python <span class="hljs-keyword">import</span> PythonREPL<br><span class="hljs-keyword">from</span> langchain.llms.openai <span class="hljs-keyword">import</span> OpenAI<br>agent_executor = create_python_agent(<br>    llm=OpenAI(temperature=<span class="hljs-number">0</span>, max_tokens=<span class="hljs-number">1000</span>),<br>    tool=PythonREPLTool(),<br>    verbose=<span class="hljs-literal">True</span><br>)<br>agent_executor.run(<span class="hljs-string">&quot;What is the 10th fibonacci number?&quot;</span>)<br>agent_executor.run(<span class="hljs-string">&quot;&quot;&quot;Understand, write a single neuron neural network in PyTorch.</span><br><span class="hljs-string">Take synthetic data for y=2x. Train for 1000 epochs and print every 100 epochs.</span><br><span class="hljs-string">Return prediction for x = 5&quot;&quot;&quot;</span>)<br></code></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> create_sql_agent<br><span class="hljs-keyword">from</span> langchain.agents.agent_toolkits <span class="hljs-keyword">import</span> SQLDatabaseToolkit<br><span class="hljs-keyword">from</span> langchain.sql_database <span class="hljs-keyword">import</span> SQLDatabase<br><span class="hljs-keyword">from</span> langchain.llms.openai <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> AgentExecutor<br>db = SQLDatabase.from_uri(<span class="hljs-string">&quot;sqlite:///../../../../notebooks/Chinook.db&quot;</span>)<br>toolkit = SQLDatabaseToolkit(db=db)<br><br><span class="hljs-comment"># pip install langchain==0.0.136</span><br>agent_executor = create_sql_agent(<br>    llm=OpenAI(temperature=<span class="hljs-number">0</span>),<br>    toolkit=toolkit,<br>    verbose=<span class="hljs-literal">True</span><br>)<br>agent_executor.run(<span class="hljs-string">&quot;Describe the playlisttrack table&quot;</span>)<br></code></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> OpenAI, VectorDBQA<br>llm = OpenAI(temperature=<span class="hljs-number">0</span>)<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> TextLoader<br>loader = TextLoader(<span class="hljs-string">&#x27;../../../state_of_the_union.txt&#x27;</span>)<br>documents = loader.load()<br>text_splitter = CharacterTextSplitter(chunk_size=<span class="hljs-number">1000</span>, chunk_overlap=<span class="hljs-number">0</span>)<br>texts = text_splitter.split_documents(documents)<br><br>embeddings = OpenAIEmbeddings()<br>state_of_union_store = Chroma.from_documents(texts, embeddings, collection_name=<span class="hljs-string">&quot;state-of-union&quot;</span>)<br><br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> WebBaseLoader<br>loader = WebBaseLoader(<span class="hljs-string">&quot;https://beta.ruff.rs/docs/faq/&quot;</span>)<br>docs = loader.load()<br>ruff_texts = text_splitter.split_documents(docs)<br>ruff_store = Chroma.from_documents(ruff_texts, embeddings, collection_name=<span class="hljs-string">&quot;ruff&quot;</span>)<br><br><span class="hljs-keyword">from</span> langchain.agents.agent_toolkits <span class="hljs-keyword">import</span> (<br>    create_vectorstore_agent,<br>    VectorStoreToolkit,<br>    VectorStoreInfo,<br>)<br>vectorstore_info = VectorStoreInfo(<br>    name=<span class="hljs-string">&quot;state_of_union_address&quot;</span>,<br>    description=<span class="hljs-string">&quot;the most recent state of the Union adress&quot;</span>,<br>    vectorstore=state_of_union_store<br>)<br>toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info)<br>agent_executor = create_vectorstore_agent(<br>    llm=llm,<br>    toolkit=toolkit,<br>    verbose=<span class="hljs-literal">True</span><br>)<br>agent_executor.run(<span class="hljs-string">&quot;What did biden say about ketanji brown jackson is the state of the union address?&quot;</span>)<br><br></code></pre></td></tr></table></figure></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="1、完成一次简单问答"><a href="#1、完成一次简单问答" class="headerlink" title="1、完成一次简单问答"></a>1、完成一次简单问答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>] = <span class="hljs-string">&#x27;你的api key&#x27;</span><br><br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><br>llm = OpenAI(model_name=<span class="hljs-string">&quot;text-davinci-003&quot;</span>,max_tokens=<span class="hljs-number">1024</span>)<br>llm(<span class="hljs-string">&quot;怎么评价人工智能&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="2、构建本地知识库问答机器人（QA）"><a href="#2、构建本地知识库问答机器人（QA）" class="headerlink" title="2、构建本地知识库问答机器人（QA）"></a>2、构建本地知识库问答机器人（QA）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> OpenAI,VectorDBQA<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> DirectoryLoader<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><span class="hljs-keyword">from</span> langchain.chains.question_answering <span class="hljs-keyword">import</span> load_qa_chain<br><br><span class="hljs-comment"># 加载文件夹中的所有txt类型的文件</span><br>loader = DirectoryLoader(<span class="hljs-string">&#x27;/content/sample_data/data/&#x27;</span>, glob=<span class="hljs-string">&#x27;**/*.txt&#x27;</span>)<br><span class="hljs-comment"># 将数据转成 document 对象，每个文件会作为一个 document</span><br>documents = loader.load()<br><br><span class="hljs-comment"># 初始化加载器</span><br>text_splitter = CharacterTextSplitter(chunk_size=<span class="hljs-number">100</span>, chunk_overlap=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># 切割加载的 document</span><br>split_docs = text_splitter.split_documents(documents)<br><br><span class="hljs-comment"># 初始化 openai 的 embeddings 对象</span><br>embeddings = OpenAIEmbeddings()<br><br><span class="hljs-comment"># 持久化数据</span><br>vector_store = Chroma.from_documents(split_docs, embeddings, persist_directory=<span class="hljs-string">&quot;D:/vector_store&quot;</span>)<br>vector_store.persist()<br><br><span class="hljs-comment"># 加载数据</span><br>vector_store = Chroma(persist_directory=<span class="hljs-string">&quot;D:/vector_store&quot;</span>, embedding_function=embeddings)<br><br><span class="hljs-comment"># 创建问答对象</span><br>qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=<span class="hljs-string">&quot;stuff&quot;</span>, vectorstore=vector_store,return_source_documents=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 进行问答</span><br>result = qa(&#123;<span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;科大讯飞今年第一季度收入是多少？&quot;</span>&#125;)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure><img src="/2023/04/18/2023-04-18-LangChain%20%E5%AE%9E%E6%88%98/1681462020394-312ad3ef-0371-4cd5-a7be-74905a60c6d7.png" class="" title="image.png"><h3 id="3、Question-Answering-over-Docs"><a href="#3、Question-Answering-over-Docs" class="headerlink" title="3、Question Answering over Docs"></a>3、Question Answering over Docs</h3><p>对您的文档数据进行问答，可以使用: load_qa_chain, RetrievalQA, VectorstoreIndexCreator, ConversationalRetrievalChain</p><ul><li><a href="https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html">Question Answering Notebook</a>: A notebook walking through how to accomplish this task.</li><li><a href="https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html">VectorDB Question Answering Notebook</a>: A notebook walking through how to do question answering over a vector database. <h3 id="4、使用GPT3-5模型构建油管频道问答机器人"><a href="#4、使用GPT3-5模型构建油管频道问答机器人" class="headerlink" title="4、使用GPT3.5模型构建油管频道问答机器人"></a>4、使用GPT3.5模型构建油管频道问答机器人</h3>在 chatgpt api（也就是 GPT-3.5-Turbo）模型出来后，因钱少活好深受大家喜爱，所以 LangChain 也加入了专属的链和模型，我们来跟着这个例子看下如何使用他。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> YoutubeLoader<br><span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> ChatVectorDBChain, ConversationalRetrievalChain<br><br><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain.prompts.chat <span class="hljs-keyword">import</span> (<br>  ChatPromptTemplate,<br>  SystemMessagePromptTemplate,<br>  HumanMessagePromptTemplate<br>)<br><br><span class="hljs-comment"># 加载 youtube 频道</span><br>loader = YoutubeLoader.from_youtube_channel(<span class="hljs-string">&#x27;https://www.youtube.com/watch?v=Dj60HHy-Kqk&#x27;</span>)<br><span class="hljs-comment"># 将数据转成 document</span><br>documents = loader.load()<br><br><span class="hljs-comment"># 初始化文本分割器</span><br>text_splitter = RecursiveCharacterTextSplitter(<br>  chunk_size=<span class="hljs-number">1000</span>,<br>  chunk_overlap=<span class="hljs-number">20</span><br>)<br><br><span class="hljs-comment"># 分割 youtube documents</span><br>documents = text_splitter.split_documents(documents)<br><br><span class="hljs-comment"># 初始化 openai embeddings</span><br>embeddings = OpenAIEmbeddings()<br><br><span class="hljs-comment"># 将数据存入向量存储</span><br>vector_store = Chroma.from_documents(documents, embeddings)<br><span class="hljs-comment"># 通过向量存储初始化检索器</span><br>retriever = vector_store.as_retriever()<br><br>system_template = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Use the following context to answer the user&#x27;s question.</span><br><span class="hljs-string">If you don&#x27;t know the answer, say you don&#x27;t, don&#x27;t try to make it up. And answer in Chinese.</span><br><span class="hljs-string">-----------</span><br><span class="hljs-string">&#123;context&#125;</span><br><span class="hljs-string">-----------</span><br><span class="hljs-string">&#123;chat_history&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 构建初始 messages 列表，这里可以理解为是 openai 传入的 messages 参数</span><br>messages = [<br>  SystemMessagePromptTemplate.from_template(system_template),<br>  HumanMessagePromptTemplate.from_template(<span class="hljs-string">&#x27;&#123;question&#125;&#x27;</span>)<br>]<br><br><span class="hljs-comment"># 初始化 prompt 对象</span><br>prompt = ChatPromptTemplate.from_messages(messages)<br><br><span class="hljs-comment"># 初始化问答链</span><br>qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=<span class="hljs-number">0.1</span>,max_tokens=<span class="hljs-number">2048</span>),retriever,qa_prompt=prompt)<br><br>chat_history = []<br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>  question = <span class="hljs-built_in">input</span>(<span class="hljs-string">&#x27;问题：&#x27;</span>)<br>  <span class="hljs-comment"># 开始发送问题 chat_history 为必须参数,用于存储对话历史</span><br>  result = qa(&#123;<span class="hljs-string">&#x27;question&#x27;</span>: question, <span class="hljs-string">&#x27;chat_history&#x27;</span>: chat_history&#125;)<br>  chat_history.append((question, result[<span class="hljs-string">&#x27;answer&#x27;</span>]))<br>  <span class="hljs-built_in">print</span>(result[<span class="hljs-string">&#x27;answer&#x27;</span>])<br></code></pre></td></tr></table></figure><img src="/2023/04/18/2023-04-18-LangChain%20%E5%AE%9E%E6%88%98/1681977935059-0e8c9a11-30a8-4301-a146-f51476132f70.png" class="" title="image.png"></li></ul><h3 id="5、langchain-zapier"><a href="#5、langchain-zapier" class="headerlink" title="5、langchain + zapier"></a>5、langchain + zapier</h3><p>我们主要是结合使用 zapier 来实现将万种工具连接起来。</p><p>所以我们第一步依旧是需要申请账号和他的自然语言 api key。</p><p><a href="https://zapier.com/l/natural-language-actions">https://zapier.com/l/natural-language-actions</a></p><p>他的 api key 虽然需要填写信息申请。但是基本填入信息后，基本可以秒在邮箱里看到审核通过的邮件。然后，我们通过右键里面的连接打开我们的api 配置页面。我们点击右侧的 Manage Actions 来配置我们要使用哪些应用。</p><details class="lake-collapse"><summary id="u04b16d10"><span class="ne-text">api 配置</span></summary><p id="u8f059fc7" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">我在这里配置了 Gmail 读取和发邮件的 action，并且所有字段都选的是通过 AI 猜。</span></p><p id="u7a2b05b7" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1681978025915-be9b653b-e4bb-4583-9fe6-1d6e13961610.png" width="650" id="u8eef1c55" class="ne-image"></p><p id="u705da2bb" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">image-20230406233319250</span></p><p id="uf73e7a68" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1681978026763-37ed8eaa-6c23-44b9-99c8-9ee5dc13e26a.png" width="620" id="u566fa606" class="ne-image"></p><p id="ucb24a1a7" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">image-20230406234827815</span></p></details><p>配置好之后，开始写代码</p><details class="lake-collapse"><summary id="udfefb8e5"><span class="ne-text">Zapier 发邮件使用示例</span></summary><pre data-language="python" id="ayqX1" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">import osos.environ["ZAPIER_NLA_API_KEY"] = ''from langchain.llms import OpenAIfrom langchain.agents import initialize_agentfrom langchain.agents.agent_toolkits import ZapierToolkitfrom langchain.utilities.zapier import ZapierNLAWrapperllm = OpenAI(temperature=.3)zapier = ZapierNLAWrapper()toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)agent = initialize_agent(toolkit.get_tools(), llm, agent="zero-shot-react-description", verbose=True)# 我们可以通过打印的方式看到我们都在 Zapier 里面配置了哪些可以用的工具for tool in toolkit.get_tools():  print (tool.name)  print (tool.description)  print ("\n\n")agent.run('请用中文总结最后一封"******@qq.com"发给我的邮件。并将总结发送给"******@qq.com"')</pre><p id="ub6132d77" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1681978269926-7b6bd947-5c4d-4b98-a6fa-675961bae56b.png" width="715" id="u5d7eba3a" class="ne-image"></p><p id="uadc640b3" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1681978284054-e83744e8-1efb-46bf-9308-c1790cca3815.png" width="455.5" id="u3abe19d8" class="ne-image"></p></details><p>这只是个小例子，因为 zapier 有数以千计的应用，所以我们可以轻松结合 openai api 搭建自己的工作流。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LangChain</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LlamaIndex 实战</title>
    <link href="/2023/04/14/2023-04-14-LlamaIndex%20%E5%AE%9E%E6%88%98/"/>
    <url>/2023/04/14/2023-04-14-LlamaIndex%20%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<h3 id="LlamaIndex-原理介绍"><a href="#LlamaIndex-原理介绍" class="headerlink" title="LlamaIndex 原理介绍"></a>LlamaIndex 原理介绍</h3><blockquote><p><a href="https://weibo.com/1727858283/MvEIhu6C2?type=repost&amp;sudaref=www.google.com.hk">llamaindex 原理介绍</a></p></blockquote><p>LlamaIndex（GPT Index）是一个对话式文档问答解决方案，可以针对特定语料进行文档检索，通过索引文件把外部语料数据和GPT连接起来。它主要帮我们做了如下几件事：<br>文档拆分、向量化、向量存储检索、基于文档对话等。</p><p>现在ChatGPT的API是无状态的，意味着你需要自己去维持会话状态，保存上下文，每次请求的时候将之前的历史消息全部发过去，但是这里面有两个问题：1. 请求内容会越来越大；2. 费用很高。</p><p>今天在Twitter上看到有人分享的一个很好的解决方案，可以借助OpenAI的embedding模型和自己的数据库，现在本地搜索数据获得上下文，然后在调用ChatGPT的API的时候，加上本地数据库中的相关内容，这样就可以让ChatGPT从你自己的数据集获得了上下文，再结合ChatGPT自己庞大的数据集给出一个更相关的理想结果。</p><p>这种模式尤其适合针对一些特定著作、资料库的搜索和问答。我想之前有人做了模拟乔布斯风格的问答应该也是基于这种模式来做的。</p><p>具体解释一下它的实现原理（参考图一）。</p><ol><li><p>首先准备好你要用来学习的文本资料，把它变成CSV或者Json这样易于处理的格式，并且分成小块（chunks），每块不要超过8191个Tokens，因为这是OpenAI embeddings模型的输入长度限制</p></li><li><p>然后用一个程序，分批调用OpenAI embedding的API，目前最新的模式是text-embedding-ada-002，将文本块变成文本向量。(参考图1从Script到OpenAI的步骤)</p></li></ol><img src="/2023/04/14/2023-04-14-LlamaIndex%20%E5%AE%9E%E6%88%98/1681439363149-e0a690e6-7887-440d-94b3-ac27ad2950d7.png" class="" title="image.png"><ol><li>需要将转换后的结果保存到本地数据库。注意一般的关系型数据库是不支持这种向量数据的，必须用特别的数据库，比如<strong>Pinecone</strong>数据库，比如Postgres数据库（需要 <strong>pgvector</strong> 扩展）。如果你数据不大，存成csv文件，然后加载到内存，借助内存搜索也是一样的。</li></ol><p>当然你保存的时候，需要把原始的文本块和数字向量一起存储，这样才能根据数字向量反向获得原始文本。这一步有点类似于全文索引中给数据建索引。(参考图一从Script到DB的步骤)</p><ol><li>等需要搜索的时候，先将你的搜索关键字，调用OpenAI embedding的API把关键字变成数字向量。</li></ol><p>(参考图一 Search App到OpenAI)</p><p>拿到这个数字向量后，再去自己的数据库进行检索，那么就可以得到一个结果集，这个结果集会根据匹配的相似度有个打分，分越高说明越匹配，这样就可以按照匹配度倒序返回一个相关结果。</p><p>(参考图一 Search App到DB的步骤)</p><ol><li>聊天问答的实现要稍微复杂一点</li></ol><p>当用户提问后，需要先根据提问内容去本地数据库中搜索到一个相关结果集。</p><p>（参考图一中Chat App到Search App的步骤）</p><p>然后根据拿到的结果集，将结果集加入到请求ChatGPT的prompt中。</p><p>（参考图一中Chat App到OpenAI的步骤）</p><p>比如说用户提了一个问题：“What’s the makers’s schedule?”，从数据库中检索到相关的文字段落是：“What I worked on…”和”Taste for Makers…”，那么最终的prompt看起来就像这样：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br>  <span class="hljs-punctuation">&#123;</span><br>    role<span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;system&quot;</span><span class="hljs-punctuation">,</span><br>    content<span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;You are a helpful assistant that accurately answers queries using Paul Graham&#x27;s essays. Use the text provided to form your answer, but avoid copying word-for-word from the essays. Try to use your own words when possible. Keep your answer under 5 sentences. Be accurate, helpful, concise, and clear.&quot;</span><br>  <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">&#123;</span><br>    role<span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span><br>    content<span class="hljs-punctuation">:</span> `Use the following passages to provide an answer<br>    to the query<span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;What&#x27;s the makers&#x27;s schedule?&quot;</span><br>    <span class="hljs-number">1.</span> What I worked on...<br>    <span class="hljs-number">2.</span> Taste for Makers...`<br>  <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><p>这样ChatGPT在返回结果的时候，就会加上你的数据集。</p><blockquote><p>项目地址：github.com/mckaywrigley/paul-graham-gpt<br>相关Twitter：twitter.com/mckaywrigley/status/1631328308116996097</p></blockquote><h3 id="LlamaIndex-实战"><a href="#LlamaIndex-实战" class="headerlink" title="LlamaIndex 实战"></a>LlamaIndex 实战</h3><blockquote><p><a href="https://gpt-index.readthedocs.io/en/latest/use_cases/queries.html">LlamaIndex (GPT Index)  官方教程</a><br><a href="https://uxdesign.cc/i-built-an-ai-that-answers-questions-based-on-my-user-research-data-7207b052e21c">Build an AI that answers questions based on user research data.</a> 这是一篇利用自己的知识库做 QA 的博客教程，里面有作者的 Colab Notebook ，一眼就会，开箱即炼。</p></blockquote><p><a href="https://github.com/jerryjliu/gpt_index">LlamaIndex</a>（原来的名字是gpt_index）库已经把上述这套逻辑封装了，可以直接使用。下面给出一个基于llama-index实现文档问答的具体demo：</p><h4 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pip3 install openai llama-index tiktoken</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> openai<br><br><span class="hljs-comment"># 设置 API key</span><br>os.environ[<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>] = <span class="hljs-string">&quot;sk-xxx&quot;</span><br>openai.api_key = os.getenv(<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>) <br><br><span class="hljs-keyword">import</span> logging <br><br><span class="hljs-comment"># 记录日志</span><br>logging.basicConfig(stream=sys.stdout, level=logging.INFO)<br>logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))<br></code></pre></td></tr></table></figure><h4 id="核心类构建"><a href="#核心类构建" class="headerlink" title="核心类构建"></a>核心类构建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings<br><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> LangchainEmbedding<br><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> (<br>    GPTSimpleVectorIndex,<br>    SimpleDirectoryReader, <br>    LLMPredictor,<br>    PromptHelper,<br>    ServiceContext,<br>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LLma</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, gptmodel, embeddingmodel</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># set maximum input size</span><br>        max_input_size = <span class="hljs-number">4096</span><br>        <span class="hljs-comment"># set number of output tokens</span><br>        num_outputs = <span class="hljs-number">512</span><br>        <span class="hljs-comment"># set maximum chunk overlap</span><br>        max_chunk_overlap = <span class="hljs-number">20</span><br>        <span class="hljs-comment"># set chunk size limit</span><br>        chunk_size_limit = <span class="hljs-number">600</span> <br>        <br>        <span class="hljs-comment"># define LLM</span><br>        llm = ChatOpenAI(model_name=gptmodel, temperature=<span class="hljs-number">0.7</span>, max_tokens=num_outputs, request_timeout=<span class="hljs-number">10</span>, max_retries=<span class="hljs-number">2</span>)<br>        self.llm_predictor = LLMPredictor(llm=llm)<br>        self.prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)<br>        <span class="hljs-comment"># 定制化 embedding</span><br>        embedding = OpenAIEmbeddings(<br>            document_model_name=embeddingmodel,<br>            query_model_name=embeddingmodel<br>        )<br>        self.embedding_llm = LangchainEmbedding(embedding)<br>        <br>        self.service_context = ServiceContext.from_defaults(<br>            llm_predictor=self.llm_predictor,<br>            embed_model=self.embedding_llm,<br>            prompt_helper=self.prompt_helper<br>        )<br><br>    <span class="hljs-comment"># 建立本地索引</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">create_index</span>(<span class="hljs-params">self,dir_path=<span class="hljs-string">&quot;./data&quot;</span>,service_context=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># 读取data文件夹下的文档</span><br>        documents = SimpleDirectoryReader(dir_path).load_data()<br>        <span class="hljs-comment"># 按最大token数600来把原文档切分为多个小的chunk，每个chunk转为向量，并构建索引</span><br>        index = GPTSimpleVectorIndex.from_documents(documents, service_context=self.service_context)<br>        <span class="hljs-comment"># 保存索引</span><br>        index.save_to_disk(<span class="hljs-string">&#x27;./index.json&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Save to localpath&quot;</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query_index</span>(<span class="hljs-params">self,input_text,index_path=<span class="hljs-string">&quot;./index.json&quot;</span></span>):<br>        <span class="hljs-comment"># 加载索引</span><br>        index = GPTSimpleVectorIndex.load_from_disk(index_path)<br>        response = index.query(input_text, response_mode=<span class="hljs-string">&quot;compact&quot;</span>)<br>        <span class="hljs-comment"># display(Markdown(f&quot;Response: &lt;b&gt;&#123;response.response&#125;&lt;/b&gt;&quot;))</span><br>        <span class="hljs-keyword">return</span> response.response<br><br><br>gptmodel = <span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>   <span class="hljs-comment"># model: gpt4</span><br>embeddingmodel = <span class="hljs-string">&quot;text-embedding-ada-002&quot;</span><br>llma = LLma(gptmodel, embeddingmodel)<br></code></pre></td></tr></table></figure><h4 id="建立索引"><a href="#建立索引" class="headerlink" title="建立索引"></a>建立索引</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 建立索引</span><br>train_dir = <span class="hljs-string">&quot;./data&quot;</span><span class="hljs-comment"># 放入多篇 TXT 文档</span><br>llma.create_index(train_dir)<br></code></pre></td></tr></table></figure><p>执行上述代码后可以看到日志打印如下：<br><figure class="highlight node-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs node-repl"><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript">[build_index_from_documents] <span class="hljs-title class_">Total</span> <span class="hljs-variable constant_">LLM</span> token <span class="hljs-attr">usage</span>: <span class="hljs-number">0</span> tokens</span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript">[build_index_from_documents] <span class="hljs-title class_">Total</span> embedding token <span class="hljs-attr">usage</span>: <span class="hljs-number">1466</span> tokens</span><br></code></pre></td></tr></table></figure><br>说明原文档中所有token数为1466，这也是请求embedding接口的调用成本。按最大token数600，则会切分为3个chunk，可以在索引文件index.json确认chunk的数目确实为3，同时index.json中也记录了每个chunk对应的embedding向量。</p><h4 id="查询索引"><a href="#查询索引" class="headerlink" title="查询索引"></a>查询索引</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查询索引</span><br>query = <span class="hljs-string">&#x27;讲一下美女蛇的故事&#x27;</span><br>answer = llma.query_index(query)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;query was:&#x27;</span>, query)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;answer was:&#x27;</span>, answer)<br></code></pre></td></tr></table></figure><p>调用query接口的时候，llama-index会构造如下的prompt:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs lsl"><span class="hljs-string">&quot;Context information is below. <span class="hljs-subst">\n</span>&quot;</span><br>    <span class="hljs-string">&quot;---------------------<span class="hljs-subst">\n</span>&quot;</span><br>    <span class="hljs-string">&quot;&#123;context_str&#125;&quot;</span><br>    <span class="hljs-string">&quot;<span class="hljs-subst">\n</span>---------------------<span class="hljs-subst">\n</span>&quot;</span><br>    <span class="hljs-string">&quot;Given the context information and not prior knowledge, &quot;</span><br>    <span class="hljs-string">&quot;answer the question: &#123;query_str&#125;<span class="hljs-subst">\n</span>&quot;</span><br></code></pre></td></tr></table></figure><br>上述代码执行后，日志打印如下：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stata">&gt; [<span class="hljs-keyword">query</span>] <span class="hljs-keyword">Total</span> LLM <span class="hljs-keyword">token</span> usage: 4894 tokens<br>&gt; [<span class="hljs-keyword">query</span>] <span class="hljs-keyword">Total</span> embedding <span class="hljs-keyword">token</span> usage: 18 tokens<br><br>美女蛇的故事是这样的：有一个读书人住在古庙里用功，晚间，在院子里纳凉的时候，突然听到有人在叫他的名字。他四面看时，却见一个美女的脸露在墙头上，向他一笑，隐去了。他很高兴，但竟给那走来夜谈的老和尚识破了机关，说他脸上有些妖气，一定遇见“美女蛇<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>LlamaIndex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开源ChatGPT平替项目汇总</title>
    <link href="/2023/04/11/2023-04-11-%E5%BC%80%E6%BA%90ChatGPT%E5%B9%B3%E6%9B%BF%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%80%BB/"/>
    <url>/2023/04/11/2023-04-11-%E5%BC%80%E6%BA%90ChatGPT%E5%B9%B3%E6%9B%BF%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://zhuanlan.zhihu.com/p/618790279?utm_medium=social&amp;utm_oi=834496487537926144&amp;utm_psn=1626225456889135104&amp;utm_source=wechat_session">开源ChatGPT替代模型项目整理</a><br><a href="https://github.com/chenking2020/FindTheChatGPTer">寻找那些ChatGPT/GPT4开源“平替”们</a></p></blockquote><p>原版用的是GPT-3.5+OpenAI自己的数据集，目前开源社区搞得最多的还是LLaMA+Alpaca这套方案，但是其他不一样的方案也是百花齐放，这里简单列一下</p><div class="table-container"><table><thead><tr><th><strong>名称</strong></th><th><strong>点赞数</strong></th><th><strong>支持语言</strong></th><th><strong>简介+基础 LLM</strong></th><th><strong>训练方法/数据集</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><a href="https://github.com/nomic-ai/gpt4all">gpt4all</a></td><td>33.7k</td><td></td><td>基于GPT-J和LLaMa训练开源的大语言模型的Demo, data, and code，类似llama.cpp、WebLLM 等作为 LLM 离线部署加速方案</td><td>自定义数据集(800k)</td><td>数据集由GPT-3.5生成</td></tr><tr><td><a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca(羊驼)</a></td><td>20.8k</td><td>en</td><td>斯坦福的羊驼模型（羊驼），基于 LLaMA 7B 微调，使用 Self-Instruct 自举生成 52K 条指令数据</td><td>Alpaca</td><td>finetune 参考 <a href="https://github.com/tloen/alpaca-lora">alpaca-lora</a></td></tr><tr><td><a href="https://github.com/tloen/alpaca-lora">alpaca-lora</a></td><td>10.2k</td><td>en</td><td>Instruct-tune LLaMA on consumer hardware 利用 LoRA 复现 Alpaca 结果</td><td>Alpaca</td><td></td></tr><tr><td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">Chinese-LLaMA-Alpaca</a></td><td>6.1k</td><td>en, zh</td><td>提供中文LLaMA模型和指令精调的Alpaca大模型，在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练</td><td>Alpaca</td><td></td></tr><tr><td><a href="https://github.com/hpcaitech/ColossalAI">ColossalChat</a></td><td>28.2k</td><td></td><td>完整RLHF流程0门槛克隆 LLaMA、OPT、BLOOM</td><td></td><td><a href="https://mp.weixin.qq.com/s/C9b_oZu9jpw0oObEuDmd9w">中文解读</a></td></tr><tr><td><a href="https://github.com/LAION-AI/Open-Assistant">Open-Assistant</a></td><td>24.1k</td><td></td><td>OpenAssistant是一个基于聊天的助手，可以理解任务，可以与第三方系统交互，并动态检索信息。</td><td></td><td></td></tr><tr><td><a href="https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-chat/chinese/README.md">DeepSpeed Chat</a></td><td>20.5k</td><td></td><td>微软开源的一键式RLHF训练，训练速度提升15倍以上，130亿参数的类ChatGPT模型，只需1.25小时就能完成训练。</td><td></td><td><a href="https://mp.weixin.qq.com/s/psX32gVrpUMxKXA7cUwcEg">中文解读</a></td></tr><tr><td><a href="https://github.com/THUDM/ChatGLM-6B">ChatGLM</a></td><td>18.1k</td><td>en, zh</td><td>清华大学出品，支持中英双语的对话语言模型</td><td>自定义数据集(1T)</td><td><a href="https://github.com/mymusise/ChatGLM-Tuning">ChatGLM-Tuning</a></td></tr><tr><td><a href="https://github.com/lm-sys/FastChat">Vicuna(小羊驼)</a></td><td>14.2k</td><td>en</td><td>利用‘用户共享对话’微调 LLaMA，Vicuna-13B 达到了 ChatGPT 90%以上的质量，同时在超过 90% 的情况下优于 LLaMA 和 Stanford Alpaca 等其他模型。 sharegpt.com 最近已经禁止抓取，这意味着该数据源不再可用，Vicuna 难以复现。</td><td>shareGPT(70k)</td><td>finetune 参考  <a href="https://github.com/Facico/Chinese-Vicuna">Chinese-Vicuna </a></td></tr><tr><td><a href="https://github.com/Vision-CAIR/MiniGPT-4">MiniGPT-4🔥</a></td><td>13k</td><td></td><td>增强视觉语言理解（识图）与先进的大型语言模型</td><td></td><td>1.识别图片，并且进行对话</td></tr><tr><td><a href="https://github.com/stability-AI/stableLM">stableLM🔥</a></td><td>10.9k</td><td>en</td><td>Stable Diffusion的初创公司Stability AI发布并开源该团队训练的大语言模型StableLM</td><td>Pile 数据集</td><td></td></tr><tr><td><a href="https://github.com/OpenLMLab/MOSS">MOSS🔥</a></td><td>4k</td><td>en, zh</td><td>国内首个类 ChatGPT 模型：复旦大学 MOSS，RTX 3090 显卡可运行</td><td></td><td></td></tr><tr><td><a href="https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama">ChatLLaMA</a></td><td>7.8k</td><td>en</td><td>初创公司 Nebuly AI 开源的 RLHF 版 LLaMA (ChatLLaMA)，构建 ChatGPT 形式的服务</td><td></td><td></td></tr><tr><td><a href="https://github.com/databrickslabs/dolly">Dolly 2.0</a></td><td>7.8k</td><td>en</td><td>Dolly是Databricks发布的开源、遵循指令的 LLM，可用于商业目的</td><td><a href="https://github.com/databrickslabs/dolly/tree/master/data">databricks-dolly-15k</a></td><td><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650873911&amp;idx=2&amp;sn=3d7e4f53f5f0c29c85b81435dc984f85&amp;scene=21#wechat_redirect">中文解读</a></td></tr><tr><td><a href="https://github.com/togethercomputer/OpenChatKit">OpenChatKit</a></td><td>7.6k</td><td>en</td><td>前OpenAI团队打造，OpenChatKit提供了一个强大的开源基础，可以为各种应用程序创建专门的和通用的聊天机器人。</td><td>OIG-43M</td><td></td></tr><tr><td><a href="https://github.com/LianjiaTech/BELLE">BELLE</a></td><td>4k</td><td>zh</td><td>基于BLOOM和LLAMA，针对ChatGPT生成的中文数据微调的对话大模型</td><td>150万中文指令微调数据集</td><td><a href="https://mp.weixin.qq.com/s/HI2VvokqYNdRZgcZwfSDqw">中文解读</a></td></tr><tr><td><a href="https://github.com/OptimalScale/LMFlow">LMFLOW</a></td><td>4.3k</td><td>en, zh</td><td>LMFlow 不是一个单一模型，而是支持很多模型的微调框架</td><td>自定义</td><td><a href="https://mp.weixin.qq.com/s/G2T3uLILlJaQtkOBeoZuKg">中文解读</a></td></tr><tr><td><a href="https://github.com/Lightning-AI/lit-llama">lit-llama</a></td><td>2.6k</td><td>en</td><td>Lightning-AI 基于nanoGPT的LLaMA语言模型的实现。支持量化，LoRA微调，预训练。</td><td>Alpaca</td></tr></tbody></table></div><p><strong>可用的开源基础 LLM，可以参考博客</strong> <a href="https://matt-rickard.com/a-list-of-1-billion-parameter-llms">A List of 1 Billion+ Parameter LLMs</a></p><p>首先，我们先简单过一下 Meta 开源的 <a href="https://arxiv.org/abs/2302.13971">LLAMA</a> (Large Language Model Meta AI)。Meta 表示，LLaMA 13B 的性能超过了 GPT-3，而 LLaMA 65B 和其他领先的模型相当；LLaMA 更小、更高性能的模型，可能是目前公开模型权重中效果最好的语言模型。但是中文支持效果比较差。我们尝试了社区精简版 LLaMA：<a href="https://github.com/juncongmoo/pyllama">https://github.com/juncongmoo/pyllama</a>，他在8GB显存就能够跑起来，具体效果如下：</p><details class="lake-collapse"><summary id="uf49f95e0"><span class="ne-text">社区精简版 pyLLaMA 使用效果一览</span></summary><pre data-language="python" id="AWoMd" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959">pip install pyllama -Upip install gradio# 模型下载# 1、通过pyllama下载python -m llama.download --model_size 7B --folder /tmp/pyllama_datapython -m llama.download --model_size 7B,30B --folder /tmp/pyllama_data# 2、直接wget下载https://ipfs.io/ipfs/Qmb9y5GCkTG7ZzbBWMu2BXwMkzyCKcUjtEKPpgdZ7GEFKm/# 配置环境变量export CKPT_DIR=/data/voiceprint2/llama/llama-dl-main/7Bexport TOKENIZER_PATH=/data/voiceprint2/llama/llama-dl-main/tokenizer.model # 运行测试git clone https://github.com/juncongmoo/pyllama.gitcd pyllamapython inference.py --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH# To load KV cache in CPU, run export KV_CAHCHE_IN_GPU=0 in the shell.# 调整max_seq_len并max_batch_size减少内存消耗以便能够在 GPU 中运行# Quantize 7B model to 8-bitpython -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 8 --save pyllama-7B8b.pt# 启动 gradio webuicd apps/gradiopython webapp_single.py  --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH# 启动网络服务器cd apps/flaskpython web_server_single.py  --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH</pre><p id="ue7e5b8a4" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="color: rgb(51, 51, 51)">Llama-7B 效果展示（显存占用约14GB）：</span></p><p id="ud7230f4e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1679040372018-ab34c5d6-634d-4cc4-8a3d-26c536651955.png" width="846" id="u926a80b3" class="ne-image"></p><p id="ue72146c6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">使用评价：</span></p><ul class="ne-ul" style="margin: 0; padding-left: 23px"><li id="ue8f789b2"><span class="ne-text">毕竟是小的模型，感觉回复的速度和质量都不尽如人意，比ChatGPT差很多</span></li><li id="uc6459f80"><span class="ne-text">回答中可能出现一些不相关的评论，明明是要求解题，却去做扩展续写的工作等等</span></li><li id="u2838f768"><span class="ne-text">目前的7B模型～可能只能做一些最简单的对话工作....</span></li></ul></details><p>Llama-7B 效果展示（显存占用约14GB）：</p><p><img src="开源ChatGPT平替项目汇总/1679040372018-ab34c5d6-634d-4cc4-8a3d-26c536651955.png" alt="img"></p><p>使用评价：</p><ul><li>毕竟是小的模型，感觉回复的速度和质量都不尽如人意，比ChatGPT差很多</li><li>回答中可能出现一些不相关的评论，明明是要求解题，却去做扩展续写的工作等等</li><li>目前的7B模型～可能只能做一些最简单的对话工作….</li></ul><p>其他平替方案里，我们挑几个简单介绍一下：</p><h3 id="Alpaca-和-Chinese-LLaMA-Alpaca"><a href="#Alpaca-和-Chinese-LLaMA-Alpaca" class="headerlink" title="Alpaca 和 Chinese-LLaMA-Alpaca"></a>Alpaca 和 Chinese-LLaMA-Alpaca</h3><p><a href="https://github.com/tatsu-lab/stanford_alpaca"><strong>Alpaca（羊驼）</strong></a>应该是目前最主流的做法，它基于 Meta 的 LLaMA 7B  模型进行指令微调。其基本原理是让 OpenAI 的 text-davinci-003 模型以 self-instruct 方式生成 52K 指令样本，以此来微调LLaMA，这个数据集本身还不是很干净导致Alpaca-lora的老哥们后来还给它清洗了一遍。</p><p>Alpaca精悍于：</p><ul><li>①直接嫖了Meta开源模型Llama-7B，在8个80GB A100上继续训练了3个小时；</li><li>②直接嫖了ChatGPT生成可靠训练数据，无需人工标注；借鉴self-instruct的方式，利用chatgpt来批量的生成alignment的数据集，约52000个指令问答训练数据；如何让模型自举生成数据参考<a href="https://zhuanlan.zhihu.com/p/615180155">链接</a></li><li>③实验效果与GPT-3.5相当。</li><li>Web Demo： <a href="https://crfm.stanford.edu/alpaca/">https://crfm.stanford.edu/alpaca/</a></li></ul><p>如果想自己尝试的话是推荐先用这个，比较稳妥。但是就以我自己测试下来的情况看，7B和13B的Alpaca在复杂问题的情况下效果距离gpt-3.5的api效果都还差很远，当然这并不意味着Alpaca上限就到这了，它用的数据集目前还是偏小，如果继续优化，还是有追上gpt-3.5的机会的，毕竟LlaMA论文里的测试结果是13B能单防gpt-3。</p><p>Chinese-LLaMA-Alpaca 开源了中文LLaMA模型和经过指令精调的Alpaca大模型。这些模型在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练，进一步提升了中文基础语义理解能力。同时，在中文LLaMA的基础上，本项目使用了中文指令数据进行指令精调，显著提升了模型对指令的理解和执行能力。但是效果一般…</p><h3 id="alpaca-lora"><a href="#alpaca-lora" class="headerlink" title="alpaca-lora"></a>alpaca-lora</h3><p>alpaca-lora是斯坦福大学的另一个巨作，其使用LoRA（low-rank adaptation）技术复现了Alpaca的结果，用了一个更加低成本的方法，只在一块RTX 4090显卡上训练5个小时得到了一个Alpaca水平相当的模型。而且，该模型可以在树莓派上运行。在该项目中，其使用了Hugging Face的PEFT来实现廉价高效的微调。PEFT 是一个库（LoRA 是其支持的技术之一），可以让你使用各种基于 Transformer的语言模型并使用LoRA对其进行微调，从而使得在一般的硬件上廉价而有效地微调模型。该项目github地址是：</p><p> <a href="https://github.com/tloen/alpaca-lora">https://github.com/tloen/alpaca-lora</a></p><p>尽管 Alpaca和alpaca-lora取得了较大的提升，但其种子任务都是英语，缺乏对中文的支持。一方面除了以上提到Belle收集到了大量的中文语料，另一方面基于alpaca-lora等前人工作，来自华中师范大学等机构的三位个人开发者开源的中文语言模型骆驼 (Luotuo)，单卡就能完成训练部署。目前该项目释放了两个模型 luotuo-lora-7b-0.1、luotuo-lora-7b-0.3，还有一个模型在计划中。其github地址是：</p><p> <a href="https://github.com/LC1332/Chinese-alpaca-lora">https://github.com/LC1332/Chinese-alpaca-lora</a></p><h3 id="ChatLLaMA"><a href="#ChatLLaMA" class="headerlink" title="ChatLLaMA"></a>ChatLLaMA</h3><p>ChatLLaMA 是一个完整的开源实现，能够基于预训练的 LLaMA（7B、13B、33B、65B）构建 ChatGPT 样式的服务。它内置了对 DeepSpeed ZERO 的支持，训练、微调和单 GPU 推理更快、成本更低。</p><details class="lake-collapse"><summary id="u87654202"><span class="ne-text" style="color: rgb(51, 51, 51)">ChatLLaMA 使用指南</span></summary><pre data-language="python" id="U2Tu1" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959"># the algorithmic implementation for RLHF training process of LLaMAfrom chatllama.rlhf.trainer import RLTrainerfrom chatllama.rlhf.config import Configpath = "path_to_config_file.yaml"config = Config(path=path)trainer = RLTrainer(config.trainer)trainer.distillate()trainer.train()trainer.training_stats.plot()</pre><p id="u704f7ecd" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text"></span></p><pre data-language="python" id="xPIST" class="ne-codeblock language-python" style="border: 1px solid #e8e8e8; border-radius: 2px; background: #f9f9f9; padding: 16px; font-size: 13px; color: #595959"># pip install chatllama-py# download the llama model weights and tokenizer.# 1 - YAML downloadwget -O artifacts.zip https://nbllabartifacts.blob.core.windows.net/chatllama/artifacts.zip\?sp\=r\&amp;st\=2023-03-08T14:53:24Z\&amp;se\=2100-03-08T22:53:24Z\&amp;spr\=https\&amp;sv\=2021-06-08\&amp;sr\=b\&amp;sig\=jqr%2B2ZkR0SW9RjV0pDOdQ%2BDulLXLjbZ36vmNd4XxxyQ%3Dunzip artifacts.zip# 2 - Dataset preparation# download the actor_training_data and the rlhf_training_data:python artifacts/download_dataset.py SHP --path ./datasets --number_of_samples 200# create the reward_training_data using davinci-003 for synthetic data generation.export OPENAI_API_KEY=YOUR_API_KEYpython artifacts/generate_rewards.py ./datasets/reward_training_data.json --model gpt-3.5-turbo//text-davinci-003# 3 - Training# Train the Reward Modelpython artifacts/main.py artifacts/config/config.yaml --type REWARD# Pre-Train the Actor Modelpython artifacts/main.py artifacts/config/config.yaml --type ACTOR# Training the Actor with reinforcement learning.python artifacts/main.py artifacts/config/config.yaml --type RL# Note that the path to the datasets and the training hyper-parameters of the training process are specified in the config.yaml file.</pre><p id="ube703d60" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"></p></details><h3 id="ChatGLM"><a href="#ChatGLM" class="headerlink" title="ChatGLM"></a>ChatGLM</h3><p>ChatGLM是清华智谱AI开源的GLM系列的对话模型，支持中英两个语种，目前开源了其62亿参数量的模型 <a href="https://github.com/THUDM/ChatGLM-6B">ChatGLM-6B</a>。其继承了GLM之前的优势，在模型架构上进行了优化，从而使得部署和应用门槛变低，实现大模型在消费级显卡上的推理应用。目前训练代码未放出，还在内测阶段，内测申请网址：<a href="https://chatglm.cn">chatglm.cn</a></p><ul><li>6G 显卡使用 int4 量化、8G 显卡使用 int8 量化、12G及以上显卡使用 fp16 完整版；</li><li>子涵在11机器上配了一套服务，可以<code>screen -r yzh</code>后<code>python cli_demo.py</code>体验；<h3 id="Dolly-2-0"><a href="#Dolly-2-0" class="headerlink" title="Dolly 2.0"></a>Dolly 2.0</h3>Dolly 1.0 在Alpaca的启发下，使用的是斯坦福大学 Alpaca 团队用 OpenAI API 创建的数据集，在GPT-J-6B上实现微调。但是，在 ChatGPT 的输出上训练的模型一直处于合法的灰色地带，知名指令遵循模型（Alpaca、Koala、GPT4All、Vicuna）都受到这种限制：禁止商业使用。</li></ul><p><strong>Dolly 2.0 是怎么诞生的？</strong>Dolly 2.0 是一个 120 亿参数的语言模型，它基于开源 EleutherAI pythia 模型系列，专门针对小型开源指令记录语料库进行了微调（databricks-dolly-15k），许可条款允许出于任何目的使用、修改和扩展，包括学术或商业应用。</p><p>「databricks-dolly-15k」数据集包含 15000 个高质量的人类生成的 prompt / 回复对，<strong>由 5000 多名 Databricks 员工在 2023 年 3 月和 4 月期间撰写</strong>，专门设计用于指令调优大型语言模型。这些训练记录自然、富有表现力，旨在代表广泛的行为，从头脑风暴、内容生成到信息提取和总结。</p><h3 id="BELLE"><a href="#BELLE" class="headerlink" title="BELLE"></a>BELLE</h3><p><a href="https://github.com/LianjiaTech/BELLE">belle</a>是基于BLOOMZ、LLama优化后的模型，针对中文做了优化，模型调优仅使用由ChatGPT生产的数据（不包含任何其他数据）。项目包括：</p><ol><li>数据开放：参考Stanford Alpaca 生成的中文数据集1M + 0.5M</li><li>基于BLOOMZ-7B1-mt优化后的模型：BELLE-7B-0.2M，BELLE-7B-0.6M，BELLE-7B-1M，BELLE-7B-2M</li><li>基于LLAMA优化后的模型：BELLE-LLAMA-7B-0.6M，BELLE-LLAMA-7B-2M</li><li>对以上模型进行量化后的轻量化模型，便于部署、推理。</li></ol><p>该项目要求开发者仅将开源的代码、数据、模型及后续用此项目生成的衍生物用于研究目的，不得用于商业！</p><p>理论上BLOOM的多语言能力是要比LLaMA好一些的，不过我测BELLE的时候发现了一个比较搞笑的情况，我在测完一些其他case之后感觉还不错，但是我突发奇想把我测英文模型的一些case机翻过来喂给BELLE，结果BELLE就开始吐中英文混杂的句子了。当然这个问题可能也不是BELLE独有的，我还没在其他中文模型上测过。如果直接拿去应用的时候出这个问题还是挺要命的，后续可以跟进看看。</p><h3 id="OpenChatKit"><a href="#OpenChatKit" class="headerlink" title="OpenChatKit"></a>OpenChatKit</h3><p>OpenChatKit由前OpenAI研究员所在的Together团队，以及LAION、Ontocord.ai团队共同打造。它的设计我觉得是很有希望的，用GPT-3的开源版本GPT-NoX-20B加上自己的数据集进行微调。同时，不同ChatGPT的强化学习，OpenChatKit采用一个60亿参数的审核模型，对不合适或者是有害的信息进行过滤，确保生成内容的安全和质量。证书是APACHE LICENSE 2.0， 真的成了直接就可以用，不受任何限制。</p><h3 id="Vicuna和Chinese-Vicuna"><a href="#Vicuna和Chinese-Vicuna" class="headerlink" title="Vicuna和Chinese-Vicuna"></a>Vicuna和Chinese-Vicuna</h3><p>斯坦福学者继推出alpaca后，联手CMU、UC伯克利等，推出一个全新模型 Vicuna-13B（俗称小羊驼、骆马），通过使用从 ShareGPT.com 收集的大约 70K 用户共享对话微调 LLaMA 基础模型创建的。<br>测试过程使用GPT-4作为评判标准，与 Alpaca 相比，<a href="https://vicuna.lmsys.org/">Vicuna</a> 能够生成更详细、结构更合理的答案（参见下面的示例），并且在超过90%的情况下实现了与ChatGPT相匹敌的能力。</p><p>UC伯克利LMSys org近期又发布了70亿参数的 Vicuna-7B，不仅体积小、效率高、能力强，而且只需两行命令就能在M1/M2芯片的Mac上运行，还能开启GPU加速！<br>github开源地址为：<a href="https://github.com/lm-sys/FastChat/">https://github.com/lm-sys/FastChat/</a></p><p>另一个中文版的进行了开源Chinese-Vicuna ，github地址为：<br> <a href="https://github.com/Facico/Chinese-Vicuna">https://github.com/Facico/Chinese-Vicuna</a></p><h3 id="gpt4all"><a href="#gpt4all" class="headerlink" title="gpt4all"></a>gpt4all</h3><p>基于 LLaMa 的 LLM 助手，提供训练代码、数据和演示，训练一个自己的 AI 助手。</p><p><img src="开源ChatGPT平替项目汇总/1681373310839.png" alt="image.png"></p><h3 id="LMFLOW"><a href="#LMFLOW" class="headerlink" title="LMFLOW"></a>LMFLOW</h3><p>低成本效仿ChatGPT，LMFlow 项目由香港科技大学统计和机器学习实验室团队发起，致力于让使用者针对专有领域支持个性化训练。例如LLaMA-7B，一张3090耗时 5 个小时即可完成训练，成本大幅降低。<br>该项目还开放了网页端即刻体验问答服务 (lmflow.com)。LMFlow的出现和开源使得普通资源可以训练问答、陪伴、写作、翻译、专家领域咨询等各种任务。目前很多研究者们正在尝试用该项目训练650亿甚至更高参数量的大模型。</p><h3 id="Lit-LLaMA"><a href="#Lit-LLaMA" class="headerlink" title="Lit-LLaMA"></a>Lit-LLaMA</h3><p>Lightning-AI 基于nanoGPT的LLaMA语言模型的实现。支持量化，LoRA微调，预训练。<br>目前还处于概念阶段，该项目的想法是把LLaMA的设计用自己的代码重写一边，这样绕过它的代码证书，再训练出一个完全开源版本的LLaMA。我觉得这个思路不错，但是是骡子是马还是得真的结果搞出来以后测了才知道</p><h3 id="StackLLaMA"><a href="#StackLLaMA" class="headerlink" title="StackLLaMA"></a>StackLLaMA</h3><p>随着斯坦福Alpaca的出现，一大堆基于LLama的羊驼家族和扩展动物家族开始出现，终于Hugging Face研究人员近期发布了一篇博客StackLLaMA：用RLHF训练LLaMA的实践指南。同时也发布了一个70亿参数的模型——StackLLaMA。这是一个通过人类反馈强化学习在LLaMA-7B微调而来的模型。详细见其博客地址：<br> <a href="https://huggingface.co/blog/stackllama">https://huggingface.co/blog/stackllama</a></p><h3 id="关于证书"><a href="#关于证书" class="headerlink" title="关于证书"></a>关于证书</h3><p>这里不得不提一下Llama的证书，分代码和模型权重两部分，它的代码是GPL 3.0的，但是权重另是一个特殊的证书，任何人不得随意发布，只允许学术用途。这意味着什么呢？直接拿Llama权重来商用肯定是不行，而且拿出来给别人下载都是不行的，已经有git上的仓库吃了DMCA被删掉了。如果使用它的代码复现，根据GPL 3.0，所有后续的项目也必须使用GPL 3.0。这一点对于很多公司乃至个人来说都是不太好接受的。这也是我为什么比较期待基于其他基干模型的方案，因为不设法绕开LLaMA(或者重新训一个开源出来），很容易后续做一些事情的时候被卡住。<br>说一点闲话，有的朋友可能说，那META也是秉承了CloseAI，不是，OpenAI的思想，怕别人用这个模型来干坏事才加了限制。但是仔细想一想，真的拿这玩意来干坏事的人，根本就不在乎你用啥证书，他搞到你的权重直接用就完事了。卡的还是想正经做点项目的人，说到这个我前两天看到一个好玩的，point-alpaca这个项目：<br><a href="https://link.zhihu.com/?target=https%3A//github.com/pointnetwork/point-alpaca">https://github.com/pointnetwork/point-alpaca</a><br>它是干什么的呢，它把整个LLaMA用alpaca的数据集训练了3个epoch（对此是否真的比lora更有效我持保留态度，我也没试）。比较好玩的是它说LLaMA的权重是META的，我们不能随便发布，那我怎么发布我的模型呢？于是它说，那我不发布整个模型，我发布我训出来的模型权重，和原版LLaMA的差，不就得了吗？<br>用户拿到原版LLaMA,加上我们这个差，就可以获得我们训过的版本啦（瞧这费劲的）</p><h3 id="一些想法"><a href="#一些想法" class="headerlink" title="一些想法"></a>一些想法</h3><p>其实就算是目前我觉得还是有很多事情需要社区加大力度，比方说</p><ul><li>规范针对类InstructGPT的评估标准，打出GPT-3.5和GPT-4的分数，量化开源模型的效果（实在没有人力可以干脆让GPT-4出分），目前很多开源模型只是给出模型对一些答案的示例，这样很难分辨实际效果</li><li>整理目前开源的数据集，不同项目间互通有无，目前看改善数据集还是能有一些提升</li><li>重炼/超越LLaMA，整出开源版。我相信这个其实已经有人在做了，我希望大佬能够加大力度。</li></ul><p>其实需要人做的有价值的事情还有很多。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于知识库+ChatGPT搭建问答机器人</title>
    <link href="/2023/04/06/2023-04-06-%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%BA%93+ChatGPT%E6%90%AD%E5%BB%BA%E9%97%AE%E7%AD%94%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    <url>/2023/04/06/2023-04-06-%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%BA%93+ChatGPT%E6%90%AD%E5%BB%BA%E9%97%AE%E7%AD%94%E6%9C%BA%E5%99%A8%E4%BA%BA/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://gist.github.com/xinqiu">xinqiu</a>/<a href="https://gist.github.com/xinqiu/521c383665e24a3de01a5cd799d5aca5">bot.py</a></p><p><a href="https://twitter-thread.com/t/1637267884786196481">基于embedding和ChatGPT的文档检索原理介绍 - 宝玉</a></p><p><a href="https://www.yeeach.com/post/2455">基于垂直行业知识库和ChatGPT搭建行业问答机器人的技术架构</a></p></blockquote><p>目前建立自己的知识库问答机器人，流行的做法有两种：</p><ol><li>一种是利用文本 embedding 的相似度搜索，构建出context，利用zero/few shot learning的方式让LLM进行Q&amp;A；</li><li>一种是使用私有数据集进行finetune，从而得到理解私有数据的LLM专属模型。</li></ol><h2 id="向量相似度搜索-chatgpt重排序"><a href="#向量相似度搜索-chatgpt重排序" class="headerlink" title="向量相似度搜索 + chatgpt重排序"></a>向量相似度搜索 + chatgpt重排序</h2><blockquote><p>一种是 <a href="https://m.okjike.com/originalPosts/6407c9291f1113c2b7c7d076?s=eyJ1IjoiNTY3YTUwZDQ2ZWY4OWMxMjAwOGE3NTc1In0%3D">m.okjike.com</a> 利用文本embedding 的相似度搜索，构建出context，利用zero/few shot learning的方式让LLM进行Q&amp;A. </p></blockquote><p><img src="2023-04-06-基于知识库+ChatGPT搭建问答机器人/1681453213713-9edf89b7-0921-4978-bac9-07004945093c.png" alt="image.png"></p><p><strong>整体流程：</strong></p><ol><li>将垂直行业的领域知识向量化，并存入向量数据库</li><li>用户提问向量化</li><li>查询向量数据库，得到TopN条匹配知识</li><li>构建Prompt，调用OpenAI API实现精排</li><li>返回回答</li></ol><h3 id="1、Embeddgings-model选择"><a href="#1、Embeddgings-model选择" class="headerlink" title="1、Embeddgings model选择"></a>1、Embeddgings model选择</h3><p>要将领域知识向量化，需要有Embeddings model，最简单的方案是使用OpenAI的Embeddings API（模型选用的是 text-embedding-ada-002）。更多embedding 模型的内容请参考 <a href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings">what-are-embeddings</a><br>由于OpenAI的Embeddings model是通用模型，对垂直行业并不是最适合的，会出现回答不准确的情况。如果数据量较大，需要反复调用Embeddgins API，效率较低、成本较高。</p><p>可以考虑自己基于知识库自训练或基于一些现成finetuned的模型，HuggingFace上有很多Embeddings model可供参考使用。</p><ol><li>直接使用现成finetune模型：<a href="https://t.co/y3Py8CTep8">Chinese-LLaMA-Alpaca</a>、<a href="https://t.co/nwMmjnMdXT">Chinese-Vicuna</a></li><li>基于知识库自训练Embedding模型，主要基于 <a href="https://t.co/zX67FyFZ52">https://github.com/tatsu-lab/stanford_alpaca…</a> 及一系列产物，比如 <a href="https://t.co/tAzzVXrut3">alpaca.cpp</a>、<a href="">alpaca-lora</a>、<a href="https://t.co/uNzZ63uZeN">FastChat</a>、<a href="https://t.co/sdoBHx5QJk">gpt4all</a> 等。这些都是利用GPT3.5/GPT4作为数据打标师，从而一方面调优Meta的LLaMA，另一方面顺着这样的思路也能进行私有数据的finetune</li></ol><ul><li>也可以使用其他方案，如fasttext/simbert等。推荐项目 <a href="https://github.com/shibing624/text2vec">text2vec</a> ，shibing624 已经给出了一个模型基于 CoSENT + MacBERT +STS-B，<a href="https://huggingface.co/shibing624/text2vec-base-chinese">shibing624/text2vec-base-chinese</a>。</li></ul><p><strong>上述两种做法对比：</strong></p><ol><li>做法一由于没有 fine-tune，所以成本就只有每次问答的成本，但由于大部分时候需要带大量的 prompts，其实隐性的成本是挺高的，我们经常可以看到一个简短的问题会带有上千个字符的 prompts。</li><li>而做法二 fine-tune 模型的训练成本是较高的，但一旦获得专属模型后，后续的聊天中就不再需要做预检索和背景知识的 prompts 了，可以直接进行关于该领域的知识问答。不仅 token 消耗会变小，而且速度也会更快。</li></ol><h3 id="2、向量数据库选择"><a href="#2、向量数据库选择" class="headerlink" title="2、向量数据库选择"></a>2、向量数据库选择</h3><p>向量数据库在相似文本搜索、个性化推荐、相似图片搜索等都有很好的应用场景。如果你数据不大，存成csv文件，然后加载到内存，借助内存搜索就 OK 了。但如果数据比较多时，就需要借助向量数据库来进行搜索。</p><p>开源的向量数据库有qdrant，weaviate，milvus，elasticsearch等，推荐qdrant。</p><ul><li>qdrant对常用的向量数据库的测试报告：<a href="https://qdrant.tech/benchmarks/">https://qdrant.tech/benchmarks/</a></li><li>（<a href="https://www.yuque.com/ningshixian/pz10h0/saa0yx">向量检索库总结·语雀</a>）</li></ul><h3 id="3、LLM框架"><a href="#3、LLM框架" class="headerlink" title="3、LLM框架"></a>3、LLM框架</h3><p>LangChain及LlamaIndex (原_GPT Index_)  这样的LLM框架，封装了很多LLM的工具，可以极大程度提升与LLM的集成效率。</p><p>LlamaIndex (原GPT Index) 入门门槛更低，入门文档也写得比较详尽。LangChain更为强大灵活，qdrant对LangChain的集成更好。二者的比较可以参考 <a href="https://news.ycombinator.com/item?id=34568343">https://news.ycombinator.com/item?id=34568343</a></p><p>选用Langchain的话，可以按照文档<a href="https://t.co/qlRZcll7UG">https://python.langchain.com/en/latest/use_cases/question_answering.html…</a>，从文本Embedding-&gt;存储VectorDB-&gt;Similarity Search-&gt;Q&amp;A with Context。</p><h3 id="4、调用OpenAI，构建的Prompt模板"><a href="#4、调用OpenAI，构建的Prompt模板" class="headerlink" title="4、调用OpenAI，构建的Prompt模板"></a>4、调用OpenAI，构建的Prompt模板</h3><p>为了能够借助 ChatGPT 优化回答内容的整体结构，需要精心构造 prompt上下文。</p><p>System：你是一个 XXX 的机器人，请使用下面提供的文本尽可能如实地回答问题，如果答案未包含在下面的文本中，请说“我不知道”</p><p>Context：向量数据库搜索结果的 TopN 知识的拼接<br>Q：用户提问</p><p>GPT3.5之后的接口支持指定role，可以将相关系统角色的prompt放在“system”中，CONTEXT 和 USER QUESTION 放到“user”中；<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>&#125;<br>&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Who won the world series in 2020?&quot;</span>&#125;<br><span class="hljs-comment"># assistant用来存储先前的回复。这是为了持续对话，提供会话的上下文。</span><br>&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;The Los Angeles Dodgers.&quot;</span>&#125;<br>&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Where was it played?&quot;</span>&#125;<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>ChatGPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一键部署私人 ChatGPT 网页应用</title>
    <link href="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/"/>
    <url>/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="开源项目"><a href="#开源项目" class="headerlink" title="开源项目"></a>开源项目</h1><p>我试用了这两个 ChatGPT Web 端都可以一键发布到 Vercel，在 Cloudflare 上配置好域名，都不需要翻墙来使用，方便搭建分享给身边不方便上网的朋友们使用。</p><h2 id="ChatGPT-Next-Web-9-4K-star"><a href="#ChatGPT-Next-Web-9-4K-star" class="headerlink" title="ChatGPT Next Web (9.4K star)"></a><a href="https://github.com/Yidadaa/ChatGPT-Next-Web">ChatGPT Next Web</a> (9.4K star)</h2><ul><li>在 1 分钟内使用 Vercel <strong>免费一键部署</strong></li><li>精心设计的 UI，响应式设计，支持深色模式</li><li>极快的首屏加载速度（~85kb）</li><li>海量的内置 prompt 列表，来自<a href="https://github.com/PlexPt/awesome-chatgpt-prompts-zh">中文</a>和<a href="https://github.com/f/awesome-chatgpt-prompts">英文</a></li><li>自动压缩上下文聊天记录，在节省 Token 的同时支持超长对话</li><li>一键导出聊天记录，完整的 Markdown 支持</li><li>拥有自己的域名？好上加好，绑定后即可在任何地方<strong>无障碍</strong>快速访问</li></ul><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680488791128-d5cb63e1-8f69-4070-a811-47ab10450686.png" class="" title="image.png"><h2 id="chatgpt-vercel-1-8K-star"><a href="#chatgpt-vercel-1-8K-star" class="headerlink" title="chatgpt-vercel (1.8K star)"></a><a href="https://github.com/ourongxing/chatgpt-vercel">chatgpt-vercel</a> (1.8K star)</h2><ul><li>基于 <a href="https://github.com/ddiu8081/chatgpt-demo">chatgpt-demo</a> 开发</li><li>修改 .env 文件中的 api_key 即可；</li></ul><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680489066235-d4ecb09c-f8ea-4525-996e-0a421b9302cf.png" class="" title="image.png"><hr><h1 id="Vercel应用绑定自己的域名"><a href="#Vercel应用绑定自己的域名" class="headerlink" title="Vercel应用绑定自己的域名"></a>Vercel应用绑定自己的域名</h1><blockquote><p><a href="https://tangly1024.com/article/vercel-domain">https://tangly1024.com/article/vercel-domain</a></p></blockquote><h2 id="准备一个域名"><a href="#准备一个域名" class="headerlink" title="准备一个域名"></a>准备一个域名</h2><p>首先购买自己的域名，您可以选择以下渠道进行购买：</p><ul><li><a href="https://www.namesilo.com/">Namesilo</a></li><li><a href="https://www.godaddy.com/zh-sg/offers/godaddy">Godaddy</a></li></ul><p>另外您还可以选择从以下平台获取到免费的域名：</p><ul><li><a href="https://freenom.com/">Freenom</a></li><li><a href="https://zhuanlan.zhihu.com/p/538005306">eu.org</a></li></ul><p>eu.org是欧盟组织的免费域名；需要英国的用户信息才能注册;</p><h3 id="首推Namesilo"><a href="#首推Namesilo" class="headerlink" title="首推Namesilo"></a>首推Namesilo</h3><p>我早先在腾讯云购买的域名，但奈何无耻的价格套路，最后选择迁入<a href="https://www.namesilo.com/">Namesilo</a>。</p><ul><li><strong>价格便宜稳定，无套路</strong></li></ul><p>Namesilo 域名本身价格就比较便宜，COM 域名 $8.99/年，除了一个一美元优惠码，基本没有其他大幅度促销活动，域名续费和首年购买价格一致。<br>套路指那些首年极其便宜，但次年续费很贵的。如 Godaddy，首年 $0.99，次年续费要 102 元人民币，而且隐私保护还需要额外再加 60 元。</p><p>Namesilo 价格表：<a href="https://link.zhihu.com/?target=https%3A//www.namesilo.com/pricing.php%3Frid%3Dd27fa32do">Domain Pricing</a></p><blockquote><p>💡填入我的优惠码 <strong>tangly1024 </strong>可以享受 1$的 优惠。</p><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492617401-ddc54b77-c16f-43d5-898f-d79a56865984.png" class="" title="image.png"></blockquote><ul><li><strong>永久免费的隐私保护</strong></li></ul><p>Namesilo 提供永久免费的域名隐私保护，防止别人通过 WHOIS 查询获取域名所有者的个人注册信息。作为对比，Godaddy 的隐私保护是 60 元/年，Namecheap 是免费提供第一年。</p><ul><li><strong>安全性高</strong></li></ul><p>支持账户登陆二次验证和 Domain Defender，保护账户和域名安全。登陆、解锁域名等，都可以设置邮件或短信提醒。</p><ul><li><strong>支付方便</strong></li></ul><p>支持支付宝、Paypal、信用卡等多种方式付款。</p><h2 id="Vercel控制台配置"><a href="#Vercel控制台配置" class="headerlink" title="Vercel控制台配置"></a>Vercel控制台配置</h2><ul><li>在Vercel控制面板中找到Setting→Domains→Add，输入域名</li></ul><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492617418-b41172c4-573e-4c86-a93b-5d805e5b0369.png" class="" title="image.png"><ul><li>Add之后，如果看到下图中的提示（Invalid Configuration）👇,说明要是这个域名解析生效，需要在你的域名商管理后台配置对应的Cname解析。</li></ul><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492617392-d9221dc9-4778-4b34-a116-581e0073d64f.png" class="" title="image.png"><h2 id="配置CNAME域名解析（CloudFlare）"><a href="#配置CNAME域名解析（CloudFlare）" class="headerlink" title="配置CNAME域名解析（CloudFlare）"></a>配置CNAME域名解析（CloudFlare）</h2><p>以下分多个平台举例，点击展开：</p><p>CloudFlare (首选推荐)CF具有Worker.js、全球无限CDN流量、网站防火墙、DDoS等特性，我个人比较喜欢将域名解析交给CloudFlare管理 。</p><p>这里主要用CF举例，打开CloudFlare打开域名控制台 <a href="https://dash.cloudflare.com/">https://dash.cloudflare.com</a></p><ul><li><strong>如果你的域名没有绑定过任何一条A记录，则添加一条域名A记录指向Vercel服务器地址76.76.21.21</strong></li></ul><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492616670-2c28963a-5aa3-4a02-9c84-65486750a491.png" class="" title="image.png"><ul><li>根据需要添加你的一条CNAME记录，值指向Vercel的CNAME服务器：cname-china.vercel-dns.com ;（例如我这里用的是二级域名 <a href="http://hexo.tangly1024.com/">hexo.tangly1024.com</a> ）</li></ul><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492617321-9c644c5a-0fa5-4f79-95c0-c49cf215b396.png" class="" title="image.png"><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492617868-94f03fd7-989a-47f3-9fcc-8aadefc0d0c4.png" class="" title="image.png"><ul><li>这里需要在ssl/tls配配置开启https加密</li></ul><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492619992-8cf59e14-2fe6-43dc-b40b-016f9507b628.png" class="" title="image.png"><p>腾讯云托管域名<a href="https://console.dnspod.cn/dns/list">https://console.dnspod.cn/dns/list</a> 点击域名进行配置，添加一条CNAME 转发</p><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492619973-617f22b3-d8f9-4d87-95f8-2a24f5993e32.png" class="" title="image.png"><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492619949-d77a4b54-82ba-458a-85e2-9dadb40a2121.png" class="" title="image.png"><p>阿里云托管域名打开控制台 <a href="https://dc.console.aliyun.com/#/domain-list/all">https://dc.console.aliyun.com/#/domain-list/all</a> 域名列表，点击域名，并添加一条cname转发</p><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492620020-7ad1e627-39c4-4181-b331-cf7bedf43223.png" class="" title="image.png"><p>Freenom 购买的域名建议将域名的Nameservers 托管给 CloudFlare，然后在CloudFlare中设置</p><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492620168-7ce06520-5948-4c86-8490-dad84ebfa359.png" class="" title="image.png"><h2 id="完成"><a href="#完成" class="headerlink" title="完成"></a>完成</h2><p>如果域名服务配置正常，Vercel的卡片会变成如图的样子：<br><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492621225-2e855386-11fa-4de1-98e4-244506b19ad3.png" class="" title="image.png"></p><blockquote><p>接下来就可以通过设置的域名（如 <a href="https://hexo.tangly1024.com/">https://hexo.tangly1024.com</a> ）访问我们的网站了</p></blockquote><h2 id="关于根域名的配置"><a href="#关于根域名的配置" class="headerlink" title="关于根域名的配置"></a>关于根域名的配置</h2><p>以上举例使用的是二级域名，如果你想像我一样使用 <a href="https://tangly1024.com/">https://tangly1024.com</a> 这样的根域名，配置参考如下：</p><h3 id="Vercel后台"><a href="#Vercel后台" class="headerlink" title="Vercel后台"></a>Vercel后台</h3><p>直接添加根域名后，vercel会提示需要添加一条@记录指向vercel的服务器 76.76.21.21</p><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492621469-6e74d7f2-68fb-4500-97a7-bf28aba053a1.png" class="" title="image.png"><h3 id="域名管理后台"><a href="#域名管理后台" class="headerlink" title="域名管理后台"></a>域名管理后台</h3><p>直接将根域名 解析到 vercel的服务器地址即可 ： 76.76.21.21</p><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492621872-3b800f7e-61c4-4dd9-bcaa-966d2502108b.png" class="" title="image.png"><h3 id="完成效果"><a href="#完成效果" class="headerlink" title="完成效果"></a>完成效果</h3><img src="/2023/04/03/2023-04-03-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E7%A7%81%E4%BA%BA%20ChatGPT%20%E7%BD%91%E9%A1%B5%E5%BA%94%E7%94%A8/1680492621914-45d5b128-6117-44fc-9d46-dfc65e8b02a1.png" class="" title="image.png"><h2 id="Vercel域名在国内被墙"><a href="#Vercel域名在国内被墙" class="headerlink" title="Vercel域名在国内被墙"></a>Vercel域名在国内被墙</h2><p>vercel.app因为被大量使用，自然而然被墙掉了，不过好在 Vercel 官方提供了单独的 IP 和 CNAME 地址给大家，对于国内的用户来说，配置一下单独的解析，依然可以享受 Vercel 提供的服务。</p><p>将上述步骤中用到的 ip和 cname地址替换成以下内容即可：</p><ul><li>A记录地址：76.223.126.88 或 76.76.21.98 等</li><li>CNAME 记录地址：cname-china.vercel-dns.com</li></ul><h3 id="扩展阅读-A记录和CNAME的区别"><a href="#扩展阅读-A记录和CNAME的区别" class="headerlink" title="扩展阅读 A记录和CNAME的区别"></a>扩展阅读 A记录和CNAME的区别</h3><p>A记录就是把一个域名解析到一个IP地址（Address，特指数字IP地址）；</p><p>CNAME记录就是把域名解析到另外一个域名。</p><p>其功能差不多，CNAME将几个主机名指向一个别名，其实跟指向IP地址是一样的，因为这个别名也要做一个A记录的。</p>]]></content>
    
    
    <categories>
      
      <category>ChatGPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ChatGPT 私人部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🇨🇳中文NLP常用开源库整理</title>
    <link href="/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87NLP%E5%B8%B8%E7%94%A8%E5%BC%80%E6%BA%90%E5%BA%93%E6%95%B4%E7%90%86/"/>
    <url>/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87NLP%E5%B8%B8%E7%94%A8%E5%BC%80%E6%BA%90%E5%BA%93%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://github.com/crownpku/Awesome-Chinese-NLP">https://github.com/crownpku/Awesome-Chinese-NLP</a><br><a href="https://github.com/fighting41love">fighting41love</a>/<a href="https://github.com/fighting41love/funNLP">funNLP</a></p></blockquote><h2 id="Toolkits-综合NLP工具包"><a href="#Toolkits-综合NLP工具包" class="headerlink" title="Toolkits 综合NLP工具包"></a>Toolkits 综合NLP工具包</h2><p>中文：</p><ul><li><a href="http://thulac.thunlp.org/">THULAC 中文词法分析工具包</a> (⭐️1.8K) by 清华 (C++/Java/Python)</li><li><a href="https://github.com/baidu/lac">BaiduLac</a> (⭐️3.4K)  by 百度，支持分词，词性标注，命名实体识别，词重要性</li><li><a href="https://github.com/hankcs">hankcs</a>/<a href="https://github.com/hankcs/HanLP">HanLP</a> (⭐️28.5K)  面向生产环境的多语种自然语言处理工具包</li><li><a href="https://github.com/isnowfy/snownlp">SnowNLP</a> (⭐️6K) ，支持中文分词、词性标注、转换成拼音、TextRank算法、BM25等</li><li><a href="https://github.com/rsanshierli/EasyBert">EasyBert</a>，基于Pytorch的Bert应用，包括命名实体识别、情感分析、文本分类以及文本相似度等</li></ul><p>英文：</p><ul><li><a href="https://github.com/stanfordnlp/stanza">Stanza</a> by Stanford (Python) A Python NLP Library for Many Human Languages</li><li><a href="http://www.nltk.org/">NLTK</a> (Python) Natural Language Toolkit</li><li><a href="https://spacy.io/">spaCy</a> (Python) Industrial-Strength Natural Language Processing with a <a href="https://course.spacy.io/">online course</a></li><li><a href="https://github.com/jbesomi/texthero">texthero</a> Text preprocessing, representation and visualization from zero to hero.</li><li><a href="https://ningshixian.github.io/2018/11/05/allennlp%E5%AE%89%E8%A3%85/">AllenNLP</a> 一个基于 PyTorch 构建的 Apache 2.0 NLP 研究库，用于在各种语言任务上开发最先进的深度学习模型。</li></ul><h2 id="中文分词工具包"><a href="#中文分词工具包" class="headerlink" title="中文分词工具包"></a>中文分词工具包</h2><ul><li><a href="https://github.com/fxsjy/jieba">Jieba 结巴中文分词</a> (Python及大量其它编程语言衍生) 做最好的 Python 中文分词组件</li><li><a href="https://github.com/lancopku/pkuseg-python">北大中文分词工具</a> (Python) 高准确度中文分词工具，简单易用，跟现有开源工具相比大幅提高了分词的准确率。</li></ul><h2 id="信息提取工具包"><a href="#信息提取工具包" class="headerlink" title="信息提取工具包"></a>信息提取工具包</h2><ul><li><a href="https://github.com/crownpku/Information-Extraction-Chinese">Information-Extraction-Chinese</a> Chinese Named Entity Recognition with IDCNN/biLSTM+CRF, and Relation Extraction with biGRU+2ATT 中文实体识别与关系提取</li><li><a href="https://github.com/baidu/Familia">Familia</a> 百度出品的 A Toolkit for Industrial Topic Modeling，可用于语义表示和语义匹配</li></ul><p><strong>关键短语挖掘库：</strong></p><ul><li><a href="https://github.com/letiantian/TextRank4ZH">TextRank4ZH</a> 从中文文本中自动提取关键词和摘要</li><li><a href="https://github.com/blmoistawinde/HarvestText#%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96">HarvestText</a> 作者对比测试优于上者（仅限关键词抽取）</li><li><a href="https://github.com/dongrixinyu/chinese_keyphrase_extractor">JioNLP</a>: 在 tfidf 方法提取的碎片化的关键词（默认使用 pkuseg 的分词工具）基础上，将在文本中相邻的关键词合并，并根据权重进行调整，同时合并较为相似的短语，并结合 LDA 模型，寻找突出主题的词汇，增加权重，组合成结果进行返回。 <a href="https://github.com/baidu/lac"> </a></li><li><a href="https://github.com/baidu/lac">LAC</a> (paddlepaddle &gt;=2.0、LAC&gt;=2.1)  + DDParser </li></ul><p><strong>文本摘要：</strong></p><ul><li><a href="https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy">基于命名实体识别的简历自动摘要</a> </li><li><a href="https://github.com/Hellisotherpeople/CX_DB8">基于BERT等最新语言模型的抽取式摘要提取</a>        </li><li><a href="https://mp.weixin.qq.com/s/gDZyTbM1nw3fbEnU--y3nQ">Python利用深度学习进行文本摘要的综合指南</a>        </li><li><a href="https://github.com/theamrzaki/text_summurization_abstractive_methods">(Colab)抽象文本摘要实现集锦</a></li></ul><h2 id="QA-amp-Chatbot-工具包"><a href="#QA-amp-Chatbot-工具包" class="headerlink" title="QA &amp; Chatbot 工具包"></a>QA &amp; Chatbot 工具包</h2><ul><li><a href="https://github.com/RasaHQ/rasa_nlu">Rasa NLU</a> (Python) turn natural language into structured data, a Chinese fork at <a href="https://github.com/crownpku/Rasa_NLU_Chi">Rasa NLU Chi</a></li><li><a href="https://github.com/RasaHQ/rasa_core">Rasa Core</a> (Python) machine learning based dialogue engine for conversational software</li><li><a href="https://github.com/deepmipt/DeepPavlov">DeepPavlov</a> (Python) An open source library for building end-to-end dialog systems and training chatbots.</li><li><a href="https://github.com/gunthercox/ChatterBot">Chatterbot</a> (Python) ChatterBot is a machine learning, conversational dialog engine for creating chat bots.</li><li><a href="https://github.com/Doragd/Chinese-Chatbot-PyTorch-Implementation">Chinese-Chatbot-PyTorch-Implementation</a> 根据自己的语料训练出自己想要的聊天机器人，可以用于智能客服、在线问答、智能聊天等场景</li><li><a href="https://github.com/GaoQ1/rasa_chatbot_cn">rasa_chatbot_cn</a> 基于最新版本rasa搭建的对话系统</li></ul><h2 id="文本匹配开源库"><a href="#文本匹配开源库" class="headerlink" title="文本匹配开源库"></a>文本匹配开源库</h2><div class="table-container"><table><thead><tr><th>资源名（Name）</th><th>描述（Description）</th><th>链接</th></tr></thead><tbody><tr><td>句子、QA相似度匹配MatchZoo</td><td>文本相似度匹配算法的集合，包含多个深度学习的方法，值得尝试。</td><td><a href="https://github.com/NTMC-Community/MatchZoo">github</a></td></tr><tr><td>中文问题句子相似度计算比赛及方案汇总</td><td></td><td><a href="https://github.com/ShuaichiLi/Chinese-sentence-similarity-task">github</a></td></tr><tr><td>similarity相似度计算工具包</td><td>java编写,用于词语、短语、句子、词法分析、情感分析、语义分析等相关的相似度计算</td><td><a href="https://github.com/shibing624/similarity">github</a></td></tr><tr><td>中文词语相似度计算方法</td><td>综合了同义词词林扩展版与知网（Hownet）的词语相似度计算方法，词汇覆盖更多、结果更准确。</td><td><a href="https://github.com/yaleimeng/Final_word_Similarity">gihtub</a></td></tr><tr><td>Python字符串相似性算法库</td><td></td><td><a href="https://github.com/luozhouyang/python-string-similarity">github</a></td></tr></tbody></table></div><h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><ul><li><p>NeuralNLP-NeuralClassifier腾讯开源深度学习文本分类工具 <a href="https://github.com/Tencent/NeuralNLP-NeuralClassifier">github</a></p><h2 id="文本聚类"><a href="#文本聚类" class="headerlink" title="文本聚类"></a>文本聚类</h2></li><li><p>TextCluster短文本聚类预处理模块 Short text cluster <a href="https://github.com/RandyPen/TextCluster">github</a></p></li></ul><h2 id="文本数据增强"><a href="#文本数据增强" class="headerlink" title="文本数据增强"></a>文本数据增强</h2><div class="table-container"><table><thead><tr><th>资源名（Name）</th><th>描述（Description）</th><th>链接</th></tr></thead><tbody><tr><td>中文NLP数据增强（EDA）工具</td><td></td><td><a href="https://github.com/zhanlaoban/eda_nlp_for_Chinese">github</a></td></tr><tr><td>英文NLP数据增强工具</td><td></td><td><a href="https://github.com/makcedward/nlpaug">github</a></td></tr><tr><td>一键中文数据增强工具</td><td></td><td><a href="https://github.com/425776024/nlpcda">github</a></td></tr><tr><td>数据增强在机器翻译及其他nlp任务中的应用及效果</td><td></td><td><a href="https://mp.weixin.qq.com/s/_aVwSWuYho_7MUT0LuFgVA">link</a></td></tr><tr><td>NLP数据增广资源集</td><td></td><td><a href="https://github.com/quincyliang/nlp-data-augmentation">github</a></td></tr></tbody></table></div><h2 id="Learning-Materials-学习资料"><a href="#Learning-Materials-学习资料" class="headerlink" title="Learning Materials 学习资料"></a>Learning Materials 学习资料</h2><ul><li><a href="https://github.com/exacity/deeplearningbook-chinese">中文Deep Learning Book</a></li><li><a href="http://web.stanford.edu/class/cs224n/syllabus.html">Stanford CS224n Natural Language Processing with Deep Learning 2017</a></li><li><a href="https://github.com/oxford-cs-deepnlp-2017">Oxford CS DeepNLP 2017</a></li><li>[Course materials for Georgia Tech CS 4650 and 7650, “Natural Language”] (<a href="https://github.com/jacobeisenstein/gt-nlp-class">https://github.com/jacobeisenstein/gt-nlp-class</a>)</li><li><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a> by Dan Jurafsky and James H. Martin</li><li><a href="http://www.52nlp.cn/">52nlp 我爱自然语言处理</a></li><li><a href="http://www.hankcs.com/">hankcs 码农场</a></li><li><a href="https://github.com/Roshanson/TextInfoExp">文本处理实践课资料</a> 文本处理实践课资料，包含文本特征提取（TF-IDF），文本分类，文本聚类，word2vec训练词向量及同义词词林中文词语相似度计算、文档自动摘要，信息抽取，情感分析与观点挖掘等实验。</li><li><a href="https://github.com/Kyubyong/nlp_tasks">nlp_tasks</a> Natural Language Processing Tasks and Selected References</li><li><a href="https://github.com/zibuyu/research_tao">NLP研究入门之道</a> from清华刘知远老师</li><li><a href="https://chinesenlp.xyz/#/">Chinese NLP</a> Shared tasks, datasets and state-of-the-art results for Chinese Natural Language Processing</li></ul><h2 id="医疗自然语言处理"><a href="#医疗自然语言处理" class="headerlink" title="医疗自然语言处理"></a>医疗自然语言处理</h2><div class="table-container"><table><thead><tr><th>资源名（Name）</th><th>描述（Description）</th><th>链接</th></tr></thead><tbody><tr><td>中文医学NLP公开资源整理</td><td></td><td><a href="https://github.com/GanjinZero/awesome_Chinese_medical_NLP">github</a></td></tr><tr><td>spaCy 医学文本挖掘与信息提取</td><td></td><td><a href="https://github.com/NLPatVCU/medaCy">github</a></td></tr><tr><td>构建医疗实体识别的模型</td><td>包含词典和语料标注，基于python</td><td><a href="https://github.com/yixiu00001/LSTM-CRF-medical">github</a></td></tr><tr><td>基于医疗领域知识图谱的问答系统</td><td></td><td><a href="https://github.com/zhihao-chen/QASystemOnMedicalGraph">github</a></td></tr><tr><td>Chinese medical dialogue data 中文医疗对话数据集</td><td></td><td><a href="https://github.com/Toyhom/Chinese-medical-dialogue-data">github</a></td></tr><tr><td>一个大规模医疗对话数据集</td><td>包含110万医学咨询，400万条医患对话</td><td><a href="https://github.com/UCSD-AI4H/Medical-Dialogue-System">github</a></td></tr><tr><td>新冠肺炎相关数据</td><td>新冠及其他类型肺炎中文医疗对话数据集；清华大学等机构的开放数据源（COVID-19）</td><td><a href="https://www.aminer.cn/data-covid19/">github</a></td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>中文NLP库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🇨🇳中文预训练模型研究进展&amp;整理</title>
    <link href="/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95&amp;%E6%95%B4%E7%90%86/"/>
    <url>/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95&amp;%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=20915">https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=20915</a></p></blockquote><p>近两年,中文预训练模型受 到广大学者的关注并取得了一定的研究成果.为了阐明现有 的中文预训练模型,本节主要从以下６个方面对现有的预训练 模型进行分类,图３展示了典型的中文预训练模型的分类图. </p><ol><li>预训练模型的方法改进,主要包括掩码方式的转变、 位置编码的转变、LN 层的位置变化、MoE 层的使用、多粒度训练和其他改进. </li><li>融入外部信息的预训练,主要包括命名实体、知识图 谱、语言学知识和特定知识.</li><li>关于多模态融合的预训练模型. </li><li>侧重于高效计算的预训练,主要包括数据处理阶段、 预训练阶段以及技术优化. </li><li>指特定领域的预训练,主要包括对话系统和其他领域 的预训练模型. </li><li>介绍一些其他变体,主要侧重于典型的英文预训练模 型开源的中文版本.</li></ol><img src="/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95&%E6%95%B4%E7%90%86/1680254358588.png" class="" title="image.png"><h2 id="开源的中文预训练模型汇总"><a href="#开源的中文预训练模型汇总" class="headerlink" title="开源的中文预训练模型汇总"></a>开源的中文预训练模型汇总</h2><blockquote><p><a href="https://github.com/lonePatient">lonePatient</a>/<a href="https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models">awesome-pretrained-chinese-nlp-models</a></p></blockquote><h3 id="NLU系列"><a href="#NLU系列" class="headerlink" title="NLU系列"></a>NLU系列</h3><div class="table-container"><table><thead><tr><th>模型</th><th>版本</th><th>作者</th><th>源地址</th><th>应用领域</th></tr></thead><tbody><tr><td>ChineseBERT</td><td>base</td><td><a href="https://github.com/ShannonAI">ShannonAI</a></td><td><a href="https://github.com/ShannonAI/ChineseBert">github</a></td><td>通用</td></tr><tr><td>NEZHA-base</td><td>base</td><td><a href="https://github.com/huawei-noah">HUAWEI</a></td><td><a href="https://github.com/huawei-noah/Pretrained-Language-Model">github</a></td><td>通用</td></tr><tr><td>MacBERT-base</td><td>base</td><td><a href="https://github.com/ymcui">Yiming Cui</a></td><td><a href="https://github.com/ymcui/MacBERT">github</a></td><td>通用</td></tr><tr><td>WoBERT</td><td>base</td><td><a href="https://github.com/ZhuiyiTechnology">ZhuiyiTechnology</a></td><td><a href="https://github.com/ZhuiyiTechnology/WoBERT">github</a></td><td>通用</td></tr><tr><td>WoBERT-plus</td><td>base</td><td><a href="https://github.com/ZhuiyiTechnology">ZhuiyiTechnology</a></td><td><a href="https://github.com/ZhuiyiTechnology/WoBERT">github</a></td><td>通用</td></tr><tr><td>ZEN-Base</td><td>base</td><td><a href="https://github.com/sinovation">Sinovation Ventures AI Institute</a></td><td><a href="https://github.com/sinovation/ZEN">github</a></td><td>通用</td></tr><tr><td>ernie-3.0-base</td><td>base</td><td><a href="https://github.com/PaddlePaddle">PaddlePaddle</a></td><td><a href="https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-3.0">github</a></td><td>通用</td></tr><tr><td>roformer</td><td>base(L12)</td><td><a href="https://github.com/ZhuiyiTechnology">ZhuiyiTechnology</a></td><td><a href="https://github.com/ZhuiyiTechnology/roformer">github</a></td><td>通用</td></tr><tr><td>roformerV2</td><td>base(L12)</td><td><a href="https://github.com/ZhuiyiTechnology">ZhuiyiTechnology</a></td><td><a href="https://github.com/ZhuiyiTechnology/roformer-v2">github</a></td><td>通用</td></tr><tr><td>LatticeBERT</td><td>base(L12)</td><td><a href="https://github.com/alibaba">Alibaba</a></td><td><a href="https://github.com/alibaba/AliceMind/tree/main/LatticeBERT">github</a></td><td>通用</td></tr><tr><td>Mengzi-BERT</td><td>base(L12)</td><td><a href="https://github.com/Langboat">Langboat</a></td><td><a href="https://github.com/Langboat/Mengzi">github</a></td><td>通用</td></tr><tr><td>bloom-6b4-zh</td><td>6B(L30)</td><td><a href="https://huggingface.co/Langboat">Langboat</a></td><td><a href="https://github.com/huggingface/transformers">github</a></td><td>通用</td></tr><tr><td>TaCL</td><td>base(L12)</td><td><a href="https://github.com/yxuansu">yxuansu</a></td><td><a href="https://github.com/yxuansu/TaCL">github</a></td><td>通用</td></tr><tr><td>chinese_GAU-alpha-char_L-24_H-768</td><td>base</td><td><a href="https://github.com/ZhuiyiTechnology">ZhuiyiTechnology</a></td><td><a href="https://github.com/ZhuiyiTechnology/GAU-alpha">github</a></td><td>通用</td></tr><tr><td>pai-ckbert-base-zh</td><td>base</td><td><a href="https://github.com/alibaba">Alibaba</a></td><td><a href="https://huggingface.co/alibaba-pai">github</a></td><td>通用</td></tr><tr><td>Chinese-LERT-base</td><td>400m</td><td><a href="https://github.com/ymcui">Yiming Cui</a></td><td><a href="https://github.com/ymcui/LERT">github</a></td><td>通用</td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="NLG系列"><a href="#NLG系列" class="headerlink" title="NLG系列"></a>NLG系列</h3><div class="table-container"><table><thead><tr><th>模型</th><th>版本</th><th>类型</th><th>源地址</th><th>应用领域</th></tr></thead><tbody><tr><td>CDial-GPTLCCC-base</td><td>base</td><td>GPT</td><td><a href="https://github.com/thu-coai/CDial-GPT">CDial-GPT</a></td><td>中文对话</td></tr><tr><td>roformer-gpt</td><td>base(L12)</td><td>GPT</td><td><a href="https://github.com/ZhuiyiTechnology/roformer">github</a></td><td>通用</td></tr><tr><td>NEZHA-Gen</td><td>base</td><td>GPT</td><td><a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow">github</a></td><td>通用</td></tr><tr><td>CPM</td><td>26亿参数</td><td><a href="https://cpm.baai.ac.cn/">项目首页</a></td><td><a href="https://github.com/TsinghuaAI/CPM-Generate">github</a></td><td>通用</td></tr><tr><td>Mengzi-T5</td><td>base(L12)</td><td>T5</td><td><a href="https://github.com/Langboat/Mengzi">github</a></td><td>通用</td></tr><tr><td>盘古α-2.6B</td><td>2.6G</td><td><a href="https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha/src/branch/master">项目首页</a></td><td><a href="https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha">github</a></td><td>通用</td></tr><tr><td>EVA2.0-base</td><td>base</td><td><a href="https://wudaoai.cn/model/detail/EVA">项目首页</a></td><td><a href="https://github.com/thu-coai/EVA">github</a></td><td>中文开放域对话</td></tr><tr><td>BART-base</td><td>base</td><td>Seq2Seq</td><td><a href="https://github.com/fastnlp/CPT">github</a></td><td>中文通用</td></tr><tr><td>Wenzhong</td><td>large(L24)</td><td>GPT2</td><td><a href="https://github.com/IDEA-CCNL">IDEA-CCNL</a></td><td><a href="https://github.com/IDEA-CCNL/Fengshenbang-LM">github</a></td></tr><tr><td>Yuyuan</td><td>large(L24)</td><td>GPT2</td><td><a href="https://github.com/IDEA-CCNL">IDEA-CCNL</a></td><td><a href="https://github.com/IDEA-CCNL/Fengshenbang-LM">github</a></td></tr><tr><td>ChatYuan</td><td>large</td><td>T5</td><td><a href="https://github.com/clue-ai">ClueAI</a></td><td><a href="https://github.com/clue-ai/ChatYuan">github</a></td></tr></tbody></table></div><h3 id="NLU-NLG系列"><a href="#NLU-NLG系列" class="headerlink" title="NLU-NLG系列"></a>NLU-NLG系列</h3><div class="table-container"><table><thead><tr><th>模型</th><th>版本</th><th>作者</th><th>源地址</th><th>应用领域</th></tr></thead><tbody><tr><td>SimBERT Base</td><td>base</td><td><a href="https://github.com/ZhuiyiTechnology">ZhuiyiTechnology</a></td><td><a href="https://github.com/ZhuiyiTechnology/pretrained-models">github</a></td><td>通用</td></tr><tr><td>roformer-sim</td><td>base(L12)</td><td><a href="https://github.com/ZhuiyiTechnology">ZhuiyiTechnology</a></td><td><a href="https://github.com/ZhuiyiTechnology/roformer-sim">github</a></td><td>通用</td></tr><tr><td>roformer-sim-v2</td><td>base(L12)</td><td><a href="https://github.com/ZhuiyiTechnology">ZhuiyiTechnology</a></td><td><a href="https://github.com/ZhuiyiTechnology/roformer-sim">github</a></td><td>通用</td></tr><tr><td>Zhouwenwang</td><td>roformer</td><td>-</td><td>-</td><td>-</td></tr><tr><td>base(L12)</td><td><a href="https://huggingface.co/IDEA-CCNL/Zhouwenwang-110M">huggingface</a></td><td><a href="https://github.com/IDEA-CCNL/Fengshenbang-LM">github</a></td><td>中文通用</td><td>-</td></tr><tr><td>CPM-2</td><td>110亿参数</td><td><a href="https://github.com/BAAI-WuDao">BAAI-WuDao</a></td><td><a href="https://github.com/BAAI-WuDao/Model">github</a></td><td>通用</td></tr><tr><td>CPT-base</td><td>base(L12)</td><td><a href="https://github.com/fastnlp">fastNLP</a></td><td><a href="https://github.com/fastnlp/CPT">github</a></td><td>通用</td></tr><tr><td>OPD</td><td>6.3B</td><td><a href="https://github.com/thu-coai">thu-coai</a></td><td><a href="https://github.com/thu-coai/OPD">github</a></td><td>中文开放域对话</td></tr></tbody></table></div><h3 id="LLM"><a href="#LLM" class="headerlink" title="LLM"></a>LLM</h3><p>大规模语言模型：表格中只罗列出参数量大于10B以上模型。</p><div class="table-container"><table><thead><tr><th>模型</th><th>大小</th><th>结构</th><th>语言</th><th>下载</th><th>机构</th><th>项目地址</th><th>时间</th><th>文献</th></tr></thead><tbody><tr><td>flan-ul2</td><td>20B</td><td>encoder-decoder</td><td>多语言</td><td><a href="https://huggingface.co/google/flan-ul2/tree/main">ckpt</a></td><td><a href="https://ai.google/research/">Google</a></td><td><a href="https://github.com/google-research/google-research/tree/master/ul2">ul2</a></td><td>2023-03</td><td><a href="https://arxiv.org/pdf/2205.05131v3.pdf">paper</a></td></tr><tr><td>CPM-Bee</td><td>10B</td><td>Decoder</td><td>中英文</td><td>待发布</td><td><a href="https://live.openbmb.org/">OpenBMB</a></td><td><a href="https://github.com/OpenBMB/CPM-Live">CPM-Live</a></td><td>2023-01</td><td>-</td></tr><tr><td>BLOOM</td><td>176B</td><td>Decoder</td><td>多语言</td><td><a href="https://huggingface.co/bigscience/bloom">ckpt-95000</a></td><td><a href="https://github.com/bigscience-workshop">BigScience</a></td><td><a href="https://github.com/bigscience-workshop/Megatron-DeepSpeed">Megatron-DeepSpeed</a></td><td>2022-11</td><td><a href="https://arxiv.org/pdf/2211.05100.pdf">paper</a></td></tr><tr><td><em>BLOOMZ</em></td><td>176B</td><td>Decoder</td><td>多语言</td><td><a href="https://huggingface.co/bigscience/bloomz">ckpt-498</a></td><td><a href="https://github.com/bigscience-workshop">BigScience</a></td><td><a href="https://github.com/bigscience-workshop/Megatron-DeepSpeed">Megatron-DeepSpeed</a></td><td>2022-11</td><td><a href="https://arxiv.org/abs/2211.01">paper</a></td></tr><tr><td>flan-t5-xxl</td><td>11B</td><td>encoder-decoder</td><td>多语言</td><td><a href="https://huggingface.co/google/flan-t5-xxl">ckpt</a></td><td><a href="https://ai.google/research/">Google</a></td><td><a href="https://github.com/google-research/t5x">t5x</a></td><td>2022-11</td><td><a href="https://arxiv.org/pdf/2210.11416.pdf">paper</a></td></tr><tr><td>CPM-Ant+</td><td>10B</td><td>Decoder</td><td>中英文</td><td><a href="http://openbmb.oss-cn-hongkong.aliyuncs.com/model_center/cpm-ant-plus-10b/cpm-ant-plus-10b.zip">ckpt</a></td><td><a href="https://live.openbmb.org/">OpenBMB</a></td><td><a href="https://github.com/OpenBMB/CPM-Live">CPM-Live</a></td><td>2022-10</td><td><a href="https://www.openbmb.org/community/blogs/blogpage?id=98afef2ce45f4fe9a4bc15a66d7ccb92">blog</a></td></tr><tr><td>GLM</td><td>130B</td><td>Decoder</td><td>中英文</td><td><a href="https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform">申请下载</a></td><td><a href="https://github.com/THUDM">清华大学</a></td><td><a href="https://github.com/THUDM/GLM-130B">GLM-130B</a></td><td>2022-10</td><td><a href="http://arxiv.org/abs/2210.02414">paper</a></td></tr><tr><td>CPM-Ant</td><td>10B</td><td>Decoder</td><td>中文</td><td><a href="https://openbmb.oss-cn-hongkong.aliyuncs.com/model_center/cpmlive-10b/cpm_live_10B.zip">ckpt</a></td><td><a href="https://live.openbmb.org/">OpenBMB</a></td><td><a href="https://github.com/OpenBMB/CPM-Live">CPM-Live</a></td><td>2022-09</td><td><a href="https://www.openbmb.org/community/blogs/blogpage?id=98afef2ce45f4fe9a4bc15a66d7ccb92">blog</a></td></tr><tr><td>GLM</td><td>10B</td><td>Decoder</td><td>中文</td><td><a href="https://lfs.aminer.cn/misc/cogview/glm-10b-chinese.zip">ckpt</a></td><td><a href="https://github.com/THUDM">清华大学</a></td><td><a href="https://github.com/THUDM/GLM">GLM</a></td><td>2022-09</td><td><a href="https://arxiv.org/abs/2103.10360">paper</a></td></tr><tr><td>源1.0</td><td>245B</td><td>Decoder</td><td>中文</td><td><a href="https://air.inspur.com/home">API申请</a></td><td><a href="https://air.inspur.com/home">浪潮</a></td><td><a href="https://github.com/Shawn-Inspur/Yuan-1.0">Yian-1.0</a></td><td>2021-09</td><td><a href="https://arxiv.org/abs/2110.04725">paper</a></td></tr><tr><td>CPM-2</td><td>11B</td><td>encoder-decoder</td><td>中文</td><td><a href="https://resource.wudao.baai.ac.cn/home?ind=2&amp;name=WuDao WenYuan&amp;id=1394901846484627456">申请下载</a></td><td><a href="https://www.baai.ac.cn/">智源研究院</a></td><td><a href="https://github.com/TsinghuaAI/CPM">CPM</a></td><td>2021-06</td><td><a href="https://arxiv.org/abs/2106.10715">paper</a></td></tr><tr><td>CPM-2</td><td>10B</td><td>encoder-decoder</td><td>中英文</td><td><a href="https://resource.wudao.baai.ac.cn/home?ind=2&amp;name=WuDao WenYuan&amp;id=1394901846484627456">申请下载</a></td><td><a href="https://www.baai.ac.cn/">智源研究院</a></td><td><a href="https://github.com/TsinghuaAI/CPM">CPM</a></td><td>2021-06</td><td><a href="https://arxiv.org/abs/2106.10715">paper</a></td></tr><tr><td>PanGu-Alpha</td><td>13B</td><td>Decoder</td><td>中文</td><td><a href="https://openi.pcl.ac.cn/PCL-Platform.Intelligence/PanGu-Alpha">ckpt</a></td><td><a href="https://openi.pcl.ac.cn/PCL-Platform.Intelligence">鹏城实验室</a></td><td><a href="https://openi.pcl.ac.cn/PCL-Platform.Intelligence/PanGu-Alpha">PanGu-Alpha</a></td><td>2021-05</td><td><a href="https://arxiv.org/pdf/2104.12369.pdf">paper</a></td></tr><tr><td>PanGu-Alpha</td><td>200B</td><td>Decoder</td><td>中文</td><td>待发布</td><td><a href="https://openi.pcl.ac.cn/PCL-Platform.Intelligence">鹏城实验室</a></td><td><a href="https://openi.pcl.ac.cn/PCL-Platform.Intelligence/PanGu-Alpha">PanGu-Alpha</a></td><td>2021-05</td><td><a href="https://arxiv.org/pdf/2104.12369.pdf">paper</a></td></tr><tr><td>PLUG</td><td>27B</td><td>encoder-decoder</td><td>中文</td><td><a href="https://www.alice-mind.com/portal#/">申请下载</a></td><td><a href="https://www.alice-mind.com/portal#/">阿里巴巴</a></td><td><a href="https://github.com/alibaba/AliceMind">AliceMind</a></td><td>2021-04</td><td>-</td></tr><tr><td>GPT-3</td><td>13B</td><td>Decoder</td><td>中文</td><td>待发布</td><td><a href="https://modelscope.cn/organization/damo">达摩院</a></td><td><a href="https://modelscope.cn/models/damo/nlp_gpt3_text-generation_13B/summary">GPT-3预训练生成模型</a></td><td>2021-04</td><td>-</td></tr><tr><td>GPT-3</td><td>30B</td><td>Decoder</td><td>中文</td><td>待发布</td><td><a href="https://modelscope.cn/organization/damo">达摩院</a></td><td><a href="https://modelscope.cn/models/damo/nlp_gpt3_text-generation_30B/summary">GPT-3预训练生成模型</a></td><td>2021-04</td><td>-</td></tr></tbody></table></div><h3 id="ChatLLM"><a href="#ChatLLM" class="headerlink" title="ChatLLM"></a>ChatLLM</h3><p>具备问答和对话等功能的大型语言模型。</p><div class="table-container"><table><thead><tr><th>模型</th><th>大小</th><th>结构</th><th>语言</th><th>下载</th><th>机构/个人</th><th>项目地址</th><th>时间</th></tr></thead><tbody><tr><td>ChatLLaMA</td><td>7B</td><td>Decoder</td><td>多语言</td><td><a href="https://huggingface.co/P01son/ChatLLaMA-zh-7B">ckpt</a></td><td><a href="https://github.com/ydli-ai">Li Yudong</a></td><td><a href="https://github.com/ydli-ai/Chinese-ChatLLaMA">Chinese-ChatLLaMA</a></td><td>2023-03</td></tr><tr><td>Chinese-Vicuna</td><td>13B</td><td>Decoder</td><td>中文</td><td><a href="https://huggingface.co/Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco">ckpt</a></td><td><a href="https://github.com/Facico">Facico</a></td><td><a href="https://github.com/Facico/Chinese-Vicuna">Chinese-Vicuna</a></td><td>2023-03</td></tr><tr><td>Chinese-Vicuna</td><td>7B</td><td>Decoder</td><td>中文</td><td><a href="https://huggingface.co/Chinese-Vicuna/Chinese-Vicuna-lora-7b-belle-and-guanaco">ckpt</a></td><td><a href="https://github.com/Facico">Facico</a></td><td><a href="https://github.com/Facico/Chinese-Vicuna">Chinese-Vicuna</a></td><td>2023-03</td></tr><tr><td>ChatYuan-V2</td><td>0.7B</td><td>Encoder-Decder</td><td>中英文</td><td><a href="https://huggingface.co/ClueAI/ChatYuan-large-v2/tree/main">ckpt</a></td><td><a href="https://github.com/clue-ai">元语智能</a></td><td><a href="https://github.com/clue-ai/ChatYuan">ChatYuan</a></td><td>2023-03</td></tr><tr><td>Chinese-LLaMA-Alpaca</td><td>7B</td><td>Decoder</td><td>中文</td><td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">lora-ckpt</a></td><td><a href="https://github.com/ymcui">Yiming Cui</a></td><td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">Chinese-LLaMA-Alpaca</a></td><td>2023-03</td></tr><tr><td>Luotuo</td><td>7B</td><td>Decoder</td><td>中文</td><td><a href="https://huggingface.co/silk-road/luotuo-lora-7b-0.3">ckpt</a></td><td>商汤科技&amp;华中师范大学</td><td><a href="https://github.com/LC1332/Chinese-alpaca-lora">Chinese-alpaca-lora</a></td><td>2023-03</td></tr><tr><td>BELLE-LLAMA</td><td>7B</td><td>Decoder</td><td>中英文</td><td><a href="https://huggingface.co/BelleGroup/BELLE-LLAMA-7B-2M">ckpt</a></td><td><a href="https://github.com/LianjiaTech">贝壳</a></td><td><a href="https://github.com/LianjiaTech/BELLE">BELLE</a></td><td>2023-03</td></tr><tr><td>BELLE-BLOOM</td><td>7B</td><td>Decoder</td><td>中英文</td><td><a href="https://huggingface.co/BelleGroup/BELLE-7B-2M">ckpt</a></td><td><a href="https://github.com/LianjiaTech">贝壳</a></td><td><a href="https://github.com/LianjiaTech/BELLE">BELLE</a></td><td>2023-03</td></tr><tr><td>ChatGLM-6B</td><td>6B</td><td>Decoder</td><td>中英双语</td><td><a href="https://huggingface.co/THUDM/chatglm-6b">ckpt</a></td><td><a href="https://github.com/THUDM">清华大学</a></td><td><a href="https://github.com/THUDM/ChatGLM-6B">ChatGLM-6B</a></td><td>2023-03</td></tr><tr><td>ChatRWKV</td><td>7B</td><td>RNN</td><td>中/英文</td><td><a href="https://huggingface.co/BlinkDL/rwkv-4-pile-7b/tree/main">ckpt</a></td><td><a href="https://github.com/BlinkDL">BlinkDL</a></td><td><a href="https://github.com/BlinkDL/ChatRWKV">ChatRWKV</a></td><td>2023-01</td></tr></tbody></table></div><hr><h2 id="预训练模型的方法改进"><a href="#预训练模型的方法改进" class="headerlink" title="预训练模型的方法改进"></a>预训练模型的方法改进</h2><h3 id="掩码方式的转变"><a href="#掩码方式的转变" class="headerlink" title="掩码方式的转变"></a>掩码方式的转变</h3><img src="/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95&%E6%95%B4%E7%90%86/1680254440948-bf274532-c2dd-4625-975b-41d06470d167.png" class="" title="image.png"><h3 id="位置编码的转变"><a href="#位置编码的转变" class="headerlink" title="位置编码的转变"></a>位置编码的转变</h3><h3 id="LN-的位置变化"><a href="#LN-的位置变化" class="headerlink" title="LN 的位置变化"></a>LN 的位置变化</h3><img src="/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95&%E6%95%B4%E7%90%86/1680254515526-1479b790-7c7e-412f-9d4a-d4b404d97f1b.png" class="" title="image.png"><h3 id="MoE层的使用"><a href="#MoE层的使用" class="headerlink" title="MoE层的使用"></a>MoE层的使用</h3><img src="/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95&%E6%95%B4%E7%90%86/1680254540425-dd3af49f-371b-457b-a79e-8fcf8842e47f.png" class="" title="image.png">]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>中文预训练模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🇨🇳中文常用语料Corpus整理</title>
    <link href="/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%96%99Corpus%E6%95%B4%E7%90%86/"/>
    <url>/2023/03/21/%F0%9F%87%A8%F0%9F%87%B3%E4%B8%AD%E6%96%87%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%96%99Corpus%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://github.com/InsaneLife">InsaneLife</a>/<a href="https://github.com/InsaneLife/ChineseNLPCorpus">ChineseNLPCorpus</a><br><a href="https://github.com/SophonPlus">SophonPlus</a>/<a href="https://github.com/SophonPlus/ChineseNlpCorpus">ChineseNlpCorpus</a><br><a href="https://github.com/brightmart">brightmart</a>/<a href="https://github.com/brightmart/nlp_chinese_corpus">nlp_chinese_corpus</a><br><a href="https://github.com/ningshixian">ningshixian</a>/<a href="https://github.com/ningshixian/NLP-zoo">NLP-zoo</a>、<a href="https://github.com/fighting41love">fighting41love</a>/<a href="https://github.com/fighting41love/funNLP">funNLP</a></p></blockquote><h2 id="中文词典"><a href="#中文词典" class="headerlink" title="中文词典"></a>中文词典</h2><ul><li><a href="https://github.com/huyingxi/Synonyms/">Synonyms:中文近义词工具包</a> 基于维基百科中文和word2vec训练的近义词库，封装为python包文件</li><li><a href="https://github.com/guotong1988/chinese_dictionary">同义词库、反义词库、否定词库</a></li><li><a href="https://github.com/wainshine/Chinese-Names-Corpus">中文人名语料库</a> 中文姓名,姓氏,名字,称呼,日本人名,翻译人名,英文人名。</li><li><a href="https://github.com/wainshine/Company-Names-Corpus">公司名、机构名语料库</a> 公司简称,缩写,品牌词,企业名。</li><li><a href="https://github.com/observerss/textfilter">中文敏感词词库 textfilter</a> 敏感词过滤的几种实现+某1w词敏感词库</li><li><a href="https://github.com/zhangyics/Chinese-abbreviation-dataset">中文简称/缩写词库</a> A corpus of Chinese abbreviation, including negative full forms.</li><li><a href="https://github.com/rainarch/SentiBridge/blob/master/Entity_Emotion_Express/CCF_data/pair_mine_result">词汇情感值</a>：如山泉水:0.400704566541、充沛: 0.37006739587</li><li><a href="http://thuctc.thunlp.org/">THU整理的词库</a>：IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库</li><li>中文字符数据：<a href="https://github.com/skishore/makemeahanzi">github</a></li><li><a href="https://github.com/liuhuanyong/DomainWordsDict">领域词典库 DomainWordsDict</a>：涵盖68个领域、共计916万词的专业词典知识库</li></ul><h2 id="超大型通用语料"><a href="#超大型通用语料" class="headerlink" title="超大型通用语料"></a>超大型通用语料</h2><div class="table-container"><table><thead><tr><th>语料名称</th><th>语料大小</th><th>语料描述</th></tr></thead><tbody><tr><td><a href="https://storage.googleapis.com/nlp_chinese_corpus/wiki_zh_2019.zip">维基百科json版(wiki2019zh)</a></td><td>104万个词条, 1.6G</td><td>做预训练的语料或构建词向量，也可以用于构建知识问答</td></tr><tr><td><a href="https://pan.baidu.com/s/1LJeq1dkA0wmYd9ZGZw72Xg">新闻语料json版(news2016zh)</a></td><td>250万篇新闻,原始数据9G</td><td>密码: film 包含了250万篇新闻。数据集划分：数据去重并分成三个部分。训练集：243万；验证集：7.7万；测试集，数万</td></tr><tr><td><a href="https://pan.baidu.com/s/12TCEwC_Q3He65HtPKN17cA">百科类问答json版(baike2018qa)</a></td><td>150万个问答,原始数据1G多</td><td>含有150万个预先过滤过的、高质量问题和答案，每个问题属于一个类别。总共有492个类别</td></tr><tr><td><a href="https://storage.googleapis.com/nlp_chinese_corpus/webtext2019zh.zip">社区问答json版(webtext2019zh)</a></td><td>410万个问答,过滤后数据3.7G</td><td>含有410万个预先过滤过的、高质量问题和回复。</td></tr></tbody></table></div><h2 id="领域特定语料"><a href="#领域特定语料" class="headerlink" title="领域特定语料"></a>领域特定语料</h2><div class="table-container"><table><thead><tr><th>语料名称</th><th>语料大小</th><th>语料描述</th></tr></thead><tbody><tr><td><a href="https://github.com/Samurais/insuranceqa-corpus-zh">保险行业QA语料库</a></td><td>未知</td><td>train_data含有问题12,889条，数据 141779条，正例：负例 = 1:10； test_data含有问题2,000条，数据 22000条，正例：负例 = 1:10；valid_data含有问题2,000条，数据 22000条，正例：负例 = 1:10</td></tr><tr><td><a href="https://github.com/smoothnlp/FinancialDatasets">FinancialDatasets</a></td><td>-</td><td>SmoothNLP 金融文本数据集(公开) Public Financial Datasets for NLP Researches Only</td></tr><tr><td><a href="https://cail.oss-cn-qingdao.aliyuncs.com/CAIL2018_ALL_DATA.zip">CAIL2018</a></td><td>-</td><td>2018中国‘法研杯’法律智能挑战赛（任务：罪名预测、法条推荐、刑期预测）的数据，数据集共包括268万刑法法律文书，共涉及183条罪名，202条法条，刑期长短包括0-25年、无期、死刑。</td></tr><tr><td><a href="https://github.com/Toyhom/Chinese-medical-dialogue-data">github</a></td><td>-</td><td>Chinese medical dialogue data 中文医疗对话数据集</td></tr><tr><td><a href="https://github.com/UCSD-AI4H/Medical-Dialogue-System">github</a></td><td>包含110万医学咨询，400万条医患对话</td><td>一个大规模医疗对话数据集</td></tr><tr><td><a href="https://github.com/abachaa/MedQuAD">github</a></td><td>-</td><td>MedQuAD(英文)医学问答数据集</td></tr></tbody></table></div><h2 id="NER-amp-POS-amp-分词"><a href="#NER-amp-POS-amp-分词" class="headerlink" title="NER &amp; POS &amp; 分词"></a>NER &amp; POS &amp; 分词</h2><div class="table-container"><table><thead><tr><th>语料名称</th><th>语料大小</th><th>语料描述</th></tr></thead><tbody><tr><td><a href="https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/MSRA">MSRA</a></td><td>5w+条</td><td>中文NER任务最常用数据之一，包含地名、人名和机构名三类</td></tr><tr><td><a href="https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/People&#39;s Daily">1998人民日报</a></td><td>137万多条</td><td>中文NER任务最常用数据之二，包含地名、人名和机构名三类实体类型。人民日报语料处理工具集 <a href="https://github.com/howl-anderson/tools_for_corpus_of_people_daily">github</a></td></tr><tr><td><a href="https://github.com/hltcoe/golden-horse">weibo NER corpus</a></td><td>1,890条</td><td>中文NER任务最常用数据之三。设计的实体有：人名、地点、组织、地理政治相关实体</td></tr><tr><td><a href="https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/boson">boson数据(不维护了)</a></td><td>2000条</td><td>包含6种实体类型：人名、地名、时间、组织名、公司名、产品名</td></tr><tr><td><a href="https://github.com/jiesutd/LatticeLSTM/tree/master/ResumeNER">Resume NER data</a></td><td>-</td><td>爬虫新浪财经的的简历数据, CoNLL format (BIOES tag scheme)，包括城市、学校、地点、人名、组织等</td></tr><tr><td><a href="https://github.com/LG-1/video_music_book_datasets">影视、音乐、书籍</a></td><td>大约10000条</td><td>包含 3 种实体：视频/音乐/书籍</td></tr><tr><td><a href="https://pan.baidu.com/s/17djsvYfpYUXrazL0H_mtoA">1300W字的新闻</a></td><td>未知</td><td>该语料可用于分词、NER、POS等任务。标记和格式请参考<a href="https://cloud.tencent.com/developer/article/1091906">此文章</a></td></tr><tr><td><a href="https://biendata.com/competition/CCKS2017_2/data/">CCKS2017中文电子病例命名实体识别</a></td><td>-</td><td>数据来源于其云医院平台的真实电子病历数据，共计800条（单个病人单次就诊记录），经脱敏处理</td></tr><tr><td><a href="https://biendata.com/competition/CCKS2018_1/data/">CCKS2018中文电子病例命名实体识别</a></td><td>-</td><td>CCKS2018的电子病历命名实体识别的评测任务提供了600份标注好的电子病历文本，共需识别含解剖部位、独立症状、症状描述、手术和药物五类实体</td></tr><tr><td><a href="https://storage.googleapis.com/cluebenchmark/tasks/cluener_public.zip">CLUE Fine-Grain NER</a></td><td>-</td><td>CLUENER2020数据集，是在清华大学开源的文本分类数据集THUCTC基础上，选出部分数据进行细粒度命名实体标注，原数据来源于Sina News RSS。数据包含10个标签类别，训练集共有10748条语料，验证集共有1343条语料</td></tr><tr><td><a href="https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset">Chinese-Literature-NER-RE-Dataset</a></td><td>-</td><td>A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text</td></tr></tbody></table></div><h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><div class="table-container"><table><thead><tr><th>语料名称</th><th>语料大小</th><th>语料描述</th></tr></thead><tbody><tr><td><a href="https://cail.oss-cn-qingdao.aliyuncs.com/CAIL2018_ALL_DATA.zip">2018中国‘法研杯’法律智能挑战赛数据</a></td><td>未知</td><td>268万刑法法律文书，共涉及183条罪名，202条法条，刑期长短包括0-25年、无期、死刑</td></tr><tr><td><a href="https://github.com/fateleak/toutiao-text-classfication-dataset">今日头条中文新闻（短文本）</a></td><td>共38万条</td><td>15个分类中，包含民生、文化、娱乐、体育、财经、房产、骑车、教育、科技、军事、旅游、国际、证券、农业、电竞</td></tr><tr><td><a href="https://pan.baidu.com/s/1bnhXX6Z">搜狗20061127新闻语料(包含分类)@百度盘</a></td><td></td><td></td></tr><tr><td><a href="http://thuctc.thunlp.org/#%E8%8E%B7%E5%8F%96%E9%93%BE%E6%8E%A5">清华新闻分类语料</a></td><td>74万篇新闻文档（2.19 GB）</td><td>根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成</td></tr><tr><td><a href="http://www.nlpir.org/?action-viewnews-itemid-145">中科大新闻分类语料库</a></td><td></td></tr></tbody></table></div><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><div class="table-container"><table><thead><tr><th>数据集</th><th>数据概览</th><th>下载地址</th></tr></thead><tbody><tr><td>ez_douban</td><td>5 万多部电影（3 万多有电影名称，2 万多没有电影名称），2.8 万 用户，280 万条评分数据</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/ez_douban/intro.ipynb">点击查看</a></td></tr><tr><td>dmsc_v2</td><td>28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/dmsc_v2/intro.ipynb">点击查看</a></td></tr><tr><td>yf_dianping</td><td>24 万家餐馆，54 万用户，440 万条评论/评分数据</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/yf_dianping/intro.ipynb">点击查看</a></td></tr><tr><td>yf_amazon</td><td>52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/yf_amazon/intro.ipynb">点击查看</a></td></tr></tbody></table></div><h2 id="阅读理解"><a href="#阅读理解" class="headerlink" title="阅读理解"></a>阅读理解</h2><p>阅读理解数据集按照方法主要有：抽取式、分类（观点提取）。按照篇章又分为单篇章、多篇章，比如有的问题答案可能需要从多个文章中提取，每个文章可能都只是一部分，那么多篇章提取就会面临怎么合并，合并的时候怎么去掉重复的，保留补充的。</p><div class="table-container"><table><thead><tr><th>名称</th><th>规模</th><th>说明</th><th>单位</th><th>下载</th><th>评测</th></tr></thead><tbody><tr><td>DuReader</td><td>30万问题 140万文档 66万答案</td><td>问答阅读理解数据集</td><td>百度</td><td><a href="https://ai.baidu.com/broad/introduction?dataset=dureader">链接</a></td><td><a href="http://mrc2018.cipsc.org.cn/">2018 NLP Challenge on MRC</a>、<a href="http://lic2019.ccf.org.cn/">2019 Language and Intelligence Challenge on MRC</a></td></tr><tr><td>DuReaderrobust</td><td>2.2万问题</td><td>单篇章、抽取式阅读理解数据集</td><td>百度</td><td><a href="https://github.com/PaddlePaddle/Research/tree/master/NLP/DuReader-Robust-BASELINE">链接</a></td><td><a href="https://aistudio.baidu.com/aistudio/competition/detail/49/?isFromLUGE=TRUE">评测</a></td></tr><tr><td>CMRC 2018</td><td>2万问题</td><td>篇章片段抽取型阅读理解</td><td>哈工大讯飞联合实验室</td><td><a href="https://github.com/ymcui/cmrc2018">链接</a></td><td><a href="https://hfl-rc.github.io/cmrc2018/">第二届“讯飞杯”中文机器阅读理解评测</a></td></tr><tr><td>DuReaderyesno</td><td>9万</td><td>观点型阅读理解数据集</td><td>百度</td><td><a href="https://aistudio.baidu.com/aistudio/competition/detail/49/?isFromLUGE=TRUE">链接</a></td><td><a href="https://aistudio.baidu.com/aistudio/competition/detail/49/?isFromLUGE=TRUE">评测</a></td></tr><tr><td>DuReaderchecklist</td><td>1万</td><td>抽取式数据集</td><td>百度</td><td><a href="https://aistudio.baidu.com/aistudio/competition/detail/49/?isFromLUGE=TRUE">链接</a></td><td>-</td></tr></tbody></table></div><h2 id="FAQ-问答"><a href="#FAQ-问答" class="headerlink" title="FAQ 问答"></a>FAQ 问答</h2><div class="table-container"><table><thead><tr><th>数据集</th><th>数据概览</th><th>下载地址</th></tr></thead><tbody><tr><td>保险知道</td><td>8000 多条保险行业问答数据，包括用户提问、网友回答、最佳回答</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/baoxianzhidao/intro.ipynb">点击查看</a></td></tr><tr><td>安徽电信知道</td><td>15.6 万条电信问答数据，包括用户提问、网友回答、最佳回答</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/anhuidianxinzhidao/intro.ipynb">点击查看</a></td></tr><tr><td>金融知道</td><td>77 万条金融行业问答数据，包括用户提问、网友回答、最佳回答</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/financezhidao/intro.ipynb">点击查看</a></td></tr><tr><td>法律知道</td><td>3.6 万条法律问答数据，包括用户提问、网友回答、最佳回答</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/lawzhidao/intro.ipynb">点击查看</a></td></tr><tr><td>联通知道</td><td>20.3 万条联通问答数据，包括用户提问、网友回答、最佳回答</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/liantongzhidao/intro.ipynb">点击查看</a></td></tr><tr><td>农行知道</td><td>4 万条农业银行问答数据，包括用户提问、网友回答、最佳回答</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/nonghangzhidao/intro.ipynb">点击查看</a></td></tr><tr><td>保险知道</td><td>58.8 万条保险行业问答数据，包括用户提问、网友回答、最佳回答</td><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master/datasets/baoxianzhidao/intro.ipynb">点击查看</a></td></tr><tr><td>百度知道问答</td><td>包括超过580万的问题，每个问题带有问题标签。问答对983万个，每个问题的答案个数1.7个，问题标签个数5824个。</td><td><a href="https://github.com/liuhuanyong/MiningZhiDaoQACorpus">点击查看</a></td></tr><tr><td>DuReader</td><td>百度开源的一个QA和MRC数据集，共140万篇文档，30万个问题，及66万个答案。</td><td><a href="http://ai.baidu.com/broad/introduction?dataset=dureader">点击查看</a></td></tr><tr><td>社区问答数据</td><td>含有410万个预先过滤过的、高质量问题和回复。每个问题属于一个话题，总共有2.8万个各式话题，话题包罗万象。从1400万个原始问答中，筛选出至少获得3个点赞以上的的答案，代表了回复的内容比较不错或有趣，从而获得高质量的数据集。除了对每个问题对应一个话题、问题的描述、一个或多个回复外，每个回复还带有点赞数、回复ID、回复者的标签</td><td><a href="https://github.com/brightmart/nlp_chinese_corpus">点击查看</a></td></tr></tbody></table></div><h2 id="任务型对话数据"><a href="#任务型对话数据" class="headerlink" title="任务型对话数据"></a>任务型对话数据</h2><div class="table-container"><table><thead><tr><th>数据集</th><th>数据概览</th><th>下载地址</th></tr></thead><tbody><tr><td>任务型对话英文数据集</td><td>【最全任务型对话数据集】主要介绍了一份任务型对话数据集大全，这份数据集大全涵盖了到目前在任务型对话领域的所有常用数据集的主要信息。此外，为了帮助研究者更好的把握领域进展的脉络，我们以Leaderboard的形式给出了几个数据集上的State-of-the-art实验结果。</td><td><a href="https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey">github</a></td></tr><tr><td>Medical DS</td><td>复旦大学发布的基于百度拇指医生上真实对话数据的，面向任务型对话的中文医疗诊断数据集。</td><td><a href="http://www.sdspeople.fudan.edu.cn/zywei/data/acl2018-mds.zip">链接</a></td></tr><tr><td>千言数据集</td><td>包含知识对话、推荐对话、画像对话。千言里面还有很多数据集，见: <a href="https://www.luge.ai/#/">https://www.luge.ai/#/</a></td><td><a href="https://aistudio.baidu.com/aistudio/competition/detail/48/?isFromLUGE=TRUE">官网</a></td></tr><tr><td>JD客服对话数据</td><td>42GB的JD客服对话数据(CSDD)</td><td><a href="https://github.com/jd-aig/nlp_baai/tree/master/pretrained_models_and_embeddings">github</a></td></tr><tr><td><a href="https://sites.google.com/view/catslu/handbook">CATSLU</a></td><td>之前的一些对话数据集集中于语义理解，而工业界真实情况ASR也会有错误，往往被忽略。CATSLU而是一个中文语音+NLU文本理解的对话数据集，可以从语音信号到理解端到端进行实验，例如直接从音素建模语言理解（而非word or token）。</td><td><a href="https://sites.google.com/view/CATSLU/home">链接</a></td></tr><tr><td>NLPCC2018 Shared Task 4</td><td>中文真实商用车载语音任务型对话系统的对话日志.</td><td><a href="http://tcci.ccf.org.cn/conference/2018/dldoc/trainingdata04.zip">训练开发集</a> <a href="http://tcci.ccf.org.cn/conference/2018/dldoc/tasktestdata04.zip">测试集</a></td></tr><tr><td>SMP-2020-ECDT小样本对话语言理解数据集</td><td>来自于讯飞AIUI开放平台上真实用户语料和专家构造的语料(比例大概为3：7)</td><td><a href="https://atmahou.github.io/attachments/FewJoint.zip">链接</a></td></tr><tr><td><a href="https://github.com/HITlilingzhi/SMP2017ECDT-DATA">SMP2017中文人机对话评测数据</a></td><td>包含了两个任务的数据集，用户意图领域分类和特定域任务型人机对话在线评测。第一个数据集用得比较多。用户意图领域分类包含闲聊类、任务垂直类共三十一个类别，属于短文本分类的一个范畴</td><td><a href="https://github.com/HITlilingzhi/SMP2017ECDT-DATA">链接</a></td></tr><tr><td>SMP-2019-NLU</td><td>包含领域分类、意图识别和语义槽填充三项子任务的数据集</td><td><a href="https://github.com/InsaneLife/ChineseNLPCorpus/blob/master/dialogue/SMP-2019-NLU/train.json">trian.json</a></td></tr></tbody></table></div><h2 id="闲聊"><a href="#闲聊" class="headerlink" title="闲聊"></a>闲聊</h2><div class="table-container"><table><thead><tr><th>语料名称</th><th>语料Size</th><th>语料来源</th><th>语料描述</th></tr></thead><tbody><tr><td><a href="https://github.com/fate233/dgk_lost_conv">中文对白语料 chinese conversation corpus</a></td><td></td><td></td><td>可以用作聊天机器人的训练语料</td></tr><tr><td><a href="https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data/chinese">chatterbot</a></td><td>560</td><td>开源项目</td><td>按类型分类，质量较高</td></tr><tr><td>qingyun（青云语料）</td><td>10W</td><td>某聊天机器人交流群</td><td>相对不错，生活化</td></tr><tr><td><a href="https://github.com/candlewill/Dialog_Corpus">xiaohuangji（小黄鸡语料）</a></td><td>45W</td><td>原人人网项目语料</td><td>有一些不雅对话，少量噪音</td></tr><tr><td><a href="https://github.com/MarkWuNLP/MultiTurnResponseSelection">douban（豆瓣多轮）</a></td><td>352W</td><td>来自北航和微软的paper, 开源项目</td><td>噪音相对较少，原本是多轮（平均7.6轮）</td></tr><tr><td><a href="https://github.com/ningshixian/NLP-zoo/blob/master">weibo（微博语料）</a></td><td>443W</td><td>来自华为的paper</td><td>有一些噪音</td></tr><tr><td><a href="https://github.com/thu-coai/CDial-GPT">中文闲聊语料库LCCC</a></td><td>??W</td><td>清华大学2020</td><td>大规模的中文闲聊语料库LCCC，从开源的文件上来看，这可能是目前开源的数量最大、质量最好的闲聊语料库了</td></tr><tr><td><a href="https://github.com/rustch3n/dgk_lost_conv">dgk_lost_conv 中文对白语料</a></td><td></td><td></td><td>chinese conversation corpus</td></tr><tr><td><a href="https://github.com/candlewill/Dialog_Corpus">用于训练中英文对话系统的语料库</a></td><td></td><td></td><td>Datasets for Training Chatbot System</td></tr><tr><td><a href="https://github.com/zake7749/Gossiping-Chinese-Corpus">八卦版問答中文語料</a></td><td></td><td></td><td></td></tr><tr><td><a href="https://github.com/codemayq/chaotbot_corpus_Chinese">中文公开聊天语料库</a></td><td></td><td></td></tr></tbody></table></div><h2 id="语义相似度"><a href="#语义相似度" class="headerlink" title="语义相似度"></a>语义相似度</h2><h3 id="哈工大-LCQMC-数据集"><a href="#哈工大-LCQMC-数据集" class="headerlink" title="哈工大 LCQMC 数据集"></a>哈工大 LCQMC 数据集</h3><p>LCQMC 是哈尔滨工业大学在自然语言处理国际顶会 COLING2018 构建的问题语义匹配数据集，其目标是判断两个问题的语义是否相同。该数据集的数据预览如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs">喜欢打篮球的男生喜欢什么样的女生爱打篮球的男生喜欢什么样的女生1 <br>我手机丢了，我想换个手机    我想买个新手机，求推荐1 <br>大家觉得她好看吗大家觉得跑男好看吗？0 <br></code></pre></td></tr></table></figure><br>原始数据集链接：<a href="http://icrc.hitsz.edu.cn/Article/show/171.html">http://icrc.hitsz.edu.cn/Article/show/171.html</a></p><h3 id="AFQMC-蚂蚁金融语义相似度数据集"><a href="#AFQMC-蚂蚁金融语义相似度数据集" class="headerlink" title="AFQMC 蚂蚁金融语义相似度数据集"></a>AFQMC 蚂蚁金融语义相似度数据集</h3><p>AFQMC（Ant Financial Question Matching Corpus）蚂蚁金融语义相似度数据集，用于问题相似度计算。即：给定客服里用户描述的两句话，用算法来判断是否表示了相同的语义。每一条数据有三个属性，分别是句子1，句子2，句子相似度标签。标签 “1” ：表示两个句子的语义类似；”0”：表示两个句子的语义不同。<br>原始数据为 json 格式，本仓库将其处理成形如 LCQMC 三列的格式，每列之间使用 ‘\t’ 分隔：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs">花呗消费超过额度有什么影响吗花呗额度成负数有啥影响吗1 <br>还款还清了，为什么花呗账单显示还要还款花呗全额还清怎么显示没有还款1 <br>花呗一次性付款有限制吗解除花呗支付限制0 <br></code></pre></td></tr></table></figure><br>原始数据集链接：<a href="https://tianchi.aliyun.com/dataset/dataDetail?dataId=106411">https://tianchi.aliyun.com/dataset/dataDetail?dataId=106411</a></p><h3 id="OPPO-小布对话文本语义匹配数据集"><a href="#OPPO-小布对话文本语义匹配数据集" class="headerlink" title="OPPO 小布对话文本语义匹配数据集"></a>OPPO 小布对话文本语义匹配数据集</h3><p>该数据集通过对闲聊、智能客服、影音娱乐、信息查询等多领域真实用户交互语料进行用户信息脱敏、相似度筛选处理得到，数据主要特点是文本较短、非常口语化、存在文本高度相似而语义不同的难例。该数据集所有标签都有经过人工精标确认。<br>原始数据为 json 格式，本仓库将其处理成形如 LCQMC 三列的格式，每列之间使用 ‘\t’ 分隔：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs">我真的超级生气气死我了1 <br>你生日是几月几日你的老师生日是几月几日0 <br>打电话给爱老公给爱老公打电话1 <br></code></pre></td></tr></table></figure><br>原始数据集链接：<a href="https://tianchi.aliyun.com/competition/entrance/531851/introduction">https://tianchi.aliyun.com/competition/entrance/531851/introduction</a></p><h3 id="谷歌-PAWS-X-数据集"><a href="#谷歌-PAWS-X-数据集" class="headerlink" title="谷歌 PAWS-X 数据集"></a>谷歌 PAWS-X 数据集</h3><p>谷歌发布的同义句识别数据集，中文部分包含了释义对和非释义对，即识别一对句子是否具有相同的释义（含义），特点是具有高度重叠词汇，重点考察模型对句法结构的理解能力。该数据集的数据预览如下：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2</span><span class="hljs-number">1975</span>年的NBA赛季 -  <span class="hljs-number">76</span>赛季是全美篮球协会的第<span class="hljs-number">30</span>个赛季。<span class="hljs-number">1975</span>-<span class="hljs-number">76</span>赛季的全国篮球协会是NBA的第<span class="hljs-number">30</span>个赛季。<span class="hljs-number">1</span> <br><span class="hljs-attribute">3</span>还有具体的讨论，公众形象辩论和项目讨论。    还有公开讨论，特定档案讨论和项目讨论。<span class="hljs-number">0</span> <br><span class="hljs-attribute">4</span>当可以保持相当的流速时，结果很高。当可以保持可比较的流速时，结果很高。<span class="hljs-number">1</span> <br></code></pre></td></tr></table></figure><br>每条数据包含4列，分别表示数据 id，sentence1，sentence2 和 label，每列之间使用 ‘\t’ 分隔。<br>原始数据集链接：<a href="https://github.com/google-research-datasets/paws">https://github.com/google-research-datasets/paws</a></p><h3 id="北大中文文本复述数据集-PKU-Paraphrase-Bank"><a href="#北大中文文本复述数据集-PKU-Paraphrase-Bank" class="headerlink" title="北大中文文本复述数据集 PKU-Paraphrase-Bank"></a>北大中文文本复述数据集 PKU-Paraphrase-Bank</h3><p>北大发布的中文文本复述语料库，每条数据包含两列，分别表示两个具有相同含义的句子，列与列之间使用 ‘\t’ 分隔。该数据集一共有 509832 组句子对，平均每句 23.05 个词。<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs">莫雷尔指指肩膀，向士兵们暗示那是一个军官，应当给他找个地方暖和暖和。莫雷尔指着他的肩，向士兵们示意，这是一个军官，应当让他暖和一下。 <br>他细心地把斧头套在大衣里面的环扣里。他把斧子细心地挂在大衣里面的绳套上。 <br>仁慈的上帝！难道那时我灵魂中还有一丝精力未曾使用？仁慈的主呵！那时难道有我心灵中的任何一种能力不曾发挥么？ <br></code></pre></td></tr></table></figure><br>原始数据集链接：<a href="https://github.com/pkucoli/PKU-Paraphrase-Bank/">https://github.com/pkucoli/PKU-Paraphrase-Bank/</a></p><h3 id="Chinese-STS-B-数据集"><a href="#Chinese-STS-B-数据集" class="headerlink" title="Chinese-STS-B 数据集"></a>Chinese-STS-B 数据集</h3><p>该数据集通过翻译加部分人工修正的方法，从英文原数据集生成，可以一定程度上缓解中文语义相似度计算数据集不够的问题。每条数据包含三列，分别表示 sentence1、sentence2 和相似等级，相似等级范围为 0~5，5 表示语义一致，0 表示语义不相关。<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs">一架飞机要起飞了。一架飞机正在起飞。5 <br>一个男人在切面包。一个人在切洋葱。2 <br>一个男人在划独木舟。一个人在弹竖琴。0 <br>一个男人开着他的车。一个男人在开车。4 <br>三个男孩在跳舞。孩子们在跳舞。3 <br>一个人一只手握着一只小动物。一个男人在炫耀一只小猴子。1 <br></code></pre></td></tr></table></figure><br>原始数据集链接：<a href="https://github.com/pluto-junzeng/CNSD">https://github.com/pluto-junzeng/CNSD</a></p><h2 id="中文指令数据集"><a href="#中文指令数据集" class="headerlink" title="中文指令数据集"></a>中文指令数据集</h2><blockquote><p>收集包含中文的指令数据集，用于微调语言模型。</p></blockquote><div class="table-container"><table><thead><tr><th>模型</th><th>大小</th><th>语言</th><th>下载</th><th>作者</th><th>项目地址</th><th>备注</th></tr></thead><tbody><tr><td>Zhihu-KOL</td><td>/</td><td>中文</td><td><a href="https://huggingface.co/datasets/wangrui6/Zhihu-KOL">dataset</a></td><td><a href="https://huggingface.co/wangrui6">Rui Wang</a></td><td><a href="https://github.com/wangrui6/Zhihu-KOL">Zhihu-KOL</a></td></tr><tr><td></td></tr><tr><td>InstructionWild</td><td>104k</td><td>中英文</td><td><a href="https://github.com/XueFuzhao/InstructionWild/tree/main/data">dataset</a></td><td><a href="https://github.com/XueFuzhao">Xue Fuzhao</a></td><td><a href="https://github.com/XueFuzhao/InstructionWild">InstructionWild</a></td></tr><tr><td></td></tr><tr><td>GuanacoDataset</td><td>/</td><td>中/多语言</td><td><a href="https://huggingface.co/datasets/JosephusCheung/GuanacoDataset">dataset</a></td><td><a href="https://github.com/Guanaco-Model">Guanaco</a></td><td><a href="https://guanaco-model.github.io/">guanaco-model</a></td></tr><tr><td></td></tr><tr><td>Traditional-Chinese-alpaca</td><td>52K</td><td>中文</td><td><a href="https://github.com/ntunlplab/traditional-chinese-alpaca/tree/main/data">dataset</a></td><td><a href="https://github.com/ntunlplab">NTU NLP Lab</a></td><td><a href="https://github.com/ntunlplab/traditional-chinese-alpaca">Traditional-Chinese Alpaca</a></td><td>gpt翻译</td></tr><tr><td>alpaca_chinese_dataset</td><td>/</td><td>中文</td><td><a href="https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models/blob/main">dataset</a></td><td><a href="https://github.com/hikariming">akou</a></td><td><a href="https://github.com/hikariming/alpaca_chinese_dataset">alpaca_chinese_dataset</a></td><td>人工校验</td></tr><tr><td>alpaca-chinese-dataset</td><td>/</td><td>中文</td><td><a href="https://github.com/carbonz0/alpaca-chinese-dataset">dataset</a></td><td><a href="https://github.com/carbonz0">carbonz</a></td><td><a href="https://github.com/carbonz0/alpaca-chinese-dataset">alpaca-chinese-dataset</a></td><td>机器翻译</td></tr><tr><td>generated_train_1M_CN</td><td>1M</td><td>中文</td><td><a href="https://huggingface.co/datasets/BelleGroup/generated_train_1M_CN">dataset</a></td><td><a href="https://github.com/LianjiaTech">Ke Technologies</a></td><td><a href="https://github.com/LianjiaTech/BELLE">BELLE</a></td></tr><tr><td></td></tr><tr><td>generated_train_0.5M_CN</td><td>0.5M</td><td>中文</td><td><a href="https://huggingface.co/datasets/BelleGroup/generated_train_0.5M_CN">dataset</a></td><td><a href="https://github.com/LianjiaTech">Ke Technologies</a></td><td><a href="https://github.com/LianjiaTech/BELLE">BELLE</a></td></tr><tr><td></td></tr><tr><td>HC3 人类-ChatGPT 问答对比语料集（中文）</td><td>/</td><td>中文</td><td><a href="https://www.modelscope.cn/datasets/simpleai/HC3-Chinese/summary">dataset</a></td><td><a href="https://github.com/Hello-SimpleAI">SimpleAI</a></td><td><a href="https://github.com/Hello-SimpleAI/chatgpt-comparison-detection">chatgpt-comparison-detection</a></td></tr><tr><td></td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Corpus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompting, Instruction, RLHF</title>
    <link href="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/"/>
    <url>/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf?continueFlag=3a2d0ebeb780301d86fd8485c9416eed">斯坦福大学CS224N——深度学习自然语言处理Lecture 11课件-prompting和RLHF</a></p></blockquote><p>之前看了李宏毅老师的深度强化学习课程，内容从浅入深，娓娓道来，但是过了两天知识点全忘了（实际是记不住，也有点理解不到位）…..今天偶然间看到了斯坦福大学CS224N《深度学习自然语言处理》的 <strong>Prompting, Instruction Finetuning, and RLHF</strong> 这一讲的课件，读完之后顿觉醍醐灌顶，再加上课件本身逻辑清晰、内容层层推进且覆盖了NLP领域最新进展（2023年冬季课程），于是对此课件进行简要总结，以备后续不时之需。</p><p>具体分为了下面三节：</p><ul><li>Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning</li><li>Instruction finetuning——指令微调</li><li>Reinforcement Learning from Human Feedback (RLHF)——人类反馈的强化学习</li></ul><h2 id="Zero-Shot-ZS-and-Few-Shot-FS-In-Context-Learning"><a href="#Zero-Shot-ZS-and-Few-Shot-FS-In-Context-Learning" class="headerlink" title="Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning"></a><strong>Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning</strong></h2><p>这一节先从GPT模型的演进引入，首先表述了随着模型参数和训练数据的增大，语言模型逐步涌现（emerging）出了一些能力，这些从GPT对应的论文就可以看出端倪。</p><p>GPT-2对应的论文 <a href="https://link.zhihu.com/?target=https%3A//d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a>，讲述了语言模型是无监督的多任务学习器，在论文中，进一步阐述了预训练好的GTP-2模型在没有进行微调和参数更新的的情况下，在8个零样本学习的任务中取得了7个任务的SOTA。这表明此时的GPT-2已经涌现出了零样本学习的能力。</p><p>GPT-3的论文 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a>，讲述了语言模型是少样本学习器。在这篇论文里，作者们阐述了在简单的任务前添加少量样例的情况下（Specify a task by simply prepending examples of the task before your example），语言模型也能够SOTA的结果。这说明GPT-3已经涌现出了基于上下文的少样本学习能力。同时，添加到任务前的样例也可以看作是一种prompting。</p><p>但是，对于复杂的任务（数学运算、逻辑推理等等），简单的给出结果并不能使模型给出正确的结果。此时，需要将提示词更换一种形式，这就引出了思维链提示词（Chain of Thought prompting，以下简称CoT Prompting）。简单地说，就是将推导的过程添加到提示词中。如下图所示：</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678437974601-c151d4fd-7432-4907-84a0-d80502413ed9.webp" class=""><p>CoT Prompting 样例</p><p>同时，Chain of Thought Prompting这个能力也是随着模型尺度的增大而出现的。</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678437974667-6198edb0-2369-4a99-a3b1-368722e361f1.webp" class=""><p>CoT Prompting 随模型尺度变化曲线</p><p>同时，也有工作探索了Zero-shot CoT Prompting和few-shot CoT Prompting，进而发现了一片新大陆——prompting engineering。</p><h2 id="Instruction-Finetuning"><a href="#Instruction-Finetuning" class="headerlink" title="Instruction Finetuning"></a><strong>Instruction Finetuning</strong></h2><blockquote><p>《<a href="https://zhuanlan.zhihu.com/p/408166011">Instruction Tuning｜谷歌Quoc V.Le团队提出又一精调范式</a>》</p></blockquote><p>之所以有这个工作，是因为大模型生成的结果，与人类的偏好还是有些偏差，或者说没有跟用户的意图对齐（not aligned with user intent）。</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678438829492-09a9928e-9afa-454c-bbc7-0f2051664472.png" class="" title="image.png"><p>为了保持模型现有的zero-shot、few-shot、CoT能力，自然而然地想到可以在预训练好的大模型上进行微调（finetuing)，只不过这时的微调与之前的微调有所区别。之前 Tuning是在某个下游任务 + 少量的 labels → 去微调/适配 PTM，而 <strong>Instruction</strong>是<strong>多任务的微调，在大量的任务指令+大量的 labels → 去适配不同的任务！</strong></p><p>这里，我想花点时间介绍下Instruction Tuning和之前的Prompting、Fine Tuning。三者都是针对预训练模型进行微调的方法，它们的区别如下：</p><ol><li>Fine Tuning：在预训练模型的基础上，针对特定任务进行微调，比如针对猫狗分类任务进行微调。Fine Tuning需要有大量的任务相关数据，且可能会导致预训练的通用性能力下降。</li><li>Prompting：在Fine Tuning之前，先给定一些文本片段作为Prompt，让模型在生成文本时遵循这些Prompt的要求，例如给定“请分类这张图片是否是猫”作为Prompt，让模型输出是或不是。这种方法可以让模型更好地遵循任务的要求，但是需要手动设计Prompt，且精调后的模型只能应用于特定的单一任务！</li><li>Instruction Tuning：在Fine Tuning的基础上，通过给定一系列的任务指令，<strong>经过多任务精调后，能够在 unseen 任务上做zero-shot！</strong>。Instruction Tuning可以减少手动设计Prompt的工作量，同时也能够保持预训练模型的通用性能力。</li></ol><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1676959646417-7cb45ab1-1efc-44a2-a3f3-520eaa7d5ac2.webp" class=""><p>Instruction Tuning和Prompt的核心一样，就是去发掘语言模型本身具备的知识。而他们的不同点就在于，Prompt是去激发语言模型的<strong>补全能力</strong>，比如给出上半句生成下半句、或者做完形填空，<strong>都还是像在做language model任务</strong>，它的模版是这样的：</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1676959646393-4688b64b-5901-4c6e-bea8-507d859e59de.jpeg" class=""><p>而Instruction Tuning则是激发语言模型的<strong>理解能力</strong>，通过给出更明显的指令/指示，让模型去理解并做出正确的action。比如NLI/分类任务：</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1676959646493-07a6c2ac-711d-41e7-a761-40a3e3f8bb12.webp" class=""><p><strong>怎么做Instruction Tuning？</strong><br>具体而言，就是将不同的任务 通过指令模版 抽象为（instruction, output）二元组，喂给模型，同时更新模型参数。</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678438054817-04a6ad97-fe85-49f1-9c30-9eba78932d16.webp" class=""><p>Instruction Finetuing</p><p>需要注意的是，这一步就有点像预训练了，需要很大的数据集和很大的模型。所以课件中戏称instruction finetuning为instruction pretraining，成了两阶段预训练。</p><p>instruction finetuing 的缺点有两个：</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678438405701-b6f77533-a64e-4a51-ab09-bd4f95ffe509.png" class="" title="image.png"><p>为了解决上述两个缺点，RLHF出场了，explicitly attempt to satisfy human preferences?</p><h2 id="Reinforcement-Learning-from-Human-Feedback-RLHF"><a href="#Reinforcement-Learning-from-Human-Feedback-RLHF" class="headerlink" title="Reinforcement Learning from Human Feedback (RLHF)"></a><strong>Reinforcement Learning from Human Feedback (RLHF)</strong></h2><blockquote><p>强化学习基本知识可以参考王树森和张志华老师写的一本书——<a href="https://link.zhihu.com/?target=https%3A//www.math.pku.edu.cn/teachers/zhzhang/drl_v1.pdf">《深度强化学习》</a>，这本书还有对应的<a href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV12o4y197US/">视频教程</a>，讲解深入浅出，值得一看。<br><a href="https://huggingface.co/blog">图解人工反馈强化学习(RLHF) - Hugging Face</a></p></blockquote><p>因为训练得到的模型并不是非常可控的，模型可以看做对训练集分布的一个拟合。那么反馈到生成模型中，训练数据的分布便是影响生成内容的质量最重要的一个因素。有时候我们希望模型并不仅仅只受训练数据的影响，而是人为可控的，从而保证生成数据的有用性，真实性和无害性。论文中多次提到了对齐（Alignment）问题，我们可以理解为模型的输出内容和人类喜欢的输出内容的对齐，人类喜欢的不止包括生成内容的流畅性和语法的正确性，还包括生成内容的有用性、真实性和无害性。</p><p>我们知道强化学习通过奖励（Reward）机制来指导模型训练，奖励机制可以看做传统模训练机制的损失函数。奖励的计算要比损失函数更灵活和多样（AlphaGO的奖励是对局的胜负），这带来的代价是奖励的计算是不可导的，因此不能直接拿来做反向传播。强化学习的思路是通过对奖励的大量采样来拟合损失函数，从而实现模型的训练。同样人类反馈也是不可导的，那么我们也可以将人工反馈作为强化学习的奖励，基于人工反馈的强化学习便应运而生。</p><p>RLHF最早可以追溯到Google在2017年发表的《<a href="https://arxiv.org/pdf/1706.03741.pdf">Deep Reinforcement Learning from Human Preferences</a>》，它通过人工标注作为反馈，提升了强化学习在模拟机器人以及雅达利游戏上的表现效果。</p><p>为了应用强化学习，需要构造出一个reward函数，该函数将语言模型生成的句子作为输入，输出句子对应的评分。比如在文本摘要任务，对于某个句子 𝑠，假设我们有一种方法可以获得 LM 输出摘要的人类奖励：$𝑅(𝑠) ∈ ℝ$，越高越好。训练的目标就是最大化 LM 的期望奖励：</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678439624364-36e48b75-77b1-42f7-a667-bd4292786d60.png" class="" title="image.png"><p>那该如何做呢？最先想到的肯定是梯度下降，但是会存在两个问题：①如何估计这个期望？②$𝑅(𝑠)$奖励函数不可导怎么办？不用担心，强化学习中的策略梯度方法（Policy gradient）已经解决了这些问题！</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678439770604-cf10da31-df4b-4c0e-9861-2ce7b96e5fac.png" class="" title="image.png"><p>策略梯度的关键思想是提高导致更高回报的行动的概率，并降低导致较低回报的行动的概率，直到你达到最优政策。简单的策略梯度介绍如下：</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678440172927-6e8d8c55-4f4b-455a-a56e-ea3e7cd927e8.png" class="" title="image.png"><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678440207786-02db6f4f-b1e2-4890-9670-19745b79671b.png" class="" title="image.png"><p><strong>想了解详细的策略梯度以及 PPO 算法，可移步到《</strong><a href="https://www.yuque.com/ningshixian/pz10h0/uegpsbdxnxm4zmsl"><strong>PPO笔记（转载-李宏毅DRL）</strong></a><strong>》</strong></p><p>Awesome：现在对于任意的、不可微分的奖励函数$𝑅(𝑠)$，我们都可以训练一个语言模型来最大化期望奖励。Not so fast! (Why not?) 。对于奖励函数$𝑅(𝑠)$的构造，还需要考虑如下的两个问题：</p><ul><li>问题 1：人在整个回路中的成本很高！ </li><li><p>解决方案：与其直接询问人类的偏好，不如将他们的偏好建模为一个单独的 (NLP) 问题！让 AI 训练 AI….<strong>采用一个神经网络来对奖励进行学习</strong></p></li><li><p>问题 2：人类的判断是嘈杂和主观的！ </p></li><li>解决方案：不要求奖励模型对句子直接评分，而是要求<strong>成对比较</strong>，这样训练更可靠</li></ul><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678441143728-4c2f0a80-158b-4acf-b4f1-a50d63a78d88.png" class="" title="image.png"><p>有了 reward 函数，以及大语言模型，再有一个依据reward对语言模型参数进行更新的策略，就可以进行<strong>RLHF</strong>了。具体如下图所示：</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678441257900-c866583f-f642-4a89-86f5-0d98210927c7.png" class="" title="image.png"><p>对仅预训练、Instruction Finetuning、RLHF的效果进行了对比，结果如下：</p><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678438054792-e41194b8-a84a-48be-8952-e79ba08089c9.webp" class=""><p>RLHF、Instruction Finetuning、仅预训练的效果对比</p><p>但是，最后还要说明一点，采用了RLHF方法的模型，虽然能够产生看上去更权威更有帮助的结果，但生成结果的<strong>真实性</strong>却值得商榷。（Chatbots are rewarded to produce responses that seem authoritative and helpful, regardless of truth）</p><p>这一节的内容就到这里，最后做个总结：</p><ul><li>InstructGPT: scaling up RLHF to tens of thousands of tasks</li><li>ChatGPT: Instruction Finetuning + RLHF for dialog agents</li></ul><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678441620646-bc2c218e-d9ae-4c39-8bbf-6246d7b408e8.png" class="" title="image.png"><img src="/2023/03/21/2023-03-21-Prompting,%20Instruction,%20RLHF/1678438054830-829b826d-6a06-41fc-8810-30338413e64f.webp" class="" title="https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;610182044"><p>从语言模型到AI助手</p>]]></content>
    
    
    <categories>
      
      <category>ChatGPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PPO算法</title>
    <link href="/2023/03/14/PPO%E7%AE%97%E6%B3%95/"/>
    <url>/2023/03/14/PPO%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><ul><li>根据 OpenAI 的<a href="https://blog.openai.com/openai-baselines-ppo/">官方博客</a>, PPO 已经成为他们在强化学习上的默认算法. <strong>如果一句话概括 PPO: OpenAI 提出的一种解决 Policy Gradient 不好确定 Learning rate (或者 Step size) 的问题. 因为如果 step size 过大, 学出来的 Policy 会一直乱动, 不会收敛, 但如果 Step Size 太小, 对于完成训练, 我们会等到绝望. PPO 利用 New Policy 和 Old Policy 的比例, 限制了 New Policy 的更新幅度, 让 Policy Gradient 对稍微大点的 Step size 不那么敏感.</strong></li><li>总的来说 PPO 是一套 Actor-Critic 结构, Actor 想<strong>最大化</strong> J_PPO, Critic 想<strong>最小化</strong> L_BL.</li></ul><h2 id="PG-Add-Constraint-→-PPO"><a href="#PG-Add-Constraint-→-PPO" class="headerlink" title="PG Add Constraint → PPO"></a>PG Add Constraint → PPO</h2><p>简单来说，PPO就是Policy Gradient的”off-policy”版本。为了满足<strong>Importance Sampling</strong>的使用条件，即防止$p_{\theta}$和$p_{\theta_{old}}$两个概率分布相差太多，PPO提供了两个解决方案：</p><ol><li>TRPO（Trust Region Policy Optimization）在目标函数外使用KL Penalty (惩罚项）来限制策略更新，希望在训练的过程中，new Policy 和 old Policy 的输出不要相差太大（因为输出的 action 是概率分布，也即计算两个概率分布之间的差别）。但是这种方法实现起来很复杂，需要更多的计算时间。</li></ol><p>$L^{PPO}(\theta)=E_{t}\left[r_t(\theta) * A_{t}\right]-\beta·KL[\pi_{\theta_{init}}|\pi_{\theta}]$</p><ol><li>PPO-Clip 在目标函数中使用 <strong>Clipped surrogate objective function </strong>来直接裁剪概率比率。所要做的事情本质上和TRPO是一样的，都是为了让两个分布（$θ$和$θ’$）之间的差距不致过大</li></ol><p>$L^{C L I P}(\theta)=\hat{\mathbb{E}}_{t}\left[\min \left(r_{t}(\theta) \hat{A}_{t}, \operatorname{clip}\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right]$</p><p>其中，$\beta$是可以动态调整的，称之为自适应KL惩罚（adaptive KL penalty）；$r_t(\theta)$表示Ratio Function，指产生同样的 token，在 Policy Model 和 Alignment Model 下的概率比值（It’s the probability of taking action a_t at state s_t in the current policy divided by the previous one. ）</p><p>$r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$</p><p>正如我们所看到的，$r_t(\theta)$表示当前策略和旧策略之间的概率比率，<strong>是估计旧策略和当前策略之间差异的一种简单方法</strong>。</p><ul><li>如果$r_t(\theta)&gt;1$，则在状态$s_t$下，动作$a_t$在当前策略中比旧策略更有可能执行。</li><li>如果$0&lt;r_t(\theta)&lt;1$，则在当前策略下执行该动作的可能性比旧策略下低。</li></ul><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1677118748202.png" class="" title="image.png"><h3 id="PPO-Clip-算法直观理解"><a href="#PPO-Clip-算法直观理解" class="headerlink" title="PPO-Clip 算法直观理解"></a>PPO-Clip 算法直观理解</h3><p>$L^{C L I P}(\theta)=\hat{\mathbb{E}}_{t}\left[\min \left(r_{t}(\theta) \hat{A}_{t}, \operatorname{clip}\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right]$<br>$r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$</p><p>整个目标函数在$min$这个大括号里有两部分，最终对比两部分哪部分更小，就取哪部分的值。<br>在括号的第二部分中，</p><ul><li>首先是裁剪函数$clip$：如果$p_{\theta}(a_t|s_t)$和$p_{\theta^k}(a_t|s_t)$之间的概率比落在范围$(1-ε)$和$(1+ε)$之外，$\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}$将被剪裁，使得其值最小不小于$(1-ε)$，最大不大于$(1+ε)$</li></ul><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1677049573025.jpeg" class="" title="image.png"><ul><li>然后是$clip$括号外乘以$A^{\theta’}(s_t,a_t)$：当$A&gt;0$，则说明这是好动作，那么希望增大这个action的几率$p_{\theta}(a_t|s_t)$，但是又不希望两者差异，即比值$\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}$太悬殊，所以增大到比值为$1+ε$就不要再增加了；当$A&lt;0$，则说明该动作不是好动作，那么希望这个action出现的几率$p_{\theta}(a_t|s_t)$越小越好，但$\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}$最小不能小过$(1-ε)$</li></ul><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1677049702486-4c83f34e-d965-4fd1-8337-135872b51c60.png" class="" title="image.png"><p>换言之，这个裁剪算法和KL散度约束所要做的事情本质上是一样的，都是为了让两个分布之间的差距不致过大，但裁剪算法相对好实现，别看看起来复杂，其实代码很好写，示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">// ratios即为重要性权重<br>// 括号里的environment_log_probs代表用于与环境交互的策略<br>ratios = torch.exp(log_probs - environment_log_probs)<br> <br>// 分别用sur_1、sur_2来计算公式的两部分<br><br>// 第一部分是重要性权重乘以优势函数<br>sur_1 = ratios * advs<br> <br>// 第二部分是具体的裁剪过程<br>sur_2 = torch.clamp(ratios, <span class="hljs-number">1</span> - clip_eps, <span class="hljs-number">1</span> + clip_eps) * advs<br> <br>// 最终看谁更小则取谁<br>clip_loss = -torch.<span class="hljs-built_in">min</span>(sur_1,sur_2).mean()<br></code></pre></td></tr></table></figure><h2 id="简单-PPO-的代码解读"><a href="#简单-PPO-的代码解读" class="headerlink" title="简单 PPO 的代码解读"></a>简单 PPO 的代码解读</h2><blockquote><p><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DPPO">https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DPPO</a></p></blockquote><p>我们用 Tensorflow 搭建神经网络, tensorboard 中可以看清晰的看到我们是如果搭建的:</p><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1679394322067.png" class="" title="image.png"><p>图中的 pi 就是我们的 Actor 了. 每次要进行 PPO 更新 Actor 和 Critic 的时候, 我们有需要将 pi 的参数复制给 oldpi. 这就是 update_oldpi 这个 operation 在做的事. Critic 和 Actor 的内部结构, 我们不会打开细说了. 因为就是一堆的神经网络而已. 这里的 Actor 使用了 normal distribution 正态分布输出动作.</p><p>这个 PPO 我们可以用一个 Python 的 class 代替:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 建 Actor Critic 网络</span><br>        <span class="hljs-comment"># 搭计算图纸 graph</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s, a, r</span>):<br>        <span class="hljs-comment"># 更新 PPO</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-comment"># 选动作</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_v</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-comment"># 算 state value</span><br></code></pre></td></tr></table></figure><p>而这个 PPO 和 env 环境的互动可以简化成这样.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">ppo = PPO()<br><span class="hljs-keyword">for</span> ep <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EP_MAX):<br>    s = env.reset()<br>    buffer_s, buffer_a, buffer_r = [], [], []<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EP_LEN):<br>        env.render()<br>        a = ppo.choose_action(s)<br>        s_, r, done, _ = env.step(a)<br>        buffer_s.append(s)<br>        buffer_a.append(a)<br>        buffer_r.append((r+<span class="hljs-number">8</span>)/<span class="hljs-number">8</span>)    <span class="hljs-comment"># normalize reward, 发现有帮助</span><br>        s = s_<br><br>        <span class="hljs-comment"># 如果 buffer 收集一个 batch 了或者 episode 完了</span><br>        <span class="hljs-keyword">if</span> (t+<span class="hljs-number">1</span>) % BATCH == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> t == EP_LEN-<span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># 计算 discounted reward</span><br>            v_s_ = ppo.get_v(s_)<br>            discounted_r = []<br>            <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> buffer_r[::-<span class="hljs-number">1</span>]:<br>                v_s_ = r + GAMMA * v_s_<br>                discounted_r.append(v_s_)<br>            discounted_r.reverse()<br><br>            bs, ba, br = batch(buffer_s, buffer_a, discounted_r)<br>            <span class="hljs-comment"># 清空 buffer</span><br>            buffer_s, buffer_a, buffer_r = [], [], []<br>            ppo.update(bs, ba, br)  <span class="hljs-comment"># 更新 PPO</span><br></code></pre></td></tr></table></figure><p>了解了这些更新步骤, 我们就来看看如何更新我们的 PPO. 我们更新 Critic 的时候是根据 刚刚计算的 discounted_r 和自己分析出来的 state value 这两者的差 (advantage). 然后最小化这个差值:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.advantage = self.tfdc_r - self.v   <span class="hljs-comment"># discounted reward - Critic 出来的 state value</span><br>        self.closs = tf.reduce_mean(tf.square(self.advantage))<br>        self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)<br><br></code></pre></td></tr></table></figure><p>两种更新 Actor 的方式 KL penalty 和 clipped surrogate objective</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.tfa = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, A_DIM], <span class="hljs-string">&#x27;action&#x27;</span>)<br>        self.tfadv = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;advantage&#x27;</span>)<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;loss&#x27;</span>):<br>            <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;surrogate&#x27;</span>):<br>                ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)<br>                surr = ratio * self.tfadv   <span class="hljs-comment"># surrogate objective</span><br>            <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span>:      <span class="hljs-comment"># 如果用 KL penatily</span><br>                self.tflam = tf.placeholder(tf.float32, <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;lambda&#x27;</span>)<br>                kl = kl_divergence(oldpi, pi)<br>                self.kl_mean = tf.reduce_mean(kl)<br>                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))<br>            <span class="hljs-keyword">else</span>:                               <span class="hljs-comment"># 如果用 clipping 的方式</span><br>                self.aloss = -tf.reduce_mean(tf.minimum(<br>                    surr,<br>                    tf.clip_by_value(ratio, <span class="hljs-number">1.</span>-METHOD[<span class="hljs-string">&#x27;epsilon&#x27;</span>], <span class="hljs-number">1.</span>+METHOD[<span class="hljs-string">&#x27;epsilon&#x27;</span>])*self.tfadv))<br><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;atrain&#x27;</span>):<br>            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)<br><br></code></pre></td></tr></table></figure><p>好了, 接下来就是最重要的更新 PPO 时间了, 同样, 如果觉得我这些代码省略的很严重, 请直接前往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/simply_PPO.py">Github 看全套代码</a>. 注意的是, 这个 update 的步骤里, 我们用 for loop 更新了很多遍 Actor 和 Critic, 在 loop 之前, pi 和 old pi 是一样的, 每次 loop 的之后, pi 会变动, 而 old pi 不变, 这样这个 surrogate 就会开始变动了. 这就是 PPO 的精辟.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s, a, r</span>):<br>        <span class="hljs-comment"># 先要将 oldpi 里的参数更新 pi 中的</span><br>        self.sess.run(self.update_oldpi_op)<br><br>        <span class="hljs-comment"># 更新 Actor 时, kl penalty 和 clipping 方式是不同的</span><br>        <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span>:  <span class="hljs-comment"># 如果用 KL penalty</span><br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A_UPDATE_STEPS):<br>                _, kl = self.sess.run(<br>                        [self.atrain_op, self.kl_mean],<br>                        &#123;self.tfs: s, self.tfa: a, self.tfadv: adv, self.tflam: METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>]&#125;)<br>                <span class="hljs-comment"># 之后根据 kl 的值, 调整 METHOD[&#x27;lam&#x27;] 这个参数</span><br>        <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># 如果用 clipping 的方法</span><br>            [self.sess.run(self.atrain_op, &#123;self.tfs: s, self.tfa: a, self.tfadv: adv&#125;) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A_UPDATE_STEPS)]<br><br>        <span class="hljs-comment"># 更新 Critic 的时候, 他们是一样的</span><br>        [self.sess.run(self.ctrain_op, &#123;self.tfs: s, self.tfdc_r: r&#125;) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(C_UPDATE_STEPS)]<br><br></code></pre></td></tr></table></figure><p>最后我们看一张学习的效果图:</p><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1679394584080.png" class="" title="image.png"><p>好了这就是整个 PPO 的主要流程了, 其他的步骤都没那么重要了, 可以直接在我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/simply_PPO.py">Github 看全套代码</a> 中轻松弄懂. </p><h2 id="PPO-Actor-Critic-Loss"><a href="#PPO-Actor-Critic-Loss" class="headerlink" title="PPO Actor-Critic Loss"></a>PPO Actor-Critic Loss</h2><p>PPO Actor-Critic 风格的最终 Clipped Surrogate Objective Loss 看起来像这样，它是 Clipped Surrogate Objective 函数、Value Loss Function 和 Entropy bonus 的组合：</p><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/image.png" class="" title="image.png"><p>DeepMind 总结 OpenAI conference 上的 PPO 的伪代码</p><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/image2.png" class="" title="image.png"><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.51cto.com/u_15721703/5575736">李宏毅老师的深度强化学习课程</a><br><a href="https://huggingface.co/deep-rl-course/unit8/introduction-sf?fw=pt">Unit 8. Introduction to PPO - Hugging Face</a><br><a href="https://huggingface.co/blog">图解人工反馈强化学习(RLHF) - Hugging Face</a></p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PPO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PPO要点摘录</title>
    <link href="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/"/>
    <url>/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>PPO（Proximal Policy Optimization）算法中，状态、动作、价值函数、奖励和策略模型的含义如下：</p><ol><li>状态（state）：状态是指环境当前的状态，如：对话历史与之前生成的序列。在强化学习中，智能体需要根据当前状态来做出决策。</li><li>动作（action）：动作是指智能体在当前状态下选择的行动，如：生成每一个token，从词表中采样即是一个动作。在强化学习中，智能体需要根据当前状态选择一个最优的动作。</li><li>价值函数（Value Function）：价值函数则是智能体对自己行为好坏的评估，它可以帮助智能体更好地指导自己的决策。价值函数可以根据当前状态或状态-动作对，预测智能体能够获得的期望回报。比如，在一个赛车游戏中，如果智能体在某个状态下选择了某个动作，那么价值函数可以预测出这个动作会获得多少奖励分数。通过不断地优化价值函数，智能体可以学习到在环境中做出最优的决策。</li><li>奖励（Reward）：奖励函数就像是老师对学生的评分一样，通过环境的反馈，对智能体的行为进行评价，给出一个分数用来衡量智能体做出的动作是否正确。比如，在一个赛车游戏中，如果智能体成功完成一次绕过障碍的动作，那么就可以获得一定的奖励分数。通过不断地累积奖励分数，智能体可以学习到在环境中做出最优的动作。</li><li>策略模型（Policy Model）：策略模型是指智能体根据当前状态选择动作的概率分布。在PPO算法中，策略模型通常使用神经网络来进行建模，输出每个动作的概率。</li></ol><p>综上所述，PPO算法需要使用神经网络来表示价值函数和策略模型，根据当前状态选择动作，并通过环境的反馈来获得奖励。通过不断优化策略和价值函数，PPO算法可以使智能体在环境中获得更高的奖励，从而实现智能体的学习和决策能力的提高。</p><blockquote><p>参考：<a href="https://spinningup.openai.com/en/latest/algorithms/vpg.html">https://spinningup.openai.com/en/latest/algorithms/vpg.html</a></p></blockquote><p>actor在进行训练之前，会先与环境进行交互，然后得到一组训练数据（由多条状态序列构成）。</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/46d23d87a5c5abf0ca1c48a7d816d46e.svg" class=""><p>每回合Trajectory<img src="https://cdn.nlark.com/yuque/__latex/ec1cc44b87fcbbced12dabd7375d36d3.svg#from=url&amp;id=SwJ8M&amp;originHeight=18&amp;originWidth=16&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">的奖励</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/299758ac4817013ed2822cdfc136f77a.svg" class=""><p>最终目标就是要使<strong>期望奖励最大</strong>！</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/56897ebb6501e4a47704b198d7bcf5b9.svg" class=""><p>根据链式法则公式$\nabla \log f(x) = \frac{1}{f(x)} \nabla f(x)$，计算式(2)的梯度</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678462247057-e3c72c54-7cbc-4ab6-a0da-f50a57ffe335.png" class="" title="image.png"><blockquote><p> <img src="https://cdn.nlark.com/yuque/0/2023/svg/8420697/1678463145871-d64a91f7-2168-4694-8f90-ad3b099a879a.svg#clientId=u6f13600c-cac6-4&amp;from=paste&amp;id=u2bca4fe6&amp;originHeight=13&amp;originWidth=26&amp;originalType=url&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u3fe21edd-a2d2-441f-9acc-87d0a71c77c&amp;title=" alt="">是当前策略的优势函数<br>$\pi_{\theta}$表示带有参数$θ$的策略，$J(\pi_{\theta})$表示该策略的 期望回报</p></blockquote><p>策略梯度算法的工作原理是通过策略性能的随机梯度上升来更新策略参数：</p><p>$\theta ← \theta+\alpha\nabla{R}$</p><p>策略梯度（Policy Gradient）方法伪代码</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678463814708-26dacbd5-6edf-4056-bdec-5cb2cea3e278.png" class="" title="image.png"><hr><p>为了使得训练资料可以反复使用，使用两个actor</p><p>其中，一个固定参数，另一个可训练，通过<strong>Importance Sampling</strong>来联系两个分布：</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678285382558-74f3aa01-8cff-4c62-9528-a4b64c099e7c.png" class="" title="image.png"><blockquote><p>代入式4，梯度计算为：</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678285679173-c8804740-750d-4df1-8344-982403686226.png" class="" title="image.png"><p><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1678285693021-0331d447-0af9-4991-8da0-54016205c657.png#averageHue=%23f2f5f4&amp;clientId=uef89bebf-095c-4&amp;from=paste&amp;height=55&amp;id=u1633ed96&amp;name=image.png&amp;originHeight=110&amp;originWidth=804&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=61955&amp;status=done&amp;style=none&amp;taskId=u234f4b37-ec52-4e28-a067-40e220a7611&amp;title=&amp;width=402" alt="image.png"></p></blockquote><p>代入式2，新的目标函数为：</p><p>$L=E_{\tau\in p_{\theta}(\tau)} [A(\tau)]=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[r_{\theta}(t)A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]$</p><p>$r_{\theta}(t)=\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} \mid s_{t}\right)}$</p><p>Advantages are computed using Generalized Advantage Estimation (GAE): $A_t$叫做策略优势估计，是PPO算法的核心!</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678443935731-6516a589-895f-43e6-bcb3-59c661d81ad8.png" class="" title="image.png"><p>PPO 进一步给目标函数添加约束项，防止$p_{\theta}$和$p_{\theta’}$两个概率分布相差太多！PPO 是一种 on-policy 算法，有两种主要变体：PPO-Penalty 和 PPO-Clip。</p><ul><li>PPO-Penalty：</li></ul><p>$L(s,a,θ_k,θ) = r_{\theta}(t)·A^{\pi_{\theta k}}(s,a)-\beta·KL(θ,θ_k)$</p><p>$r_{\theta}(t)=\frac{\pi_{\theta}\left(a \mid s\right)}{\pi_{\theta^{\prime}}\left(a \mid s\right)}$</p><ul><li>PPO-Clip：</li></ul><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678464277268-b4d8e192-dcc1-4e7f-9544-add909d41690.png" class="" title="image.png"><p>PPO方法伪代码</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678464873211-15bbc7ba-8930-4e13-abaf-a106d6df8151.png" class="" title="image.png"><p>符号解释：</p><ul><li>$p_{\theta}$表示可训练策略模型</li><li>$p_{\theta’}$表示参数固定的策略模型</li><li>$\frac{p_{\theta}(·)}{p_{\theta{‘}}(·)}$表示 Importance Sampling 系数，指产生同样的 token，在 Policy Model 和 Alignment Model 下的概率比值</li><li>$A$ 或$R$表示奖励，其给出一种利用长期奖励$r_t$与短期奖励$V({s_t})$计算当前步的好坏，在第t步的$A_t$分数越高，则代表该位置生成的质量越高。<ul><li>Reward Model，是一个固定参数的模型，其输出值$r_t$</li><li>Value Function Model，是一个可训练的价值函数，其输出值$V({s_t})$代表生成的每一个token的质量，是一个短期的奖励</li></ul></li><li>$KL()$表示KL散度，用作一个惩罚项，约束着每一次梯度更新以后不要产生与原模型相距甚远的回复。保证了模型训练的稳定性。</li></ul>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PPO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ChatGPT复现总结</title>
    <link href="/2023/03/02/2023-03-02-ChatGPT%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/"/>
    <url>/2023/03/02/2023-03-02-ChatGPT%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://zhuanlan.zhihu.com/p/607847588?utm_medium=social&amp;utm_oi=556189566109925376&amp;utm_psn=1611295462803406848&amp;utm_source=wechat_session">复现ChatGPT的难点与平替</a><br><a href="https://zhuanlan.zhihu.com/p/607744955">复现和使用GPT-3/ChatGPT，我们应该注意什么？</a></p></blockquote><p>1.3B的模型+RLHF就可以很强，在真正的落地中，训一个for单一生成任务的定制化ChatGPT不再那么遥不可及，一两张A100和十万级别的数据就可以了——《追赶ChatGPT的难点与平替》</p><p>在人力、算力、时间有限的情况下，<strong>效率最优的路径是直接在1.3B模型上迭代，大概10万标注数据，复现一个低配小型ChatGPT，验证整个流程的有效性，再去做175B的模型。——《</strong>追赶ChatGPT的难点与平替<strong>》</strong></p><img src="/2023/03/02/2023-03-02-ChatGPT%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/1679295939637-8e94487d-80ae-4e1e-aa5b-d1a4243fe198.png" class="" title="image.png"><h2 id="LLM-资源"><a href="#LLM-资源" class="headerlink" title="LLM 资源"></a>LLM 资源</h2><ul><li>GPT-3 和 PaLM，这两个模型都不是公开的</li><li>OPT 和 Bloom 是公开模型，基于<strong>GPT架构</strong>，但是效果达不到GPT-3；<ul><li>OPT只支持英文，Bloom支持中英；</li><li>OPT-IML (OPT + Instruction Meta-Learning) is a set of instruction-tuned versions of OPT, ①OPT-IML trained on 1500 tasks with several tasks held-out for purposes of downstream evaluation, and ②OPT-IML-Max trained on all ~2000 tasks</li><li><a href="https://huggingface.co/bigscience/bloom-1b1">bigscience/bloom-1b1</a>、</li><li><a href="https://huggingface.co/facebook/opt-350m">facebook/opt-350m</a>、<a href="https://huggingface.co/facebook/opt-125m">facebook/opt-125m</a>、<a href="https://huggingface.co/facebook/opt-iml-max-1.3b">facebook/opt-iml-max-1.3b</a></li></ul></li><li><strong>另外还有 mT5、GLM、LLAMA 等不基于 GPT 架构；</strong><ul><li><a href="https://huggingface.co/miguelvictor/multilingual-gpt2-large">miguelvictor/multilingual-gpt2-large</a></li><li><a href="https://huggingface.co/models?sort=downloads&amp;search=llama+7">LLAMA-7B</a></li></ul></li></ul><div class="table-container"><table><thead><tr><th></th><th><strong>预训练语料库的token数</strong></th><th><strong>在预训练过程能看到的token数</strong></th></tr></thead><tbody><tr><td>PaLM</td><td>780B</td><td>770B</td></tr><tr><td>GPT-3</td><td>500B</td><td>300B</td></tr><tr><td>OPT</td><td>180B</td><td>300B</td></tr><tr><td>BLOOM</td><td>341B</td><td>366B</td></tr><tr><td>LLAMA</td><td>&gt;1T</td><td>&gt;1T</td></tr></tbody></table></div><h2 id="平替方案"><a href="#平替方案" class="headerlink" title="平替方案"></a>平替方案</h2><p><a href="https://github.com/hpcaitech/ColossalAI">ColossalAI</a>框架，提供了在单张 GPU 上即可尝试的 ChatGPT 训练流程</p><ul><li>对于基于 1.2 亿参数小模型的 ChatGPT 训练，最低仅需 1.62GB 显存，任意单张消费级 GPU 即可满足</li><li>ChatGPT 提供了开源基础模型 GPT，OPT 和 BLOOM</li></ul><p>预训练数据要求：</p><ul><li><strong>通过数据筛选以得到高质量数据。</strong>一个用更少但质量更高的数据集训练的预训练模型，可以在性能上超过另一个用更多的混合质量数据集训练的模型。</li><li><strong>通过数据去重避免记忆和过拟合。</strong>去重有助于避免预训练模型多次面对相同的数据后记住它们或者在其上过拟合，因此有助于提高模型的泛化能力。</li><li><strong>保证数据多样性以确保LLM的泛化性。</strong>包括领域多样性、格式多样性（例如：文本、代码和表格）和语言多样性。</li></ul><p>训练三部曲：</p><ul><li>阶段 1：监督微调 (SFT，也可以叫 Instruction tuning）解锁知识和涌现能力<ul><li>收集各种NLP数据集，构造【指令+回复】语料；</li><li>使用构造的数据监督微调一个大模型，如 opt 1.3B、bloom 1.1B(中文)；</li></ul></li></ul><img src="/2023/03/02/2023-03-02-ChatGPT%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/1679287538189-e1794b1e-3c7b-460c-97e3-86810e38163c.png" class="" title="未命名绘图.drawio.png"><ul><li><p>阶段 2：训练奖励模型（RM do not with lora），评价文本生成的好坏</p><ul><li><p>Second, given the fine-tuned model and a prompt, the model will generate several model outputs. A labeler gives the desired score and ranks the output to compose a comparison dataset, which is used to train the reward model.</p><p>给定微调模型和采样Prompts，预测生成多个模型输出，然后需要人工打分排序，以组成比较数据集，该数据集用于训练奖励模型。</p></li><li>Prompts 的收集是个难题！可以考虑中英互译、爬虫知乎问答、已有 prompts 自己写答案…</li></ul></li></ul><img src="/2023/03/02/2023-03-02-ChatGPT%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/1679287654270-36317103-d54c-4a4c-a04e-86d85fc39f17.png" class="" title="未命名绘图.drawio.png"><ul><li>阶段 3：通过强化学习让模型去拟合人的偏好<ul><li>Finally, the fine-tuned model (ChatGPT) is optimized against the reward model using the Proximal Policy Optimization (PPO) RL algorithm</li><li>最后，使用 PPO 针对训练有素的奖励模型对大模型进行微调，以使 LLM 与人类偏好保持一致</li></ul></li></ul><img src="/2023/03/02/2023-03-02-ChatGPT%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/1679302137322-1ef27792-adac-40f2-8bb1-756fcd24d13a.png" class="" title="image.png"><h3 id="1、Instruction-tuning-→-SFT"><a href="#1、Instruction-tuning-→-SFT" class="headerlink" title="1、Instruction-tuning → SFT"></a>1、Instruction-tuning → SFT</h3><blockquote><p>这一阶段是对语言模型进行指令微调，使语言模型能够更好地遵循指令；</p><p><a href="https://github.com/zhilizju/Awesome-instruction-tuning">https://github.com/zhilizju/Awesome-instruction-tuning</a></p></blockquote><p>以下是英文的<a href="https://github.com/zhilizju/Awesome-instruction-tuning">【指令+答案】数据集</a>：</p><div class="table-container"><table><thead><tr><th>Modified from Traditional NLP</th><th></th><th></th></tr></thead><tbody><tr><td>Datasets</td><td>Number of Tasks</td><td>Number of Instances</td></tr><tr><td><a href="https://github.com/orhonovich/unnatural-instructions">Unnatural Inst.</a> ⭐️75</td><td>117</td><td>64 k</td></tr><tr><td>core_data.jsonl，包含 64k 个【指令-输入-输出】三元组，以及 full_data.jsonl，包含完整的240,670个不自然指令示例。</td><td></td><td></td></tr><tr><td><a href="https://github.com/allenai/natural-instructions">Super-Natural Inst.</a></td><td>1613</td><td>5M</td></tr><tr><td><a href="https://instructions.apps.allenai.org/">Natural Inst v1.0</a></td><td>61</td><td>620 k</td></tr><tr><td><a href="https://github.com/google-research/FLAN/tree/main#flan-2021">Flan 2021</a></td><td>62</td><td>4.4M</td></tr><tr><td><a href="https://github.com/bigscience-workshop/promptsource">P3</a></td><td>62</td><td>12M</td></tr><tr><td><a href="https://github.com/allenai/unifiedqa">UnifiedQA</a></td><td>46</td><td>750k</td></tr><tr><td><a href="https://github.com/tatsu-lab/stanford_alpaca">tatsu-lab/Alpaca</a></td><td></td><td></td></tr><tr><td><a href="https://github.com/gururise/AlpacaDataCleaned">gururise/Cleaned Alpaca</a></td><td></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>Generated by LLMs</th><th></th><th></th></tr></thead><tbody><tr><td>Datasets</td><td>Number of Tasks</td><td>Number of Instances</td></tr><tr><td><a href="https://huggingface.co/datasets/yizhongw/self_instruct">GPT-3 Self-Instruct</a> ⭐️180</td><td>41 k</td><td>82 k</td></tr><tr><td>data/gpt3-generations/batch_221203/all_instances_82K.jsonl：一共包含41K条指令，82K个实例输入和输出配对，调整重点是多轮对话、<strong>问答</strong>、分类、提取和摘要等几个任务data/finetuning/self_instruct_221203：干净的GPT3微调格式 (prompt + completion) ，这种格式是用于训练GPT3模型的一种特定的JSONL文件格式</td><td></td><td></td></tr><tr><td><a href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json">alpaca_data</a></td><td>En</td><td>52 k</td></tr><tr><td><a href="https://github.com/XueFuzhao/InstructionWild">ColossalChat InstructionWild</a></td><td>En Zh</td><td>104 k</td></tr></tbody></table></div><p><strong>一阶段训练代码如下：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">nohup python -u run_clm_no_trainer.py --train_file data/all_instances_82K.csv  --model_name_or_path /data/voiceprint2/FlexGen-main/opt-iml-<span class="hljs-built_in">max</span>-<span class="hljs-number">1.3</span>b  --block_size <span class="hljs-number">512</span> --per_device_train_batch_size <span class="hljs-number">2</span> --per_device_eval_batch_size <span class="hljs-number">2</span> --num_train_epochs <span class="hljs-number">4</span> --output_dir opt_1b3_Selfdata_epoch4 --learning_rate <span class="hljs-number">2e-5</span> &gt; logs/finetune_onSelfInstruct_opt1b3.log &amp;<br><br><span class="hljs-comment"># 添加梯度累计</span><br>python -u run_clm_no_trainer.py --train_file data/all_instances_82K.csv  --model_name_or_path /data/voiceprint2/FlexGen-main/opt-iml-<span class="hljs-built_in">max</span>-<span class="hljs-number">1.3</span>b  --block_size <span class="hljs-number">512</span> --per_device_train_batch_size <span class="hljs-number">2</span> --per_device_eval_batch_size <span class="hljs-number">2</span> --num_train_epochs <span class="hljs-number">4</span> --output_dir opt_1b3_Selfdata_epoch4 --learning_rate <span class="hljs-number">2e-5</span> --gradient_accumulation_steps <span class="hljs-number">15</span><br><br><span class="hljs-comment"># Lora (SFT效果会变差？)</span><br>CUDA_VISIBLE_DEVICES=<span class="hljs-number">2</span> nohup python train_sft.py --dataset data/gpt3_generations.csv --strategy naive --model opt --pretrain /data/voiceprint2/FlexGen-main/opt-iml-<span class="hljs-built_in">max</span>-<span class="hljs-number">1.3</span>b --max_epochs <span class="hljs-number">4</span> --batch_size <span class="hljs-number">8</span> --lora_rank <span class="hljs-number">16</span> &gt; logs/finetune_onSelfInstruct_opt1b3.log &amp;<br></code></pre></td></tr></table></figure><p>ps：机器为4卡，GPU型号是Tesla V100-PCIE 4卡 32G</p><p>ps：block_size相当于序列长度至多 512 否则 OOM，batch-size最大至多 3，学习率默认是 5e-5；</p><p>ps：训练 4 个 epoch</p><h3 id="2、Reward-Model-Training"><a href="#2、Reward-Model-Training" class="headerlink" title="2、Reward Model Training"></a>2、Reward Model Training</h3><p>以下是用于训练奖励模型的英文数据集：</p><ul><li><a href="https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences">Dataset Card for H4 Stack Exchange Preferences Dataset</a><ul><li>来自 Stack Exchange 的1000多万个问题的问答数据集，带多个答案以及评分；</li></ul></li><li><a href="https://huggingface.co/datasets/Anthropic/hh-rlhf">Anthropic/hh-rlhf</a><ul><li>features: [‘chosen’, ‘rejected’]</li><li>160800 条训练数据， 8553 条测试数据</li><li>—dataset Anthropic/hh-rlhf</li></ul></li><li><a href="https://huggingface.co/datasets/Dahoas/rm-static/tree/main">Dahoas/rm-static</a><ul><li>features: [‘prompt’, ‘response’, ‘chosen’, ‘rejected’]</li><li>76256 条训练数据，5103 条测试数据</li><li>—dataset Dahoas/rm-static</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">nohup python -u train_reward_model.py --strategy naive --pretrain facebook/opt-125m --model opt --dataset Anthropic/hh-rlhf --save_path no_lora_rm_opt125m.pth --loss_fn log_sig --max_epochs <span class="hljs-number">4</span> --batch_size <span class="hljs-number">8</span> &gt; logs/opt_rm.log &amp;<br><br><span class="hljs-comment"># 分布式训练要先切换编译器</span><br>scl enable devtoolset-<span class="hljs-number">7</span> bash<br>source ~/.bashrc<br>source activate xxenv<br><br>CUDA_VISIBLE_DEVICES=<span class="hljs-number">0</span>,<span class="hljs-number">1</span> nohup torchrun --standalone --nproc_per_node <span class="hljs-number">2</span> train_reward_model.py  --strategy colossalai_zero2 --pretrain facebook/opt-350m --dataset Anthropic/hh-rlhf --save_path no_lora_rm_opt350m.pth --loss_fn log_sig --max_epochs <span class="hljs-number">2</span> --batch_size <span class="hljs-number">8</span> &gt; logs/opt_rm.log &amp;<br></code></pre></td></tr></table></figure><p>注意：</p><p>①可能遇到如下问题。原因是我们 reward model 任务是一个回归任务，，而我们加载的是预训练语言模型 PLM（_AutoModelForCausalLM是在一系列token之后预测token的任务_），所以会提示我们加载模型的时候扔掉了一些不匹配/未使用的神经网络参数（比如：PLM 的预测 head 被扔掉了，同时随机初始化了一个回归任务的预测 head）<a href="https://zhuanlan.zhihu.com/p/403496010">https://zhuanlan.zhihu.com/p/403496010</a></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">Some</span> weights <span class="hljs-keyword">of</span> the model <span class="hljs-keyword">checkpoint</span> at facebook/opt<span class="hljs-number">-125</span>m were <span class="hljs-keyword">not</span> used <span class="hljs-keyword">when</span> initializing OPTModel: [<span class="hljs-string">&#x27;lm_head.weight&#x27;</span>]<br></code></pre></td></tr></table></figure><p>②卡到No pre-built kernel is found, build and load the cpu_adam kernel during runtime now<br>的时候 删除这个/home/lhadmin/.cache/colossalai 文件夹就行</p><p>③单机多卡进行分布式（gemini）训练时会遇到错误，导致无法保存！改为zero2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># torch.save(trainer.model)</span><br><br>NotImplementedError<br>WARNING:torch.distributed.elastic.multiprocessing.api:Sending process <span class="hljs-number">17278</span> closing signal SIGTERM<br>ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: <span class="hljs-number">1</span>) local_rank: <span class="hljs-number">1</span> (pid: <span class="hljs-number">17279</span>) of binary: /usr/local/anaconda3/envs/nlp_env39/<span class="hljs-built_in">bin</span>/python<br></code></pre></td></tr></table></figure><p>④ 测试 reward model 效果，所有 input 的预测得分都一样？</p><p>解决：将opt350m 改为opt125m + 增大batch_size=8</p><h3 id="3、PPO-训练"><a href="#3、PPO-训练" class="headerlink" title="3、PPO 训练"></a>3、PPO 训练</h3><p>ColossalAI使用Hugging Face 的<a href="https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/tree/main">awesome-chatgpt-prompts</a>作为训练数据，因此将此prompts.csv下载到工作文件夹。该测试使用一个小数据集，大约 100 个提示。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">nohup python train_prompts.py data/prompts.csv --strategy naive --lora_rank <span class="hljs-number">16</span> --model opt --pretrain opt_1b3_Selfdata_epoch4 --rm_path no_lora_rm_opt350m.pth --max_epochs <span class="hljs-number">5</span> &gt; logs/opt1b3Actor_RM_opt_ep5_Lora16_RL.log &amp;<br><br><span class="hljs-comment"># 分布式训练要先切换编译器</span><br>scl enable devtoolset-<span class="hljs-number">7</span> bash<br>source ~/.bashrc<br>source activate xxenv<br><br>CUDA_VISIBLE_DEVICES=<span class="hljs-number">1</span>,<span class="hljs-number">2</span> torchrun --standalone --nproc_per_node <span class="hljs-number">2</span> train_prompts.py data/prompts.csv --strategy colossalai_zero2 --lora_rank <span class="hljs-number">16</span> --model opt --pretrain opt_1b3_Selfdata_epoch4 --rm_path no_lora_rm_opt350m.pth --max_epochs <span class="hljs-number">5</span><br><br><span class="hljs-comment">#rm_path lora_rm_model.pth是前一步训练好的reward model 地址 必须指定正确才能正常工作</span><br></code></pre></td></tr></table></figure></p><ul><li>OPTActor.model = OPTForCausalLM</li><li>OPTCritic.model = OPTModel    OPTCritic.value_head = nn.Linear</li><li>OPTRM.model = OPTModel        OPTRM.value_head = nn.Linear</li></ul><p>注意 1：ColossalAI的 ChatGPT 复现不包括 SFT 的训练部分。另外训练好的 Reward 模型暂时还不支持放到 RL 里面去用，需要自行解决。</p><img src="/2023/03/02/2023-03-02-ChatGPT%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/1677659173665-772a024e-3d03-4990-bf8b-b4a4d778261c.png" class="" title="image.png">]]></content>
    
    
    <categories>
      
      <category>ChatGPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer详解          </title>
    <link href="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/"/>
    <url>/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p><a href="https://arxiv.org/abs/1706.03762"><strong>Transformer</strong></a>抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder，并在机器翻译中取得了BLEU值得新高。</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1673943315513-662666c1-47e8-41bf-906c-2c2c0e95765c.png" class="" title="image.png"><h2 id="Encoder-结构"><a href="#Encoder-结构" class="headerlink" title="Encoder 结构"></a>Encoder 结构</h2><p>Transformer 包括编码器和解码器两部分。本节主要介绍Transformer的编码器部分，其结构如图2.5（a）所示。它主要包括位置编码（Position Embedding）、多头注意力（Multi-Head Attention）以及前馈神经网络：</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1655463549843-5147cd6f-62b2-41c0-976f-4e8a64cbb321.png" class="" title="5.Transformer.tif"><h3 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h3><blockquote><p><a href="https://kexue.fm/archives/8130">让研究人员绞尽脑汁的Transformer位置编码-科学空间</a><br><a href="https://vaclavkosar.com/ml/transformer-positional-embeddings-and-encodings">Transformer Positional Embeddings and Encodings</a></p></blockquote><p>我们知道，文字的先后顺序，很重要。比如<code>吃饭没</code>、<code>没吃饭</code>、<code>没饭吃</code>、<code>饭吃没</code>、<code>饭没吃</code>，同样三个字，顺序颠倒，所表达的含义就不同了。</p><p>不同于RNN、CNN等模型，对于Transformer模型来说，位置编码的加入是必不可少的，因为纯粹的Attention模块是无法捕捉输入顺序的，即无法区分不同位置的Token。为此我们大体有两个选择：1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了相对位置编码的一般做法。</p><ul><li>绝对位置编码是相对简单的一种方案，①可以直接将位置编码当作可训练参数，比如最大长度为512，编码维度为768，那么就初始化一个 <strong>512×768 </strong>的矩阵作为位置向量，让它随着训练过程更新。现在的BERT、GPT等模型所用的就是这种位置编码；②三角函数式位置编码，一般也称为Sinusoidal位置编码，是Google的论文<a href="https://arxiv.org/abs/1706.03762">《Attention is All You Need》</a>所提出来的一个显式解，下文会详细介绍；③先接一层RNN学习位置信息，然后再接Transformer，那么理论上就不需要加位置编码了。<ul><li>输入$x_k$与绝对位置编码$p_k$的组合方式一般是$x_k+p_k$</li></ul></li><li>相对位置并没有完整建模每个输入的位置信息，而是在算Attention的时候考虑当前位置与被Attention的位置的相对距离，由于自然语言一般更依赖于相对位置，所以相对位置编码通常也有着优秀的表现。对于相对位置编码来说，它的灵活性更大，更加体现出了研究人员的“天马行空”。</li></ul><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1655623701153-d74c30d4-7df5-4068-8ca7-ad43638b1792.png" class="" title="image.png"><p>为了能够对位置信息进行编码，Transformer为序列中的每个单词引入了位置编码特征。通过融合词向量和位置向量，来为每一个词引入了一定的位置信息。<code>Tranformer</code> 采用的是 <code>sin-cos</code> 三角函数式位置编码，计算公式如下：</p><p>$\begin{aligned} P E_{(\text {pos}, 2 i)} &amp;=\sin \left(\text {pos} / 10000^{2 i / d}\right) \\ P E_{(\text {pos}, 2 i+1)} &amp;=\cos \left(\text {pos} / 10000^{2 i / d}\right) \end{aligned}$</p><p>其中， <code>pos</code> 表示位置编号，目标是将其被映射为一个$d$维的位置向量，该向量的第$i$个元素值通过公式$PE$进行计算。由于$sin(α+β)=sinαcosβ+cosαsinβ$以及$cos(\alpha+\beta)=cos{\alpha}cos{\beta}-sin{\alpha}sin{\beta}$，这表明位置α+β的向量可以表示成位置α和位置β的向量组合，这提供了表达相对位置信息的可能性。但很奇怪的是，现在我们很少能看到直接使用这种形式的绝对位置编码的工作，原因不详。</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1655614243125-029b80db-6101-47a2-8033-3967c3dce5b4.png" class="" title="image.png"><p>可以用代码，简单看下效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入依赖库</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_positional_encoding</span>(<span class="hljs-params">max_seq_len, embed_dim</span>):<br>    <span class="hljs-comment"># 初始化一个positional encoding</span><br>    <span class="hljs-comment"># embed_dim: 字嵌入的维度</span><br>    <span class="hljs-comment"># max_seq_len: 最大的序列长度</span><br>    positional_encoding = np.array([<br>        [pos / np.power(<span class="hljs-number">10000</span>, <span class="hljs-number">2</span> * i / embed_dim) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(embed_dim)]<br>        <span class="hljs-keyword">if</span> pos != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> np.zeros(embed_dim) <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_seq_len)])<br>    positional_encoding[<span class="hljs-number">1</span>:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = np.sin(positional_encoding[<span class="hljs-number">1</span>:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>])  <span class="hljs-comment"># dim 2i 偶数</span><br>    positional_encoding[<span class="hljs-number">1</span>:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = np.cos(positional_encoding[<span class="hljs-number">1</span>:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>])  <span class="hljs-comment"># dim 2i+1 奇数</span><br>    <span class="hljs-comment"># 归一化, 用位置嵌入的每一行除以它的模长</span><br>    <span class="hljs-comment"># denominator = np.sqrt(np.sum(position_enc**2, axis=1, keepdims=True))</span><br>    <span class="hljs-comment"># position_enc = position_enc / (denominator + 1e-8)</span><br>    <span class="hljs-keyword">return</span> positional_encoding<br>    <br>positional_encoding = get_positional_encoding(max_seq_len=<span class="hljs-number">100</span>, embed_dim=<span class="hljs-number">16</span>)<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>sns.heatmap(positional_encoding)<br>plt.title(<span class="hljs-string">&quot;Sinusoidal Function&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;hidden dimension&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;sequence length&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="Attention层"><a href="#Attention层" class="headerlink" title="Attention层"></a>Attention层</h3><p><strong>Attention层的好处是能够**</strong>一步到位捕捉到全局的联系<strong>**，因为它直接把序列两两比较（代价是计算量变为 </strong>$O(n^2)$<strong>，当然由于是纯矩阵运算，这个计算量相当也不是很严重）；相比之下，RNN需要一步步递推才能捕捉到，而CNN则需要通过层叠来扩大感受野，这是Attention层的明显优势。</strong></p><p>Self Attention 具体操作如下：首先，每个词都要通过三个矩阵Wq, Wk, Wv进行一次线性变化，一分为三，生成每个词自己的<strong>query, key, vector</strong>三个向量。以一个词为中心进行Self Attention时，都是用这个词的key向量与每个词的query向量做点积，再通过Softmax归一化出权重。然后通过这些权重算出所有词的vector的加权和，作为这个词的输出。</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1675762746226-139e737c-412a-4f06-9999-10c2438f5b8a.png" class="" title="image.png"><p>Google给出的Attention的定义:</p><p>${Attention}(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</p><p>其中，$\boldsymbol{Q} \in \mathbb{R}^{n \times d{k}}, \boldsymbol{K} \in \mathbb{R}^{m \times d{k}}, \boldsymbol{V} \in \mathbb{R}^{m \times d_{v}} , Z是归一化因子$。单头注意力通过「放缩点积注意力」（Scaled dot-product attention）来将查询$Q$与$K$进行点积并缩放，再馈送到Softmax函数以获得与$V$对应的相似度权重。根据这些权重对序列自身$V$进行加权求和，建模序列内部联系，从而得到$n$个$d$维的输出向量。其中因子$\sqrt{d_k}$起到调节作用，使得内积不至于太大（太大的话softmax后就非0即1了，不够“soft”了）。</p><p>逐个向量来看:</p><p>$Attention \left(\boldsymbol{q}_{t}, \boldsymbol{K}, \boldsymbol{V}\right)=\sum_{s=1}^{m} \frac{1}{Z} \exp \left(\frac{\left\langle\boldsymbol{q}_{t}, \boldsymbol{k}_{s}\right\rangle}{\sqrt{d_{k}}}\right) \boldsymbol{v}_{s}$</p><p>其中，$q,k,v$ 分别是 $query,key,value$ 的简写。</p><p>最终Self Attention将$n×d_k$的输入序列$Q$编码成了一个新的$n×dv$的输出序列.</p><p>小结：所谓Self Attention，其实就是$Attention(X,X,X)$，$X$ 就是前面说的输入序列。也就是说，在序列内部做Attention，寻找序列内部的联系。</p><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>所谓“多头”指的是同样的操作（参数不共享）重复多遍，然后把结果拼接起来。多头注意力的结构如图2.5（b）所示，把$Q,K,V$通过参数矩阵映射一下，然后做单头注意力（自注意力），把这个过程重复做 $h$ 次，结果拼接起来，最后得到一个 $n×(hd_v)$ 的序列。</p><p>${head}_{i} = {Attention}(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V})<br>\\<br>{MultiHead}(Q, K, V) ={Concat}({head}_{1}, \cdots, {head}_{h})$</p><p>其中，$W^Q,W^K,W^V$ 对应线性变换的权重矩阵。</p><h3 id="Add-amp-Norm-层"><a href="#Add-amp-Norm-层" class="headerlink" title="Add &amp; Norm 层"></a>Add &amp; Norm 层</h3><p>Add表示残差，将一层的输入与其标准化后的输出进行相加即可。Norm表示层归一化（Layer Normalization），具体可看《<a href="https://www.yuque.com/ningshixian/pz2e1w/vnsumg">归一化方法介绍</a>》</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1675763228866-9827ed28-f907-4f57-9580-eb97847a75c8.png" class="" title="image.png"><h3 id="前馈神经网络FFN"><a href="#前馈神经网络FFN" class="headerlink" title="前馈神经网络FFN"></a>前馈神经网络FFN</h3><p>序列经过Add &amp; Norm层后，会被送入 FFN层 中进行降维处理。前馈神经网络包含两个线性变换和一个非线性ReLU激活函数，计算公式如下：</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1655464747502-7bb4c1cb-a985-4d14-bb41-544fffee76f7.png" class="" title="image.png"><p>其中，$W_1,W_2,b_1,b_2$ 是可训练的参数。</p><h2 id="Decoder-结构"><a href="#Decoder-结构" class="headerlink" title="Decoder 结构"></a>Decoder 结构</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/338817680">基于 transformer-decoder 的 CLM 是如何工作的？</a></p></blockquote><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1678178718832-81217fc5-bb6c-4301-9950-c41773f81a21.webp" class=""><p>Transformer Decoder block</p><p>上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p><ul><li>包含两个 Multi-Head Attention 层。</li><li>第一个 Multi-Head Attention 层采用了 Masked 操作。</li><li>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个 Decoder block 的输出计算。</li><li>最后有一个 Softmax 层计算下一个翻译单词的概率。</li></ul><h3 id="5-1-第一个-Multi-Head-Attention"><a href="#5-1-第一个-Multi-Head-Attention" class="headerlink" title="5.1 第一个 Multi-Head Attention"></a>5.1 第一个 Multi-Head Attention</h3><p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。</p><p>下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “<Begin>“ 预测出第一个单词为 “I”，然后根据输入 “<Begin> I” 预测下一个单词 “have”。</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1678178718869-d5a71936-50a9-4ad2-a27e-c81081539e16.png" class=""><p>Decoder 预测</p><p>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (<Begin> I have a cat) 和对应输出 (I have a cat <end>) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<strong>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “<Begin> I have a cat <end>“。</strong></p><p><strong>第一步：</strong>是 Decoder 的输入矩阵和 <strong>Mask </strong>矩阵，输入矩阵包含 “<Begin> I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask </strong>是一个 5×5 的矩阵。在 <strong>Mask </strong>可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1678178718871-b5ecd220-c3bc-4470-973e-21e6614beb5a.png" class=""><p>输入矩阵与 Mask 矩阵</p><p><strong>第二步：</strong>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵<strong>X</strong>计算得到<strong>Q,K,V</strong>矩阵。然后计算<strong>Q</strong>和 $K^T$ 的乘积 $QK^T$ 。</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1678178718860-9820b351-4c44-4ea6-a874-495623593a4a.webp" class=""><p>Q乘以K的转置</p><p><strong>第三步：</strong>在得到 $QK^T$ 之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用<strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1678178718836-b24fba3a-e3fa-4f17-bf1f-e85879b9b700.webp" class=""><p>Softmax 之前 Mask</p><p>得到 <strong>Mask</strong> $QK^T$ 之后在 <strong>Mask </strong>$QK^T$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p><p><strong>第四步：</strong>使用 <strong>Mask </strong>$QK^T$与矩阵<strong> V</strong>相乘，得到输出 <strong>Z</strong>，则单词 1 的输出向量 $Z_1$ 是只包含单词 1 信息的。</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1678178719311-0edf5276-28c7-4909-84b7-a94c8e99c270.png" class=""><p>Mask 之后的输出</p><p><strong>第五步：</strong>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $Z_i$ ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出Zi 然后计算得到第一个 Multi-Head Attention 的输出<strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p><h3 id="5-2-第二个-Multi-Head-Attention"><a href="#5-2-第二个-Multi-Head-Attention" class="headerlink" title="5.2 第二个 Multi-Head Attention"></a>5.2 第二个 Multi-Head Attention</h3><p>Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 <strong>K, V</strong>矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 <strong>Encoder 的编码信息矩阵 C </strong>计算的。</p><p>根据 Encoder 的输出 <strong>C</strong>计算得到 <strong>K, V</strong>，根据上一个 Decoder block 的输出<strong> Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block 则使用输入矩阵 <strong>X</strong> 进行计算)，后续的计算方法与之前描述的一致。</p><p>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <strong>Mask</strong>)。</p><h3 id="5-3-Softmax-预测输出单词"><a href="#5-3-Softmax-预测输出单词" class="headerlink" title="5.3 Softmax 预测输出单词"></a>5.3 Softmax 预测输出单词</h3><p>Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1678178719370-daea7cf1-60ff-483f-81af-8f60702f9f1a.webp" class=""><p>Decoder Softmax 之前的 Z</p><p>Softmax 根据输出矩阵的每一行预测下一个单词：</p><img src="/2023/02/26/2023-02-26-Transformer%E8%AF%A6%E8%A7%A3/1678178719381-1f80b4f1-ff65-4c5e-ba21-08a27bed633d.png" class=""><p>Decoder Softmax 预测</p><p>这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。</p><h1 id="Transformer-总结"><a href="#Transformer-总结" class="headerlink" title="Transformer 总结"></a>Transformer 总结</h1><ol><li>Transformer摆脱了nlp任务对于rnn，lstm的依赖，在长距离上的建模能力更强；</li><li>使用了 <code>self-attention</code> 可以并行化地对上下文进行建模，提高了训练和推理的速度；</li><li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</li><li>Transformer 的重点是 Self-Attention 结构，其中用到的 Q, K, V矩阵通过输出进行线性变换得到。</li><li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</li><li>Transformer也是后续更强大的nlp预训练模型的基础（bert系列使用了transformer的encoder，gpt系列transformer的decoder）</li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://kexue.fm/archives/4765">Attention is All You Need》浅读（简介+代码）</a></li><li><a href="https://zhuanlan.zhihu.com/p/345680792">Transformer 一篇就够了（一）： Self-attenstion</a></li><li><a href="https://zhuanlan.zhihu.com/p/347709112">Transformer 一篇就够了（三）： Transformer的实现</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> <ul><li><a href="https://zhuanlan.zhihu.com/p/347904940">保姆级教程：图解Transformer</a></li></ul></li><li><a href="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a> “带注释”版本的pytorch代码详细走读(harvard)</li><li><a href="https://kexue.fm/archives/8620">浅谈Transformer的初始化、参数化与标准化</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>llm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🍺预训练语言模型小酌new          </title>
    <link href="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/"/>
    <url>/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文为 Pre-trained Models for Natural Language Processing: A Survey 和相关模型的笔记</p></blockquote><p><a href="https://www.yuque.com/ningshixian/pz10h0/mu9a48/edit#luHYs">一、为什么要进行预训练？</a></p><p><a href="https://www.yuque.com/ningshixian/pz10h0/mu9a48/edit#FOh7K">二、PTMs 有哪两大范式？</a></p><p><a href="https://www.yuque.com/ningshixian/pz10h0/mu9a48/edit#YfxPH">三、PTMs 按照任务类型如何分类？</a></p><p><a href="https://www.yuque.com/ningshixian/pz10h0/mu9a48/edit#FRdXy">四、PTMs 有哪些拓展？</a></p><p><a href="https://www.yuque.com/ningshixian/pz10h0/mu9a48/edit#P4x2t">五、如何对 PTMs 进行迁移学习？</a></p><p><a href="https://www.yuque.com/ningshixian/pz10h0/mu9a48/edit#a8f55deb">预训练模型详解</a></p><ul><li>ELMo</li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#gpt-2018-radford2018improving">GPT (2018)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#bert-2018-devlin2018bert">BERT (2018)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#unilm-2019-dong2019unified">UniLM (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#transformer-xl-2019-dai2019transformer">Transformer-XL (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#xlnet-2019-yang2019xlnet">XLNet (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#mass-2019-song2019mass">MASS (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#roberta-2019-liu2019roberta">RoBERTa (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#bart-2019-lewis2019bart">BART (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#t5-2019-raffel2019exploring">T5 (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#ernie-baidu-2019-sun2019ernie-sun2019ernie2">ERNIE (Baidu, 2019)</a></li><li>Albert / DistillBERT / TINYBERT</li><li>BERT 拓展</li></ul><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1650275584551-bacbbf55-45d2-43b2-89c8-ba0e09771416.png" class="" title="PTM 总结.png"><h1 id="一、为什么要进行预训练？"><a href="#一、为什么要进行预训练？" class="headerlink" title="一、为什么要进行预训练？"></a>一、为什么要进行预训练？</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/115014536">NLP算法面试必备！PTMs：NLP预训练模型的全面总结 —- JayJay</a></p></blockquote><p>深度学习时代，为了充分训练深层模型参数并防止过拟合，通常需要更多标注数据喂养。在 NLP 领域，标注数据更是一个昂贵资源。PTMs 从大量无标注数据中进行预训练使许多 NLP 任务获得显著的性能提升。<strong>总的来看，预训练模型 PTMs 的优势包括：</strong></p><ol><li>在庞大的无标注数据上进行预训练可以获取更通用的语言表示，并有利于下游任务；</li><li>为模型提供了一个更好的初始化参数，在目标任务上具备更好的泛化性能、并加速收敛；</li><li>是一种有效的正则化手段，避免在小数据集上过拟合（一个随机初始化的深层模型容易对小数据集过拟合）</li></ol><h1 id="二、PTMs-有哪两大范式？"><a href="#二、PTMs-有哪两大范式？" class="headerlink" title="二、PTMs 有哪两大范式？"></a>二、PTMs 有哪两大范式？</h1><blockquote><p><a href="https://www.yuque.com/ningshixian/pz10h0/opzyty?singleDoc#">https://www.yuque.com/ningshixian/pz10h0/opzyty?singleDoc#</a> 《语言模型的前世今生》</p></blockquote><h1 id="三、PTMs-的训练任务如何分类？"><a href="#三、PTMs-的训练任务如何分类？" class="headerlink" title="三、PTMs 的训练任务如何分类？"></a>三、PTMs 的训练任务如何分类？</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/115014536">NLP算法面试必备！PTMs：NLP预训练模型的全面总结 —- JayJay</a></p></blockquote><p>PTMs 按照任务类型可分为 2 大类：监督学习 和 无监督学习/自监督学习。</p><p>监督学习在 NLP-PTMs 中的主要代表就是 <strong>CoVe</strong>，CoVe 作为机器翻译的 encoder 部分可以应用于多种 NLP 下游任务。除了 CoVe 外，NLP 中的绝大多数 PTMs 属于自监督学习。</p><p>自监督学习是无监督学习的一种方法，自监督学习主要是利用辅助任务从大规模的无监督数据中挖掘自身的监督信息，通过这种构造的监督信息对网络进行训练，从而可以学习到对下游任务有价值的表征。因此，从“构造监督信息”这个角度来看，自监督也可看作是监督学习和无监督学习的一种融合。严格地讲，从是否由人工标注来看，自监督学习属于无监督学习的范畴。</p><p>综合各种自监督学习的分类方式，笔者将 NLP-PTMs 在自监督学习中分为两种类型：基于上下文（Context Based）和基于对比（Contrastive Based）。</p><p><strong>1、基于上下文（Context Based）</strong></p><p>基于上下文的 PTMs，主要基于数据本身的上下文信息构造辅助任务，在 NLP 中我们通常引入语言模型作为训练目标。PTMs 中的语言模型主要分为三大类：</p><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1650276292664-4bd14080-8096-42d4-9e80-31d8b39718e6.png" class="" title="image.png"><p><strong>第一类：自回归语言模型（Autoregressive LM）</strong></p><p>在ELMO／BERT出来之前，大家通常讲的语言模型其实是根据上文内容预测下一个可能跟随的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词，这种类型的LM被称为自回归语言模型。auto-regressive的loss的定义如下：</p><p>$\max _{\theta} \log p_{\theta}(\mathbf{x})=\sum_{t=1}^{T} \log p_{\theta}\left(x_{t} | \mathbf{x}_{&lt;t}\right)=\sum_{t=1}^{T} \log \frac{\exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x^{\prime}\right)\right)}$</p><p>GPT 就是典型的自回归语言模型。ELMO尽管看上去利用了上文，也利用了下文，但是本质上仍然是自回归LM，这个跟模型具体怎么实现有关系。ELMO是做了两个方向（从左到右以及从右到左两个方向的语言模型），但是是分别有两个方向的自回归LM，然后把LSTM的两个方向的隐节点状态拼接到一起，来体现双向语言模型这个事情的。所以其实是两个自回归语言模型的拼接，本质上仍然是自回归语言模型。</p><p>自回归语言模型有优点有缺点，缺点是只能利用上文或者下文的信息，不能同时利用上文和下文的信息，当然，貌似ELMO这种双向都做，然后拼接看上去能够解决这个问题，因为融合模式过于简单，所以效果其实并不是太好。它的优点，其实跟下游NLP任务有关，比如生成类NLP任务，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。而Bert这种DAE模式，在生成类NLP任务中，就面临训练过程和应用过程不一致的问题，导致生成类的NLP任务到目前为止都做不太好。</p><p><strong>第二类：自编码语言模型（Autoencoder LM）</strong></p><p>自回归语言模型只能根据上文预测下一个单词，或者反过来，只能根据下文预测前面一个单词。相比而言，Bert通过在输入X中随机Mask掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被Mask掉的单词，如果你对Denoising Autoencoder比较熟悉的话，会看出，这确实是典型的DAE的思路。那些被Mask掉的单词就是在输入侧加入的所谓噪音。类似Bert这种预训练模式，被称为DAE LM（旨在采用部分损坏的输入，旨在恢复原始的未失真输入）。</p><p>DAE LM的loss的定义如下：(如果当前 token 被预测，则 $m_t=1$否则 $m_t=0$，$\widetilde x$为原始文本被替换后的输入)</p><p>$\max_{\theta} \log p_{\theta}(\overline{\mathbf{x}} | \hat{\mathbf{x}}) \approx \sum_{t=1}^{T} m_{t} \log p_{\theta}\left(x_{t} | \hat{\mathbf{x}}\right)=\sum_{t=1}^{T} m_{t} \log \frac{\exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x^{\prime}\right)\right)}$</p><p>这种DAE LM的优缺点正好和自回归LM反过来，它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，这是好处。缺点是啥呢？主要在输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的。DAE吗，就要引入噪音，[Mask] 标记就是引入噪音的手段，这个正常。</p><p>两种不同的预训练目标的优劣势对比如下：</p><ol><li><strong>独立假设</strong>：BERT 中联合条件概率 $p(\overline{\mathbf{x}} | \hat{\mathbf{x}})$ 假设在给定的 x^ 下，遮挡的词条 $\overline{\mathbf{x}}$ 是相关独立的，而 AR(auto-regressive) 语言模型则没有这样的假设。</li><li><strong>输入噪声</strong>：BERT 在预训练是使用了特殊标记 <code>[MASK]</code>，在下游任务微调时不会出现，而 AR 语言模型则不会存在这个问题。</li><li><strong>上下文依赖</strong>：AR 语言模型仅考虑了词条左侧的上下文，而 BERT 则可以捕获两个方向的上下文。</li></ol><p><strong>第三类：排列语言模型（PLM）</strong></p><p>为了能够结合自回归LM和DAE LM两者的优点，XLNet 提出了排序语言模型 Permuted Language Model(PLM)。对于一个长度为 T 序列 x，共有 $T!$ 种不同的方式进行 AR 分解，随机选择一部分作为模型预训练的输入，如果模型共享不同分解顺序的参数，那么模型就能学习到两侧所有位置的信息。令 $Z_T$ 为长度为 T 的索引序列 $[1,2,…,T]$ 的所有可能排列，$z_t$ 和 $z_{&lt;t}$ 分别表示一个排列 $z∈Z_T$ 第 t 个和前 t−1 个元素。则排列语言模型的优化目标为：</p><p>$\max_{\theta} \quad \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\sum_{t=1}^{T} \log p_{\theta}\left(x_{z_{t}} | \mathbf{x}_{\mathbf{z}_{&lt;t}}\right)\right]$</p><p>PLM一样采用单个Transformer模型作为主干结构，但是从训练方法上来说，是个很另类也很有创意的做法，是种“形为AR，实为AE”的做法。在语言模型预训练过程中，它看上去遵循AR从左到右的输入过程，这符合一般生成任务的外在表现形式，但是在内部通过Attention Mask，实际做法其实是AE的做法，无非是把AE的做法隐藏在Transformer内部。它和AE从细节来说，主要有两个区别：首先，预训练过程中，输入句子去掉了Mask标记，改为内部Attention Mask，以保持预训练过程和下游任务Fine-tuning的一致性；其次，也是它和AE的最主要区别，PLM认为被Mask掉的单词之间是相互有影响的，先产生的被Mask掉的单词，应该对后生成的被Mask掉的单词，在预测的时候发生作用，而标准的AE则认为被Mask掉的单词是相互独立的，相互之间不产生作用。</p><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1652349654559-7213f045-17a2-4e0b-870a-3a6c35ba564e.png" class="" title="image.png"><p>最后，我们对基于上述三类语言模型的 PTMs 进行总结：</p><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1650276293002-13009e34-eaaa-4cc1-8074-baec51a63d0e.png" class="" title="image.png"><p><strong>2、基于对比（Contrastive Based）</strong></p><p>基于对比（Contrastive Based），不同于 <strong>Context Based</strong> 主要基于数据本身的上下文信息构造辅助任利用，Contrastive Based 主要利用样本间的约束信息构造辅助任务，这类方法也是 Contrastive learning（CTL）。CTL 假设观察到的文本对（正样本）在语义上比随机采样的文本（负样本）更相似。CTL 背后的原理是<strong>「在对比中学习」</strong>。相较于语言建模，CTL 的计算复杂度更低，因而在预训练中是理想的替代训练标准。</p><p>CTL 通过构建正样本（positive）和负样本（negative），然后度量正负样本的距离来实现自监督学习：可以使用点积的方式构造距离函数，然后构造一个 softmax 分类器，以正确分类正样本和负样本。鼓励相似性度量函数将较大的值分配给正例，将较小的值分配给负例：</p><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1650277182234-6929b86e-81f9-47c2-b164-53fddbf76b8c.png" class="" title="image.png"><p>-</p><p>基于对比的PTMs 主要有如下 4 类形式：</p><p><strong>第一类： Deep InfoMax (DIM)</strong></p><p>DIM 方法来源于 CV 领域，对于全局的特征（编码器最终的输出）和局部特征（编码器中间层的特征），DIM 需要判断全局特征和局部特征是否来自同一图像。</p><p><strong>InfoWord</strong> 将 DIM 引入到 NLP 中，用 Mutual Information 的一个下界 InfoNCE 来重新解释 BERT 和 XLNET 的 objective，并提出一个新的 DIM objective 以最大化一个句子的 global representation 和其中一个 ngram 的 local representation 之间的 Mutual Information。</p><p><strong>第二类：Replaced Token Detection (RTD)</strong></p><p>噪声对比估计（Noise-Contrastive Estimation，NCE）通过训练一个二元分类器来区分真实样本和假样本，可以很好的训练词嵌入。RTD 于与 NCE 相同，根据上下文语境来预测 token 是否替换 。</p><ul><li><strong>word2vec*</strong> 中的 negative sampling 可看作是 RTD，负样本从词表中进行带权采样。</li><li><strong>ELECTRA*</strong> 提出了一种新的预训练任务框架，构建生成器-判别器，生成器通过 MLM 任务对被 mask 的 token 进行预测，迭代器判断原始句子中的每个 token 是否被 replace 过。生成器相当于对输入进行了筛选，使判别器的任务更难，从而学习到更好的表示。生成器-判别器共享 embedding，生成器部分采用 small-bert，判别器部分对每一个 token 采用 sigmoid 计算 loss。finetune 阶段只采用判别器部分。RTD 也被看作解决 MLM 中「MASK」在预训练和 finetune 间差异的一种手段。</li><li><strong>WKLM*</strong> 在实体 level 进行替换，替换为具有相同实体类型的实体名称。</li></ul><p><strong>第三类：Next Sentence Prediction (NSP)</strong></p><p>NSP 区分两个输入句子是否为训练语料库中的连续片段，第二个句子 50% 为第一句子实际的连续片段，50% 从其他语料随机选择。NSP 可以引导模型理解两个输入句子之间的关系，从而使对此信息敏感的下游任务受益，如 QA 任务。而 RoBERTa 表明：NSP 在对单个文档中的文本块进行训练时，去除 NSP 任务或在下游任务上可以稍微提高性能。</p><p><strong>第四类：Sentence Order Prediction (SOP)</strong></p><p>SOP 使用同一文档中的两个连续片段作为正样本，而相同的两个连续片段互换顺序作为负样本。NSP 融合了主题预测和相关性预测，主题预测更容易，这使得模型进行预测时仅依赖于主题学习。与 NSP 不同，SOP 使用同一文档中的两个连续段作为正样本，但顺序互换为负样本。采取 SOP 任务的 PTMs 有 ALBERT、StructBERT、BERTje。</p><p>最后，我们对基于对比（Contrastive Based）的四类 PTMs 进行总结：</p><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1650276292083-ec453eff-7eac-46b8-8291-b4edf0fbcc4f.png" class="" title="基于对比（Contrastive Based）的四类 PTMs 总结"><h1 id="四、PTMs-有哪些拓展？"><a href="#四、PTMs-有哪些拓展？" class="headerlink" title="四、PTMs 有哪些拓展？"></a>四、PTMs 有哪些拓展？</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/115014536">NLP算法面试必备！PTMs：NLP预训练模型的全面总结 —- JayJay</a></p></blockquote><p><strong>1、引入知识</strong></p><p>PTMs 通常从通用大型文本语料库中学习通用语言表示，但是缺少特定领域的知识。PTMs 中设计一些辅助的预训练任务，将外部知识库中的领域知识整合到 PTMs 中被证明是有效的。</p><ul><li><strong>ERNIE-THU</strong> 将在知识图谱中预先训练的实体嵌入与文本中相应的实体提及相结合，以增强文本表示。由于语言表征的预训练过程和知识表征过程有很大的不同，会产生两个独立的向量空间。为解决上述问题，在有实体输入的位置，将实体向量和文本表示通过非线性变换进行融合，以融合词汇、句法和知识信息。</li><li><strong>LIBERT</strong>（语言知识的 BERT）通过附加的语言约束任务整合了语言知识。</li><li><strong>SentiLR</strong> 集成了每个单词的情感极性，以将 MLM 扩展到标签感知 MLM（LA-MLM），ABSA 任务上都达到 SOTA。</li><li><strong>SenseBERT</strong> 不仅能够预测被 mask 的 token，还能预测它们在给定语境下的实际含义。使用英语词汇数据库 WordNet 作为标注参照系统，预测单词在语境中的实际含义，显著提升词汇消歧能力。</li><li><strong>KnowBERT</strong> 与实体链接模型以端到端的方式合并实体表示。</li><li><strong>KG-BERT</strong> 显示输入三元组形式，采取两种方式进行预测：构建三元组识别和关系分类，共同优化知识嵌入和语言建模目标。这些工作通过实体嵌入注入知识图的结构信息。</li><li><strong>K-BERT</strong> 将从 KG 提取的相关三元组显式地注入句子中，以获得 BERT 的扩展树形输入。</li><li><strong>K-Adapter</strong> 通过针对不同的预训练任务独立地训练不同的适配器来注入多种知识，从而可以不断地注入知识，以解决注入多种知识时可能会出现灾难性遗忘问题。</li><li>此外，这类 PTMs 还有 WKLM、KEPLER 等。</li></ul><p><strong>2、模型压缩</strong></p><p>由于预训练的语言模型通常包含至少数亿个参数，因此很难将它们部署在现实应用程序中的在线服务和资源受限的设备上。模型压缩是减小模型尺寸并提高计算效率的有效方法。5 种 PTMs 的压缩方法为：</p><ul><li><strong>pruning（剪枝）</strong>：将模型中影响较小的部分舍弃。<ul><li>如 Compressing BERT，还有结构化剪枝 LayerDrop* ，其在训练时进行 Dropout，预测时再剪掉 Layer，不像知识蒸馏需要提前固定 student 模型的尺寸大小。</li></ul></li><li><strong>quantization（量化）</strong>：将高精度模型用低精度来表示；<ul><li>如 Q-BERT 和 Q8BERT ，量化通常需要兼容的硬件。</li></ul></li><li><strong>parameter sharing （参数共享）</strong>：相似模型单元间的参数共享；<ul><li>ALBERT 主要是通过矩阵分解和跨层参数共享来做到对参数量的减少。</li></ul></li><li><strong>module replacing（模块替换）</strong>：<ul><li>BERT-of-Theseus 根据伯努利分布进行采样，决定使用原始的大模型模块还是小模型，只使用 task loss。</li></ul></li><li><strong>knowledge distillation （知识蒸馏）</strong>：通过一些优化目标从大型、知识丰富、fixed 的 teacher 模型学习一个小型的 student 模型。蒸馏机制主要分为 3 种类型：<ul><li>从软标签蒸馏：DistilBERT、EnsembleBERT</li><li>从其他知识蒸馏：TinyBERT、BERT-PKD、MobileBERT、 MiniLM、DualTrain</li><li>蒸馏到其他结构：Distilled-BiLSTM</li></ul></li></ul><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1650277767178-60527907-256a-43b6-ac20-87d3b59119b3.png" class="" title="不同的知识蒸馏 PTMs"><p><strong>3、多模态</strong></p><p>随着 PTMs 在 NLP 领域的成功，许多研究者开始关注多模态领域的 PTMs，主要为通用的视觉和语言特征编码表示而设计。多模态的 PTMs 在一些庞大的跨模式数据语料库（带有文字的语音、视频、图像）上进行了预训练，如带有文字的语音、视频、图像等，主要有 VideoBERT、CBT 、UniViLM、 ViL-BERT 、 LXMERT、 VisualBERT 、 B2T2 、Unicoder-VL 、UNITER 、 VL-BERT 、 SpeechBERT。</p><p><strong>4、从两阶段模型到四阶段模型</strong></p><p>经典的预训练模型框架下，一般我们解决NLP问题有两个阶段：第一阶段是模型预训练阶段，预训练模型从文本等信息中学习语言知识；第二阶段是Fine-tuning阶段，根据手上的有监督数据，对模型参数进行微调，以获得更好的任务效果。</p><p>论文“Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks”也通过大量实验验证了领域数据预训练（DAPT）的有效性，再结合它得出的另外一个重要结论：用手上的任务数据，无论大小，如果做一次任务级数据预训练（TAPT），也就是拿着手上任务数据，在通用预训练模型基础上，再做一次预训练，也能够有效提升任务效果。综合这个文章和其它有关文章的结论，我们不难看出，要想更好地提升任务效果，我们应该从传统的两阶段模型，拓展到如下四阶段模型：</p><p>第一个阶段：通用预训练</p><p>这就是传统两阶段模式中的第一阶段。这个阶段不仅仅追求效果好，也追求领域通用性。它的优化目标是：在尽可能多的下游任务场景中，效果都尽可能好，但不单独考虑某个特殊领域的效果如何。这个阶段，目前看总的发展趋势是：在数据质量有保证的前提下，增加数据数量，以及数据的多样性，同时提升模型复杂度，这样可以提供普遍有效的模型增强能力。很明显，这个阶段，一般只有土豪公司才能做得起，而且从趋势看，会越来越如此。将来的发展模式可能是，超级土豪公司不断优化这个模型，然后放出来供大家用，有能力做这个事情的人，应该会越来越少。</p><p>第二个阶段：领域预训练</p><p>在第一阶段训练好的通用预训练模型基础上，利用不同领域的自由文本，构建多个、不同领域的领域预训练模型。比如我们可以分别收集计算机领域、生物领域、电商领域…等等，多个不同领域的无标注自由文本数据。在第一阶段通用模型基础上，分别用各个领域数据，再分别做一次预训练，这样我们就得到了适合解决各个不同领域的预训练模型：计算机领域、生物领域、电商领域…..等等多个不同的预训练模型。下游任务可以根据自己任务的领域，选择适配性好的领域预训练模型来使用。</p><p>这个阶段的预训练模型，在训练的时候，有个独特的问题需要解决：灾难遗忘问题。所谓“灾难遗忘”，就是说，当你用领域数据进行预训练的时候，因为会调整第一阶段预训练模型的参数，这种偏向领域性的参数调整，可能会导致第一阶段模型学好的参数被改写，这意味着：经过第二阶段预训练，第一阶段预训练模型里学会的很多通用语言知识，可能会被冲掉。灾难遗忘就是这个意思。灾难遗忘问题，对于预训练模型，尤其是领域预训练模型来说，是个很关键也很重要的问题，目前也有一些解决方案，限于篇幅，这里就不展开了。</p><p>这个阶段的预训练，因为数据量相比第一阶段会小很多，所以其实中农公司甚至贫农公司也能做得起，不存在土豪门槛，大家应该都能做。当然，一般我们只做跟自己手头任务相关的领域的预训练模型。如果你想做很多领域的预训练模型，那估计也要备足银行卡。估计后续也会有土豪公司做好很多不同领域的预训练模型，供大家个性化适配使用，虽说目前还没有，但是推断起来，这是个大概率会发生的事件。</p><p>第三个阶段：任务预训练</p><p>在前两个预训练模型基础上，比如从第二个阶段里面的多个不同的领域预训练模型中，选择和手头任务适配的那个领域预训练模型，在这个模型基础上，用手头数据，抛掉数据标签，再做一次预训练，无论手上任务数据有多少。比如手上任务是计算机领域的，那么从第二阶段的多个领域模型里面，选择计算机领域适配过的预训练模型，在这个模型基础上进行一次任务级别的预训练。这样应该能明显提升任务效果。</p><p>第四阶段：任务Fine-tuning</p><p>这是传统两阶段的第二阶段，做法一样，没什么好讲的。</p><p>当然，如果你手上的任务没有那么强的领域性，可以跳过第二阶段，也就是那个领域预训练模型阶段，走剩余的三阶段模式即可，无论如何，任务预训练都是值得做的一个事情。</p><p><strong>5、多语言和特定语言</strong></p><p>学习跨语言共享的多语言文本表示形式对于许多跨语言的 NLP 任务起着重要的作用。</p><ul><li><strong>Multilingual-BERT</strong> 在 104 种 Wikipedia 文本上进行 MLM 训练（共享词表），每个训练样本都是单语言文档，没有专门设计的跨语言目标，也没有任何跨语言数据，M-BERT 也可以很好的执行跨语言任务。</li><li><strong>XLM</strong> 通过融合跨语言任务（翻译语言模型）改进了 M-BERT，该任务通过拼接平行语料句子对进行 MLM 训练。</li><li><strong>Unicoder</strong> 提出了 3 种跨语言预训练任务：1)cross-lingual word recovery；2) cross-lingual paraphrase classification;3) cross-lingual masked language model.</li></ul><p>虽然多语言的 PTMs 在跨语言上任务表现良好，但用单一语言训练的 PTMs 明显好于多语言的 PTMs。此外一些单语言的 PTMs 被提出：BERT-wwm，ZEN，NEZHA , ERNIE-Baidu, BERTje, CamemBERT, FlauBERT, RobBERT。</p><h1 id="五、如何对-PTMs-进行迁移学习？"><a href="#五、如何对-PTMs-进行迁移学习？" class="headerlink" title="五、如何对 PTMs 进行迁移学习？"></a>五、如何对 PTMs 进行迁移学习？</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/115014536">NLP算法面试必备！PTMs：NLP预训练模型的全面总结 —- JayJay</a></p></blockquote><p>PTMs 从大型语料库中获取通用语言知识，如何有效地将其知识适应下游任务是一个关键问题。迁移学习的方式主要有归纳迁移（顺序迁移学习、多任务学习）、领域自适应（转导迁移）、跨语言学习等。NLP 中 PTMs 的迁移方式是顺序迁移学习。</p><p><strong>1、如何迁移？</strong></p><ol><li>选择合适的<strong>预训练任务</strong>：语言模型是 PTM 是最为流行的预训练任务；同的预训练任务有其自身的偏置，并且对不同的任务会产生不同的效果。例如，NSP 任务可以使诸如问答（QA）和自然语言推论（NLI）之类的下游任务受益。</li><li>选择合适的<strong>模型架构</strong>：例如 BERT 采用的 MLM 策略和 Transformer-Encoder 结构，导致其不适合直接处理生成任务。</li><li>选择合适的<strong>数据</strong>：下游任务的数据应该近似于 PTMs 的预训练任务，现在已有有很多现成的 PTMs 可以方便地用于各种特定领域或特定语言的下游任务。</li><li>选择合适的 <strong>layers</strong> 进行 transfer：主要包括 Embedding 迁移、top layer 迁移和 all layer 迁移。如 word2vec 和 Glove 可采用 Embedding 迁移，BERT 可采用 top layer 迁移，Elmo 可采用 all layer 迁移。</li><li><strong>特征集成</strong> 还是 <strong>fine-tune</strong>？对于特征集成预训练参数是 freeze 的，而 fine-tune 是 unfreeze 的。特征集成方式却需要特定任务的体系结构，fine-tune 方法通常比特征提取方法更为通用和方便。</li></ol><p><strong>2、fine-tune 策略：</strong> 通过更好的微调策略进一步激发 PTMs 性能</p><ul><li>两阶段 fine-tune 策略：如第一阶段对中间任务或语料进行 finetune，第二阶段再对目标任务 fine-tune。第一阶段通常可根据特定任务的数据继续进行 fine-tune 预训练。</li><li>多任务 fine-tune：MTDNN 在多任务学习框架下对 BERT 进行了 fine-tune，这表明多任务学习和预训练是互补的技术。</li><li>采取额外的适配器：fine-tune 的主要缺点是其参数效率低，每个下游任务都有自己的 fine-tune 参数。因此，更好的解决方案是在固定原始参数的同时，将一些可 fine-tune 的适配器注入 PTMs。</li><li>逐层阶段：逐渐冻结而不是同时对所有层进行 fine-tune，也是一种有效的 fine-tune 策略。</li></ul><h1 id="经典预训练模型"><a href="#经典预训练模型" class="headerlink" title="经典预训练模型"></a>经典预训练模型</h1><h2 id="Skip-thoughts-2015"><a href="#Skip-thoughts-2015" class="headerlink" title="#. Skip-thoughts (2015)"></a>#. Skip-thoughts (2015)</h2><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1651816836376-68aead3d-6b67-4dde-b344-830eb8954771.png" class="" title="image.png"><p>Skip-thoughts 是一个15年的模型。它的选择的是<strong>大规模的图书语料</strong>。图书语料其实是一个很合适的选择，因为图书的题材可以很多，比如历史、爱情、科幻，以及图书包含了很多人类的知识。在编码器方面。选择了当时比较流行的GRU。它的目标是很经典的语言模型单词预测任务，通过一个句子的前n-1个单词的信息来预测第n个单词。如果在预训练阶段模型能把单词预测的任务做好，那么我们认为它充分学习到了人类语言的知识，对下游任务会有很大的帮助。</p><p>Skip-thoughts这个模型还有一个额外的贡献点，那就是它<strong>用了相邻句子的信息来预测第n个单词</strong>。如图所示，灰色、红色和绿色代表不同的句子。我们对红色的句子进行单词预测的时候，不仅使用红色句子中前n-1个单词的信息预测第n个单词，还会使用相邻句子的信息，也就是灰色的句子。<strong>最终，模型会将相邻的句子进行编码之后和前面的n-1个单词一起预测第n个单词。</strong></p><h2 id="Quick-thoughts-2018"><a href="#Quick-thoughts-2018" class="headerlink" title="#. Quick-thoughts (2018)"></a>#. Quick-thoughts (2018)</h2><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1651816836340-a65367fb-112b-4f09-9703-0b2c7044d4ca.png" class="" title="image.png"><p>Quick-thoughts 其实是Skip-thoughts模型的近亲。在语料方面，它用了和Skip-thoughts一样的大规模图书语料，同时它还增加了更多的互联网的语料。在编码器方面，它跟Skip-thoughts一样，用的也是GRU模型。这篇论文的贡献，也就是和Skip-thoughts主要的不同点在于它的预训练目标：它使用了句子级别的预测模型，<strong>即用中心句子预测周围的句子</strong>。如图所示，第一句话“春天来了”的下面有三句候选，我们要从这三句候选句子中挑选出哪一句话是真正的下一句话。所以我们会对这三个候选的句子，通过GRU编码器进行编码，最后通过一个分类器进行预测。第二个候选句子“但是谷物还没有生长”是正确的答案。这个预训练任务的目标就是通过预测句子之间的关系来让模型学习到高质量的句子表示。</p><p>这个模型的名字叫做Quick-thoughts，之所以有quick这个名字，是因为它的训练非常的快。我们知道，一般语言模型是比较慢的，因为语言模型对单词的预测需要在整个词典中找出合适的单词。举一个简单的例子，词典大小是两万，那就要从两万个单词中去选择那个合适的单词。另一方面，语言模型需要对每个单词进行预测。但Quick-thoughts针对句子进行预测是可以很快的。<strong>直观的说，如果进行负采样，Quick-thoughts只选两个句子作为负样本，到最后就是一个简单的三分类任务，所以这也是为什么它叫Quick-thoughts的原因。</strong></p><h2 id="CoVe-2017"><a href="#CoVe-2017" class="headerlink" title="#. CoVe (2017)"></a>#. CoVe (2017)</h2><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1651816836226-bc53c8ec-bdf7-42f3-bd72-f1a4cd1e1dc5.png" class="" title="image.png"><p>接下来是17年的一个模型，是当时在预训练领域比较有名气的一个工作，叫做CoVe。它最大的贡献是<strong>提出了用机器翻译作为训练目标进行预训练</strong>。因为机器翻译是一个非常困难的一个任务，被称为人工智能皇冠上的明珠。如果模型把这么难的任务都能做好的话，那它应该能学习到很多的语言知识，这些知识对于其他的下游任务就会有很大的帮助。CoVe的预训练目标选择了机器翻译之后，很自然的就会选择平行语料作为预训练的语料，这里使用了英德平行语料。编码器则选择了当时最为流行的双层LSTM，解码器也是双层LSTM。如图所示，在预训练的过程中，编码器对应的是英语，解码器对应的是德语。由于模型的下游任务都是英语，所以会把编码器学习到的英语的知识迁移到下游任务上，然后利用学习到的权重去更好的提升下游任务上的表现。</p><h2 id="InferSent-2017"><a href="#InferSent-2017" class="headerlink" title="#. InferSent (2017)"></a>#. InferSent (2017)</h2><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1651816836386-7c64406e-e3cd-4e37-a217-47adfb3c4cf3.png" class="" title="image.png"><p>下一个介绍的是InferSent。这个工作的动机和上个介绍的工作是类似的，认为自然语言推理是一个很难的任务。如果把这个任务做好了，那模型也就能学习到有价值的人类语言知识，对于下游任务也会有很大的帮助。自然语言推理是一个文本对三分类任务：给定两个句子，如果第一句能推理出第二句是正确的，这两个句子就是蕴含关系；如果第一句能推出第二句是错误的，这两个句子就是矛盾关系；如果第一句既推不出第二句是正确，也推不出第二句是错误的话，那两者就是中立关系。简单来说这篇文章的核心工作就是使用分类任务作为预训练目标。这个工作的语料是五十七万的语言推理数据集。在编码器方面。用到了在Transformer之前常用的各种编码器，包括LSTM/GRU加上不同的Pooling层、以及多层的CNN、LSTM加上Attention。</p><p>这篇文章最后给出了一个结论，认为LSTM加上max Pooling是最好的选择。其实我们之前自己的实验也有相似的结论，LSTM确实在很多场景下还是一个非常鲁棒的模型，在和max pooling配合的时候，在很多数据集上都能取得非常好的效果，不一定比Transformer要差。</p><h1 id="自回归语言模型详解"><a href="#自回归语言模型详解" class="headerlink" title="自回归语言模型详解"></a>自回归语言模型详解</h1><h2 id="1-ELMo-2018"><a href="#1-ELMo-2018" class="headerlink" title="1. ELMo (2018)"></a>1. ELMo (2018)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/kq901lwtmot3ec5a">https://www.yuque.com/ningshixian/pz10h0/kq901lwtmot3ec5a</a></p><h2 id="2-GPT"><a href="#2-GPT" class="headerlink" title="2. GPT"></a>2. GPT</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/oboycd6mxxhveub2">https://www.yuque.com/ningshixian/pz10h0/oboycd6mxxhveub2</a></p><h1 id="自编码语言模型详解"><a href="#自编码语言模型详解" class="headerlink" title="自编码语言模型详解"></a>自编码语言模型详解</h1><h2 id="1-BERT-2018"><a href="#1-BERT-2018" class="headerlink" title="1. BERT (2018)"></a>1. BERT (2018)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/ggf6f5se8w5w5004">https://www.yuque.com/ningshixian/pz10h0/ggf6f5se8w5w5004</a></p><h2 id="2-RoBERTa-2019"><a href="#2-RoBERTa-2019" class="headerlink" title="2. RoBERTa (2019)"></a>2. RoBERTa (2019)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/fw3tok5d2e7lbmoq">https://www.yuque.com/ningshixian/pz10h0/fw3tok5d2e7lbmoq</a></p><h2 id="3-UniLM-2019"><a href="#3-UniLM-2019" class="headerlink" title="3. UniLM (2019)"></a>3. UniLM (2019)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/uwhgtl6fsdqv48m8">https://www.yuque.com/ningshixian/pz10h0/uwhgtl6fsdqv48m8</a></p><h2 id="4-MASS-2019"><a href="#4-MASS-2019" class="headerlink" title="4. MASS (2019)"></a>4. MASS (2019)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/wp2d21vm3ynmqv9q">https://www.yuque.com/ningshixian/pz10h0/wp2d21vm3ynmqv9q</a></p><h2 id="5-BART-2019"><a href="#5-BART-2019" class="headerlink" title="5. BART (2019)"></a>5. BART (2019)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/ixi2eil4u4kce32l">https://www.yuque.com/ningshixian/pz10h0/ixi2eil4u4kce32l</a></p><h2 id="6-T5-2019"><a href="#6-T5-2019" class="headerlink" title="6. T5 (2019)"></a>6. T5 (2019)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/lbk203cipy4mc1za">https://www.yuque.com/ningshixian/pz10h0/lbk203cipy4mc1za</a></p><h2 id="7-SpanBERT"><a href="#7-SpanBERT" class="headerlink" title="7. SpanBERT"></a>7. SpanBERT</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/rhvq0bzgl8tia5sy">https://www.yuque.com/ningshixian/pz10h0/rhvq0bzgl8tia5sy</a></p><h2 id="8-MacBERT"><a href="#8-MacBERT" class="headerlink" title="8. MacBERT"></a>8. MacBERT</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/nrkzl091247ui7sg">https://www.yuque.com/ningshixian/pz10h0/nrkzl091247ui7sg</a></p><h1 id="全排列语言模型"><a href="#全排列语言模型" class="headerlink" title="全排列语言模型"></a>全排列语言模型</h1><h2 id="1-Transformer-XL-2019"><a href="#1-Transformer-XL-2019" class="headerlink" title="1. Transformer-XL (2019)"></a>1. Transformer-XL (2019)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/lli9fsaidass95ko">https://www.yuque.com/ningshixian/pz10h0/lli9fsaidass95ko</a></p><h2 id="2-XLNet-2019"><a href="#2-XLNet-2019" class="headerlink" title="2. XLNet (2019)"></a>2. XLNet (2019)</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/ggsm2eqel1sdgwac">https://www.yuque.com/ningshixian/pz10h0/ggsm2eqel1sdgwac</a></p><h1 id="基于对比学习的语言模型"><a href="#基于对比学习的语言模型" class="headerlink" title="基于对比学习的语言模型"></a>基于对比学习的语言模型</h1><h2 id="1-ELECTRA-2020"><a href="#1-ELECTRA-2020" class="headerlink" title="1. ELECTRA (2020)"></a>1. ELECTRA (2020)</h2><blockquote><p>苏剑林. (Oct. 29, 2020). 《用ALBERT和ELECTRA之前，请确认你真的了解它们 》[Blog post]. Retrieved from <a href="https://kexue.fm/archives/7846">https://kexue.fm/archives/7846</a></p></blockquote><p>ELECTRA则来自论文<a href="https://arxiv.org/abs/2003.10555">《ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators》</a>。说实话，ELECTRA真是一个一言难尽的模型，它刚出来的时候让很多人兴奋过，后来正式发布开源后又让很多人失望过，目前的实战表现虽然不能说差，却也不能说多好。</p><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1651142611439-48661325-6f81-464e-94fd-23ca3ddc01eb.png" class="" title="image.png"><p>ELECTRA的出发点是觉得BERT的MLM模型随机选择一部分Token进行Mask的这个操作过于简单了，想要增加一下它的难度。所以它借鉴了GAN的思想，用普通的方式训练一个MLM模型（生成器），然后根据MLM模型对输入句子进行采样替换，将处理后的句子输入到另外一个大模型（判别器）中，判断句子哪些 token 是被替换过的，哪些是被没被替换的（二分类任务）。该任务被称为替代词检测(<strong>RTD</strong>)任务。生成器和判别器是同步训练的，因此随着生成器的训练，判断难度会慢慢增加，直观想象有利于模型学到更有价值的内容。最后只保留判别器的Encoder来用，生成器一般就不要了。</p><p><strong>Q：ELECTRA模型介绍</strong><br><strong>A：</strong>首先，按照一定的比例对于原始输入序列进行随机MASK操作得到新序列；<br>其次，将新序列作为生成器模型的输入，生成器模型对MASK的token进行预测，获取生成序列；<br>之后，将生成序列作为判别器模型的输入，判别器模型对生成序列每一个元素进行预测，判断是否是原始文本；<br>然后，将生成器模型和判别器模型的损失进行加和，分别反向传播；<br>最后，将判别器模型用于下游任务。</p><p><strong>Q：ELECTRA模型任务本质</strong><br><strong>A：</strong>任务本质是获取效果较好的判别器，而判别器任务，就是一个Replaced Token Detection任务，预测替换词语是否为原始文本词语；而存在生成器模型的原因是直接使用随机替换的词语效果不好。</p><p><strong>Q：ELECTRA模型与GAN的关系？</strong><br><strong>A：</strong>其实不是GAN模型，句子的字词是离散的，梯度在判别器使用生成器结果时就断了，判别器的梯度无法传给生成器，生成器的训练目标还是MLM。</p><p><strong>Q：Loss如何分布</strong><br><strong>A：</strong>判别器的任务相对来说容易些，RTD loss相对MLM loss会很小，因此加上一个系数，论文提供的参数为50。</p><p><strong>Q：判别器Loss是计算所有Token，为什么不只计算15%被mask的token</strong><br><strong>A：</strong>效果好。</p><p><strong>Q：生成器和判别器的权重共享是否可以提升效果呢？</strong><br><strong>A：</strong>在相同参数下（生成器和判别器结构一致），不共享权重下的模型效果最差，共享所有权重的效果最好，只共享token embedding层的效果只比共享所有权重差一点点。原因是，生成器是一个MLM任务，在模型预测时softmax建立在词典的所有词之上，反向传播会更新所有token 的embedding，因此生成器对token embedding层的学习效果更好。最后论文作者只使用了token embedding共享策略。并且实验发现生成器的大小在判别器的1/4到1/2之间效果是最好的。</p><p><strong>ELECTRA论文</strong>见：<a href="https://arxiv.org/abs/2003.10555">https://arxiv.org/abs/2003.10555</a></p><h2 id="2-StructBERT"><a href="#2-StructBERT" class="headerlink" title="2. StructBERT"></a>2. StructBERT</h2><p>word structural objective：给定一个被打乱顺序的序列，尝试预测每个被移位的词的原始正确的位置：</p><ul><li>在句子中引入词乱序（“乱序不改其义”的思想，增大理解难度）。</li></ul><p>Sentence Structural Objective：对于两句话(S1, S2)，预测二者相对关系，3分类问题。</p><ul><li>1/3的概率是segment B是segment A的下一句</li><li>1/3的概率是segment A是segment B的下一句</li><li>1/3的概率是segment A和segment B来自2篇不同的文本</li></ul><h1 id="引入知识的语言模型"><a href="#引入知识的语言模型" class="headerlink" title="引入知识的语言模型"></a>引入知识的语言模型</h1><h2 id="1-ERNIE-系列-Baidu"><a href="#1-ERNIE-系列-Baidu" class="headerlink" title="1. ERNIE 系列 (Baidu)"></a>1. ERNIE 系列 (Baidu)</h2><p><strong>ERNIE 1.0</strong> 通过建模海量数据中的词、实体及实体关系，学习真实世界的语义知识。相较于 BERT 学习原始语言信号，百度ERNIE 1.0 可以概括为 2 点改进：</p><ol><li>Knowledge Masking：直接对先验语义知识单元进行建模，借助知识改进masking策略，包括 3 种：传统mask + 实体mask + 短语mask 。例如</li></ol><blockquote><p>BERT ：哈 [mask] 滨是 [mask] 龙江的省会，[mask] 际冰 [mask] 文化名城。<br>ERNIE：[mask] [mask] [mask] 是黑龙江的省会，国际 [mask] [mask] 文化名城。</p></blockquote><p>在 BERT 模型中，我们通过『哈』与『滨』的局部共现，即可判断出『尔』字，模型没有学习与『哈尔滨』相关的任何知识。而 ERNIE 通过学习词与实体的表达，使模型能够建模出『哈尔滨』与『黑龙江』的关系，学到『哈尔滨』是 『黑龙江』的省会以及『哈尔滨』是个冰雪城市。</p><ol><li>Dialog Language Model (DLM)：训练数据方面，除百科类、资讯类中文语料外，ERNIE 还引入了论坛对话类数据，利用 DLM（Dialogue Language Model）建模 Query-Response 对话结构，将对话 Pair 对作为输入，引入 Dialogue Embedding 标识对话的角色，利用 Dialogue Response Loss 学习对话的隐式关系，进一步提升模型的语义表示能力。</li></ol><hr><p><strong>ERNIE 2.0</strong> 是基于持续学习的语义理解预训练框架，使用多任务学习增量式构建预训练任务。ERNIE 2.0 中，新构建的预训练任务类型可以无缝的加入训练框架，持续的进行语义理解学习。 </p><blockquote><p><strong>ERNIE2.0论文</strong>见：<a href="https://arxiv.org/abs/1907.12412">https://arxiv.org/abs/1907.12412</a></p></blockquote><p>在预训练引入多任务学习，主要包含3个方面的任务：</p><ul><li>词层面任务<ul><li>知识掩码任务（Knowledge Masking Task）：同ERNIE1.0</li><li>词语-文档关系预测任务（Token-Document Relation Prediction Task）：一篇文档出现的词语，是否在另外一篇出现；</li><li>大写预测任务（Capitalization Prediction Task）：预测一个word的首字母是否大写，一般大写词语都是较重要的词语；</li></ul></li><li>句子层面<ul><li>句子重排序任务（Sentence Reordering Task）：将文本句子打乱顺序，然后预测正确排序；</li><li>句子距离预测任务（SentenceDistance Task）：预测句子对之间的相对距离–&gt;三分类问题</li></ul></li><li>语义层面<ul><li>句子关系任务（Discourse Relation Task）：根据句子之间的关键词判断，两个句子之间的语义关系</li><li>检索关系任务（IR Relevance Task）：判断一条用户查询和一篇文档题目的相关性程度，包含强相关、弱相关和无关</li></ul></li></ul><hr><p><strong>ERNIE 3.0</strong> </p><ul><li>设计了上下两层网络结构，一个用于聚焦自然语言理解，一个用于聚焦自然语言生成任务<ul><li>Universal Representation Module</li><li>Task-specific Representation Module</li></ul></li><li>使用Transformer-XL作为骨干网络</li></ul><h2 id="2-ERNIE-THU"><a href="#2-ERNIE-THU" class="headerlink" title="2. ERNIE (THU)"></a>2. ERNIE (THU)</h2><blockquote><p><a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/THU-ERNIE.html">https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/THU-ERNIE.html</a></p></blockquote><p>相比百度ERNIE，清华ERNIE则是另外一种思路：我们已经有些结构化知识或者实体关系知识等现成的外部知识库，可以在预训练的过程中，通过工具找出句中的命名实体，句中的命名实体可以触发知识库中其它相关实体，然后预训练模型通过特殊的结构，来融合文本和结构化知识，以进一步促进语言的理解。</p><p><strong>THU-ERNIE</strong>是由两种类型的Encoder堆叠而成：<strong>T-Encoder</strong>和<strong>K-Encoder</strong>。<strong>T-Encoder</strong>负责从输入序列中捕获词法和句法信息；<strong>K-Encoder</strong>负责将KG知识和从<strong>T-Encoder</strong>中提取的文本信息进行融合，其中KG知识在这里主要是实体，这些实体是通过TransE模型训练出来的。</p><p>具体来说：</p><ol><li>使用<a href="https://arxiv.org/pdf/1006.3498v1.pdf">TAGME</a>提取文本中的实体，并将这些实体链指到KG中的对应实体对象，然后找出这些实体对象对应的embedding，这些embedding是由一些知识表示方法，例如<a href="https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf">TransE</a>训练得到的。</li><li>将实体向量和文本表示通过非线性变换进行融合，以融合词汇、句法和知识信息；</li><li>除了MLM、NSP任务外，重新添加了一个和KG相关的预训练目标Denoising entity auto-encoder (DEA)。dEA将随机地Mask掉一些token-entity对，然后要求模型在这些对齐的token上利用图谱去预测相应的实体分布，完成对齐。</li></ol><img src="/2023/02/26/2023-02-26-%F0%9F%8D%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8Cnew/1652352024792-00cda75c-67d2-4f8d-aa9b-ce97a12af0d6.png" class="" title="image.png"><p>THU-ERNIE的模型架构</p><h2 id="3-K-BERT"><a href="#3-K-BERT" class="headerlink" title="3. K-BERT"></a>3. K-BERT</h2><p>将实体关系的三元组知识显式地嵌入到原始文本</p><ul><li>原始句子：“Tim Cook is visiting Beijing now”</li><li>改造句子：“Tim Cook CEO Apple is visiting Beijing capital China is_a City now”</li></ul><h2 id="4-LIBERT"><a href="#4-LIBERT" class="headerlink" title="4. LIBERT"></a>4. LIBERT</h2><p>添加了语言知识约束</p><ul><li>基于预训练好的 BERT，利用同义词/上下位词对进行fine-tuning</li></ul><h2 id="5-其他"><a href="#5-其他" class="headerlink" title="5. 其他"></a>5. 其他</h2><p>KnowBERT</p><p>SentiLR</p><p>KEPLER</p><p>WKLM</p><p>CoLAKE</p><h1 id="轻量化BERT详解"><a href="#轻量化BERT详解" class="headerlink" title="轻量化BERT详解"></a>轻量化BERT详解</h1><h2 id="1-Albert"><a href="#1-Albert" class="headerlink" title="1. Albert"></a>1. Albert</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/utnz6o1aps5m1cge">https://www.yuque.com/ningshixian/pz10h0/utnz6o1aps5m1cge</a></p><h2 id="2-DistillBert"><a href="#2-DistillBert" class="headerlink" title="2. DistillBert"></a>2. DistillBert</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/mqhw8qymy5tmal0s">https://www.yuque.com/ningshixian/pz10h0/mqhw8qymy5tmal0s</a></p><h2 id="3-TINYBERT"><a href="#3-TINYBERT" class="headerlink" title="3. TINYBERT"></a>3. TINYBERT</h2><p><a href="https://www.yuque.com/ningshixian/pz10h0/gg28qb7cmr3mc4ue">https://www.yuque.com/ningshixian/pz10h0/gg28qb7cmr3mc4ue</a></p><h1 id="State-of-Art"><a href="#State-of-Art" class="headerlink" title="State-of-Art"></a>State-of-Art</h1><p>NLP 任务的 State-of-Art 模型详见：</p><ul><li><a href="https://gluebenchmark.com/leaderboard">GLUE Leaderboard</a></li><li><a href="https://super.gluebenchmark.com/leaderboard">SuperGLUE Leaderboard</a></li><li><a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a></li><li><a href="https://nlpprogress.com/">NLP-progress</a></li><li><a href="https://www.cluebenchmarks.com/">中文任务基准测评</a></li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2652057750&amp;idx=4&amp;sn=799877cde18a3a4ce259a1a1654c9dfc&amp;chksm=f1204467c657cd71891649bbf14ecf20549a76911fcd087b5b0e6406ff5717f052d672d5db8b&amp;mpshare=1&amp;scene=23&amp;srcid=&amp;sharer_sharetime=1573370368071&amp;sharer_shareid=5f83a05212e1b63a8a185c7ff5d2a2be#rd">绝对干货！NLP预训练模型：从transformer到albert</a></p><p><a href="https://kexue.fm/archives/4765">《Attention is All You Need》浅读（简介+代码）</a></p><p><a href="https://tech.meituan.com/2019/11/14/nlp-bert-practice.html">美团BERT的探索和实践</a></p><p><a href="https://www.bookstack.cn/books/huaxiaozhuan-ai"> AI算法工程师手册</a></p><p><strong>⭐️</strong><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/"><strong>预训练自然语言模型 (Pre-trained Models for NLP)-范叶亮</strong></a></p><p><a href="https://www.6aiq.com/article/1587401784826">贝壳找房【语言模型系列】原理篇二：从 ELMo 到 ALBERT</a></p><p><a href="https://mp.weixin.qq.com/s/rsiV1ZXlGdFBawxGYbEyCQ">常用预训练语言模型（PTMs）总结-刘聪 NLP</a></p>]]></content>
    
    
    <categories>
      
      <category>llm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ptm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《深度学习推荐系统》-极客时间</title>
    <link href="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"/>
    <url>/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h1 id="一、互联网的增长引擎—推荐系统"><a href="#一、互联网的增长引擎—推荐系统" class="headerlink" title="一、互联网的增长引擎—推荐系统"></a>一、互联网的增长引擎—推荐系统</h1><h2 id="1、推荐系统的作用"><a href="#1、推荐系统的作用" class="headerlink" title="1、推荐系统的作用"></a>1、推荐系统的作用</h2><ul><li>解决在信息过载的情况下，用户高效获得感兴趣信息的问题</li><li><p>提高产品的用户转化率，得到公司商业目标连续增长的目的</p><h2 id="2、推荐系统的架构"><a href="#2、推荐系统的架构" class="headerlink" title="2、推荐系统的架构"></a>2、推荐系统的架构</h2><h3 id="逻辑框架"><a href="#逻辑框架" class="headerlink" title="逻辑框架"></a>逻辑框架</h3></li><li><p>对于用户U，在特定场景C下，针对海量的物品信息，构建一个函数f(U,I,C)，预测用户对特定候选物品I的喜好程度</p><h3 id="技术架构"><a href="#技术架构" class="headerlink" title="技术架构"></a>技术架构</h3></li><li><p>数据部分</p><ul><li>数据收集<ul><li>推荐模型所需的样本数据</li><li>推荐模型所需特征</li><li>系统监控、商业智能所需的统计数据</li></ul></li><li>数据加工 <ul><li>客户端及服务器端实时数据处理</li><li>流处理平台准实时数据处理</li><li>大数据平台离线数据处理</li></ul></li></ul></li><li>模型部分<ul><li>召回层</li><li>排序层</li><li>补充策略与算法层</li><li>评估模块：离线评估和线上A／B测试</li></ul></li></ul><img src="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666836394220-254761b4-92dd-45e3-ae9d-c67323647832.png" class="" title="image.png"><h1 id="二、前深度学习时代-推荐系统的进化之路"><a href="#二、前深度学习时代-推荐系统的进化之路" class="headerlink" title="二、前深度学习时代-推荐系统的进化之路"></a>二、前深度学习时代-推荐系统的进化之路</h1><h2 id="1、协同过滤—经典的推荐算法"><a href="#1、协同过滤—经典的推荐算法" class="headerlink" title="1、协同过滤—经典的推荐算法"></a>1、协同过滤—经典的推荐算法</h2><ul><li>协同过滤是协同大家的反馈、评价和意见一起对海量的信息进行过滤，从中筛选出目标用户可能感兴趣的信息的推荐过程</li><li>基于用户的协同过滤UserCF<ul><li>更强的社交性</li><li>适用于发现热点</li></ul></li><li>基于物品的协同过滤ItemCF<ul><li>适用于兴趣变化较为稳定的应用</li></ul></li><li><p>推荐结果的头部效应明显，处理稀疏向量能力弱</p><h2 id="2、矩阵分解算法-协同过滤的进化"><a href="#2、矩阵分解算法-协同过滤的进化" class="headerlink" title="2、矩阵分解算法-协同过滤的进化"></a>2、矩阵分解算法-协同过滤的进化</h2><h3 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h3></li><li><p>特征值分解：只适用于方阵</p></li><li>奇异值分解：计算复杂度高</li><li>梯度下降：主流方法<ul><li>目的是原始评分与用户向量和物品向量的内积的差尽量小</li><li>加入正则化避免过拟合</li><li>加入用户和物品的偏差向量消除偏差</li></ul></li><li><p>优缺点</p><ul><li>优点：泛化能力强、空间复杂度低、更好的扩展性和灵活性</li><li>缺点：不方便加入用户、物品和上下文特征<h2 id="3、逻辑回归-融合多种特征的推荐模型"><a href="#3、逻辑回归-融合多种特征的推荐模型" class="headerlink" title="3、逻辑回归-融合多种特征的推荐模型"></a>3、逻辑回归-融合多种特征的推荐模型</h2></li></ul></li><li><p>综合利用用户、物品、上下文多种不同特征，生成较为全面的推荐模型</p></li><li>训练方法：梯度下降法、牛顿法、拟牛顿法</li><li>优势<ul><li>数字含义上的支撑：广义线性模型的一种，假设y服从伯努利分布</li><li>可解释性强</li><li>工程化的需要：并行化、模型简单、训练开销小</li></ul></li><li><p>局限性</p><ul><li>表达能力不强、无法进行特征交叉、特征筛选等<h2 id="4、从FM到FFM—自动特征交叉的解决方案"><a href="#4、从FM到FFM—自动特征交叉的解决方案" class="headerlink" title="4、从FM到FFM—自动特征交叉的解决方案"></a>4、从FM到FFM—自动特征交叉的解决方案</h2></li></ul></li><li><p>多维度特征交叉的重要性：“辛普森悖论”</p></li><li>POLY2模型—特征交叉的开始<ul><li>暴力进行特征组合</li><li>训练复杂度高、稀疏数据下大部分权重得不到有效训练</li></ul></li><li>FM模型-隐向量特征交叉<ul><li>为每一个特征赋予一个对应的隐向量</li><li>更好地解决数据稀疏性问题</li><li>线上推断过程简单，更容易进行线上部署</li><li>不易扩展到三阶特征交叉</li></ul></li><li><p>FFM模型-引入特征域的概念</p><ul><li>域可以简单理解为采用one-hot编码形成的一段one-hot特征向量</li><li>每个特征对应每一个域，都有一个隐向量</li><li>缺点：训练开销大<h2 id="5、GBDT-LR—特征工程化的开端"><a href="#5、GBDT-LR—特征工程化的开端" class="headerlink" title="5、GBDT+LR—特征工程化的开端"></a>5、GBDT+LR—特征工程化的开端</h2></li></ul></li><li><p>利用GBDT构建特征工程、利用LR预估CTR</p></li><li><p>大大推进了特征工程模型化这一趋势</p><h2 id="6、LS-PLM-阿里巴巴曾经的主流推荐模型"><a href="#6、LS-PLM-阿里巴巴曾经的主流推荐模型" class="headerlink" title="6、LS-PLM-阿里巴巴曾经的主流推荐模型"></a>6、LS-PLM-阿里巴巴曾经的主流推荐模型</h2></li><li><p>对样本先进行分片，再在样本分片中应用LR</p></li><li>优势<ul><li>端到端的非线性学习能力</li><li>稀疏性强、部署更加轻量级</li></ul></li><li>可以看作一个加入了注意力机制的三层NN</li></ul><img src="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666836394386-15cdc3b9-cbc8-4fb7-89f6-4e4363b73805.png" class="" title="image.png"><h1 id="三、浪潮之巅-深度学习在推荐系统中的应用"><a href="#三、浪潮之巅-深度学习在推荐系统中的应用" class="headerlink" title="三、浪潮之巅-深度学习在推荐系统中的应用"></a>三、浪潮之巅-深度学习在推荐系统中的应用</h1><h2 id="1、深度学习时代的模型"><a href="#1、深度学习时代的模型" class="headerlink" title="1、深度学习时代的模型"></a>1、深度学习时代的模型</h2><ul><li>表达能力更强，能够挖掘更多数据中潜藏的模式</li><li>结构灵活，能够结合业务灵活选择模型结构</li><li><p>两类思路</p><ul><li>特征工程自动化的思路：PNN、Wide&amp;Deep、Deep &amp; Cross、FNN、DeepFM、NFM等</li><li>模型结构的尝试：注意力机制（AFM、DIN）、序列模型（DIEN）、强化学习（DRN）<h2 id="2、AutoRec-单隐层神经网络推荐模型"><a href="#2、AutoRec-单隐层神经网络推荐模型" class="headerlink" title="2、AutoRec-单隐层神经网络推荐模型"></a>2、AutoRec-单隐层神经网络推荐模型</h2></li></ul></li><li><p>自编码器思想和协同过滤结合</p></li><li><p>模型较为简单，存在一定表达能力不足问题</p><h2 id="3、Deep-Crossing模型-经典的深度学习架构"><a href="#3、Deep-Crossing模型-经典的深度学习架构" class="headerlink" title="3、Deep Crossing模型-经典的深度学习架构"></a>3、Deep Crossing模型-经典的深度学习架构</h2></li><li><p>分Embedding层、Stacking层（concatenate层）、Multiple Residual Units层和Scoring层</p></li><li><p>全部特征交叉的任务交给模型，是对特征交叉方法的革命</p><h2 id="4、NeuralCF模型-CF与深度学习的结合"><a href="#4、NeuralCF模型-CF与深度学习的结合" class="headerlink" title="4、NeuralCF模型-CF与深度学习的结合"></a>4、NeuralCF模型-CF与深度学习的结合</h2></li><li><p>利用互操作层代替简单的用户和物品向量求内积的方式，广义矩阵分解模型</p></li><li>互操作：用户向量和物品向量拼接后输入多层神经网络／元素乘连接（GMF层）</li><li><p>没有引入更多其他类型的特征</p><h2 id="5、PNN模型—加强特征交叉的能力"><a href="#5、PNN模型—加强特征交叉的能力" class="headerlink" title="5、PNN模型—加强特征交叉的能力"></a>5、PNN模型—加强特征交叉的能力</h2></li><li><p>用Product layer代替了Deep Crossing中的Stack层，使用Product操作对特征进行两两交互</p></li><li>分为IPNN和OPNN</li><li>Product操作更强调不同特征之间的交互，模型更容易捕获特征的交叉信息</li><li><p>对所有特征进行无差别的交叉，一定程度忽略了原始特征中包含的有价值信息</p><h2 id="6、Wide-amp-Deep模型—记忆能力和泛化能力的综合"><a href="#6、Wide-amp-Deep模型—记忆能力和泛化能力的综合" class="headerlink" title="6、Wide&amp;Deep模型—记忆能力和泛化能力的综合"></a>6、Wide&amp;Deep模型—记忆能力和泛化能力的综合</h2></li><li><p>wide部分让模型具有较强的记忆能力，记忆能力可以被理解为模型直接学习并利用历史数据中物品或者特征共现频率的能力</p></li><li><p>Deep部分让模型具有泛化能力，泛化能力可以被理解为模型传递特征的相关性，以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力</p><h2 id="7、Wide-amp-Deep模型进化-Deep-amp-Cross模型"><a href="#7、Wide-amp-Deep模型进化-Deep-amp-Cross模型" class="headerlink" title="7、Wide&amp;Deep模型进化-Deep &amp; Cross模型"></a>7、Wide&amp;Deep模型进化-Deep &amp; Cross模型</h2></li><li><p>利用Cross Net代替Wide部分，增加特征交叉力度</p><h2 id="8、FM与深度学习模型的结合"><a href="#8、FM与深度学习模型的结合" class="headerlink" title="8、FM与深度学习模型的结合"></a>8、FM与深度学习模型的结合</h2></li><li><p>结合FM思路的深度学习模型，基本特点是在经典多层神经网络的基础上加入针对性的特征交叉操作，让模型具有更强的非线性能力</p></li><li>FNN：用FM训练好的隐向量来初始化Embedding层，为Embedding预训练提供了借鉴思路</li><li>DeepFM：用FM代替了Wide&amp;Deep的Wide部分，加强了浅层网络部分特征组合的能力</li><li><p>NFM：用一个表达能力更强的函数代替FM中二阶隐向量内积的部分</p><h2 id="9、注意力机制在推荐模型中的应用"><a href="#9、注意力机制在推荐模型中的应用" class="headerlink" title="9、注意力机制在推荐模型中的应用"></a>9、注意力机制在推荐模型中的应用</h2></li><li><p>AFM：引入注意力机制的FM</p><ul><li>在特征交叉层和最终输出层之间加入attention，为每一组交叉特征提供权重</li></ul></li><li><p>DIN：引入注意力机制的深度学习网络</p><ul><li>候选商品和历史行为商品之间计算权重<h2 id="10、DIEN-序列模型与推荐系统的结合"><a href="#10、DIEN-序列模型与推荐系统的结合" class="headerlink" title="10、DIEN-序列模型与推荐系统的结合"></a>10、DIEN-序列模型与推荐系统的结合</h2></li></ul></li><li><p>用序列模型模拟了用户兴趣的进化过程</p></li><li>序列信息的重要性<ul><li>加强了最近行为对下次行为预测的影响</li><li>能够学习到购买趋势的信息</li></ul></li><li>模型主要结构<ul><li>行为序列层</li><li>兴趣抽取层：模拟用户兴趣迁移过程，抽取用户兴趣</li><li>兴趣进化层：针对性地模拟与目标广告相关的兴趣进化路径</li></ul></li><li><p>序列模型非常适合预估用户经过一系列行为后的下一次动作，但由于是串行推断，需要在工程上着重优化</p><h2 id="11、DRN：强化学习与推荐系统的结合"><a href="#11、DRN：强化学习与推荐系统的结合" class="headerlink" title="11、DRN：强化学习与推荐系统的结合"></a>11、DRN：强化学习与推荐系统的结合</h2></li><li><p>Dueling-DQN结构</p></li><li>在线学习方法：竞争梯度下降算法</li><li>与其他深度学习模型的不同之处在于变静态为动态，把模型学习的实时性提到了一个空前重要的位置</li></ul><img src="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666836394390-8c427348-9b46-4359-9af3-b7779e9d4726.png" class="" title="image.png"><p><img src="https://cdn.nlark.com/yuque/0/2022/png/8420697/1666836394331-37c2c66b-e688-4ab1-9fd7-0a8ef9b6e51f.png#averageHue=%23e7eff5&amp;clientId=u3ffa787c-0924-4&amp;from=paste&amp;id=u4a0873dc&amp;name=image.png&amp;originHeight=612&amp;originWidth=1080&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=179745&amp;status=done&amp;style=none&amp;taskId=u4fb2a4b1-a309-47dc-89ab-8665496d020&amp;title=" alt="image.png"></p><h1 id="四、Embedding技术在推荐系统中的应用"><a href="#四、Embedding技术在推荐系统中的应用" class="headerlink" title="四、Embedding技术在推荐系统中的应用"></a>四、Embedding技术在推荐系统中的应用</h1><h2 id="1、什么是Embedding"><a href="#1、什么是Embedding" class="headerlink" title="1、什么是Embedding"></a>1、什么是Embedding</h2><ul><li>用一个低维稠密向量“表示”一个对象。“表示”意味着Embedding向量能够表达相应对象的某些特征，同时向量之间的距离反映了对象之间的相似性。</li><li><p>Embedding在推荐场景的重要性</p><ul><li>Embdding层将高维稀疏特征向量转换成稠密低维特征向量</li><li>Embedding本身就是极其重要的特征向量</li><li>Embedding对物品、用户相似度的计算是常用的推荐系统召回层技术<h2 id="2、Word2Vec：经典的Embedding方法"><a href="#2、Word2Vec：经典的Embedding方法" class="headerlink" title="2、Word2Vec：经典的Embedding方法"></a>2、Word2Vec：经典的Embedding方法</h2></li></ul></li><li><p>模型结构</p><ul><li>CBOW模型：输入周边词，预测中间词</li><li>Skip-Gram模型：输入中间词，预测周边词，经验上讲Skip-Gram模型效果更好</li></ul></li><li><p>训练方法</p><ul><li>负采样</li><li>层级softmax<h2 id="3、Item2Vec：Word2Vec在推荐系统领域的推广"><a href="#3、Item2Vec：Word2Vec在推荐系统领域的推广" class="headerlink" title="3、Item2Vec：Word2Vec在推荐系统领域的推广"></a>3、Item2Vec：Word2Vec在推荐系统领域的推广</h2></li></ul></li><li><p>利用用户历史行为记录，摒弃时间窗口概念，认为两两物品都相关</p></li><li><p>广义的Item2Vec：任何能够生成物品向量的方法，如双塔模型</p><h2 id="4、Graph-Embedding：引入更多结构信息的图嵌入技术"><a href="#4、Graph-Embedding：引入更多结构信息的图嵌入技术" class="headerlink" title="4、Graph Embedding：引入更多结构信息的图嵌入技术"></a>4、Graph Embedding：引入更多结构信息的图嵌入技术</h2></li><li><p>互联网场景下，数据更多呈现图结构</p></li><li>DeepWalk：在图上随机游走，产生物品序列，然后使用word2vec进行训练，游走的概率取决于边的权重</li><li>Node2Vec：通过调整随机游走权重的方法使Graph Embedding的结果更倾向于体现网络的同质性和结构性</li><li><p>EGES：在DeepWalk的基础上补充Side Information</p><h2 id="5、Embedding与深度学习推荐系统的结合"><a href="#5、Embedding与深度学习推荐系统的结合" class="headerlink" title="5、Embedding与深度学习推荐系统的结合"></a>5、Embedding与深度学习推荐系统的结合</h2></li><li><p>深度学习中的Embedding层</p></li><li>作为预训练的Embedding特征向量</li><li><p>计算物品和用户的相似度，作为召回层的策略</p><ul><li>youtube推荐系统召回层<h2 id="6、局部敏感Hash"><a href="#6、局部敏感Hash" class="headerlink" title="6、局部敏感Hash"></a>6、局部敏感Hash</h2></li></ul></li><li><p>快速Embedding向量最近邻搜索方法</p></li><li>基本思想是让相邻的点落入同一个桶</li></ul><img src="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666836394377-8f7d7a2d-caf4-4c83-a57d-406c43450ef7.png" class="" title="image.png"><h1 id="五、多角度审视推荐系统"><a href="#五、多角度审视推荐系统" class="headerlink" title="五、多角度审视推荐系统"></a>五、多角度审视推荐系统</h1><h2 id="1、推荐系统的特征工程"><a href="#1、推荐系统的特征工程" class="headerlink" title="1、推荐系统的特征工程"></a>1、推荐系统的特征工程</h2><ul><li>原则：尽可能地让特征工程抽取出的一组特征能够保留推荐环境及用户行为过程的所有有用信息，尽量摒弃冗余信息</li><li>特征工程需要深入了解业务的运行模式，了解用户在业务场景下的思考方式和行为动机</li><li>常用特征<ul><li><a href="https://cloud.tencent.com/product/ida?from=10680">用户行为数据</a><ul><li>显性反馈行为</li><li>隐性反馈行为</li></ul></li><li>用户关系数据</li><li>属性、标签类数据</li><li>内容类数据：描述型文字、图片、视频</li><li>上下文信息：推荐行为产生的场景的信息</li><li>统计类特征：历史CTR、CVR等</li><li>组合类特征</li></ul></li><li><p>常用的特征处理方法</p><ul><li>连续型：归一化、离散化、加非线性函数</li><li>类别型：one／multi-hot + embedding<h2 id="2、推荐系统召回层主要策略"><a href="#2、推荐系统召回层主要策略" class="headerlink" title="2、推荐系统召回层主要策略"></a>2、推荐系统召回层主要策略</h2></li></ul></li><li><p>召回层的特点：候选集合大、速度快、模型简单、特征较少</p></li><li>多路召回策略（主流方法）<ul><li>如信息流应用中：热门新闻、兴趣标签、协同过滤、最近流行、朋友喜欢</li></ul></li><li><p>基于Embedding的方法</p><ul><li>如youtube召回策略<h2 id="3、推荐系统的实时性"><a href="#3、推荐系统的实时性" class="headerlink" title="3、推荐系统的实时性"></a>3、推荐系统的实时性</h2></li></ul></li><li><p>特征的实时性</p><ul><li>客户端实时特征</li><li>流计算平台的准实时特征处理</li><li>分布式批处理平台的全量特征处理</li></ul></li><li>模型的实时性<ul><li>全量更新</li><li>增量更新</li><li>在线学习：获得一个新样本就更新模型，一个附带问题是稀疏性不强，相关研究包括FOBOS、FTRL</li><li>局部更新：降低训练效率低部分的更新频率，提高训练效率高的部分的更新频率，如果GBDT+LR、Embedding层 + 神经网络</li><li>客户端模型实时更新：探索阶段</li></ul></li></ul><img src="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666865183918-f49b65d7-f957-4aa5-9799-eb4e82badf1f.png" class="" title="image.png"><h2 id="4、如何合理设定推荐系统中的优化目标"><a href="#4、如何合理设定推荐系统中的优化目标" class="headerlink" title="4、如何合理设定推荐系统中的优化目标"></a>4、如何合理设定推荐系统中的优化目标</h2><ul><li>推荐系统的目标是完成公司的某个商业目的：如Youtube以观看时长为优化目标</li><li>优化场景和应用场景的统一性：阿里ESMM</li><li><p>构建成功的推荐系统需要和其他团队协调一致</p><h2 id="5、推荐系统中比模型结构更重要的是什么"><a href="#5、推荐系统中比模型结构更重要的是什么" class="headerlink" title="5、推荐系统中比模型结构更重要的是什么"></a>5、推荐系统中比模型结构更重要的是什么</h2></li><li><p>在构建推荐模型的过程中，从应用场景出发，基于用户行为和数据的特点，提出合理的改进模型的动机才是最重要的</p><h2 id="6、冷启动的解决方法"><a href="#6、冷启动的解决方法" class="headerlink" title="6、冷启动的解决方法"></a>6、冷启动的解决方法</h2></li><li><p>分类：用户冷启动、物品冷启动、系统冷启动</p></li><li><p>冷启动策略</p><ul><li>基于规则<ul><li>用户冷启动：热门排行榜、最近流行趋势等</li><li>物品冷启动：利用相似物品的推荐逻辑</li></ul></li><li>丰富冷启动过程中可获得的用户和物品特征<ul><li>用户的注册信息</li><li>第三方数据管理平台提供的数据</li><li>物品的内容特征</li><li>引导用户输入的冷启动特征</li></ul></li><li>利用主动学习、迁移学习和“探索与利用”机制<h2 id="7、探索与利用"><a href="#7、探索与利用" class="headerlink" title="7、探索与利用"></a>7、探索与利用</h2></li></ul></li><li><p>传统方法</p><ul><li>简化为多臂老虎机问题，方法有e-Greedy，Thompson Sampling 和 UCB</li><li>无法引入用户的上下文和个性化信息</li></ul></li><li>个性化方法<ul><li>在传统方法的基础上，引入个性化信息，如LinUCB方法</li><li>无法与深度学习模型有效的整合</li></ul></li><li>基于模型方法<ul><li>如DRN中的探索网络</li></ul></li><li>作用<ul><li>物品冷启动</li><li>发掘用户新兴趣</li><li>增加结果多样性</li></ul></li></ul><img src="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666836395418-f70809a7-f423-4572-8989-563d038c3fb0.png" class="" title="image.png"><h1 id="六、深度学习推荐系统的工程实现"><a href="#六、深度学习推荐系统的工程实现" class="headerlink" title="六、深度学习推荐系统的工程实现"></a>六、深度学习推荐系统的工程实现</h1><h2 id="1、推荐系统的数据流"><a href="#1、推荐系统的数据流" class="headerlink" title="1、推荐系统的数据流"></a>1、推荐系统的数据流</h2><ul><li>批处理大数据架构<ul><li>分布式文件系统如HDFS和Map Reduce</li><li>数据处理延迟较大，影响相关应用的实时性</li></ul></li><li>流计算大数据架构<ul><li>Storm、Spark Streaming、Flink等</li><li>数据处理的延迟小，数据流的灵活性非常强</li></ul></li><li>Lambda架构<ul><li>数据通道分实时流和离线处理</li><li>实时流保持了流计算架构、离线处理以批处理为主</li><li>实时流和离线处理部分存在大量逻辑冗余，需要重复地进行编码工作，浪费了大量计算资源</li></ul></li><li>Kappa架构<ul><li>一切皆是流</li><li>批处理部分看作时间窗口较大的流处理过程</li></ul></li><li><p>大数据平台与推荐系统的融合</p><ul><li>训练数据的处理</li><li>特征的预计算<h2 id="2、深度学习推荐模型的分布式离线训练"><a href="#2、深度学习推荐模型的分布式离线训练" class="headerlink" title="2、深度学习推荐模型的分布式离线训练"></a>2、深度学习推荐模型的分布式离线训练</h2></li></ul></li><li><p>Spark MLlib</p><ul><li>全局广播：全量参数广播到所有partition</li><li>阻断式梯度下降方式：所有节点计算完成后再汇总，效率低</li></ul></li><li>Parameter Server<ul><li>异步非阻断式梯度下降</li><li>要在一致性和并行效率之间取舍</li><li>应用一致性Hash管理每个节点负责的参数</li></ul></li><li><p>Tensorflow</p><ul><li>基于Parameter Server架构的数据并行训练过程</li><li>每个worker节点内部，CPU+<a href="https://cloud.tencent.com/product/gpu?from=10680">GPU</a>任务级别的<a href="https://cloud.tencent.com/product/gpu?from=10680">并行计算</a>工程，CPU主要负责数据和任务的调度，GPU负责计算密集度高的张量运算<h2 id="3、深度学习推荐模型的线上部署"><a href="#3、深度学习推荐模型的线上部署" class="headerlink" title="3、深度学习推荐模型的线上部署"></a>3、深度学习推荐模型的线上部署</h2></li></ul></li><li><p>预训练Embedding + 轻量级线上模型</p></li><li>利用PMML转换并部署模型</li><li><p>Tensorflow Serving</p><h2 id="4、工程与理论之间的权衡"><a href="#4、工程与理论之间的权衡" class="headerlink" title="4、工程与理论之间的权衡"></a>4、工程与理论之间的权衡</h2></li><li><p>在现有实际条件的制约下，以工程完成和技术落地为目的，寻找并实现最优的解决方案</p></li><li>关注模型的稀疏性，关注主要特征，舍弃次要特征</li><li>技术升级兼顾日常开发进度</li><li>在有限的硬件资源下优化模型相关的一切工程实现</li></ul><img src="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666836395847-b12d0bc0-6d74-4799-bbb8-dc5c8bd7142d.png" class="" title="image.png"><h1 id="七、推荐系统的评估"><a href="#七、推荐系统的评估" class="headerlink" title="七、推荐系统的评估"></a>七、推荐系统的评估</h1><h2 id="1、离线评估方法和指标"><a href="#1、离线评估方法和指标" class="headerlink" title="1、离线评估方法和指标"></a>1、离线评估方法和指标</h2><ul><li>主要方法<ul><li>Holdout检验</li><li>交叉验证</li><li>自助法Bootstrap</li><li>Replay：逐一样本回放的精确线上仿真过程</li></ul></li><li><p>评估指标</p><ul><li>准确率</li><li>精确率和召回率</li><li>均方误差RMSE</li><li>对数损失函数</li><li>P-R曲线</li><li>ROC曲线与AUC</li><li>平均精度均值MAP</li><li>归一化折扣累计收益NDCG<h2 id="2、A／B测试与线上评估指标"><a href="#2、A／B测试与线上评估指标" class="headerlink" title="2、A／B测试与线上评估指标"></a>2、A／B测试与线上评估指标</h2></li></ul></li><li><p>又称为分桶测试，在利用控制变量法保持单一变量的前提下，将A、B两组数据进行对比</p></li><li>需要注意样本的独立性和采样方式的无偏性</li><li>层与层之间流量正交、同层流量互斥</li><li><p>A/B测试通常是模型上线前的最后一道测试 ，A／B测试的指标应与线上业务的核心指标保持一致</p><h2 id="3、快速线上评估方法-Interleaving"><a href="#3、快速线上评估方法-Interleaving" class="headerlink" title="3、快速线上评估方法 Interleaving"></a>3、快速线上评估方法 Interleaving</h2></li><li><p>不区分A／B组，同时把A和B模型的推荐结果推荐给同一批用户，通过指标衡量两个模型的效果</p></li><li>需要注意位置偏差，不同模型的推荐结果 等概率交替领先</li><li>所需样本少、测试速度快、与A／B测试结果无明显差异</li><li>工程框架复杂、只是对“用户对算法推荐结果偏好程度”的相对测量，不能得出一个算法真实的表现</li></ul><img src="/2022/10/27/2022-10-27-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666836396115-f0382e8b-3c8e-4ff9-8a3a-4f524469dd17.png" class="" title="image.png">]]></content>
    
    
    
    <tags>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《推荐系统三十六式》-极客时间</title>
    <link href="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"/>
    <url>/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<blockquote><p>参考：<a href="https://cloud.tencent.com/developer/article/1042402">推荐算法概览</a><br>参考：<a href="https://cloud.tencent.com/developer/article/1706381">学姐问我推荐系统是怎么做的？我用23张图带她搞懂！</a><br><strong>参考：</strong><a href="https://time.geekbang.org/column/intro/100005101?tab=catalog"><strong>推荐系统三十六式-刑无刀</strong></a><br>参考：<a href="https://mp.weixin.qq.com/s/l0gfLuiRam-kIsIWrZUv5A">万字长文详述对话推荐系统的逻辑与演化</a><br>参考：<a href="https://www.6aiq.com/article/1647476196657">图文解读：推荐算法架构——精排！</a></p></blockquote><p>课程学习，推荐系统的基础知识总结！</p><p><strong>概念篇</strong></p><div class="table-container"><table><thead><tr><th>推荐系统需要可以找到用户和物品之间是否有关联； 这个关联是一种概率上的可能性，而不是强关联；</th></tr></thead><tbody><tr><td>输入：推荐系统需要已经存在的连接，从已有的连接去预测未来的连接。</td></tr><tr><td>预测问题模式<strong>评分预测</strong>（显式）<strong>：</strong><br />1. 预测一个用户对每一个物品会打多少分<br />2. <strong>行为预测</strong>（隐式√）<strong>：</strong>预测一个用户“是否点击”某个物品，构建二分类模型，即 CTR 预估<br />评分预测问题很典型，但并不大众，毕竟在实际的应用中，评分数据很难收集到，属于典型的精英问题；与之相对的另一类问题行为预测，才是平民级推荐问题，处处可见。</td></tr><tr><td>推荐系统 4 大关键要素：UI 和 UE→数据→领域知识→算法</td></tr><tr><td>推荐系统强调的是目标思维和不确定性思维<br />- 目标思维：对最终目标进行量化，追求的是目标的增长，而不是一城一池的得失<br />- 不确定性思维：绝大多数推荐算法都是概率算法，不能用因果逻辑严丝合缝地提前推演，而是用概率的眼光去看结果。不能停留在“感觉推荐很精准”或者“感觉推荐得很不准”这样的玄学层面</td></tr></tbody></table></div><p><strong>基于内容过滤的推荐算法概览</strong></p><div class="table-container"><table><thead><tr><th>以item内容信息为评判主体，基于对用户的特征、兴趣的判断，评估用户与item的关联性基于内容的推荐，最重要的不是推荐算法，而是<strong>内容挖掘和分析</strong>。</th></tr></thead><tbody><tr><td>最简单的做法：用户画像构建+挖掘标签，将用户的画像内容就表示为稀疏的向量，同时内容端也有对应的稀疏向量，两者之间计算余弦相似度，根据相似度对推荐物品排序。</td></tr><tr><td>输入：用户画像（用户信息的向量化表示）/ 物品内容的稀疏向量</td></tr><tr><td>类型：<br />- 兴趣标签、注册信息筛选、最近流行、朋友喜欢等<br />- 进一步通过TF-IDF、BM25等方法学习不同特征的重要性（特征加权）</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>- 无需冷启动<br />- 推荐结果直观，容易解释；<br />- 无需领域知识</td><td>新用户问题<br />没有惊喜<br />特征向量稀疏，复杂特征难处理；</td></tr></tbody></table></div><p><strong>协同过滤推荐算法概览</strong></p><div class="table-container"><table><thead><tr><th>协同过滤算法的核心就是「找相似」，它基于用户的历史行为（浏览、收藏、评论等），去发现用户对物品的喜好，并对喜好进行度量和打分，最终筛选出推荐集合。简单总结就是，<strong>“物以类聚，人以群分”</strong></th></tr></thead><tbody><tr><td>输入：用户相似度矩阵 / 物品相似度矩阵</td></tr><tr><td>基于协同过滤的推荐算法类型：<br />- <strong>基于用户的协同过滤：</strong>先根据历史消费行为帮你找到一群和你口味很相似的用户；然后根据这些和你很相似的用户再消费了什么新的、你没有见过的物品，都可以推荐给你。<br />- <strong>基于物品的协同过滤：</strong>首先计算相似物品，然后再根据用户消费过、或者正在消费的物品为其推荐相似的基于模型的协同过滤：如矩阵分解、FM、SVD 等</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>- 无需领域知识<br />- 自动化程度高<br />- 性能随时间的积累而提高；</td><td>- 新用户问题<br />- 冷启动问题。可以通过基于内容或 bandit 算法解决<br />- <a href="https://www.bilibili.com/video/BV1Uv41117hx/?spm_id_from=333.999.0.0&amp;vd_source=64e24aaa8fce4a43b517304bd02226fc">流行度长尾问题</a>，越热门的item越容易被系统推荐，反之越难被系统推荐。可以通过流行度算法，降低热门 item 权重；<br />- user-item行为矩阵稀疏、存储压力较大。可通过MF解决</td></tr></tbody></table></div><p><strong>矩阵分解推荐算法概览</strong></p><div class="table-container"><table><thead><tr><th>矩阵分解，直观上说来简单，就是把原来的大矩阵，近似分解成两个小矩阵的乘积，在实际推荐计算时不再使用大矩阵，而是使用分解得到的两个小矩阵。<strong>主要用于评分预测！</strong></th></tr></thead><tbody><tr><td>输入：三元组&lt;𝑢,𝑖,𝑗&gt;，表示对用户u来说，商品i的优先级要高于商品j。</td></tr><tr><td>类型：<br />- 矩阵分解的典型模型就是 SVD 以及其各种变体。<br />- <strong>基于矩阵分解的贝叶斯个性化排序算法</strong>：能够较好地为用户排列出更好的物品相对顺序，而非更精确的评分。适合在海量数据中选择极少量数据做推荐</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>1. 预测的精度高于基于内容和协同过滤；<br />2. 高维矩阵映射为两个低维矩阵节省了存储空间<br />3. 比较容易编程实现，随机梯度下降法和交替最小二乘法均可训练出模型。<br />4. 良好的拓展性，可以很方便在用户特征向量和物品特征向量中添加其它因素</td><td>1. 模型训练比较费时；<br />2. 因为将用户和物品隐射到了隐因子空间，使得这些隐含特征无法使用现实生活中的概念来解释，因此推荐出的见过解释性并不好。</td></tr></tbody></table></div><p><strong>模型融合推荐算法概览</strong></p><div class="table-container"><table><thead><tr><th>多路召回的推荐结果，由于分数范围、产生机制等原因，没法直接用来进行比较排序，需要一个融合模型来给多种召回策略外挂一个<strong>融合排序</strong>。</th></tr></thead><tbody><tr><td>输入：content-based、item-based、user-based、SVD等多路召回结果</td></tr><tr><td>类型：<br />- 逻辑回归和梯度提升决策树组合：树模型的集成模型（如RF和GBDT）可以做特征组合又能有效表达出数据中的非线性；逻辑回归负责学习特征组合的权重，最后预估 CTR。<br />- Wide and Deep：用于融合排序。由逻辑回归作为最终输出单元，深模型最后一个隐藏层作为特征，与宽模型的原始特征一起接入逻辑回归，然后训练参数。</td></tr><tr><td><a href="https://www.yuque.com/ningshixian/pz10h0/om4bph#mB3T0">深宽模型具体落地</a></td></tr></tbody></table></div><p><strong>高级或“非传统”推荐算法概览</strong></p><div class="table-container"><table><thead><tr><th>包括：深度学习Bandit 推荐算法（探索/利用）….</th></tr></thead><tbody><tr><td>Bandit：<br />- Bandit 算法是一类走一步看一步的推荐算法，基本思想是：看看选择会带来多少遗憾，遗憾越少越好。套路就是：小心翼翼地试，越确定某个选择好，就多选择它，越确定某个选择差，就越来越少选择它。<br />- Bandit 算法是一种不太常用在推荐系统的算法，究其原因，是它能同时处理的物品数量不能太多。主要用于探索利用问题，也叫 Exploit－Explore 问题</td></tr><tr><td>精排：<br />- 表示学习：Youtube DNN 视频推荐做法、Airbnb embedding、双塔 DSSM、各种xx2vec<br />- 图表示学习：DeepWalk、Node2vec、EGES等<br />- 特征交叉模型：关注自动化的特征交叉，经典研究工作有：DCN、DeepFM、xDeepFM等；<br />- 序列模型：从历史行为序列特征学习用户的兴趣向量，典型的研究工作有：DIN、DSIN、DIEN、SIM等；<br />- 多任务学习：多种目标综合起来，归纳到一个模型里面进行学习，典型的算法有：ESSM、MMoE等。</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>- 深层特征自动组合和挖掘<br />- 效果好</td><td>- 难以解释<br />- 较为复杂</td></tr></tbody></table></div><h2 id="概念篇"><a href="#概念篇" class="headerlink" title="概念篇"></a>概念篇</h2><h3 id="什么是推荐系统？"><a href="#什么是推荐系统？" class="headerlink" title="什么是推荐系统？"></a>什么是推荐系统？</h3><p>随着移动互联网的发展，越来越多的信息开始在互联网上传播，产生了严重的信息过载。因此，如何从众多信息中找到用户感兴趣的信息，这个便是推荐系统的价值。精准推荐解决了用户痛点，提升了用户体验，最终便能留住用户。</p><p>推荐系统本质上就是一个信息过滤系统，通常分为：召回、排序、重排序这3个环节，每个环节逐层过滤，最终从海量的物料库中筛选出几十个用户可能感兴趣的物品推荐给用户。 </p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664332612978-ffa24ee2-4fd2-46c7-9c6a-35836db14283.png" class="" title="image.png"><ol><li>它能做什么；推荐系统需要可以找到用户和物品之间是否有关联； 这个关联是一种概率上的可能性，而不是强关联；</li><li>它需要什么；推荐系统需要已经存在的连接，从已有的连接去预测未来的连接。</li><li>它怎么做：预测用户评分和偏好；机器推荐和人工推荐，也就是通常说的“个性化推荐”和“编辑推荐”。</li></ol><h3 id="推荐系统的问题模式"><a href="#推荐系统的问题模式" class="headerlink" title="推荐系统的问题模式"></a>推荐系统的问题模式</h3><p>我们知道，推荐系统的使命是为用户和物品建立连接，建立的方式是提前找出那些隐藏的连接呈现给用户，这是一个预测问题；所以推荐系统的预测问题模式，从达成的连接目标角度区分，有两大类：</p><ol><li><strong>评分预测。</strong></li><li><strong>行为预测。</strong></li></ol><p><strong>评分预测（显式反馈）</strong><br>提前预测一个用户对每一个物品会打多少分，找出那些他可能会打高分，但是还没消费的物品，然后装作若无其事地呈现在他面前，惊不惊喜，意不意外?<br>但是评分实际不易收集，数据质量不能保障，数据伪造门槛低，加上数据分布问题导致评分预测做好比较难</p><p><strong>行为预测（隐式反馈）</strong><br>预测一个用户对某个物品可能会产品的行为。推荐系统预测行为方式有很多，常见的有两种：<strong>直接预测行为本身发生的概率，和预测物品的相对排序。</strong></p><ul><li><p>直接预测用户行为这一类技术，有一个更烂大街的名字，叫做 CTR 预估。这里的 C 原本是点击行为 Click，但这个解决问题的模式可以引申到任何其他用户行为，如收藏、购买等行为的预测</p></li><li><p>预测物品的相对排序：包括 pair-wise、list-wise等方法</p></li></ul><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665301184139-70153131-252b-45ec-ba2f-bcc7b88db0e1.png" class="" title="image.png"><p><strong>评分预测 vs. 行为预测</strong></p><ol><li>隐式反馈数据比显式反馈更加稠密</li><li>隐式反馈更代表用户的真实想法。比如不赞成川普的观点，但还是想看到他（吐槽）</li><li>评分的分布不稳定，整体评分在不同时期会差别很大，个人评分在不同时期标准不同，人和人之间的标准差别很大</li><li>隐式反馈通常更容易在AB 测试中和测试指标挂钩。比如CTR 预估当然关注的是点击率</li></ol><h3 id="几个常见顽疾"><a href="#几个常见顽疾" class="headerlink" title="几个常见顽疾"></a>几个常见顽疾</h3><p>讨论了两大类推荐系统的问题后，我们再来看几个推荐系统的隐藏顽疾。之所以说这些是隐藏顽疾，是因为它们还没有很好的通用解决方案，并且不容易被重视，这几个顽疾分别是：</p><ol><li>冷启动问题；</li><li>探索与利用问题；</li><li>安全问题。</li></ol><h3 id="对关键元素重要性的认识"><a href="#对关键元素重要性的认识" class="headerlink" title="对关键元素重要性的认识"></a>对关键元素重要性的认识</h3><p>要开发一个推荐系统产品，有这么四个关键的元素需要注意：</p><ol><li>UI 和 UE；</li><li>数据；</li><li>领域知识；</li><li>算法。</li></ol><p>他们的重要性依次递减，权重大致是 4-3-2-1。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665302972252-2007f5d2-40b2-4bd8-a373-7a19da0809e3.png" class="" title="image.png"><p>UI 和 UE<br>首先“颜值即正义”，推荐的作用是锦上添花，不能当做雪中送炭的救命稻草；</p><p>数据<br>数据与 UI、UE 是几乎同等重要的元素，它是推荐系统的食材，巧妇难为无米之炊，多少算法工程师因为加入了一家没有历史数据积累的公司，那种“拔剑四顾心茫然”的无力感，谁去谁知道；</p><p>领域知识<br>领域知识，与之对应的是常识和通识。可以这样说，没有哪个产品不涉及领域知识，每一个产品存在于市场上，总是有一部分价值是大多数其他产品无法替代的，这些在一个领域总结出来的普适规律，对于推荐系统的效果提升非常有用：有的是防止闹笑话自毁品牌形象，有的是大幅提高某些指标，有的是缩短模型训练周期；</p><p>算法<br>最后是算法，一种对算法的常见误会就是：短期高估，长期低估。在一款个性化产品诞生之初，算法所起到的作用可以忽略，我们不能指望它能让产品起死回生、一飞冲天，但就此抛出“算法无用论”也是很愚蠢的。</p><h3 id="目标思维和不确定性思维"><a href="#目标思维和不确定性思维" class="headerlink" title="目标思维和不确定性思维"></a>目标思维和不确定性思维</h3><p>传统的软件产品追求的是稳定和满足预期，背后思想强调的是逻辑和因果链条。反观推荐系统这种信息过滤系统，追求的是指标的增长，背后思想强调是目标和不确定性。</p><p>目标思维：对最终目标进行量化，才能让整个团队才能知道在为什么而战，才能知道自己所做的动作是不是有意义，才能让团队自发地去寻找优化方向，一定不能停留在“感觉推荐很精准”或者“感觉推荐得很不准”这样的玄学层面。</p><p>不确定思维：不能用因果逻辑严丝合缝地提前推演，而是用概率的眼光去看结果。比如说，出现了一个不是很合适的推荐，通常老板们会立即责问：“为什么出现这个”，这就是确定性思维在作祟，如果是不确定性思维，就会问：“出现这个的可能性有多大”。</p><p>为什么负责推荐系统产品的人一定要有不确定性思维呢？原因有以下几个。</p><ol><li>绝大多数推荐算法都是概率算法，因此本身就无法保证得到确切结果，只是概率上得到好的效果；</li><li>推荐系统追求的是目标的增长，而不是一城一池的得失；</li><li>如果去花时间为了一个 Case 而增加补丁，那么付出的成本和得到的收益将大打折扣；</li><li>本身出现意外的推荐也是有益的，可以探索用户的新兴趣，这属于推荐系统的一个经典问题：EE 问题，我也会在后面的内容中专门讲。</li></ol><h2 id="原理篇-·-内容推荐"><a href="#原理篇-·-内容推荐" class="headerlink" title="原理篇 · 内容推荐"></a>原理篇 · 内容推荐</h2><h3 id="什么是用户画像"><a href="#什么是用户画像" class="headerlink" title="什么是用户画像"></a>什么是用户画像</h3><p>对用户信息的向量化表示，就是 User Profile，俗称“用户画像”。所以，用户画像不是推荐系统的目的，而是在构建推荐系统的过程中产生的一个关键环节的副产品。用户画像是给机器看的，而不是给人看的。</p><h3 id="用户画像构建的方法"><a href="#用户画像构建的方法" class="headerlink" title="用户画像构建的方法"></a>用户画像构建的方法</h3><ol><li><strong>第一类就是查户口</strong>。直接使用原始数据作为用户画像的内容，如注册资料等人口统计学信息，或者购买历史，阅读历史等，除了数据清洗等工作，数据本身并没有做任何抽象和归纳。这就跟查户口一样，没什么技术含量，但通常对于用户冷启动等场景非常有用。</li><li><strong>第二类就是堆数据</strong>。方法就是堆积历史数据，做统计工作，这是最常见的用户画像数据，常见的兴趣标签，就是这一类，就是从历史行为数据中去挖掘出标签，然后在标签维度上做数据统计，用统计结果作为量化结果。这一类数据贡献了常见的酷炫用户画像。</li><li><strong>第三类就是黑盒子</strong>。就是用机器学习方法，学习出人类无法直观理解的稠密向量，也最不被非技术人员重视，但实际上在推荐系统中承担的作用非常大。</li></ol><h3 id="构建用户画像步骤"><a href="#构建用户画像步骤" class="headerlink" title="构建用户画像步骤"></a>构建用户画像步骤</h3><p>用户画像对于推荐系统还是非常必要的，而产品中属文本数据最多，那如何用文本数据构建出用户的画像内容呢？ 1、分析用户的文本和物品的文本，使其结构化，去粗取精，保留关键信息作为其画像内容；2、把物品的文本分析结果，按照用户历史行为把物品画像（ Item Profile ）传递给用户。</p><p><strong>结构化文本</strong></p><p>我们拿到的文本，常常是自然语言描述的，用行话说，就是“非结构化”的，但是计算机在处理时，只能使用结构化的数据索引，所以分析文本，就是为了将非结构化的数据结构化，好比是将模拟信号数字化一样，只有这样才能送入计算机，继续计算。</p><p>几种常用的文本结构化算法</p><p>关键词提取：常用 TF-IDF 和 TextRank。</p><p>实体识别：</p><p>内容分类：</p><p>文本聚类 ：</p><p>主题模型：也是一种聚类思想，主题向量也不是标签形式，也是用户画像的常用构成。</p><p>嵌入：也叫作 Embedding，如大名鼎鼎的 Word2Vec</p><p><strong>标签选择</strong></p><p>前面说到，用户端的文本，物品端的文本如何结构化，得到了诸如标签（关键词、分类等）、主题、词嵌入向量。接下来就是第二步：如何把物品的结构化信息给用户呢？</p><p>我们想一想，用户在产品上看到了很多我们用各种逻辑和理由展示给他的物品，他只从中消费了一部分物品。现在问题就是，到底是那些特性吸引了他消费呢？</p><ol><li>把用户产生过行为的物品标签直接累积在一起（不做筛选）</li><li>把用户对物品的行为，消费或者没有消费看成是一个分类问题。用户用实际行动帮我们标注了若干数据，那么挑选出他实际感兴趣的特性就变成了特征选择问题。</li></ol><p>特征选择最常用的是两个方法：</p><ul><li>卡方检验（CHI）：本质上在检验“词和某个类别 C 相互独立”这个假设是否成立，和这个假设偏离越大，就越说明这个词和类别 C 暗中有一腿，那当然这个词就是关键词了。</li><li>信息增益（IG）</li></ul><h3 id="内容推荐算法"><a href="#内容推荐算法" class="headerlink" title="内容推荐算法"></a>内容推荐算法</h3><p>基于内容的推荐，最重要的不是推荐算法，而是<strong>内容挖掘和分析</strong>。内容挖掘越深入，哪怕早期推荐算法仅仅是非常硬的规则，也能取得不俗的效果。举个例子，如果推荐物品是短视频，我们分几种情况看：</p><ol><li>如果短视频本身没有任何结构化信息，如果不挖掘内容，那么除了强推或者随机小流量，没有别的合理曝光逻辑了；</li><li>如果对视频的文本描述，比如标题等能够有内容分类，比如是娱乐类，那么对于喜欢娱乐的用户来说就很合理；</li><li>如果能够进一步分析文本的主题，那么对于类似主题感兴趣的用户就可能得到展示；</li><li>如果还能识别出内容中主角是吴亦凡，那就更精准锁定一部分用户了；</li><li>如果再对内容本身做到嵌入分析，那么潜藏的语义信息也全部抓住，更能表达内容了。</li></ol><p>对于基于内容的推荐系统，最简单的推荐算法当然是计算相似性即可，用户的画像内容就表示为稀疏的向量，同时内容端也有对应的稀疏向量，两者之间计算余弦相似度，根据相似度对推荐物品排序。如果再进一步，要更好地利用内容中的结构化信息，因为一个直观的认识是：不同字段的重要性不同。我们可以借鉴信息检索中的相关性计算方法来做推荐匹配计算：<strong>BM25F</strong> 算法</p><h2 id="原理篇-·-近邻推荐"><a href="#原理篇-·-近邻推荐" class="headerlink" title="原理篇 · 近邻推荐"></a>原理篇 · 近邻推荐</h2><h3 id="协同过滤是什么？"><a href="#协同过滤是什么？" class="headerlink" title="协同过滤是什么？"></a>协同过滤是什么？</h3><p>协同过滤的重点在于“协同”，所谓协同，也就是群体互帮互助，互相支持是集体智慧的体现，协同过滤也是这般简单直接，历久弥新。</p><p>协同过滤算法的核心就是「找相似」，它基于用户的历史行为（浏览、收藏、评论等），去发现用户对物品的喜好，并对喜好进行度量和打分，最终筛选出推荐集合。</p><p>它包括两个分支：</p><ul><li><strong>基于用户的协同过滤：</strong>User-CF</li><li><strong>基于物品的协同过滤：</strong>Item-CF</li></ul><h3 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h3><p>思想：先根据历史消费行为帮你找到一群和你口味很相似的用户；然后根据这些和你很相似的用户再消费了什么新的、你没有见过的物品，都可以推荐给你。</p><p>比如下图中，用户 A 和用户 C 都购买过物品 a 和物品 b，那么可以认为 A 和 C 是相似的，因为他们共同喜欢的物品多。这样，就可以将用户 A 购买过的物品 d 推荐给用户 C。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664332918296-dcf7ccdb-86b5-4d04-97e4-8cb8a859dd72.png" class="" title="image.png"><p>基于用户的协同过滤具体做法：</p><ol><li>构建“user-item”交互矩阵，行代表用户向量，列代表物品向量，维度上的取值是评分或点击次数占比；</li><li>两两计算用户之间的相似度。设定一个相似度阈值或者设定一个最大数量，为每个用户保留与其最相似的用户。</li><li>为每一个用户产生推荐结果。把和他“臭味相投”的用户们喜欢过的物品汇总起来，去掉用户已消费过的物品，剩下的排序输出就是推荐结果。具体的汇总方式我们用一个公式来表示：</li></ol><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666089018024-b9d6f2dc-a00c-42fd-889b-39cee70f804f.png" class="" title="未命名绘图.drawio.png"><p>这里的态度R最简单就是 0 或者 1，1 表示喜欢过，0 表示没有，如果是评分，则可以是 0 到 5 的取值。整个公式就是相似用户们的态度加权平均值。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666089103785-0fc022e9-fc5d-423c-8eff-dc49afe7d043.png" class="" title="image.png"><p>基于用户的协同过滤-有哪些踩坑</p><ol><li>用户向量很稀疏。可以采用稀疏矩阵存储格式</li><li>相似度计算本身如果遇到超大维度向量怎么办？对向量采样计算，或转换成矩阵计算</li><li>用户数量往往比较大，两两计算相似度非常吃力？不用基于用户的协同过滤。</li></ol><h3 id="基于物品的协同过滤"><a href="#基于物品的协同过滤" class="headerlink" title="基于物品的协同过滤"></a>基于物品的协同过滤</h3><p>思想：和基于用户的不同，基于物品的协同过滤首先计算相似物品，然后再根据用户消费过、或者正在消费的物品为其推荐相似的。</p><p>比如下图中，物品 a 和物品 b 同时被用户 A，B，C 购买了，那么物品 a 和 物品 b 被认为是相似的，因为它们的共现次数很高。这样，如果用户 D 购买了物品 a，则可以将和物品 a 最相似的物品 b 推荐给用户 D。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664332918658-c2bb398c-fe01-4362-bf26-c573327ba9f5.png" class="" title="image.png"><p>基于物品的协同过滤具体做法：</p><ol><li>构建“user-item”交互矩阵，行代表用户向量，列代表物品向量，维度上的取值是评分或点击次数占比；</li><li>两两计算物品之间的相似度。item 向量之间的夹角余弦。</li><li>为每一个用户产生推荐结果。<strong>出发方式是汇总和“用户已经消费过的物品”相似的物品，按照汇总后分数从高到低推出。汇总的公式是这样的：</strong></li></ol><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666090123502-83b6eb20-a387-45ce-84dc-e0e6c8ac62c4.png" class="" title="未命名绘图.drawio.png"><p>遍历用户$u$评分/点击过的所有物品，假如一共有 $m$ 个，每一个物品 j 和待计算物品 $i$ 的相似度$w_{ij}$乘以用户的评分$R_{uj}$（由于评分数据很难收集到，可以用物品$j$的被点击次数占比 进行替代$R_{uj}=\frac{c_{uj}}{\sum_k^m{c_{uk}}}$），这样加权求和后，就得到了一个加权平均评分，作为用户 u 对物品 i 的分数预测。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666090190970-121273de-2e40-465c-91c8-64876528ad95.png" class="" title="image.png"><p>举例说明：</p><p><strong>第一步：整理物品的共现矩阵</strong></p><p>假设有 A、B、C、D、E 5个用户，其中用户 A 喜欢物品 a、b、c，用户 B 喜欢物品 a、b等等。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664335290906-803caf19-aded-4c49-95bc-e81d22eb9452.png" class="" title="image.png"><p>所谓共现，即：两个物品被同一个用户喜欢了。比如物品 a 和 b，由于他们同时被用户 A、B、C 喜欢，所以 a 和 b 的共现次数是3，采用这种统计方法就可以快速构建出共现矩阵。 </p><p><strong>第二步：计算物品的相似度矩阵</strong></p><p>对于 Item-CF 算法来说，一般不采用前面提到的余弦距离来衡量物品的相似度，而是采用下面的公式：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664335290901-676eef25-ac11-4621-9a11-9eb174e99875.png" class="" title="image.png"><p>其中，N(u) 表示喜欢物品 u 的用户数，N(v) 表示喜欢物品 v 的用户数，两者的交集表示同时喜欢物品 u 和物品 v 的用户数。很显然，如果两个物品同时被很多人喜欢，那么这两个物品越相似。</p><p>基于第1步计算出来的共现矩阵以及每个物品的喜欢人数，便可以构造出物品的相似度矩阵：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664335290974-e5c60076-4ced-4a9f-8d9c-cb018ce7d568.png" class="" title="image.png"><p><strong>第三步：基于相似度矩阵推荐物品</strong></p><p>在得到物品相似度之后，接下来就是为用户推荐他可能会感兴趣的物品了，基于物品的协同过滤，有两种应用场景。</p><ol><li>第一种属于 TopK 推荐，形式上也常常属于类似“猜你喜欢”这样的。</li></ol><p><strong>出发方式是当用户访问首页时，汇总和“用户已经消费过的物品”相似的物品，按照汇总后分数从高到低推出。汇总的公式是这样的：</strong></p><p>$P_{u i}=\sum_{j=1}^{m} \mathrm{W}_{i j} * R_{u j}$</p><p>其中，${P_{u i}}$表示用户 u 对某个物品 $i$ 的感兴趣程度，值越大，越值得被推荐。</p><p>遍历用户$u$评分/点击过的所有物品，假如一共有 $m$ 个，每一个物品 j 和待计算物品 $i$ 的相似度$w_{ij}$乘以用户的评分$R_{uj}$（由于评分数据很难收集到，可以用物品$j$的被点击次数占比 进行替代$R_{uj}=\frac{c_{uj}}{\sum_k^m{c_{uk}}}$），这样加权求和后，就得到了一个加权平均评分，作为用户 u 对物品 i 的分数预测。</p><blockquote><p>注意：我们在计算时不必对所有物品 $i$ 都计算一次，只需要按照用户评分/点击过的$m$个物品，逐一取出和它们相似的 N 个物品出来就可以了。</p></blockquote><p>这个过程都是离线完成后，去掉那些用户已经消费过的，保留分数最高的 k 个结果存储。当用户访问首页时，直接查询出来即可。</p><p>上面的公式有点抽象，直接看例子更容易理解，假设我要给用户 E 推荐物品，前面我们已经知道用户 E 喜欢物品 b 和物品 c，喜欢程度假设分别为 0.6 和 0.4。那么，利用上面的公式计算出来的推荐结果如下：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664335290950-c466acf7-705c-4918-a34a-e4d5d9006b29.png" class="" title="image.png"><p>因为物品 b 和物品 c 已经被用户 E 喜欢过了，所以不再重复推荐。最终对比用户 E 对物品 a 和物品 d 的感兴趣程度，因为 0.682 &gt; 0.3，因此选择推荐物品 a。</p><ol><li>第二种属于相关推荐</li></ol><p>这类推荐不需要提前合并计算，当用户访问一个物品的详情页面时，或者完成一个物品消费的结果面，直接通过物品相似度矩阵，获取这个物品的相似物品推荐，就是“看了又看”或者“买了又买”的推荐结果了。</p><blockquote><p>基于物品的协同过滤很好的解决了基于用户的坑。<br>首先，物品的数量，或者严格的说，可以推荐的物品数量往往少于用户数量；所以一般计算物品之间的相似度就不会成为瓶颈。<br>其次，物品之间的相似度比较静态，它们变化的速度没有用户的口味变化快；所以完全解耦了用户兴趣迁移这个问题。<br>最后，物品对应的消费者数量较大，对于计算物品之间的相似度稀疏度是好过计算用户之间相似度的。</p></blockquote><h3 id="协同过滤有哪些缺点呢？"><a href="#协同过滤有哪些缺点呢？" class="headerlink" title="协同过滤有哪些缺点呢？"></a>协同过滤有哪些缺点呢？</h3><ul><li>由于大部分user只对很少一部分item有行为，导致user与item的<strong>行为矩阵十分稀疏</strong>，甚至有些user根本没有任何行为，影响了向量相似度计算准确性。</li><li>user和item数量都很大，行为矩阵存储压力很大。</li><li>矩阵稀疏也带来一个问题，就是头部热门item容易与大多数item均相似，导致极其严重的<strong>马太效应</strong>。</li></ul><h2 id="原理篇-·-矩阵分解"><a href="#原理篇-·-矩阵分解" class="headerlink" title="原理篇 · 矩阵分解"></a>原理篇 · 矩阵分解</h2><blockquote><p><a href="https://lumingdong.cn/recommendation-algorithm-based-on-matrix-decomposition.html">https://lumingdong.cn/recommendation-algorithm-based-on-matrix-decomposition.html</a></p></blockquote><p>2006 年 10 月 2 号，Netflix Prize 发出百万美元悬赏，凡是能在他们现有推荐系统基础上，把均方根误差降低 10% 的大侠，可以瓜分 100 万美元。这一<strong>评分预测问题</strong>在一百万美元的加持下，催生出无数推荐算法横空出世，其中最为著名的就是一系列矩阵分解模型，而最最著名的模型就是 SVD 以及其各种变体。</p><p><strong>矩阵分解（Matrix Factorization，MF）</strong>是协同过滤的一个分支算法（model-based CF），在推荐领域具有崇高的地位，因为它同时兼具了协同过滤、隐语义以及机器学习的特性，再加上矩阵分解易于实现和拓展，使之成为当今工业界非常普遍和流行的推荐算法。</p><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>先来说说矩阵分解几个明显的特点，它具有协同过滤的 “集体智慧”，隐语义的 “深层关系”，以及机器学习的 “以目标为导向的有监督学习”。在了解了基于邻域的协同过滤算法后，集体智慧自不必多说，我们依次从 “隐因子” 和 “有监督学习” 的角度来了解矩阵分解的基本思路。</p><p>推荐算法中的矩阵分解最初的想法是从奇异值分解（Singular Value Decomposition，SVD）借鉴来的，也仅仅是借鉴，并非是标准的奇异值分解，勉强算是一个伪奇异值分解。具体的区别留在相关算法这一小节详说。</p><p>以 Netflix 用户对电影的评分矩阵为例，矩阵分解，直观上来说就是把原来的大矩阵，近似分解成两个小矩阵的乘积，在实际推荐计算时不再使用大矩阵，而是使用分解得到的两个小矩阵。按照矩阵分解的原理，我们会发现原来 𝑚×𝑛 的大矩阵会分解成 𝑚×𝑘 和 𝑘×𝑛 的两个小矩阵，这里多出来一个 k 维向量，就是隐因子向量（Latent Factor Vector），类似的表达还有隐因子、隐向量、隐含特征、隐语义、隐变量等。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665202625343-6733bc2f-330b-4d0d-af78-1c123f578fa4.png" class="" title="image.png"><p><strong>基于矩阵分解的推荐算法的核心假设是用隐语义（隐变量）来表达用户和物品，他们的乘积关系就成为了原始的元素。这种假设之所以成立，是因为我们认为实际的交互数据是由一系列的隐变量的影响下产生的（通常隐变量带有统计分布的假设，就是隐变量之间，或者隐变量和显式变量之间的关系，我们往往认为是由某种分布产生的。），这些隐变量代表了用户和物品一部分共有的特征，在物品身上表现为属性特征，在用户身上表现为偏好特征，只不过这些因子并不具有实际意义，也不一定具有非常好的可解释性，每一个维度也没有确定的标签名字，所以才会叫做 “隐变量”。而矩阵分解后得到的两个包含隐变量的小矩阵，一个代表用户的隐含特征，一个代表物品的隐含特征，矩阵的元素值代表着相应用户或物品对各项隐因子的符合程度，有正面的也有负面的。</strong></p><p>依然以电影为例，电影可能具有一些隐藏因子：演员、题材、主题、年代……，而用户针对这些隐因子有偏好特征属性，为了便于理解，我们假设隐因子数量 k 是 2，分别代表着喜剧片和动作片两种题材，矩阵分解后的两个小矩阵，分布代表着电影对这两种题材的符合程度以及用户对这两种题材的偏好程度，如下图：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665202655366-52e99476-aef0-4f11-8650-f42ea640fe2d.png" class="" title="image.png"><p>通常情况下，隐因子数量 k 的选取要远远低于用户和电影的数量，大矩阵分解成两个小矩阵实际上是用户和电影在 k 维隐因子空间上的映射，这个方法其实是也是一种 “降维”（Dimension Reduction）过程，同时将用户和电影的表示转化为在这个 k 维空间上的分布位置，电影和用户的距离越接近表示用户越有可能喜欢这部电影，表现在数值上则是各项隐因子符合程度的正负性越一致。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665202676097-839155ca-f52a-4bc8-b6c1-33aba2cd593c.png" class="" title="image.png"><p>我们再从机器学习的角度来了解矩阵分解，我们已经知道电影评分预测实际上是一个矩阵补全的过程，在矩阵分解的时候原来的大矩阵必然是稀疏的，即有一部分有评分，有一部分是没有评过分的，不然也就没必要预测和推荐了，所以整个预测模型的最终目的是得到两个小矩阵，通过这两个小矩阵的乘积来补全大矩阵中没有评分的位置。所以对于机器学习模型来说，问题转化成了如何获得两个最优的小矩阵。因为大矩阵有一部分是有评分的，那么只要保证大矩阵有评分的位置（实际值）与两个小矩阵相乘得到的相应位置的评分（预测值）之间的误差最小即可，其实就是一个均方误差损失，这便是模型的目标函数，具体的公式可参考相关算法这一小节。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665202690139-d2d7795b-9947-4b4f-b1a7-e2801fc91c05.png" class="" title="image.png"><p>这种带有隐因子的机器学习模型通常称为<strong>隐语义模型（Latent Factor Model，LFM)</strong>，因为隐因子的概念最早在文本领域被提出，用于找到文本的隐含语义，所以隐因子有时也称隐语义。而矩阵分解是隐语义模型的代表，在很多地方，会直接使用隐语义模型代表矩阵分解的这一类模型。隐语义模型的在推荐算法中的优势是对用户和物品信息中的隐含结构进行建模，从而能够挖掘更加深层次的用户和物品关系。</p><h3 id="Traditional-SVD"><a href="#Traditional-SVD" class="headerlink" title="Traditional SVD"></a>Traditional SVD</h3><p>说到矩阵分解，我们首先想到的就是奇异值分解 SVD。理论上，SVD 也可以直接用于推荐，我们可以把 User-Item 评分矩阵 A 进行 SVD 分解，并通过选择部分较大的一些奇异值来同时进行降维，也就是说矩阵 A 此时分解为：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665202868857-3f0774d8-6d22-4b03-9a43-87f6ae22d80a.png" class="" title="image.png"><p>其中 k 是矩阵 A 中较大的部分奇异值的个数，一般会远远的小于用户数 m 和物品数 n。如下图所示：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665202769567-7563fe74-13cf-4397-be26-f8ad37f7e89d.png" class="" title="image.png"><p>从图中可以看出，我们上面用的公式其实是 Truncated SVD 形式，它是对完整 SVD 的一个极大近似，因为对于推荐系统来说，并不要求极致的精确度，我们更加追求效率和收益最大化，Truncated SVD 的形式可以极大地降低计算量，更符合实际需要。</p><p>SVD 通常有以下几种不同的表示形式（<a href="https://lumingdong.cn/go/1kd8k3">参考论文</a>）：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665202787577-d77f5da7-91db-418d-be4f-89a052fd7ebf.png" class="" title="image.png"><p>1）<strong>Full SVD（完整 SVD）</strong>：左奇异矩阵 U 和右奇异矩阵 V 都是方阵，在对角矩阵 Σ 中有 P 个奇异值，其中 P=min(M, N)，图中假设 M&gt;N，那么 U 中被虚线框出的列向量的会乘以对角矩阵超出主对角线外的 0 值行向量，也就是说，对于 U 而言，虚线框出的这部分其实已经没有什么价值了，所以才会有 Reduced SVD 的表示方法。</p><p>2）<strong>Reduced SVD / Compact SVD（精简/紧凑 SVD）</strong>：Reduced SVD 其实就是把 Full SVD 中的 U 和 Σ 多余的那部分去掉，对原矩阵 A 的表示并不受影响，而对应的 V 仍然是方阵（图中假设 M&gt;N）</p><p>3）<strong>Truncated SVD（截断 SVD）</strong>：Truncated SVD 其实是一种<strong>近似</strong>，（从大到小排列)取前 K 个奇异值来代替原来全部的奇异值，因为很多情况下，前 10% 甚至 1% 的奇异值的和就占了全部的奇异值之和的 99% 以上了，这样做可以极大地减少计算量，但却可以保留大部分信息。</p><p>通过奇异值分解，如果我们要预测第 𝑖 个用户对第 𝑗 个物品的评分 𝑎𝑖𝑗, 则只需要计算$𝑢_𝑖Σ𝑣^𝑇_𝑗$即可。通过这种方法，我们可以将评分表里面所有没有评分的位置得到一个预测评分，通过找到最高的若干个评分对应的物品推荐给用户。</p><p>思路很简单，但是奇异值分解在直接用于推荐的使用过程中存在很多问题：</p><ol><li>SVD 分解要求矩阵是稠密的，而现实场景中的评分矩阵是稀疏的，有大量空白，无法直接使用 SVD 分解的。要想使用 SVD，必须对评分矩阵中的缺失值进行简单的补全，比如用全局平均值或者用用户物品平均值补全，得到补全后的矩阵。接着可以用 SVD 分解并降维。但填充本身会造成很多问题，其一，填充大大增加了数据量，增加了算法复杂度。其二，简单粗暴的数据填充很容易造成数据失真。</li><li>SVD 分解的复杂度比较高，假设对一个 𝑚×𝑛 的矩阵进行分解，时间复杂度为$𝑂(𝑛^2∗𝑚+𝑛∗𝑚^2)$，其实就是 𝑂(𝑛3)。对于 m、n 比较小的情况，还是勉强可以接受的，但是在推荐场景的海量数据下，m 和 n 的值通常会比较大，可能是百万级别上的数据，这个时候如果再进行 SVD 分解需要的计算代价就是很大的。</li></ol><h3 id="贝叶斯个性化排序"><a href="#贝叶斯个性化排序" class="headerlink" title="贝叶斯个性化排序"></a>贝叶斯个性化排序</h3><p>在推荐或搜索领域，排序学习（Leaning To Rank）占有非常重要的地位，因为对于推荐或者搜索，本质上我们还是会更加关注相关内容的排序，越相关就希望它的排序越靠前。其实矩阵分解也属于排序学习的一种，也就是最为常见的 Pointwise 类型，其中 point 意思就是：只单独考虑每个物品，每个物品像是空间中孤立的点一样。而基于矩阵分解构建的<strong>贝叶斯个性化排序（Bayesian personalized ranking，BPR</strong>）则是一种 Pairwise 类型的排序学习算法，pair，顾名思义就是成对成双。确切来讲，BPR 实际上是一个基于 Pointwise 之上的算法框架，但最常见的就是基于矩阵分解的构建形式，借助于矩阵分解得到排序学习的损失函数，然后完成排序学习的任务。<br>BPR 详细的介绍也可移步到 <a href="https://www.cnblogs.com/pinard/p/9128682.html">https://www.cnblogs.com/pinard/p/9128682.html</a></p><h3 id="相对排序的评测指标-AUC"><a href="#相对排序的评测指标-AUC" class="headerlink" title="相对排序的评测指标-AUC"></a>相对排序的评测指标-AUC</h3><p>AUC 全称是 Area Under Curve，意思是曲线下的面积，这里的曲线就是 ROC 曲线。AUC 非常适合用来评价模型的排序效果，AUC 这个值在数学上等价于：模型把关心的那一类样本排在其他样本前面的概率。最大是 1，完美结果，而 0.5 就是随机排列，0 就是完美地全部排错。</p><p>AUC 怎么计算呢？一般步骤如下。</p><ol><li>用模型给样本计算推荐分数，比如样本都是用户和物品这样一对一对的，同时还包含了有无反馈的标识；</li><li>得到打过分的样本，每条样本保留两个信息，第一个是分数，第二个是 0 或者 1，1 表示用户消费过，是正样本，0 表示没有，是负样本；</li><li>按照分数对样本重新排序，降序排列；</li><li>给每一个样本赋一个排序值，第一位 r1 = n，第二位 r2 = n-1，以此类推；其中要注意，如果几个样本分数一样，需要将其排序值调整为他们的平均值；</li><li>最终按照下面这个公式计算就可以得到 AUC 值。</li></ol><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665128167856-15440372-d0da-4808-8200-5bf0af8d078a.png" class="" title="image.png"><p>这个公式看上去复杂，其实很简单，由两部分构成：</p><ul><li>第一部分： 分母是所有我们关心的那类样本，也就是正样本，有 M 个，以及其他样本有 N 个，这两类样本相对排序总共的组合可能性，是 M x N；</li><li>第二部分： 分子也不复杂，原本是这样算的：第一名的排序值是 r1，它在排序上不但比过了所有的负样本，而且比过了自己以外的正样本。但后者是自己人，所以组合数要排除，于是就有 n - M 种组合，以此类推，排序值为 rM 的就贡献了 rM - 1，把这些加起来就是分子。</li></ul><p>详细介绍可移步到 <a href="https://www.cnblogs.com/gatherstars/p/6084696.html">https://www.cnblogs.com/gatherstars/p/6084696.html</a> ROC曲线和AUC值</p><h2 id="原理篇-·-模型融合"><a href="#原理篇-·-模型融合" class="headerlink" title="原理篇 · 模型融合"></a>原理篇 · 模型融合</h2><p>推荐系统在技术实现上一般划分为三个阶段：<strong>挖掘、召回、排序</strong>。</p><ul><li>挖掘的工作就是对用户和物品做非常深入的结构化分析，挖掘特征供召回阶段使用</li><li>召回就是用一些手段从全量的物品中筛选出一部分比较靠谱的</li><li>最后就是排序，针对筛选出的一部分靠谱的做一个统一的论资排辈，最后这个统一的排序就是今天要讲的主题：融合。</li></ul><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664464734627-0fc95390-6e21-430c-9ad0-2d793a6c5e3d.png" class="" title="image.png"><p>在召回阶段，一般会使用多种算法来产生一批推荐结果，这些不同算法产生的推荐分数，是<strong>没办法直接用来进行比较排序</strong>的，原因如下：</p><ol><li>有个算法可能只给出结果，不给分数，比如用决策树产生一些推荐结果；</li><li>每种算法给出结果时如果有分数，分数的范围不一定一样，所以不能互相比较，大家各自家庭背景不一样；</li><li>即使强行把所有分数都归一化，仍然不能互相比较，因为产生的机制不同，有的可能普遍偏高，有的可能普遍偏低。</li></ol><p>既然来自各个地方的状元凑在一起，谁也不服谁，那只能再举行一次入学考试了，这个入学考试就是融合模型。也就是，不同算法只负责推举出候选结果，真正最终是否推荐给用户，由另一个统一的模型说了算，这个就叫做模型的融合。</p><h3 id="GBDT-LR：逻辑回归和梯度提升决策树组合"><a href="#GBDT-LR：逻辑回归和梯度提升决策树组合" class="headerlink" title="GBDT+LR：逻辑回归和梯度提升决策树组合"></a>GBDT+LR：逻辑回归和梯度提升决策树组合</h3><p>给多种召回策略外挂一个融合排序，要以产品目标为导向。举个简单的例子，信息流推荐，如果以提高 CTR 为目标，则融合模型就要把预估 CTR 作为本职工作，这个工作谁最能胜任呢，一直以来就是逻辑回归。</p><p>CTR 预估本质是一个分类问题，故完全可以采用经典的逻辑回归来解决。LR类方法的解决思路与协同过滤CF不同，它将问题构建成一个有监督分类问题。同时可以<strong>利用use和item侧的丰富特征</strong>，表达能力比CF类要强。</p><p>在对召回阶段不同算法给出的候选物品计算 CTR 预估时，需要两个东西：</p><ul><li>特征</li><li>权重</li></ul><p><strong>1、训练数据构造</strong></p><p>有监督分类需要以数据作为驱动，一般用曝光点击作为正样本，曝光未点击作为负样本。样本方面主要问题有：</p><ul><li><strong>正负样本不均衡</strong> ：比如CTR任务，如果点击率是5%，则正负样本1: 20，负样本远远多于正样本，导致样本不均衡。分类问题中样本不均衡，会导致模型整体偏向样本多的那个类别，导致其他类别不准确。解决方法主要有：<ul><li>负采样：对负样本进行采样，可以直接采用随机负采样。一方面可以减少样本存储和模型训练的压力，另一方面可以缓解正负样本不均衡问题。但负样本其实也有很多信息的，直接丢弃实在可惜，特别是小场景样本不足的时候。</li><li>focal loss：何凯明老师在图像多分类样本不均衡中采用的方法，也可以使用到推荐场景中。通过减少负样本（易分类）在loss中的权重，使得模型更加专注于正样本（样本少，难分类）</li></ul></li><li><strong>样本置信度问题</strong> ：曝光点击可以认为置信度较高，但曝光未点击就一定表名用户不会点击，一定是负样本吗？点击与否可能也与用户需求是否已经得到满足有关系。美团采用了skip-above采样方式，对用户最后一个点击位置以下的负样本，直接丢弃。</li></ul><p><strong>2、特征工程</strong><br>特征工程一定要结合业务理解，在具体业务场景上，想象自己就是一个实际用户，会有哪些特征对你是否点击、是否转化有比较大的影响。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664348602921-1d7dce2a-4e58-4baa-a8db-1dc646b095e5.png" class="" title="image.png"><p><strong>3、高阶特征组合用 GBDT</strong></p><p>逻辑回归本身对原始特征仅仅是线性加权，并不能很好地刻画特征组合关系；而且特征组合的难点在于：组合数目非常庞大，而且并不是所有组合都有效，只有少数组合有效。<strong>树模型可以做到对原始的特征做各种有效的组合，一棵树一个叶子节点就是一种特征组合</strong>，树模型的集成模型（如RF和GBDT）效果更佳！</p><p><strong>4、权重学习用逻辑回归</strong></p><p>权重的学习主要看两个方面：损失函数的最小化，就是模型的偏差是否足够小；另一个就是模型的正则化，就是看模型的方差是否足够小；</p><p>将两者结合在一起，用于做模型融合阶段的 CTR 预估。这是 Facebook 在其广告系统中使用的方法，GBDT所起的作用就是对原始的特征做各种有效的组合，<strong>从根结点到叶子节点的这条路径就可以看成是不同特征进行的特征组合</strong>；LR 负责学习特征组合的权重。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664465875578-65a56211-ad5c-40d1-8ace-e36dbb3e50b0.png" class="" title="image.png"><h3 id="FM：一网打尽协同过滤、矩阵分解和线性模型"><a href="#FM：一网打尽协同过滤、矩阵分解和线性模型" class="headerlink" title="FM：一网打尽协同过滤、矩阵分解和线性模型"></a>FM：一网打尽协同过滤、矩阵分解和线性模型</h3><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666090375506-1d2182d8-faf0-43f9-a0eb-bcb2adeac3f3.png" class="" title="image.png"><p>分解机 FM（Factorization Machine）也称因子分解机。是由 Konstanz 大学 Steffen Rendle 于 2010 年最早提出的，旨在解决稀疏数据下的特征组合问题。FM 设计灵感来源于广义线性模型和矩阵分解。</p><p>在线性模型中，我们会单独考察每个特征对 Label 的影响，一种策略是使用 One-hot 编码每个特征，然后使用线性模型来进行回归，但是 one-hot 编码后，一者，数据会变得稀疏，二者，很多时候，单个特征和 Label 相关性不够高。最终导致模型性能不好。为了引入关联特征，可以引入二阶项到线性模型中进行建模：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665207740424-b223bb5e-7354-4910-b9d0-4e61749092c3.png" class="" title="image.png"><p>𝑥𝑖、𝑥𝑗 是经过 One-hot 编码后的特征，取 0 或 1。只有当二者都为 1 时，𝑤𝑖𝑗 权重才能得以学习。然后由于稀疏性的存在，满足 𝑥𝑖，𝑥𝑗 都非零的样本很少，导致组合特征权重参数缺乏足够多的样本进行学习。</p><p><strong>FM 模型公式</strong></p><p>矩阵分解思想此时发挥作用。借鉴协同过滤中，评分矩阵的分解思想，我们也可以对 𝑤𝑖𝑗 组成的二阶项参数矩阵进行分解，所有二次项参数 𝑤𝑖𝑗 可以组成一个对称阵 𝑊，那么这个矩阵就可以分解为$𝑊=𝑉^𝑇𝑉$，𝑉 的第 𝑗 列便是第 𝑗 维特征的隐向量。换句话说，每个参数 𝑤𝑖𝑗=⟨𝑣𝑖,𝑣𝑗⟩，这就是 FM 模型的核心思想。因此，FM 的模型方程为（常用的是二阶，本文不讨论 FM 的高阶形式）:</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665207793162-ca9665b9-6d71-4e6d-ab39-d42461ef7671.png" class="" title="image.png"><p>其中，𝑣𝑖 是第 i 维特征的隐向量，⟨⋅,⋅⟩ 代表向量点积，计算公式为</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665207806470-b17ab514-8dd2-4923-9fe1-c676f0cdb9b7.png" class="" title="image.png"><p>隐向量的长度为 𝑘(𝑘&lt;&lt;𝑛)，包含 𝑘 个隐因子。 具体解读一下这个公式</p><ul><li><strong>线性模型+交叉项</strong>：直观地看 FM 模型表达式，前两项是线性回归模型的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将两个互异的特征分量之间的关联信息考虑进来。用交叉项表示组合特征，从而建立特征与结果之间的非线性关系。</li><li><strong>交叉项系数 → 隐向量内积</strong>：由于 FM 模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数 𝑤𝑖𝑗（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积 ⟨𝑣𝑖,𝑣𝑗⟩ 表示 𝑤𝑖𝑗。具体的，FM 求解过程中的做法是：对每一个特征分量 𝑥𝑖 引入隐向量 𝑣𝑖＝(𝑣𝑖,1,𝑣𝑖,2,⋯,𝑣𝑖,𝑘)，利用$𝑣_𝑖𝑣^𝑇_𝑗$内积结果对交叉项的系数 𝑤𝑖𝑗 进行估计，公式表示：$𝑤̂ _{𝑖𝑗}=𝑣_𝑖𝑣^𝑇_𝑗$。</li></ul><p>根据上式，二次项的参数数量减少为 𝑘𝑛 个，远少于多项式模型的参数数量。</p><blockquote><p><strong>FM 模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力，在工业界使用非常多</strong>。</p></blockquote><h3 id="Wide-amp-Deep：深度和宽度兼具的融合模型"><a href="#Wide-amp-Deep：深度和宽度兼具的融合模型" class="headerlink" title="Wide &amp; Deep：深度和宽度兼具的融合模型"></a>Wide &amp; Deep：深度和宽度兼具的融合模型</h3><p>一个典型的推荐系统架构，其实很类似一个搜索引擎，搜索由检索和排序构成。推荐系统也有召回和排序两部构成，不过，推荐系统的检索过程并不一定有显式的检索语句，通常是拿着用户特征和场景特征去检索召回，其中用户特征也就是在前面的专栏中提到的用户画像。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664467019538-db6b08fe-6fad-4d54-bd81-d0f290de5b2f.png" class="" title="image.png"><p>今天要说的深宽模型就是专门用于融合排序的，它是 Google 在 2016 年发表的 CTR 预估方法。<br>Wide&amp;Deep 由逻辑回归作为最终输出单元，深模型最后一个隐藏层作为特征，与宽模型的原始特征一起接入逻辑回归，然后训练参数。它与机器学习中的集成学习方法有所区别，集成学习的子模型是独立训练的，只在融合阶段才会学习权重，这里是整体。Wide&amp;Deep 由两部分组成：</p><ul><li>wide部分是一个广义的线性模型，输入的特征主要有两部分组成，一是原始的部分特征，二是原始特征的交叉特征； </li><li>Deep部分是一个DNN模型，输入的特征主要分为两大类，一类是数值特征，一类是类别特征</li></ul><p>Wide部分有利于增强模型的“记忆能力”，Deep部分有利于增强模型的“泛化能力” 。两部分的输出一起接入逻辑回归，做最终的预测，输出概率值。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664467074761-bdb8ce88-7e11-41fd-b0f2-71a61b23b56b.png" class="" title="image.png"><p><strong>深宽模型具体落地</strong></p><p>1 数据生成</p><p>数据生成有几个要点：</p><ul><li>每一条曝光日志就生成一条样本，标签就是 1/0，安装了 App 就是 1，否则就是 0。</li><li>将字符串形式的特征映射为 ID，需要用一个阈值过滤掉那些出现样本较少的特征。</li><li>对连续值做归一化</li></ul><p>2 模型训练</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664467945506-f36dcf76-4980-4855-9027-1ece26ef0837.png" class="" title="image.png"><p>整个模型的示意如图所示。其要点，在深度模型侧：</p><ol><li>每个类别特征 embedding 成一个 32 维向量；</li><li>将所有类别特征的 embedding 变量连成一个 1200 维度左右的大向量；</li><li>1200 维度向量就送进三层以 ReLU 作为激活函数的隐藏层；</li><li>最终从 Logistic Regreesion 输出。</li></ol><p>宽模型侧就是传统的做法：特征交叉组合。</p><p>当新的样本集合到来时，先是用上一次的模型来初始化模型参数，然后在此基础上进行训练。</p><p>新模型上线前，会先跑一遍，看看会不会出事，算是一个冒烟测试。</p><p>3 模型应用</p><p>模型验证后，就发布到模型服务器。模型服务，每次网络请求输入的是来自召回模块的 App 候选列表以及用户特征，再对输入的每个 App 进行评分。评分就是用我们的“深宽模型”计算，再按照计算的 CTR 从高到低排序。</p><p>为了让每次请求响应时间在 10ms 量级，每次并不是串行地对每个候选 App 计算，而是多线程并行，将候选 App 分成若干并行批量计算。</p><h2 id="原理篇-·-探索和利用问题"><a href="#原理篇-·-探索和利用问题" class="headerlink" title="原理篇 · 探索和利用问题"></a>原理篇 · 探索和利用问题</h2><blockquote><p><a href="https://lumingdong.cn/exploration-and-exploitation-in-the-recommendation-system.html">https://lumingdong.cn/exploration-and-exploitation-in-the-recommendation-system.html</a></p></blockquote><p><strong>探索与利用（Exploration and Exploitation）</strong>问题简称 EE 问题，是计算广告和推荐系统里最常见的两大问题之一（另外一个是冷启动问题）。EE 问题中的利用（Exploitation），表示对用户比较确定的兴趣，要利用开采迎合；而探索（Exploration）则表示光对着用户已知的兴趣使用，用户很快会腻，所以要不断探索用户新的兴趣才行。</p><p>之所以会有 EE 问题，是因为给用户推荐物品本身就是一个选择的问题，从选择什么物品推荐上升到最根本的推荐策略的选择，不同的策略起到的效果是不一样的。一个极端表现就是总是按照已知用户兴趣来推荐，会让用户觉得总是重复推荐类似的东西，没有惊喜感，而如果完全随意地给用户推荐各种东西，推荐的多样性是有了，但可能大部分物品是用户不喜欢的，让用户觉得推荐得不准确。可以看出，这两种极端的选择策略本身就是矛盾的，因此实践中应以平衡推荐系统的准确性和多样性为标准来进行选择，如何能够来衡量呢？最常用的就是利用 Bandit 算法。</p><h3 id="Bandit算法原理"><a href="#Bandit算法原理" class="headerlink" title="Bandit算法原理"></a>Bandit算法原理</h3><p>Bandit 算法是解决 EE 问题的一类有效算法，并不是指一个算法。Bandit 算法来源于历史悠久的赌博学，它要解决的问题是这样的：一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是<strong>多臂老虎机问题（Multi-armed bandit problem, K-armed bandit problem, MAB）</strong>，因此 EE 问题也常被称为 MAB 问题。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665215453017-05622839-2e97-43c0-9fb9-066cba74872e.png" class="" title="image.png"><p>Bandit 算法需要量化一个核心问题：错误的选择到底有多大的遗憾？能不能遗憾少一些？所以我们便有了<strong>衡量 Bandit 算法的一个指标——累积遗憾 (regret)</strong>：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665215500944-0179ec98-be79-4898-a662-bf7fe0c605ea.png" class="" title="image.png"><p>其中，$𝑟_{𝑡,𝑎^∗_𝑡}$表示第 t 轮最优的那个 arm 所获得的收益，而$𝑟_{𝑡,𝑎_𝑡}$表示第 t 轮实际选择的 arm 所获的收益，每次都会计算当前选择的 arm 获取的收益与最优 arm 期望最大收益之间的差距，把每次差距累加起来就是总的遗憾。</p><p>Bandit 常用的算法如下：</p><p><strong>朴素 Bandit 算法</strong></p><p>朴素算法 Bandit 算法也是一种贪心算法，其思想是：先随机试若干次，计算每个臂的平均收益，一直选均值最大那个臂。这个算法是人类在实际中最常采用的，不可否认，它还是比随机乱猜要好。</p><p><strong>Epsilon-Greedy 算法</strong></p><p>这也是一个朴素的 bandit 算法：</p><ol><li>选一个 (0, 1) 之间较小的数作为 epsilon；</li><li>每次以 epsilon 的概率随机选取一个臂（用于探索）；</li><li>每次以 1-epsilon 的概率选取当前平均收益收益最大的那个臂（用于利用）。</li></ol><p>epsilon 的值可以控制对 Exploit 和 Explore 的偏好程度，越接近 0，越保守；越接近于 1，越冒险。epsilon 可以是固定的，也可以设定为逐渐衰减的，类似于模拟退火。</p><p><strong>UCB 算法</strong></p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665299319023-b72be2e1-f2fd-481e-a98d-ad9c1020ea24.png" class="" title="image.png"><p><strong>Thompson sampling 算法</strong></p><p>略</p><h2 id="原理篇-·-深度学习"><a href="#原理篇-·-深度学习" class="headerlink" title="原理篇 · 深度学习"></a>原理篇 · 深度学习</h2><h3 id="各种-2vec"><a href="#各种-2vec" class="headerlink" title="各种 2vec"></a>各种 2vec</h3><p>自 Embedding 的概念问世以来，Embedding 的探索和应用就没有停止过，Word2Vec、Sentence2Vec、Doc2Vec、Item2Vec，甚至 Everything2Vec。对，“万物皆可 Embedding”。几年来，Embedding 在推荐系统中的应用也越来越多，方式众多，技法新颖。</p><ul><li><a href="https://www.yuque.com/ningshixian/pz10h0/fg4zr8">Word2vec</a>，用于学习词嵌入向量。当把一个词表示成一个稠密的向量后，就可以计算词的相似度，进而可以计算句子的相似度，也可以直接把这个稠密向量作为特征输入给高级的预测模型。Word2vec 可以直接应用在基于内容的推荐算法中。内容推荐的一个常用方法是通过理解内容（挖掘内容属性）去挖掘用户的兴趣点来构建推荐模型，此时 Embedding 可以作为一个非常有效的方法来辅助理解内容</li><li>Sentence2vec，把一个句子表示成一个嵌入向量，通常是把其包含的词嵌入向量加起来就完事了。</li><li>Doc2vec，在word2vec的基础上增加一个段落向量，该模型也有两个方法：①试图给定上下文和段落ID的情况下预测下一单词的概率。在一个句子或者段落文档训练过程中，段落ID保存不变，共享同一个段落向量；②只给定段落向量的情况下预测段落中一组（窗口）随机单词的概率。</li><li>AutoEncoder，自动编码器。它是一种输入和输出一模一样的神经网络，这个神经网络就一个目的，更加清楚地认识自己，在这个优化目标指导下，学到的网络连接权重都是不同的嵌入向量。</li><li>Item2Vec（MSRA）， Item2Vec 是 Word2Vec 在推荐系统领域的一个转化，它把 Word2vec 的 Skip-gram with Negative Sampling (SGNS) 的算法思路迁移到基于物品的协同过滤 (Item-Based CF) 上，以物品的共现性作为自然语言中的上下文关系，构建神经网络学习出物品在隐空间的向量表示。</li></ul><h3 id="各种-Graph-Embedding"><a href="#各种-Graph-Embedding" class="headerlink" title="各种 Graph Embedding"></a>各种 Graph Embedding</h3><p>Word2Vec 通过序列（sequence）式的样本：句子，学习单词的真实含义。仿照 Word2Vec 思想而生的 Item2Vec 也通过商品的组合去生成商品的 Embedding，这里商品的组合也是序列式的，我们可以称他们为 “Sequence Embedding”。然而，在更多场景下，数据对象之间更多以图（网络）的结构呈现，这种结构生成 Embedding 的方法，我们称之为图嵌入（Graph Embedding）。Graph Embedding 比 Item2Vec 能够获得更加高阶的相似和关联信息</p><ul><li>DeepWalk，DeepWalk认为如果一个网络中两个节点的结构相似，那么这两个点的表示也应该相似。它重点解决的就是如何从图采样到序列，其主要思想是在由物品组成的有向加权图上进行<strong>随机游走（Random Walk）</strong>，产生 K 个物品序列，然后将这些物品序列作为训练样本输入 word2vec 进行训练，得到物品的 Embedding。</li></ul><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666091355809-1df621bb-eee7-4286-9f62-5d7ca2b0d1cd.png" class="" title="image.png"><ul><li>缺陷：只能针对已有的 Item 而无法应对新的 Item，即无法解决冷启动问题</li></ul><ul><li>Node2vec</li></ul><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665240203946-30d2ecbb-d701-40fb-bb33-8c426436c330.png" class="" title="image.png"><ul><li>EGES(2018 淘宝)(Enhanced Graph Embedding with Side information)<ul><li>一定程度上解决<strong>物品冷启动</strong>问题</li></ul></li></ul><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665240240177-9230fc50-fa88-4f48-84a0-92acdfb695da.png" class="" title="image.png"><h3 id="YouTubeDNN-思路"><a href="#YouTubeDNN-思路" class="headerlink" title="YouTubeDNN 思路"></a>YouTubeDNN 思路</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/52504407">YouTube深度学习推荐系统的十大工程问题</a></p></blockquote><p>首先，Youtube 把推荐的预测任务看成是一个 “超大规模” 多分类，这个和之前常规的推荐系统要么预测行为要么预测评分的做法不太一样，而是把候选物品当成多个类别，预测用户下一个会观看哪个视频，用一个 Softmax 公式表示这个条件概率：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664982593677-bfa06d1f-9851-4000-b054-75bf2d8d5e5c.png" class="" title="image.png"><p>其中，U 是用户，C 是场景，输入是视频的嵌入向量和用户的嵌入向量。这里就涉及了先要使用深度神经网络，从用户历史反馈行为和场景信息中学习物品和用户的嵌入向量。整个<strong>推荐召回模型</strong>示意图如下：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665225803548-57ec9b5b-80bd-407e-ad7c-6d376033a9e7.png" class="" title="image.png"><p>看这个推荐模型示意图，就可以看到深度学习应用在了哪些地方。</p><ol><li>模型输入包括：用户浏览历史，用户的搜索历史，地理信息以及其余上下文信息（设备、登录状态等），这些输入信息从先将变长的稀疏 ID 特征转换成固定宽度的 Embedding 向量化表示（平均）</li><li>除了 Embedding 类的特征，还有一些简单的二元特征以及连续型特征（也即用户画像，比如用户性别、年龄等）归一化到 [0, 1] 区间上的实数值</li><li>将<strong>多种信息拼接（concat）成一个特别宽的输入向量</strong>，经过一个三层 ReLU 全连接层，在输出层以 Softmax 作为输出函数，预测每一个视频的概率分布。</li></ol><p><strong>注意：</strong><br><strong>1、Softmax的前一层输出作为 User Embedding， Softmax 层中的权重矩阵的每一行向量作为 Video Embedding；</strong><br><strong>2、在模型训练时，以 Softmax 作为输出层，但是在实际线上预测服务时，由于模型关心相对顺序，所以并不需要真的去计算 Softmax，而是拿着用户的特征向量做近似的近邻搜索 ANN，只召回最相近的一些推荐结果</strong>。</p><p><strong>排序阶段的模型结</strong>构和召回阶段非常类似，可见下图：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665214901091-cd46f595-2cc2-49c7-94c9-10cb3d794191.png" class="" title="image.png"><p>与召回阶段不同的是，排序阶段需要处理计算的数据量仅仅是百数量级的，为了提高预测精度，排序阶段使用了更多精细的特征。除此之外，排序阶段本身就可以整合多源召回，上面提到的召回模型可能仅仅是一种召回策略，通常召回阶段的来源往往很多。</p><h3 id="双塔模型思路"><a href="#双塔模型思路" class="headerlink" title="双塔模型思路"></a>双塔模型思路</h3><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666090554395-33d28a21-7f20-40a1-bc04-96b4813ee928.png" class="" title="image.png"><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666090576855-55d14903-1444-43b2-a184-ac9cd8998f3c.png" class="" title="image.png"><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666090600996-49ec0255-4fd5-49a3-80d0-b86a589fcaf9.png" class="" title="image.png"><h3 id="深度排序模型"><a href="#深度排序模型" class="headerlink" title="深度排序模型"></a>深度排序模型</h3><p>进入深度模型时代后，精排模型发展更为迅猛，借用王喆老师《<a href="https://time.geekbang.org/column/intro/100060801">深度学习推荐系统实战</a>》课程的一张图，如下：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664349135540-9eaf1153-070f-42b4-9303-ef425acaa427.png" class="" title="image.png"><p>想了解深度推荐模型细节，可参考董辉的双周会分享《<a href="https://www.jianguoyun.com/p/DeXIDVkQ-_3RBhjiwd0EIAA">推荐排序CTR.ppt</a>》。深度排序主要分成 4 个方向：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666091028138-5dadb465-9a9a-4a36-ba5f-b5a14515315a.png" class="" title="image.png"><p>特征交叉学习，主要研究如何将显式和隐式特征交互与普通全连接网络（即MLP）结合起来。代表是 DeepFM 和 DCN：</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666091108324-17db757a-e4a2-476d-9bf9-352dd779b44e.png" class="" title="image.png"><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666091127524-fda9b17d-23cd-47ac-96a8-33ac61f93170.png" class="" title="image.png"><p>序列建模主要考虑用户行为序列 LastN 特征</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1666091190763-02104347-2782-4071-b539-54a2d8c29b97.png" class="" title="image.png"><h2 id="模型优化目标：Learning-to-rank"><a href="#模型优化目标：Learning-to-rank" class="headerlink" title="模型优化目标：Learning to rank"></a>模型优化目标：<a href="https://www.yuque.com/ningshixian/pz10h0/zhhpol">Learning to rank</a></h2><p>排序学习是以点击率作为衡量标准，可以根据用户反馈（点击／停留时间）来持续优化搜索结果。但随着相关度函数中特征的增多，使调参变得极其的困难。LTR通过机器学习算法训练来学习一个最佳的拟合公式，从而对文档打分并排序。</p><p>以排序的评价指标作为优化目标但是评判排序好坏的这些指标的特点是<strong>不平滑、不连续（?）</strong>，无法求梯度，因此无法直接用梯度下降法求解。所以诞生了3种近似求解的训练方法（ LTR优化目标/策略）：</p><ul><li>pointwise将其转化为多类分类或者回归问题（LR、GBDT、DNN），</li><li>pairwise将其转化为pair分类问题，</li><li>Listwise为对查询下的一整个候选集作为学习目标（LambdaMART）</li></ul><p><strong>三者的区别在于loss不同、以及相应的标签标注方式和优化方法的不同。</strong></p><p>在推荐系统领域，最常用就是二元分类的 Point-wise，比如常见的点击率（CTR）预估问题，之所以用得多，是因为二元分类的 Pointwise 模型的复杂度通常比 Pairwise 和 Listwise 要低，而且可以借助用户的点击反馈自然地完成正负样例的标注，而其他 Pairwise 和 Listwise 的模型标注就没那么容易了。成功地将排序问题转化成分类问题，也就意味着我们机器学习中那些常用的分类方法都可以直接用来解决排序问题。</p><blockquote><p>推荐中使用较多的 Pointwise 方法是 LR、GBDT、SVM、FM 以及结合 DNN 的各种排序算法。</p></blockquote><p>在推荐领域，排序学习可能会用在以下场景：</p><ol><li><strong>用于多路召回策略的融合排序</strong>一个推荐系统的召回阶段可能会用到多种不同召回策略，比如基于内容的召回、基于热门物品的召回、基于协同过滤的召回、基于类别和标签的召回等等，不同的召回策略召回的物品无法确切地分辨哪个策略召回更重要，所以需要一个排序学习模型，根据用户的行为反馈来对多路召回的物品进行排序推荐，这也是排序模块的一大重要作用。</li><li><strong>一个排序模型完成召回排序</strong>某些简单的推荐场景下，召回排序过程区分的不够开，目标比较单一，比如相关推荐，对相关性要求比较高，此时可以用 LTR，一个排序模型就可以完成召回排序任务。</li><li><strong>解决多目标排序问题</strong>通常一个推荐系统的目标是提高点击率（CTR）或转化率（CVR），但是这些并不能完全反映用户对推荐物品的满意度，以此为目标的推荐系统也无法保证用户存留。为了提高用户对推荐物品满意度，可能还需要依赖更多的指标，比如加购、收藏、分享等等。由此便产生了多种目标，如何优化最终推荐列表的顺序来使得众多指标在不同的场景下近可能达到最优或者满意，这就是推荐系统中的多目标排序问题，而排序学习是解决多目标排序问题的一种重要方法。</li></ol><h2 id="工程篇-·-常见架构"><a href="#工程篇-·-常见架构" class="headerlink" title="工程篇 · 常见架构"></a><strong>工程篇 · 常见架构</strong></h2><h3 id="推荐候选池的去重策略"><a href="#推荐候选池的去重策略" class="headerlink" title="推荐候选池的去重策略"></a>推荐候选池的去重策略</h3><p>在推荐系统中，有一个刚需就是去重，那么说在哪些地方有去重的需求呢？主要是在两个地方：一个是内容源去重，另一个是不重复给用户推荐。</p><ul><li><strong>内容去重：</strong>内容重复检测，最常用的方法，即 Simhash</li><li><strong>不重复推荐</strong>：给用户推荐的内容不要重复，推荐过的内容就不再出现在推荐候选集中</li></ul><h3 id="推荐系统的整体架构"><a href="#推荐系统的整体架构" class="headerlink" title="推荐系统的整体架构"></a>推荐系统的整体架构</h3><ul><li><strong>召回层：</strong>包括各种推荐策略或者算法，比如经典的协同过滤，基于内容的召回，基于向量的召回，用于托底的热门推荐等。为了应对线上高并发的流量，召回结果通常会预计算好，建立好倒排索引后存入缓存中。</li><li><strong>融合过滤层：</strong>触发多路召回，由于召回层的每个召回源都会返回一个候选集，因此这一层需要进行融合和过滤。</li><li><strong>排序层：</strong>利用机器学习或者深度学习模型（如 LG/GBDT/Wide&amp;Deep/DeepFM），以及更丰富的特征进行重排序，筛选出更小、更精准的推荐集合返回给上层业务。</li></ul><h3 id="总览推荐架构和搜索、广告的关系"><a href="#总览推荐架构和搜索、广告的关系" class="headerlink" title="总览推荐架构和搜索、广告的关系"></a>总览推荐架构和搜索、广告的关系</h3><p>搜索，推荐和广告本质上都在解决信息过载的问题，各自解决的手段、目标不相同，各自诞生在产品生命周期不同阶段，以至于系统实现也不尽相同</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664984792552-87eed2e1-090f-46a1-9af5-8925ad3481e2.png" class="" title="image.png"><ul><li><strong>搜索：</strong>用户有明确的搜索意图，搜索出来的结果和用户的搜索词相关。最重要的目标是降低延迟和提高相关性（精确快速）</li><li><strong>推荐：</strong>不具有目的性，更多的是起一个“锦上添花”的作用，依赖用户的历史行为和画像数据进行个性化推荐。满足多样性要求，“相对准确性”要求不高！</li><li><strong>广告：</strong>借助搜索和推荐技术实现广告的精准投放，可以将广告理解成搜索推荐的一种应用场景，技术方案更复杂，涉及到智能预算控制、广告竞价等。<strong>搜索和推荐都是为人找信息，而广告是为信息找人</strong>。</li></ul><div class="table-container"><table><thead><tr><th><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665300378028-667c23c6-525c-473e-b9ef-bdba72086a5f.png" class="" title="image.png"></th><th><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665300386021-d08e3fd2-0def-424d-9e99-accc8f10de71.png" class="" title="image.png"></th><th><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1665300399004-ee8454c2-5501-4eb7-87de-0aed18847eb3.png" class="" title="image.png"></th></tr></thead><tbody><tr><td></td></tr></tbody></table></div><h2 id="工程篇-·-效果保证"><a href="#工程篇-·-效果保证" class="headerlink" title="工程篇 · 效果保证"></a>工程篇 · 效果保证</h2><h3 id="推荐系统的测试方法及常用指标介绍"><a href="#推荐系统的测试方法及常用指标介绍" class="headerlink" title="推荐系统的测试方法及常用指标介绍"></a>推荐系统的测试方法及常用指标介绍</h3><blockquote><p><a href="https://lumingdong.cn/criteria-for-evaluating-recommendation-systems-in-industry.html">https://lumingdong.cn/criteria-for-evaluating-recommendation-systems-in-industry.html</a></p></blockquote><p>推荐系统的测试方法有四种： 业务规则扫描、离线模拟测试、在线对比测试、用户访谈。</p><ul><li>业务规则扫描：本质上就是传统软件的功能测试</li><li><strong>离线模拟测试</strong>：通常做法是先收集业务数据，也就是根据业务场景特点，构造用户访问推荐接口的参数。这些参数要尽量还原当时场景，然后拿这些参数数据去实时访问推荐，产生推荐结果日志，收集这些结果日志并计算评测指标，比如，可以评测<strong>推荐结果的 TopK 的准确率，或者排序效果 AUC</strong>。</li><li>在线对比测试：<strong>ABTest</strong>，即在线对比测试，分流量做真实的评测。</li><li>用户访谈</li></ul><p><strong>推荐系统的离线评价指标</strong></p><ol><li>评分准确度。通常就是均方根误差 RMSE，或者其他误差类指标，反映预测评分效果的好坏。</li><li>排序。检测推荐系统排序能力非常重要，因为把用户偏爱的物品放在前面是推荐系统的天职。由于推荐系统输出结果是非常个人化的，除了用户本人，其他人都很难替他回答哪个好哪个不好，所以通常评价推荐系统排序效果很少采用搜索引擎排序指标，例如 MAP，MRR，NDCG。搜索引擎评价搜索结果和查询相关性，具有很强的客观属性，可以他人代替评价。<strong>推荐系统评价排序通常采用 AUC。</strong></li><li>分类准确率。<strong>这个指标也是针对行为预测的，而行为预测就是分类问题</strong>，所以评价准确度就很自然。在推荐系统中，评价准确度略微特殊，一般评价 <strong>TopK 准确率</strong>，与之对应还有 TopK 召回率</li></ol><p>TopK 准确度计算方式如下：</p><p>如果日志中用户有 A、B 两个物品有正反馈行为，推荐系统推出一个物品列表，长度为 K，这个列表中就有可能包含 A、B 两个物品中的一个或多个，下面这个表格就说明了 TopK 准确率和 TopK 召回率的含义。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664985735987-8f9a5c82-d427-4e20-9295-ed6d26f37808.png" class="" title="image.png"><h3 id="和推荐系统有关的开源工具及框架介绍"><a href="#和推荐系统有关的开源工具及框架介绍" class="headerlink" title="和推荐系统有关的开源工具及框架介绍"></a>和推荐系统有关的开源工具及框架介绍</h3><p>内容分析</p><p>基于内容的推荐，主要工作集中在处理文本，或者把数据视为文本去处理。文本分析相关的工作就是将非结构化的文本转换为结构化。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664985967647-24533dcf-b321-4cdc-b03b-b1f31d015dd4.png" class="" title="image.png"><p>协同过滤</p><p>基于用户、基于物品的协同过滤，矩阵分解，都依赖对用户物品关系矩阵的利用，这里面常常要涉及的工作有下面几种：KNN 相似度计算；SVD 矩阵分解；SVD++ 矩阵分解；ALS 矩阵分解；BPR 矩阵分解；低维稠密向量近邻搜索。可以做这些工作的开源工具有下面几种。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664986008627-8bc3080c-f531-480e-b82f-4ff7a7060393.png" class="" title="image.png"><p>模型融合</p><p>模型融合这部分，有线性模型、梯度提升树模型。</p><img src="/2022/09/29/2022-09-29-%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F%E3%80%8B-%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/1664986083943-16a306c8-6a84-4aeb-85a1-4d8da7afb81c.png" class="" title="image.png"><p>向量检索工具</p><p><a href="https://www.yuque.com/ningshixian/pz10h0/saa0yx">https://www.yuque.com/ningshixian/pz10h0/saa0yx</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>向量检索库总结</title>
    <link href="/2022/09/23/2022-09-23-%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E5%BA%93%E6%80%BB%E7%BB%93/"/>
    <url>/2022/09/23/2022-09-23-%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E5%BA%93%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://mp.weixin.qq.com/s/YO3VTBBv8uan6yPTOgS1jA">NLP向量搜索开源工具必备：常用向量化检索工具的优缺点与试用场景介绍</a><br><a href="https://mp.weixin.qq.com/s/lESqFhYzwiuN7gzu57bOGg">干货：向量检索库总结|建议收藏</a></p></blockquote><p>向量相似度检索，即根据一个向量Q从海量的向量库中寻找TopK个与Q最相似或者距离最近的向量，其在工业中有着广泛的应用场景，比如图像检索、文本语义检索以及推荐系统中基于User与Item的Embedding向量召回等。在生产环境中，被查找的向量库往往是海量，甚至超过了内存的限制，而且面临着高并发与低延迟的需求。当前涌现出了一系列高质量的向量化工具。</p><div class="table-container"><table><thead><tr><th>向量检索工具</th><th>发布</th><th>支持算法</th><th>分布式支持</th><th>GPU 加速</th><th>备注</th></tr></thead><tbody><tr><td>Faiss</td><td>Facebook</td><td>全量计算LSHPQ近邻图</td><td>不支持</td><td>部分算法支持</td><td>开源目前最为成熟</td></tr><tr><td>Milvus</td><td><strong>国产</strong></td><td>全量计算LSHPQ近邻图搜索树</td><td>支持</td><td>部分算法支持</td><td><strong>开源</strong>社区活跃</td></tr><tr><td>Elastic Search</td><td></td><td>暴力搜索近邻图</td><td>支持</td><td>不支持</td><td></td></tr><tr><td>Vearch</td><td>JD</td><td>HNSWLSHPQ搜索树</td><td>支持</td><td>部分算法支持</td><td>基于 ES 二次开发</td></tr><tr><td>SPTAG</td><td>Microsoft</td><td>近邻图搜索树</td><td>不支持</td><td>不支持</td><td>基于KD-Tree</td></tr><tr><td>Annoy</td><td>Spotify</td><td>近邻图搜索树</td><td>不支持</td><td>不支持</td><td>基于二叉树</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th><a href="https://github.com/weaviate/weaviate">向量数据库 weaviate</a></th><th>开源的向量数据库，可以存储对象和向量，允许将向量搜索与结构化过滤相结合，并具有云原生数据库的容错性和可扩展性，可通过 GraphQL、REST 和各种语言客户端进行访问。</th></tr></thead><tbody><tr><td><a href="https://www.pinecone.io/">向量数据库 PineCone</a></td><td>Pinecone为向量数据提供了数据存储解决方案。</td></tr><tr><td><a href="https://github.com/chroma-core/chroma">嵌入式数据库 chroma</a></td><td>Chroma 是一个用于 Python / JavaScript LLM 应用程序的嵌入式数据库，它具有内存快速访问的优势</td></tr></tbody></table></div><h2 id="1、Gensim"><a href="#1、Gensim" class="headerlink" title="1、Gensim"></a>1、Gensim</h2><p><strong>Gensim</strong>是 <strong>Radim Řehůřek开源</strong>的一个主题建模、文本向量化计算工具库，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达，支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，提供了针对向量的多种操作，如相似度计算，信息检索等一些常用任务的API接口，如找到与一个词相似度最高的词语集合，比较两个词语之间的相似度值。</p><p><strong>地址：</strong><a href="https://radimrehurek.com/gensim/"><strong>https://radimrehurek.com/gensim/</strong></a></p><p>Gensim的提供了wordvec模块提供了cbow和skipgram两种词向量训练接口，用户可以通过训练自有语料来得到特定的向量文件。</p><p>因此，我们一方面可以直接使用该向量文件实现检索操作，也可以预先将预先得到的embedding【如DeepWalk、Node2vec得到的向量，根据TFIDF得到的文本向量，从其他开源渠道下载得到的向量等】按照gensim所规定的格式【一般是文件首行为词表大小、空格、向量维度，第二行至最后一行为每个词、空格、以空格连接的各维度向量】，调用该工具完成加载和使用，<strong>实验表明，</strong>gensim加载模型耗时很长，会将所有的词向量加载进入内存，占用内存很大，most_similar函数耗时较长。</p><h2 id="2、Annoy"><a href="#2、Annoy" class="headerlink" title="2、Annoy"></a>2、Annoy</h2><p>Annoy是<strong>Spotify开源</strong>的一个用于<strong>高维空间求近似最近邻的一个开源库</strong>，全称：Approximate Nearest Neighbors Oh Yeah，是一种适合实际应用的快速相似查找算法。Annoy对内存使用进行了优化，索引可以在硬盘保存或者加载，提供欧式距离，曼哈顿距离，余弦距离，汉明距离，內积距离等距离的度量方法，可以使用 Annoy 对 word2vec 等向量建立索引。</p><p>不过，Annoy仅支持树结构的索引类型，且不支持批量插入和查询，仅支持一种索引类型，单步查询速度快，另外，annoy中向量的item-id只接受非负数，如果自己的数据不符合要求需要自己维护一份映射。</p><p><strong>地址：</strong><a href="https://github.com/spotify/annoy"><strong>https://github.com/spotify/annoy</strong></a></p><h2 id="3、FAISS"><a href="#3、FAISS" class="headerlink" title="3、FAISS"></a>3、FAISS</h2><p><a href="https://github.com/facebookresearch/faiss">Faiss</a>是Facebook AI团队开源的针对聚类和相似性搜索的开源库，为稠密向量提供高效相似度搜索服务，支持十亿级别向量的搜索，是目前最为成熟的近似近邻搜索库之一。</p><p>Faiss提供了高效的索引类库。是向量化检索开山鼻祖的应用。</p><p>Faiss 支持多种向量检索方式，包括内积、欧氏距离等，同时支持精确检索与模糊搜索，并使用 GPU 来获得更高的内存带宽和计算吞吐量。</p><p>不过，Faiss本身只是一个能够单机运行的支持各种向量检索模型的机器学习算法基础库，不支持分布式实时索引和检索，同时也不支持标量字段的存储和索引等功能。</p><p><strong>地址：</strong><a href="https://github.com/facebookresearch/faiss"><strong>https://github.com/facebookresearch/faiss</strong></a></p><p>tutorial: <a href="https://github.com/facebookresearch/faiss/wiki/Getting-started">https://github.com/facebookresearch/faiss/wiki/Getting-started</a></p><h3 id="Faiss的使用"><a href="#Faiss的使用" class="headerlink" title="Faiss的使用"></a><strong>Faiss的使用</strong></h3><p>直接通过 pip install faiss-cpu —no-cache 进行安装。</p><p>faiss的使用方法也比较简单，归纳为以下三个步骤：</p><ol><li>构建向量库，对已知的数据进行向量，最终以矩阵的形式表示</li><li>为矩阵选择合适的index，将第一步得到的矩阵add到index中</li><li>search得到最终结果<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs perl">import numpy as np <br>import faiss <br><br>d = <span class="hljs-number">64</span> <br>nb = <span class="hljs-number">100000</span><br>nq = <span class="hljs-number">10000</span><br><span class="hljs-comment"># 构建向量库</span><br>xb = np.random.random((nb, d)).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)  <br>xb[:, <span class="hljs-number">0</span>] += np.arange(nb) / <span class="hljs-number">1000</span>.<br>xq = np.random.random((nq, d)).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br><span class="hljs-keyword">x</span><span class="hljs-string">q[:, 0]</span> += np.arange(nq) / <span class="hljs-number">1000</span>.<br><br><span class="hljs-comment"># 关键步骤，build index</span><br><span class="hljs-keyword">index</span> = faiss.IndexFlatL2(d)   <br>index.add(xb)<br><br>k = <span class="hljs-number">4</span> <br>D, I = index.search(<span class="hljs-keyword">x</span><span class="hljs-string">q[:5]</span>, k)   <span class="hljs-comment"># 分别返回距离和索引</span><br></code></pre></td></tr></table></figure></li></ol><h2 id="4、SPTAG"><a href="#4、SPTAG" class="headerlink" title="4、SPTAG"></a>4、SPTAG</h2><p>SPTAG(空间分区树和图)是<strong>微软开源</strong>的BING搜索算法库，作为一种分布式近似最近邻域搜索（ANN）库，可用于大规模矢量搜索场景提供高质量矢量的索引构建，搜索和分布式在线服务。SPTAG内置L2 距离或余弦距离来计算向量之间的相似度，并提供KD-Tree 和相对邻域图（SPTAG-KDT）、以及平衡 k-means 树和相对邻域图（SPTAG-BKT）两种搜索算法。前者在指数构建成本方面能够有效降低成本，后者则在非常高维数据中保持较高的搜索精度。</p><p><strong>地址：</strong><a href="https://github.com/microsoft/SPTAG"><strong>https://github.com/microsoft/SPTAG</strong></a></p><h2 id="5、Vearch"><a href="#5、Vearch" class="headerlink" title="5、Vearch"></a>5、Vearch</h2><p>Vearch 是由<strong>京东开源</strong>的一个分布式向量搜索系统，考虑到开发及可扩展性，vearch 中的 Master，Router 和 PS 均采用 GO 语言编写。出于性能考虑，核心的存储检索引擎 gamma 基于 faiss 采用 c++ 语言实现， 提供了快速的向量检索功能，以及类似 Elasticsearch 的 Restful API 可以方便地对数据及表结构进行管理查询等工作。</p><p>此外，为满足实际业务场景需要，Vearch 还提供了算法插件服务模块，通过选择默认的 VGG，Resnet 或自定义算法模型等，能够提供端到端的图像检索，视频流智能监控等业务应用场景的实现。</p><p><strong>地址：</strong><a href="https://github.com/vearch/vearch"><strong>https://github.com/vearch/vearch</strong></a></p><h2 id="6、Milvus"><a href="#6、Milvus" class="headerlink" title="6、Milvus"></a>6、Milvus</h2><p>Milvus 是一款<strong>国产开源</strong>的特征向量相似度搜索引擎，使用方便、实用可靠、易于扩展、稳定高效和搜索迅速。</p><ul><li>高性能：涵盖如Faiss、Annoy和hnswlib等主流第三方索引库，性能高，支持对海量向量数据进行相似搜索。</li><li>高可用、高可靠：Milvus支持使用Kubernetes部署，支持在云上扩展。其容灾能力能够保证服务的高可用。Milvus依照日志及数据的理念，使用如Pulsar、Kafka等消息队列的技术实现组件间的通信，对组件进行解耦，拥抱云原生。</li><li>混合查询：Milvus支持在向量检索过程中进行标量字段过滤，实现混合查询。</li><li>开发者友好：支持多语言、多工具的Milvus生态。如今Milvus已经支持Python、Java、Go和Node.js，未来可能还会扩展对更多语言的支持。Milvus提供了如Attu等工具，帮助用户简化操作。</li></ul><p>在实际实验中发现，与FAISS相比，Milvus多平台通用，mac，windows和linux都是支持的，可以通过docker部署，在平台通用性上好了不少，并且支持Java，c，c++和python等多种编程语言。值得注意的是，Milvus 专门开通了训练营们，对了解向量数据库的操作及各种应用场景做了索引，例如如何进行 Milvus 性能测评，搭建智能问答机器人、推荐系统、以图搜图系统、分子式检索系统。</p><p><strong>地址：</strong><a href="https://milvus.io/"><strong>https://milvus.io/</strong></a></p><h2 id="嵌入式数据库：chroma"><a href="#嵌入式数据库：chroma" class="headerlink" title="嵌入式数据库：chroma"></a><a href="https://github.com/chroma-core/chroma">嵌入式数据库：chroma</a></h2><p>Chroma 是一个用于 Python / JavaScript LLM 应用程序的嵌入式数据库，它具有内存快速访问的优势。它只有 4 个核心函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> chromadb<br><span class="hljs-comment"># setup Chroma in-memory, for easy prototyping. Can add persistence easily!</span><br>client = chromadb.Client()<br><br><span class="hljs-comment"># Create collection. get_collection, get_or_create_collection, delete_collection also available!</span><br>collection = client.create_collection(<span class="hljs-string">&quot;all-my-documents&quot;</span>) <br><br><span class="hljs-comment"># Add docs to the collection. Can also update and delete. Row-based API coming soon!</span><br>collection.add(<br>    documents=[<span class="hljs-string">&quot;This is document1&quot;</span>, <span class="hljs-string">&quot;This is document2&quot;</span>], <span class="hljs-comment"># we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well</span><br>    metadatas=[&#123;<span class="hljs-string">&quot;source&quot;</span>: <span class="hljs-string">&quot;notion&quot;</span>&#125;, &#123;<span class="hljs-string">&quot;source&quot;</span>: <span class="hljs-string">&quot;google-docs&quot;</span>&#125;], <span class="hljs-comment"># filter on these!</span><br>    ids=[<span class="hljs-string">&quot;doc1&quot;</span>, <span class="hljs-string">&quot;doc2&quot;</span>], <span class="hljs-comment"># unique for each doc </span><br>)<br><br><span class="hljs-comment"># Query/search 2 most similar results. You can also .get by id</span><br>results = collection.query(<br>    query_texts=[<span class="hljs-string">&quot;This is a query document&quot;</span>],<br>    n_results=<span class="hljs-number">2</span>,<br>    <span class="hljs-comment"># where=&#123;&quot;metadata_field&quot;: &quot;is_equal_to_this&quot;&#125;, # optional filter</span><br>    <span class="hljs-comment"># where_document=&#123;&quot;$contains&quot;:&quot;search_string&quot;&#125;  # optional filter</span><br>)<br></code></pre></td></tr></table></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在实践中，可以发现的是，gensim适合小规模试验的场景当中，只需要简单的命令，将向量文件表示为对应的格式，就可以完成相应的处理。Annoy、FAISS、SPTAG是较为底层的相似度算法库，FAISS目前广泛应用于搜索推荐场景当中，后两者在facebook以及微软的实际业务中成功的应用，直接证明了两个系统的稳定性和工业落地性。</p><p>此外，通过对比faiss和SPTAG，Vearch和Milvus两者属于同类型产品，Milvus和Vearch是两款基于现有的开发库，开箱即用的应用，在实现基本的相似计算功能的基础上，围绕服务整体易用性、部署、稳定性等方面做了更多工作。此外，Milvus对比Vearch，在社区活跃度、支持度上具有更明显的优势。</p><p>总之，我们在实际的业务落地场景中，要考虑自身的数据、资源、业务需求，合适的选用向量化方法和向量化搜索框架，将现有数据价值发挥到最大。</p>]]></content>
    
    
    <categories>
      
      <category>ChatGPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>向量检索库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>并行训练大型神经网络（转载）</title>
    <link href="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
    <url>/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>参考资料：</p><p><a href="https://openai.com/blog/techniques-for-training-large-neural-networks/">OpenAI一篇文章总结</a></p><p><a href="https://mp.weixin.qq.com/s/CJK40HDZNmiqWXKTsUiOjA">实操教程 | GPU多卡并行训练总结（以pytorch为例）</a></p></blockquote><p>最近OpenAI发布了一篇文章，详细介绍了一些训练大型神经网络的相关技术及底层原理，彻底消除你对并行的恐惧！</p><p>比如以并行训练一个三层的神经网络为例，其中并行可以分为数据并行、pipeline并行，trensor并行和专家并行，图中不同颜色代表不同层、虚线隔开的是不同的GPU。</p><img src="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1659508649396-f7bb43a6-e466-4bc9-bbce-694d2d649b44.png" class="" title="image.png"><p>听上去很多，但理解这些并行技术实际上只需要对计算结构进行一些假设，然后对数据包的流动方向有所了解即可。</p><h1 id="训练流程无并行"><a href="#训练流程无并行" class="headerlink" title="训练流程无并行"></a><strong>训练流程无并行</strong></h1><p>训练一个神经网络是一个迭代的过程。</p><p>在一次迭代中，输入数据经过模型的层，前向传递后即可为一个batch数据中的每个训练实例计算输出。</p><p>然后各层再向后传递，通过计算每个参数的梯度来传播每个参数对最终输出的影响程度。</p><p>每个batch数据的平均梯度、参数和一些每个参数的优化状态被传递给一个优化算法，比如Adam可以计算下一个迭代的参数（在你的数据上应该有更好的性能）和新的每个参数的优化状态。</p><p>随着训练在大量数据上的迭代，模型不断进化，产生越来越精确的输出。</p><img src="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1659508866982-ff573ea3-2ab4-48a2-93ce-8ba6f947d37c.png" class="" title="image.png"><p>在整个训练过程中，会有不同的并行技术在不同的维度上进行切割，包括：</p><p>1、数据并行，即在不同的GPU上运行一个batch的不同子集；</p><p>2、pipeline并行，即在不同的GPU上运行模型的不同层；</p><p>3、tensor并行，即将单一操作（如矩阵乘法）的数学运算拆分到不同的GPU上 </p><p>4、专家混合（Mixture of Experts, MoE），即只用每层的一部分来处理每个输入实例。</p><p>并行中说的GPU并非仅局限于GPU，对于其他神经网络加速器的用户来说，这些想法同样有效。</p><h1 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a><strong>数据并行</strong></h1><p>数据并行训练意味着将相同的参数复制到多个GPU（通常称为worker），并将不同的实例分配给每个GPU同时进行处理。</p><p>单纯的数据并行仍然需要模型符合单个GPU的内存要求，如果你利用多个GPU进行计算，那代价就是存储许多重复的参数副本。有一些策略可以增加你的GPU可用的有效RAM，比如在两次使用之间将参数暂时卸载到CPU内存。</p><img src="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1659508887274-47a0d907-2595-4ab8-a4ee-4aef8b3054ee.png" class="" title="image.png"><p>当每个数据并行worker更新其参数副本时，他们需要协调以确保每个worker继续拥有类似的参数。</p><p>最简单的方法是在worker之间引入阻塞式通信：</p><p>1、在每个worker上独立计算梯度；</p><p>2、在各worker上平均梯度；</p><p>3、每个worker上独立计算相同的新参数。</p><p>其中步骤2是的阻塞需要传输相当多的数据（与worker的数量乘以参数量的大小成正比），非常有可能降低训练吞吐量。</p><p>虽然有各种异步同步方案来消除这种开销，但它们会损害学习效率；在实践中，研究人员通常还是会坚持使用同步方法。</p><h1 id="Pipeline并行"><a href="#Pipeline并行" class="headerlink" title="Pipeline并行"></a><strong>Pipeline并行</strong></h1><p>pipeline并行训练的意思是将模型的顺序块分割到不同的GPU上，每个GPU只持有一部分参数，因此，同一个模型在每个GPU上消耗的内存比例较小。</p><p>将一个大的模型分割成连续层的大块是很直接的一种方式。然而，各层的输入和输出之间存在着顺序上的依赖性，所以一个朴素的实现可能会导致大量的空闲时间，而wroker在等待前一个机器的输出被用作其输入。</p><p>这些等待时间块被称为气泡（bubbles），浪费了空闲机器可以完成的计算。</p><img src="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1659508713787-88303d28-db33-41bc-8101-a7c534608718.png" class="" title="image.png"><p>我们可以重用数据并行的思想，通过让每个worker一次只处理一个数据元素的子集来降低气泡的成本，巧妙地将新的计算与等待时间重叠起来。</p><p>核心思想是将一个batch分成多个microbatches；每个微批的处理速度应该是成比例的，每个worker在下一个微批可用时就开始工作，从而加速管道的执行。</p><p>有了足够的微批，worker在大部分时间内都处于工作状态，并且在每个step的开始和结束时气泡最小。梯度是所有微批的平均值，只有所有微批完成之后才会进行参数更新。</p><p>模型被分割的worker的数量通常被称为pipeline深度。</p><p>在前向传递期间，worker只需要将其大块层的输出（也叫激活）发送给下一个worker；在后向传递期间，它只将这些激活的梯度发送给前一个worker。如何调度这些传递过程以及如何在微批中聚合梯度，仍然有很大的设计空间。</p><p>GPipe的做法是让每个worker连续地处理前向和后向的传递，然后在最后同步地聚合来自多个微批的梯度。而PipeDream则安排每个工作者交替地处理前向和后向通道。</p><img src="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1659508796577-6ef68263-c418-49bd-8612-6af5d12de473.png" class="" title="image.png"> <h1 id="Tensor并行"><a href="#Tensor并行" class="headerlink" title="Tensor并行"></a><strong>Tensor并行</strong></h1><p>Pipeline并行是将一个模型按层「垂直」分割，而Tensor并行则是在一个层内「横向」分割某些操作。</p><p>对于现代模型（如Transformer）来说，计算瓶颈主要来自激活批矩阵与大权重矩阵相乘。矩阵乘法可以被认为是成对的行和列之间的点积，所以是有可能在不同的GPU上独立计算点积，或者在不同的GPU上计算每个点积的一部分，最后再将结果相加。无论采用哪种策略，我们都可以将权重矩阵切成偶数大小的「碎片」，将每个碎片放在不同的GPU上，并使用该碎片来计算整个矩阵乘积的相关部分，然后再进行GPU间通信来合并结果。</p><img src="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1659508910706-17443af0-1078-468c-bf29-2007b50f8d07.png" class="" title="image.png"><p>Megatron-LM采用的就是这种方式，它在Transformer的自注意力和MLP层内并行化矩阵乘法。PTD-P使用Tensor、数据和Pipeline并行；它的pipeline调度将多个不连续的层分配给每个设备，以更多的网络通信为代价减少气泡的开销。</p><p>有时网络的输入也可以在一个维度上进行并行化，相对于交叉通信来说，并行计算的程度很高。序列并行就是这样一个想法，一个输入序列在不同时间被分割成多个子实例，通过以更细粒度的实例进行计算，可以按比例减少峰值内存消耗。</p><h1 id="混合专家系统（MoE）"><a href="#混合专家系统（MoE）" class="headerlink" title="混合专家系统（MoE）"></a><strong>混合专家系统（MoE）</strong></h1><p>对于任何一个输入，MoE策略都只有一部分网络被用来计算输出。</p><p>比如说一个网络里有很多套权重，网络可以在推理时通过门控机制选择具体使用哪套。这样就可以在不增加计算成本的情况下增加参数量。</p><p>每组权重被称为一个「专家」，训练目标是希望网络能够学会将专门的计算和技能分配给每个专家。不同的专家可以托管在不同的GPU上，为扩大模型使用的GPU数量提供了一个明确的方法。</p><img src="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1659508837385-108bd27b-a8c6-47b2-a7cd-09ccd763607a.png" class="" title="image.png"><p>GShard可以将MoE Transformer的规模扩大到6000亿个参数，其方案是只有MoE层被分割到多个TPU设备上，而其他层则是完全重复的。</p><p>Switch Transformer则是通过将一个输入路由（routing）到一个专家，成功将模型规模扩展到数万亿的参数，甚至稀疏度更高。</p><h1 id="省内存小妙招"><a href="#省内存小妙招" class="headerlink" title="省内存小妙招"></a><strong>省内存小妙招</strong></h1><p>除了买GPU外，还有一些计算策略可以帮助节省内存，方便训练更大的神经网络。</p><p>1、为了计算梯度，你可能需要保存原始激活值，这可能会消耗大量的设备内存。检查点（Checkpointing, 也被称为激活再计算）可以存储任何激活子集，并在后向通道中以just-in-time的方式重新计算中间激活。</p><p>这种方式可以节省大量的内存，而计算成本最多就是多出一个完整的前向传递。我们也可以通过选择性的激活再计算来不断地在计算和内存成本之间进行权衡，也就是检查那些存储成本相对较高但计算成本较低的激活子集。</p><p>2、混合精度训练（Mixed Precision Training）是使用较低精度的数字（最常见的是FP16）来训练模型。现代的计算加速器可以用低精度数字达到更高的FLOP数，而且你还可以节省设备RAM。只要处理得当，这种方式训练得到的模型在性能上几乎不会有太大损失。</p><img src="/2022/08/03/2022-08-03-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1659508936311-4b67c312-f834-4933-8c35-d23ffb7cff2d.png" class="" title="image.png"><p>3、卸载（Offloading）是将未使用的数据暂时卸载到CPU或不同的设备中，然后在需要时再将其读回。朴素的实现方式会大大降低训练速度，但复杂的实现方式会预先获取数据，这样设备就不需要再等待了。</p><p>这个想法的一个具体实现是ZeRO，它将参数、梯度和优化器状态分割到所有可用的硬件上，并根据实际需要再将它们具体化。</p><p>4、内存效率优化器（Memory Efficient Optimizer）可以减少优化器所维护的运行状态的内存占用，如Adafactor。</p><p>5、压缩（Compression）也可用于存储网络中的中间结果。例如，Gist对为后向传递而保存的激活进行压缩；DALL-E在同步梯度之前压缩了梯度。</p>]]></content>
    
    
    <categories>
      
      <category>llm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>坐席辅助话术推荐1-方案调研&amp;设计</title>
    <link href="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&amp;%E8%AE%BE%E8%AE%A1/"/>
    <url>/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&amp;%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="方案调研"><a href="#方案调研" class="headerlink" title="方案调研"></a>方案调研</h1><p>关注点： </p><ul><li>这个方案的baseline是什么，最简单和快捷的方式是哪些，都有什么优缺点。</li><li>大厂常用的方案是什么，有没有什么特别地操作，为什么要做这个操作。</li><li>论文，科研界的主要方式是什么，需要关注哪些方面。</li></ul><h2 id="1、美团人工辅助——话术推荐"><a href="#1、美团人工辅助——话术推荐" class="headerlink" title="1、美团人工辅助——话术推荐"></a>1、<a href="https://tech.meituan.com/2021/09/30/artificial-intelligence-customer-service.html">美团人工辅助——话术推荐</a></h2><blockquote><p><a href="https://tech.meituan.com/2021/09/30/artificial-intelligence-customer-service.html">美团智能客服核心技术与实践</a></p></blockquote><p><strong>智能辅助目的：</strong></p><ul><li>自动匹配历史对话日志，便于人工座席了解客户背景诉求；</li><li>自动匹配历史优秀座席回答话术，供其他座席参考；</li><li>场景话术推荐，分步规范座席对话流程；</li></ul><p>面向座席的场景：</p><p>座席在与用户的对话聊天中经常回复相似甚至相同的话术，提供<strong>话术推荐</strong>的能力可以提升人工座席的工作效率，改善人工座席的工作体验。</p><p>那么，话术推荐具体要怎么做呢？常见的做法是先准备好常用通用话术库（直接从对话日志里面挖回复），部分座席或商家也会准备个人常见话术库，然后系统根据用户的Query及上下文来检索最合适的话术，预测接下来可能的回复话术</p><p>美团的做法是：将历史聊天记录构建成“N+1”QA问答对的形式建模，前N句看作问题Q，后1句作为回复话术A，整个框架就可以转化成检索式的问答模型。在召回阶段，除了文本信息召回外，还加入了上文多轮槽位标签，Topic标签等召回优化（我理解是 ES 建向量索引，top 相似度召回）；排序为基于BERT的模型，将“N+1”QA问答对拼接，并加入角色信息建模（角色为用户、商家或者座席）作为输入，利用 0/1 二分类做训练，利用xx相似度匹配做推理….</p><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&%E8%AE%BE%E8%AE%A1/1647238292862-27d4822d-7389-477c-bfce-8383df113cd8.png" class="" title="image.png"><h2 id="2、基于LM的生成式多轮对话模型"><a href="#2、基于LM的生成式多轮对话模型" class="headerlink" title="2、基于LM的生成式多轮对话模型"></a>2、基于LM的生成式多轮对话模型</h2><blockquote><p><a href="https://spaces.ac.cn/archives/7718">动手做个DialoGPT：基于LM的生成式多轮对话模型</a></p></blockquote><ol><li>数据集：大规模的中文闲聊语料库LCCC，包含了部分多轮对话聊天…所有样本都被处理成双人对话。</li><li>考虑训练一个模型，预测下一个该回复什么。同时还要求这个模型支持多轮对话。</li><li>考虑对话历史的最简单的方式，就是把直到当前句的所有历史对话都拼接成单句文本，来作为模型的输入信息了。</li><li>选择就是单向语言模型（LM、GPT），做法如下图所示。选择当前主流的Transformer模型，按照BERT / GPT 的常规输入格式，将每句对话用[SEP]拼接起来，然后就训练一个从左往右的单向语言模型。为了区分不同的说话角色，我们对不同的说话者用不同的Segment Id区分。</li><li>此外，考虑到BERT和GPT都是用了绝对位置编码，可处理的文本长度存在一个上限，而对话轮数理论上是无限的，这里可以采用相对位置编码的<strong>NEZHA</strong>作为基本结构，并使用NEZHA的预训练权重作为模型的初始化权重。</li></ol><p><a href="https://spaces.ac.cn/usr/uploads/2020/09/603395458.png"><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&%E8%AE%BE%E8%AE%A1/1647360987158-49f5b087-2579-42cd-ab28-06ea3acd4feb.png" class=""></a></p><p>利用单向语言模型做多轮对话示意图</p><p>另外，<a href="https://github.com/thu-coai/CDial-GPT">CDial-GPT</a>也开源了自己训练的预训练模型，如下图：</p><p><a href="https://spaces.ac.cn/usr/uploads/2020/09/727199313.png"><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&%E8%AE%BE%E8%AE%A1/1647361118030-703dab57-1908-4b4b-95d7-ea7a5ff33e02.png" class=""></a></p><p>CDial-GPT模型示意图</p><p>如图所示，CDial-GPT跟前述设计的主要不同是多轮对话之间的拼接方式，我们之前是直接用[SEP]连接，它是用[speaker1]、[speaker2]（图中简记为S1、S2）这样的角色标记来连接，最后才用一个[SEP]表示回复结束。这样一来，由于预测部分的格式跟历史的格式不一样，因此每次只能训练一句回复，多轮对话要拆分为多个样本来训练，理论上是增加了训练复杂性的（要训练多步才能把一个多轮对话样本训练完）。</p><h2 id="3、Seq2Seq-前缀树⭐️"><a href="#3、Seq2Seq-前缀树⭐️" class="headerlink" title="3、Seq2Seq+前缀树⭐️"></a>3、Seq2Seq+前缀树⭐️</h2><blockquote><p><a href="https://spaces.ac.cn/archives/8802">Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）</a></p></blockquote><p>对于相似问检索来说，我们输入一个问句，希望输出数据库中与之最相近的句子；对于话术推荐，输入一个问句，希望输出话术库中与最合适的回复。这些任务本质即 <a href="https://www.yuque.com/ningshixian/pz10h0/ep4qy2">seq2seq</a>，再利用<a href="https://www.yuque.com/ningshixian/pz10h0/wob6k0">前缀树</a>来约束解码过程，即可保证生成的结果在数据库/话术库中。</p><p>具体来说，我们用<strong>UNILM方案</strong>来训练一个Seq2Seq模型。将用户上下文 query 当作Seq2Seq的输入，将坐席客服回复（type=out）用[SEP]连接起来作为目标；推理/解码的时候，我们先把所有的坐席回复（即答案）建立成前缀树，然后按照前缀树进行 <a href="https://www.yuque.com/ningshixian/pz10h0/occzbt">beam search 解码</a> and 输出结果。</p><p>注意，利用前缀树约束Seq2Seq解码其实很简单，即根据树上的每一条路径（以[BOS]开头、[EOS]结尾）查找到以某个前缀开头的字/句有哪些。然后，把模型预测其他字的概率都置零，这样模型只可能从这些字中选其一。依此类推，通过将不在前缀树上的候选token置零，保证解码过程只走前缀树的分支，而且必须走到最后，这样解码出来的结果必然是话术库中已有的句子。<br><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&%E8%AE%BE%E8%AE%A1/1647872975533-856f89df-836d-411d-bc03-5c1ca78ecb2e.png" class="" title="image.png"></p><p>在“Seq2Seq+前缀树”方案中，Seq2Seq（Roberta / UER / RoFormer+UniLM)并不是真的要去生成任意文本，而是在前缀树的约束下做本质上是检索的操作。</p><p>“Seq2Seq+前缀树”对标的是传统的稠密向量检索任务。理论上“Seq2Seq+前缀树”的检索时间是固定的，也就是说不管数据库中有多少句子，其预测时间都是常数级别。待检索句子的多少，影响的是前缀树的空间占用，基本不影响检索速度。</p><h1 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h1><h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><ul><li>数据：C2 坐席半年的对话数据(20211001~20220301)，通过二八分拆出训练集和评测集；<ol><li>把直到当前句的所有历史对话都拼接成单句文本（q1+a1+…+qn+an）作为输入，预测回复话术(分类)；</li><li>将历史聊天记录构建成“N+1”QA问答对的形式，前N句看作问题Q，后1句作为回复话术A，通过[SEP]占位符拼接；</li></ol></li><li>在线更新。需要设计在线实时更新的pipeline。</li><li>机器。阿波罗的多gpu的机器（8 卡），搭建Docker+配置Conda+安装依赖包；</li><li>模型的训练策略：<ol><li>当做单标签多分类任务，类别数很大，效果不一定好；</li><li>类似nlu度量学习，转化成检索式的问答模型：多分类任务训练，保存编码模型，最后向量相似度检索；</li><li>交互精排：拼接【问题+话术】作为模型的输入，二分类任务（相似/不相似），需要一个召回模型~</li><li>双塔模型，问题bert+话术bert（二者可共享权重），预测（相似/不相似）；</li><li>seq2seq+前缀树（参考方案调研 3）</li></ol></li><li>算法效果评估，分类方案使用<strong> pearson和spearman</strong>系数双50%，生成方案使用 <a href="https://coladrill.github.io/2018/10/20/%E6%B5%85%E8%B0%88BLEU%E8%AF%84%E5%88%86/"><strong>BLEU评分</strong></a>；<ul><li>_坐席推荐选 pearson和spearman 系数进行评测，这里说下原因：（_评测集<q,a>的情况下_）_<ol><li>_省去了需要召回阶段才能评测，省事_</li><li>_直接对QA对进行打分，然后用pearson和spearman去计算 socre list 和 label list 的趋势契合度_</li><li>_如果以 ACC/F1 来评测，需要人工卡阈值，才能评估模型的分类准确性；_</li><li>_如果以 pearson/spearman 评测，看趋势，可以评估模型的排序相关性；_</li><li>_话术推荐是一个非二分类问题，不关心绝对得分大小，而是关心相对得分大小，只要整体得分趋势接近，即可用于特征的对比排序_<ol><li>_好比有一个q，它有一个正例a1和一个难负例a2，推理出的分数为0.9和0.8，如果是走ACC，默认0.5的阈值，会得到俩1，0.5的ACC。而如果是走spearman或pearson，会是[0.8,0.9]对应[0,1]的label，趋势是一致的_</li></ol></li></ol></li></ul></li></ul><p><strong>模型的训练策略C，精排策略的具体细节：</strong></p><ol><li>问题的拼接：<br>首先将查询和每一个候选文档一起作为Bert模型的输入，开始加入[CLS]标记。查询和文档之间加入[SEP]标记。利用BPE算法等进行分词，得到Bert模型的输入特征向量。</li><li>相似度计算：<br>将特征向量输入Bert后，经计算将得到BERT的输出（句子中每个词的向量表示），取[CLS] 标记的向量表示，通过一个单层或多层的线性神经网络，得到两个文档的相似度得分（相似的概率)。</li></ol><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&%E8%AE%BE%E8%AE%A1/1647236107118-7e705281-8676-4e53-b32f-ed37d93ba3fa.png" class="" title="image.png"><p><strong>模型的训练策略D，孪生策略的具体细节：</strong></p><ol><li>BERT 模型共享权重；</li><li>将两个 CLS 取出作差 or 级联，最后通过Dense+Softmax进行分类；</li><li>使用交叉熵计算二分类损失 or margin loss</li></ol><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&%E8%AE%BE%E8%AE%A1/1647236335834-dca470a9-f60b-4e7f-a95a-301e6b3c687e.png" class="" title="image.png"><p>1、通用 margin loss 如下：</p><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&%E8%AE%BE%E8%AE%A1/1647237281146-36d6b508-10fb-40c4-a64d-8f5cee122267.png" class="" title="image.png"><p>2、Triple loss function为：</p><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%901-%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94&%E8%AE%BE%E8%AE%A1/image-20230424202724667.png" class="" title="image-20230424202724667"><h2 id="工程设计"><a href="#工程设计" class="headerlink" title="工程设计"></a>工程设计</h2><ul><li>算法模块在整个流程中的角色是什么，上下游接口怎么样。</li><li>数据流是怎么走的，需要确认，包括离在线，搜索系统的话不仅考虑用户query等信息，还有物料的信息怎么存储，另外如果个性化，还有用户画像的数据问题。</li><li>中间件的使用和维护，如常用的数据库mysql、ES、redis之类的，还有些类似kafka、hadoop全家桶之类的。</li><li>……</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://mp.weixin.qq.com/s/J5f39bTGjjEuHZHd1DomhQ">心法利器[56] | 算法技术设计思路小结</a></p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>坐席辅助</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>坐席辅助话术推荐2-详细技术方案</title>
    <link href="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%902-%E8%AF%A6%E7%BB%86%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88/"/>
    <url>/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%902-%E8%AF%A6%E7%BB%86%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h1 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h1><ul><li>C2 坐席半年的对话日志数据(20220301~20220420)，通过二八分拆出训练集和评测集；</li></ul><h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><ul><li>全小写</li><li>替换html标签、时间（年月日时分秒）、手机号&amp;尾号等</li><li>过滤无意义数据（如：“转人工”、“服务评价”、“感谢您的咨询”）</li><li>过滤无意义单字Query</li><li>添加特殊占位符[pic]、[http]、[subphone]、[phone]、[alnum]、[ques]、[know]</li><li>按时间 createtime 对dataframe排序</li><li>过滤重复上下文  &amp;&amp; 过滤重复答案</li><li>全局随机采样负例（easy），使得测试集中「正负比例」=1:1</li></ul><h1 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h1><p>对于话术推荐，输入一个问句，希望输出话术库中与其最合适的回复（以坐席的回复作为话术库）。抛开某些约束规则不说，可以发现该任务本质上都可以抽象为Seq2Seq，即：“输入一个句子，输出另一个句子”。进一步利用前缀树来约束解码过程，即可保证生成的结果在要求的话术库中。</p><h1 id="模型输入数据构造"><a href="#模型输入数据构造" class="headerlink" title="模型输入数据构造"></a>模型输入数据构造</h1><p>将用户上下文 query （context=3）以及坐席客服回复 A（type=out）用[SEP]占位符拼接；</p><ul><li>$单条样本示例：[CLS] Q1 [SEP] Q2 [SEP] Q3 [SEP] A [SEP]$</li></ul><p>其中 Q 表示用户输入 query（type=in），每条的最大长度为 32；A 表示坐席回复 answer（type=out），每条的最大长度为 64。注意：如果存在坐席连续回复，则通过句号将连续的多个回复拼接起来；</p><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h2 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h2><p>以“BERT+UniLM”为基础架构，训练一个Seq2Seq模型。</p><ul><li>采用Seq2Seq的经典训练方式 <strong>Teacher Forcing</strong>，输入上下文Q和坐席回复A[:-1]，预测A[1:]，即将目标A错开一位来训练。PLM 使用<a href="https://github.com/dbiir/UER-py">腾讯UER</a>开源的BERT权重，优化器是Adam，学习率是2e-5，batch_size=16，大概需要训练15~20个epoch，以 bleu 评测结果作为模型保存依据。</li></ul><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%902-%E8%AF%A6%E7%BB%86%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88/1649318286901-f3c3d740-7b88-43d4-ba1e-2398c20c6b22.png" class="" title="seq2seq+trie.drawio.png"><ul><li>借鉴 <a href="https://arxiv.org/abs/1905.03197">UNILM</a> 方案，通过添加一个特别的Mask矩阵，直接用单个Bert模型实现 Seq2Seq LM 任务（无需修改模型架构，且可以直接沿用Bert的 Masked Language Model 预训练权重）。Mask矩阵如下图所示，作用是让Bert输入部分的Attention是双向的，输出部分的Attention是单向，从而满足Seq2Seq的要求。</li></ul><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%902-%E8%AF%A6%E7%BB%86%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88/1649320201338-5d5f6559-271a-4c3d-a9b5-1d5155958645.png" class="" title="seq2seq+mask.drawio.png"><p>白色方格代表0，蓝色方格代表 1</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>seq2seq 在生成输出序列的时候是一个time_step生成一个字，换句话说，在每个时间步都在解决一个分类问题。所以这里选择最常用的<strong>交叉熵损失函数</strong>，但需要利用上述的Mask来屏蔽掉上下文部分的loss（置为零）</p><h2 id="推理阶段"><a href="#推理阶段" class="headerlink" title="推理阶段"></a>推理阶段</h2><p>利用坐席回复（即答案A）构建<a href="https://www.yuque.com/ningshixian/pz10h0/wob6k0">前缀树</a>，然后按照前缀树进行 <a href="https://www.yuque.com/ningshixian/pz10h0/occzbt">beam search 解码</a>。</p><ul><li>在 beam search 中采用基于Trie树的剪枝方法：将目标词建立成Trie树结构，在decode过程中根据beam中的前项序列在Trie树中搜索候选的后项token。然后，把模型预测其他字的概率都置零，保证解码过程只走前缀树的分支，而且必须走到最后，这样可以大幅缩减搜索空间及计算开销，同时也保证了生成的句子必然在话术库中；</li></ul><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%902-%E8%AF%A6%E7%BB%86%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88/1649401519965-43740992-cfc8-4469-81c0-5e024e1bf696.png" class="" title="image.png"><ul><li>前缀树叶节点保存坐席回复对应的富文本格式；</li><li>目前支持beam_search 和 random_sample 两种解码策略，通过如下公式计算序列概率：</li></ul><img src="/2022/06/07/2022-06-07-%E5%9D%90%E5%B8%AD%E8%BE%85%E5%8A%A9%E8%AF%9D%E6%9C%AF%E6%8E%A8%E8%8D%902-%E8%AF%A6%E7%BB%86%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88/image-20230424203704809.png" class="" title="image-20230424203704809"><h1 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h1><p>参考《<a href="https://www.yuque.com/ningshixian/pz10h0/rpgivp">2022-03-18-生成模型的评估指标</a>》，主要以 BLEU 指标作为选模型的依据；</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>坐席辅助</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>中文分词算法综述（转载）</title>
    <link href="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
    <url>/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://zhuanlan.zhihu.com/p/50444885?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=30098367447040">NLP分词算法深度综述</a></p></blockquote><p>之前总是在看前沿文章，真正落实到工业级任务还是需要实打实的硬核基础，我司选用了HANLP作为分词组件，在使用的过程中才感受到自己基础的薄弱，决定最近好好把分词的底层算法梳理一下。</p><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a><strong>1. 简介</strong></h2><p>NLP的底层任务由易到难大致可以分为词法分析、句法分析和语义分析。分词是词法分析（还包括词性标注和命名实体识别）中最基本的任务，可以说既简单又复杂。说简单是因为分词的算法研究已经很成熟了，大部分的准确率都可以达到95%以上，说复杂是因为剩下的5%很难有突破，主要因为三点：</p><ol><li>粒度，不同应用对粒度的要求不一样，比如“苹果手机”可以是一个词也可以是两个词</li><li>歧义，比如“下雨天留人天留我不留”</li><li>未登录词，比如“skrrr”、“打call”等新兴词语</li></ol><p>然而，在真实的应用中往往会因为以上的难点造成分词效果欠佳，进而影响之后的任务。对于追求算法表现的童鞋来说，不仅要会调分词包，也要对这些基础技术有一定的了解，在做真正的工业级应用时有能力对分词器进行调整。这篇文章不是着重介绍某个SOTA成果，而是对常用的分词算法（不仅是机器学习或神经网络，还包括动态规划等）以及其核心思想进行介绍。</p><h2 id="2-分词算法"><a href="#2-分词算法" class="headerlink" title="2. 分词算法"></a><strong>2. 分词算法</strong></h2><p>我认为分词算法根据其核心思想主要分为两种，第一种是基于字典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；第二种是基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。归根结底，上述两种方法都可以归结为在图或者概率图上寻找最短路径的问题。接下来将以“<strong>他说的确实在理</strong>”这句话为例，讲解各个不同的分词算法核心思想。</p><h2 id="2-1-基于词典的分词"><a href="#2-1-基于词典的分词" class="headerlink" title="2.1 基于词典的分词"></a><strong>2.1 基于词典的分词</strong></h2><h2 id="2-1-1-最大正向匹配"><a href="#2-1-1-最大正向匹配" class="headerlink" title="2.1.1 最大正向匹配"></a><strong>2.1.1 最大正向匹配</strong></h2><p>最大匹配分词寻找最优组合的方式是将匹配到的最长词组合在一起。主要的思路是先将词典构造成一棵Trie树，也称为字典树，如下图：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904677841-7946898c-48a2-43c4-9dcb-94b6ed187523.png" class="" title="image.png"><p>Trie树由词的公共前缀构成节点，降低了存储空间的同时提升查找效率。最大匹配分词将句子与Trie树进行匹配，在匹配到根结点时由下一个字重新开始进行查找。比如正向（从左至右）匹配“他说的确实在理”，得出的结果为“他／说／的确／实在／理”。如果进行反向最大匹配，则为“他／说／的／确实／在理”。</p><p>可见，词典分词虽然可以在O(n)时间对句子进行分词，但是效果很差，在实际情况中基本不使用此种方法。</p><h2 id="2-1-2-最短路径分词算法"><a href="#2-1-2-最短路径分词算法" class="headerlink" title="2.1.2 最短路径分词算法"></a><strong>2.1.2 最短路径分词算法</strong></h2><p>最短路径分词算法首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的最短路径作为最佳组合方式，引用《统计自然语言处理》中的图：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904677865-9e0bca91-c8b3-43cc-92f8-ba31626e106f.png" class="" title="image.png"><p>我们认为图中每个词的权重都是相等的，因此每条边的权重都为1。</p><p>在求解DAG图的最短路径问题时，总是要利用到一种性质：即两点之间的最短路径也包含了路径上其他顶点间的最短路径。比如S-&gt;A-&gt;B-&gt;E为S到E到最短路径，那S-&gt;A-&gt;B一定是S到B到最短路径，否则会存在一点C使得d(S-&gt;C-&gt;B)<d(S->A-&gt;B)，那S到E的最短路径也会变为S-&gt;C-&gt;B-&gt;E，这就与假设矛盾了。利用上述的最优子结构性质，可以利用贪心算法或动态规划两种求解算法：</p><p><strong>1. 最短路径分词算法</strong></p><p>基于Dijkstra算法求解最短路径。该算法适用于所有带权有向图，求解源节点到其他所有节点的最短路径，并可以求得全局最优解。Dijkstra本质为贪心算法，在每一步走到当前路径最短的节点，递推地更新原节点到其他节点的距离。针对当前问题，Dijkstra算法的计算结果为：“他／说／的／确实／在理“。可见最短路径分词算法可以满足部分分词要求。但当存在多条距离相同的最短路径时，Dijkstra只保存一条，对其他路径不公平，也缺乏理论依据。</p><p><strong>2. N-最短路径分词算法</strong></p><p>N-最短路径分词是对Dijkstra算法的扩展，在每一步保存最短的N条路径，并记录这些路径上当前节点的前驱，在最后求得最优解时回溯得到最短路径。该方法的准确率优于Dijkstra算法，但在时间和空间复杂度上都更大。</p><h2 id="2-1-3-基于n-gram-model的分词算法"><a href="#2-1-3-基于n-gram-model的分词算法" class="headerlink" title="2.1.3 基于n-gram model的分词算法"></a><strong>2.1.3 基于n-gram model的分词算法</strong></h2><p>在前文的词图中，边的权重都为1。而现实中却不一样，常用词的出现频率／概率肯定比罕见词要大。因此可以将求解词图最短路径的问题转化为求解最大概率路径的问题，即分词结果为“最有可能的词的组合“。计算词出现的概率，仅有词典是不够的，还需要有充足的语料。因此分词任务已经从单纯的“算法”上升到了“建模”，即利用统计学方法结合大数据挖掘，对“语言”进行建模。</p><p>语言模型的目的是构建一句话出现的概率p(s)，根据条件概率公式我们知道：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904677768-dc8029f5-2990-41b8-a9ee-fb0d2037c988.svg" class=""><p>而要真正计算“他说的确实在理”出现的概率，就必须计算出上述所有形如 <img src="https://cdn.nlark.com/yuque/0/2022/svg/8420697/1653904677786-29829c65-1f9d-490c-9614-08568c92c1e0.svg#clientId=u7f1ecb66-be4a-4&amp;from=paste&amp;id=uc04cbd33&amp;originHeight=23&amp;originWidth=139&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u8c4f5f9e-530f-4737-80fa-fd2c690af9f&amp;title=" alt=""> n=1,…,6 的概率，计算量太过庞大，因此我们近似地认为：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904677869-d5147b9c-de61-46ff-a9fe-e66b57b99d27.svg" class=""><p>其中 $s=(w_!,w_2,…,w_l), s_i$ 为字或单词。我们将上述模型成为二元语言模型 (2-gram model)。类似的，如果只对词频进行统计，则为一元语言模型。由于计算量的限制，在实际应用中n一般取3。</p><p>我们将基于词的语言模型所统计出的概率分布应用到词图中，可以得到词的<strong>概率图</strong>：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904678267-4dae9b21-f9f7-467c-9b8c-0bca444642cd.png" class="" title="image.png"><p>对该词图用2.1.2中的算法求解最大概率的路径，即可得到分词结果。</p><h2 id="2-2-基于字的分词"><a href="#2-2-基于字的分词" class="headerlink" title="2.2 基于字的分词"></a><strong>2.2 基于字的分词</strong></h2><p>与基于词典的分词不同的是，基于字的分词事先不对句子进行词的匹配，而是将分词看成序列标注问题，把一个字标记成B(Begin), I(Inside), O(Outside), E(End), S(Single)。因此也可以看成是每个字的分类问题，输入为每个字及其前后字所构成的特征，输出为分类标记。对于分类问题，可以用统计机器学习或神经网络的方法求解。</p><p>统计机器学习方法通过一系列算法对问题进行抽象，进而得到模型，再用得到的模型去解决相似的问题。也可以将模型看成一个函数，输入X，得到f(X)=Y。另外，机器学习中一般将模型分为两类：生成式模型和判别式模型，两者的本质区别在于X和Y的生成关系。生成式模型以“输出Y按照一定的规律生成输入X”为假设对P(X,Y)联合概率进行建模；判别式模型认为Y由X决定，直接对后验概率P(Y|X)进行建模。两者各有利弊，生成模型对变量的关系描述更加清晰，而判别式模型容易建立和学习。下面对几种序列标注方法做简要介绍。</p><h2 id="2-2-1-生成式模型分词算法"><a href="#2-2-1-生成式模型分词算法" class="headerlink" title="2.2.1 生成式模型分词算法"></a><strong>2.2.1 生成式模型分词算法</strong></h2><p>生成式模型主要有n-gram模型、HMM隐马尔可夫模型、朴素贝叶斯分类等。在分词中应用比较多的是n-gram模型和HMM模型。如果将2.1.3中的节点由词改成字，则可基于字的n-gram模型进行分词，不过这种方法的效果没有基于词的效果要好。</p><p>HMM模型认为在解决序列标注问题时存在两种序列，一种是观测序列，即人们显性观察到的句子，而序列标签是隐状态序列，即观测序列为X，隐状态序列是Y，因果关系为Y-&gt;X。因此要得到标注结果Y，必须对X的概率、Y的概率、P(X|Y)进行计算，即建立P(X,Y)的概率分布模型。例句的隐马尔科夫序列如下图：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904678491-670fa71a-6703-4251-8658-b91ce80a7909.png" class="" title="image.png"><p>HMM模型是常用的分词模型，基于Python的jieba分词器和基于Java的HanLP分词器都使用了HMM。要注意的是，该模型创建的概率图与上文中的DAG图并不同，因为节点具有观测概率，所以不能再用上文中的算法求解，而应该使用Viterbi算法求解最大概率的路径。</p><h2 id="2-2-2-判别式模型分词算法"><a href="#2-2-2-判别式模型分词算法" class="headerlink" title="2.2.2 判别式模型分词算法"></a><strong>2.2.2 判别式模型分词算法</strong></h2><p>判别式模型主要有感知机、SVM支持向量机、CRF条件随机场、最大熵模型等。在分词中常用的有感知机模型和CRF模型：</p><p><strong>1. 平均感知机分词算法</strong></p><p>感知机是一种简单的二分类线性模型，通过构造超平面，将特征空间（输入空间）中的样本分为正负两类。通过组合，感知机也可以处理多分类问题。但由于每次迭代都会更新模型的所有权重，被误分类的样本会造成很大影响，因此采用平均的方法，在处理完一部分样本后对更新的权重进行平均。</p><p><strong>2. CRF分词算法</strong></p><p>CRF可以看作一个无向图模型，对于给定的标注序列Y和观测序列X，对条件概率P(Y|X)进行定义，而不是对联合概率建模。CRF可以说是目前最常用的分词、词性标注和实体识别算法，它对未登陆词有很好的识别能力，但开销较大。</p><h2 id="2-2-3-神经网络分词算法"><a href="#2-2-3-神经网络分词算法" class="headerlink" title="2.2.3 神经网络分词算法"></a><strong>2.2.3 神经网络分词算法</strong></h2><p>在NLP中，最常用的神经网络为循环神经网络（RNN，Recurrent Neural Network），它在处理变长输入和序列输入问题中有着巨大的优势。LSTM为RNN变种的一种，在一定程度上解决了RNN在训练过程中梯度消失和梯度爆炸的问题。双向（Bidirectional）循环神经网络分别从句子的开头和结尾开始对输入进行处理，将上下文信息进行编码，提升预测效果。</p><p>目前对于序列标注任务，公认效果最好的模型是BiLSTM+CRF。结构如图：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904678602-e3e2a4eb-9cc1-4860-a3e9-8a85c84dc725.png" class="" title="image.png"><p>利用双向循环神经网络BiLSTM，相比于上述其它模型，可以更好的编码当前字等上下文信息，并在最终增加CRF层，核心是用Viterbi算法进行解码，以得到全局最优解，避免B,S,E这种标记结果的出现。</p><h2 id="3-分词算法中的数据结构"><a href="#3-分词算法中的数据结构" class="headerlink" title="3. 分词算法中的数据结构"></a><strong>3. 分词算法中的数据结构</strong></h2><p>前文主要讲了分词任务中所用到的算法和模型，但在实际的工业级应用中，仅仅有算法是不够的，还需要高效的数据结构进行辅助。</p><h2 id="3-1-词典"><a href="#3-1-词典" class="headerlink" title="3.1 词典"></a><strong>3.1 词典</strong></h2><p>中文有7000多个常用字，56000多个常用词，要将这些数据加载到内存虽然容易，但进行高并发毫秒级运算是困难的，这就需要设计巧妙的数据结构和存储方式。前文提到的Trie树只可以在O(n)时间完成单模式匹配，识别出“的确”后到达Trie树对也节点，句子指针接着指向“实”，再识别“实在”，而无法识别“确实”这个词。如果要在O(n)时间完成多模式匹配，构建词图，就需要用到Aho-Corasick算法将模式串预处理为有限状态自动机，如模式串是he/she/his/hers，文本为“ushers”。构建的自动机如图：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904678776-27f40b51-7e97-4280-88e6-d1d3b3760890.png" class="" title="image.png"><p>这样，在第一次到叶节点5时，下一步的匹配可以直接从节点2开始，一次遍历就可以识别出所有的模式串。</p><p>对于数据结构的存储，一般可以用链表或者数组，两者在查找、插入和删除操作的复杂度上各有千秋。在基于Java的高性能分词器HanLP中，作者使用双数组完成了Trie树和自动机的存储。</p><h2 id="3-2-词图"><a href="#3-2-词图" class="headerlink" title="3.2 词图"></a><strong>3.2 词图</strong></h2><p>图作为一种常见的数据结构，其存储方式一般有两种：</p><p><strong>1. 邻接矩阵</strong></p><p>邻接矩阵用数组下标代表节点，值代表边的权重，即d[i][j]=v代表节点i和节点j间的边权重为v。如下图：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904678692-17812548-dbd9-48c2-845e-ed6c8be7330f.png" class="" title="image.png"><p>用矩阵存储图的空间复杂度较高，在存储稀疏图时不建议使用。</p><p><strong>2. 邻接表</strong></p><p>邻接表对图中的每个节点建立一个单链表，对于稀疏图可以极大地节省存储空间。第i个单链表中的节点表示依附于顶点i的边，如下图：</p><img src="/2022/05/30/2022-05-30-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/1653904679088-79db1a87-a3ef-4e53-a1d2-44994b2e4ca5.png" class="" title="image.png"><p>在实际应用中，尤其是用Viterbi算法求解最优路径时，由于是按照广度优先的策略对图进行遍历，最好是使用邻接表对图进行存储，便于访问某个节点下的所有节点。</p><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a><strong>4. 总结</strong></h2><p>分词作为NLP底层任务之一，既简单又重要，很多时候上层算法的错误都是由分词结果导致的。因此，对于底层实现的算法工程师，不仅需要深入理解分词算法，更需要懂得如何高效地实现。而对于上层应用的算法工程师，在实际分词时，需要根据业务场景有选择地应用上述算法，比如在搜索引擎对大规模网页进行内容解析时，对分词对速度要求大于精度，而在智能问答中由于句子较短，对分词的精度要求大于速度。</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>中文分词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>度量学习之损失函数⭐️</title>
    <link href="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E2%AD%90%EF%B8%8F/"/>
    <url>/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E2%AD%90%EF%B8%8F/</url>
    
    <content type="html"><![CDATA[<p>metric learning希望使同源的向量相似度尽可能的高，而非同源的向量相似度尽可能的低，即<strong>类内相近，类间分离</strong>。通过distance metric<strong>引导分类器可以学习到能区分不同类的特征组合</strong>，所以多被用于 CV 的人脸识别，NLP 的语义匹配等。</p><p>在语义模型的训练框架里，Deep Metric Learning 大致可以分为两类：分类和排序。</p><ol><li>采用分类的方法，一般最后一层接的是多类别的softmax，即输入是用户Q，分类结果是所属的标准Q类别。</li><li>排序学习有三种类型：point-wise, pair-wise和list-wise。在QA中我们常用的是point-wise和pair-wise。其中point-wise的方法直接把问题转换成二分类，判断当前用户问题是否属于带匹配的问题，最后根据隶属概率值可以得到问题的排序。而pair-wise学习的是和两两之间的排序关系，训练目标是最大化正样本对和负样本对的距离。</li></ol><div class="table-container"><table><thead><tr><th>分类的方法</th><th>point-wise</th><th>pair-wise</th></tr></thead><tbody><tr><td><img src="度量学习之损失函数⭐️/基于分类的模型结构.png" alt="基于分类的模型结构"></td><td><img src="度量学习之损失函数⭐️/基于point-wise的模型结构.png" alt="基于point-wise的模型结构"></td><td><img src="度量学习之损失函数⭐️/基于pair-wise的模型结构.png" alt="基于pair-wise的模型结构"></td></tr></tbody></table></div><p><strong>对应损失函数的形式大致如下：</strong></p><div class="table-container"><table><thead><tr><th><img src="度量学习之损失函数⭐️/1673405195163.png" alt="image.png"></th><th><img src="度量学习之损失函数⭐️/1673405218698.png" alt="image.png"></th></tr></thead><tbody><tr><td></td></tr></tbody></table></div><h1 id="Ranking-based-Loss"><a href="#Ranking-based-Loss" class="headerlink" title="Ranking-based Loss"></a>Ranking<strong>-based Loss</strong></h1><h2 id="Contrastive-loss"><a href="#Contrastive-loss" class="headerlink" title="Contrastive loss"></a>Contrastive loss</h2><p>想要学习一个pair的相似度，最容易想到的就是把它当作一个分类问题，即同一个人的人脸pair的<strong>欧式距离</strong>d（a0，a1）作为正例（label=1），不同人的人脸pair（a0，b1）为负例（label=0）。那么如果用经典的二元交叉熵损失如下：</p><p>$Loss=-\left(y_{i} \log \hat{y}_{i}+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right)\right)$</p><p>cross entropy loss希望正例趋近于1，负例趋近于0。当y=1时，p=1时loss最小；当y=0时，p=0时loss最小。那么我们可以直接把<a href="https://www.zhihu.com/search?q=cross+entropy&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157495428%7D">cross entropy</a>中的p替换成距离d吗？不能。原因有三：一是d的值域不属于[0,1]；二是我们的任务中希望y=1时距离d越小越好；三是希望y=0时d越大越好。后两点与cross entropy的优化目标正好相反。因此，需要对loss进行一些改进，例如把<a href="https://www.zhihu.com/search?q=log%E5%87%BD%E6%95%B0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157495428%7D">log函数</a>改为平方函数：</p><p>$Loss=yd^2+(1-y)(1-d^2)$</p><p>这样改进之后解决了第二个问题，当y=1时d越小loss越小；但此时y=0时优化的目标是d越接近于1越好，这与我们的任务期待不符，继续改进：</p><p>$Loss=yd^2+(1-y)max(margin-d, 0)^2$</p><p>其中margin是可根据任务调节的超参数。经过这次改进后，问题一和三也被解决。当y=0时，d的优化目标为比margin大，在某种程度上这个loss通过margin参数达到了“类内相近，类间分离”的作用。这个loss即为<a href="https://www.zhihu.com/search?q=contrastive+loss&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157495428%7D">contrastive loss</a>，出自Yann LeCun在2015发表的<a href="https://link.zhihu.com/?target=http%3A//yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">Dimensionality Reduction by Learning an Invariant Mapping</a>，官方的损失公式如下所示：</p><p>$L=\frac{1}{2N} \sum_{n=1}^{N}Y\left(D_{W}\right)^{2}+(1-Y)\left\{\max \left(0, margin-D_{W}\right)\right\}^{2}$</p><p>$D_{W}\left(\vec{X}_{1}, \vec{X}_{2}\right)=\left|G_{W}\left(\vec{X}_{1}\right)-G_{W}\left(\vec{X}_{2}\right)\right|_{2}=\sqrt{\sum_{i=1}^{n}\left(x_{1i}-x_{2i}\right)^{2}}$</p><p>从拓扑的观点来看，Contrastive Loss使得网络学习到一种映射关系(神经网络或转换矩阵)，把<strong>向量从原始空间映射到新的空间从而使得向量在新的空间有更好的拓扑性质</strong>，即<strong>类内尽可能紧凑类间尽可能分离</strong>。</p><p><img src="度量学习之损失函数⭐️/v2-7c277d3e03c9813d6332fb9024a782bc_1440w.jpg" alt="Network"></p><p><strong>Contrastive loss的缺点</strong>：尽管它很受欢迎，但在大多数检索任务(通常用作基线)中，这种对比性损失的表现很不起眼。大多数高级损失需要一个三元组$(x_i,x_j,x_k)$，其中$(x_i,x_j)$属于同一类，$(x_i,x_k)$属于不同类。这种三元组样本在无监督学习中很难获得。因此，尽管对比损失在检索方面的表现不佳，但在无监督学习和自监督学习中仍普遍使用。</p><p><a href="https://blog.csdn.net/nima1994/article/details/83862502">代码参考1：</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">contrastive_loss</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;Contrastive loss from Hadsell-et-al.&#x27;06</span><br><span class="hljs-string">    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    margin = <span class="hljs-number">1</span><br>    sqaure_pred = K.square(y_pred)<br>    margin_square = K.square(K.maximum(margin - y_pred, <span class="hljs-number">0</span>))<br>    <span class="hljs-keyword">return</span> K.mean(y_true * sqaure_pred + (<span class="hljs-number">1</span> - y_true) * margin_square)<br><br>model.<span class="hljs-built_in">compile</span>(loss=contrastive_loss, ...)<br></code></pre></td></tr></table></figure><p><a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/losses/metric_learning/contrastive_loss">代码参考2：</a></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">tf<span class="hljs-selector-class">.contrib</span><span class="hljs-selector-class">.losses</span><span class="hljs-selector-class">.metric_learning</span><span class="hljs-selector-class">.contrastive_loss</span>(<br>    labels, embeddings_anchor, embeddings_positive, <span class="hljs-attribute">margin</span>=<span class="hljs-number">1.0</span><br>)<br></code></pre></td></tr></table></figure><h2 id="Triplet-loss"><a href="#Triplet-loss" class="headerlink" title="Triplet loss"></a>Triplet loss</h2><blockquote><p>参考资料：<br><a href="http://daniel-at-world.blogspot.com/2019/07/implementing-triplet-loss-function-in.html">Implementing Triplet Loss Function in Tensorflow 2.0</a><br><a href="https://blog.csdn.net/qq_36387683/article/details/83583099">Tensorflow实现Triplet Loss</a><br><a href="https://zhuanlan.zhihu.com/p/295512971">深度学习从入门到放飞自我：完全解析triplet loss</a><br><a href="https://omoindrot.github.io/triplet-loss">Triplet Loss and Online Triplet Mining in TensorFlow</a><br><a href="https://www.zhihu.com/question/365370142">triplet loss稳定在margin附近?—hardTri &amp; l2_normalize</a><br><a href="https://bindog.github.io/blog/2019/10/23/why-triplet-loss-works/">为什么triplet loss有效？从直观上说明为什么triplet loss不稳定?</a></p></blockquote><p>contrastive loss是一个严格的loss，它要求正例距离趋近于0，负例距离大于margin。然而，有一些距离较近的负例，它们的正例也较近；有一些距离较远的负例，它们的正例也较远，统<strong>一对待这两种情况，模型可能无法很好的训练</strong>。考虑一个正脸的数据集里有一张稍微偏一些的侧脸，如果这张侧脸图片作为contrastive loss中的a0，它的pair的正例/负例距离明显大于其它pair的正例/负例距离。如果它的优化目标和其它pair一样，正例趋近于0，负例大于margin，是不合理的。那么是否有一种没有这么严格的loss呢？有，triplet loss。</p><p><img src="度量学习之损失函数⭐️/1639413138897.png" alt="tri.png"></p><p>Triplet loss 出自2014 年的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1503.03832.pdf">FaceNet</a> 论文，经常用在人脸识别任务中，目的是学习度量嵌入（to learn a metric embedding）。它的输入是一个三元组（anchor，positive，negative），具体的loss function为：</p><p>$\mathcal{L}_{\mathrm{tri}}^{m}\left(x, x^{+}, x^{-} ; f\right)=\max \left(0,\left|f-f^{+}\right|_{2}^{2}-\left|f-f^{-}\right|_{2}^{2}+m\right)$</p><p>triplet loss的优化目标就是让Anchor与负例的欧式比Anchor与正例的距离大至少一个margin。还是以上述的侧脸图片为例，侧脸图片作为anchor时，triplet loss不要求正例趋近于0，负例大于margin，而是负例&gt;正例+margin，这就解决了刚才的问题。<strong>当dis(A, N) – dis(A, P) &lt; margin时，loss会大于0。</strong></p><p>Triplet Loss 训练过程动图：</p><p><img src="度量学习之损失函数⭐️/tri.gif" alt=""></p><p><strong>Triplet loss 的不足之处：</strong></p><ul><li>Triplet loss 对噪声数据很敏感，因此随机负采样会影响其性能； </li><li>Triplet loss 的实现不是很简单，比较tricky的地方是如何计算embedding的距离，以及怎样识别并抛弃掉invalid和easy triplet（需要较好的采样策略）； </li><li>Triplet loss 训练过程不稳定，收敛速度慢，需要极大的耐心去调参； </li><li><strong> </strong><a href="https://bindog.github.io/blog/2019/10/23/why-triplet-loss-works/"><strong>来分析一下为什么单纯使用triplet loss效果不好</strong></a><strong>，我们对比softmax的损失函数以及triplet loss上界版本的损失函数不难发现：softmax损失函数和triplet loss上界版本中，每一个batch的loss都能够兼顾全局的信息，并进行权重更新，这一点能够保证整个训练过程相对平滑稳定。其中softmax是天然如此，而triplet loss上界版则是通过引入centroid实现的。反观原始的triplet loss的形式，每个batch所涉及和更新的信息是非常局限的(只含2个类别)，如果不能设计合理的采样和训练策略，很容易出现的一种情况是某个类别的embedding分布不稳定、出现突变和跃迁，导致训练反复、难以收敛。 </strong></li></ul><p>分析<strong>triplet loss</strong>在<strong> hard negtive </strong>下坍塌问题</p><blockquote><p>问题描述：<br>Triplet models are susceptible to mapping each input to the same point. When this happens, the distances in (*) go to zero the loss gets stuck at a and the model is basically done updating. Semi-hard negative mining can also help prevent this from happening. In my experience, the loss tending towards a is a clear signal that training isn’t working as desired and the embeddings are not informative. You can check whether this is the case by examining the embedding vectors: if the classes tend to be close together, there’s a problem.</p></blockquote><p>分析：</p><ul><li>坍塌问题（collapse issue），这意味着所有的句子都倾向于编码到一个较小的局部区域内</li><li><a href="https://www.zhihu.com/question/322954623/answer/1135390437">为什么triplet loss选择hard negative会导致collapsed models? - 八月的回答 - 知乎</a></li><li>当模型有了坍塌的倾向时，最终loss趋近于margin，近乎于停止更新</li></ul><p><img src="度量学习之损失函数⭐️/1659942647929.png" alt="image.png"></p><p>使用 offline 离线挖掘策略的Triplet loss实现：</p><p><strong>代码参考</strong>1：<a href="https://zhangruochi.com/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/">Create a Siamese Network with Triplet Loss in Keras</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">alpha = <span class="hljs-number">0.2</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">triplet_loss</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:<span class="hljs-number">2</span>*emb_size], y_pred[:,<span class="hljs-number">2</span>*emb_size:]<br>    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=<span class="hljs-number">1</span>)<br>    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> tf.maximum(positive_dist - negative_dist + alpha, <span class="hljs-number">0.</span>)<br></code></pre></td></tr></table></figure><p><strong>代码参考</strong>2：<a href="https://medium.com/@crimy/one-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352">One Shot learning, Siamese networks and Triplet Loss with Keras</a></p><blockquote><p>The triplet loss function, implemented as a custom Keras layer</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TripletLossLayer</span>(<span class="hljs-title class_ inherited__">Layer</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, alpha, **kwargs</span>):<br>        self.alpha = alpha<br>        <span class="hljs-built_in">super</span>(TripletLossLayer, self).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">triplet_loss</span>(<span class="hljs-params">self, inputs</span>):<br>        a, p, n = inputs<br>        p_dist = K.sqrt(K.<span class="hljs-built_in">sum</span>(K.square(a - p), axis=-<span class="hljs-number">1</span>))<br>        n_dist = K.sqrt(K.<span class="hljs-built_in">sum</span>(K.square(a - n), axis=-<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> K.mean(K.maximum(p_dist - n_dist + self.alpha, <span class="hljs-number">0</span>), axis=<span class="hljs-number">0</span>) + <span class="hljs-number">1e-9</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs</span>):<br>        loss = self.triplet_loss(inputs)<br>        self.add_loss(loss)<br>        <span class="hljs-keyword">return</span> loss<br><br>margin=<span class="hljs-number">0.2</span><br>loss_layer = TripletLossLayer(alpha=margin)([encoded_a,encoded_p,encoded_n])<br>network_train = Model(inputs=[anchor_input,positive_input,negative_input],outputs=loss_layer)<br></code></pre></td></tr></table></figure><p><strong>代码参考</strong>3：tensorflow-addons 的<a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/losses/metric_learning/triplet_semihard_loss">Triplet loss实现</a></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import tensorflow_addons as tfa<br><br>model<span class="hljs-selector-class">.compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>              loss=tfa<span class="hljs-selector-class">.losses</span><span class="hljs-selector-class">.TripletSemiHardLoss</span>(),<br>              metrics=<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;accuracy&#x27;</span>]</span>)import tensorflow_addons as tfa<br><br>tfa<span class="hljs-selector-class">.losses</span><span class="hljs-selector-class">.metric_learning</span><span class="hljs-selector-class">.triplet_semihard_loss</span>(<br>    labels, embeddings, <span class="hljs-attribute">margin</span>=<span class="hljs-number">1.0</span><br>)<br></code></pre></td></tr></table></figure><p>使用online挖掘的策略的Triplet loss实现：</p><ul><li><a href="https://github.com/eroj333/learning-cv-ml/blob/master/SNN/Online%20Triplet%20Mining.ipynb">https://github.com/eroj333/learning-cv-ml/blob/master/SNN/Online Triplet Mining.ipynb</a></li><li>PyTorch已经集成进去了<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.TripletMarginLoss">TripletMarginLoss</a></li></ul><h2 id="Lifted-Structured-Loss"><a href="#Lifted-Structured-Loss" class="headerlink" title="Lifted Structured Loss"></a>Lifted Structured Loss</h2><p><strong>Lifted Structured Loss</strong> (<a href="https://arxiv.org/abs/1511.06452">Song et al. 2015</a>) 利用一个训练批次中的所有成对边来提高计算效率。</p><p><img src="度量学习之损失函数⭐️/1639475408589.png" alt="image.png"></p><p><img src="度量学习之损失函数⭐️/1673406211010.png" alt="image.png"></p><h2 id="N-Pairs-Loss"><a href="#N-Pairs-Loss" class="headerlink" title="N-Pairs Loss"></a>N-Pairs Loss</h2><blockquote><p><a href="https://link.zhihu.com/?target=https%3A//papers.nips.cc/paper/6200-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective.pdf">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</a></p></blockquote><p>contrastive loss和triplet loss存在着比如收敛慢，陷入局部最小值等问题，相当部分原因就是因为loss function仅仅只使用了一个negative样本，在每次更新时，与其他的negative的类没有交互。因此，<strong>Multi-Class N-pair loss</strong> (<a href="https://papers.nips.cc/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html">Sohn 2016</a>) 提出了一个考虑多个negative样本的方法：即从（N-1）个negative样本中区分一个positive样本，当N=2时，即是triplet loss。训练样本为 $\left\{x, x^{+}, x_{1}, \cdots, x_{N-1}\right\}$： $x^{+}$ 是一个positive样本， $\left\{x_{i}\right\}_{i=1}^{N-1}$ 是（N-1）个negative 样本。</p><p><img src="度量学习之损失函数⭐️/1663316290728-e31c0de8-bb8b-44e7-9b52-f580706ebdc3.png" alt="image.png"></p><p>Triplet loss在将positive样本拉近的同时一次只能推离一个negative样本；而(N+1)-tuplet loss基于样本之间的相似性，一次可以将（N-1）个negative样本推离。另外，对比损失和三元组损失都利用<strong>欧氏距离</strong>来量化点之间的相似性。而 N-Pairs损失利用<strong>余弦相似度</strong>来量化点之间的相似度，由于余弦相似度度量（以及概率）是尺度不变的（如下图所示），N-pair loss 往往对训练过程中的特征变化具有鲁棒性。<strong>Multi-Class N-pair loss</strong> 公式如下：</p><p>$\begin{aligned}<br>\mathcal{L}_{\mathrm{tri}}^{m}\left(x, x^{+}, x^{-} ; f\right)&amp;=\max \left(0,\left|f-f^{+}\right|_{2}^{2}-\left|f-f^{-}\right|_{2}^{2}+m\right) \\<br>&amp;→Euclidean Distance→Cosine Similar,m=0 \\<br>&amp;=\max (0,f^T f_i-f^T f^{+}) \\<br>&amp;→softplus(x)=log(1+e^x)≈max(0,x) \\<br>&amp;=log (1+exp(f^T f_i-f^T f^{+})) \\<br>\end{aligned}$</p><p>扩展到(N-1)个negative样本：</p><p><img src="度量学习之损失函数⭐️/image-20230424125145643.png" alt="image-20230424125145643"></p><p>N-pairs的直觉是利用batch中的所有负样本来指导梯度更新，从而加速收敛。N-pairs损失通常优于三元组损失，而且没什么要注意的东西。训练batch的大小的上限是由训练类的数量决定的，因为每个类只允许有一个正样本对。相比之下，三元组损失和对比损失batch的大小仅受GPU内存的限制。此外，N-pairs损失学习了一个没有归一化的嵌入。这有两个结果：(1)不同类之间的边界是用角度来定义的，(2)可以避免退化的嵌入增长到无限大，一个正则化器，来约束嵌入空间，是必需的。</p><p><a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/losses/metric_learning/npairs_loss">代码参考：</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import tensorflow_addons as tfa<br><br>tf.contrib.losses.metric_learning.npairs_loss(<br>    labels, embeddings_anchor, embeddings_positive, <span class="hljs-attribute">reg_lambda</span>=0.002,<br>    <span class="hljs-attribute">print_losses</span>=<span class="hljs-literal">False</span><br>)<br></code></pre></td></tr></table></figure><h2 id="Constellation-Loss"><a href="#Constellation-Loss" class="headerlink" title="Constellation Loss"></a>Constellation Loss</h2><blockquote><p>论文题目：Constellation Loss: Improving the efficiency of deep metric learning loss functions for optimal embedding<br>Source code is available at: <a href="https://git.code.tecnalia.com/comvis_public/piccolo/ constellation_loss/">https://git.code.tecnalia.com/comvis_public/piccolo/ constellation_loss/</a></p></blockquote><p>论文主要贡献：把Triplet loss和Multiclass-N-pair loss的函数结合起来，构造了一个新的损失函数，在一个大肠癌组织切片的数据集上获得了比Triplet loss和Multiclass-N-pair loss更好的分类效果。</p><div class="table-container"><table><thead><tr><th><img src="度量学习之损失函数⭐️/1664185635777-cd0979ec-38ba-42f9-80e7-f6f44f2d763a.png" alt="image.png"></th><th><img src="度量学习之损失函数⭐️/1664185654433-67c1dd15-3a63-4448-bb4f-3feb17dab9de.png" alt="image.png"></th></tr></thead><tbody><tr><td><img src="度量学习之损失函数⭐️/1664185671969-c0e37e09-2705-44c5-9f03-48fd5d81344e.png" alt="image.png"></td></tr></tbody></table></div><p>Triplet loss的损失函数：$\mathcal{L}_{\text {triplet}}=\frac{1}{N} \sum_{i=1}^{N} \max \left(0,\left|f_{i}^{a}-f_{i}^{p}\right|_{2}^{2}-\left|f_{i}^{a}-f_{i}^{n}\right|_{2}^{2}+\alpha\right)$</p><p>Multiclass-N-pair loss损失函数：$\mathcal{L}_{m-c}=\frac{1}{N} \sum_{i=1}^{N} \log \left(1+\sum_{j \neq i} \exp \left(f_{i}^{\top} f_{j}^{+}-f_{i}^{\top} f_{i}^{+}\right)\right)$</p><p>Constellation Loss损失函数为：$\mathcal{L}_{\text {constellation}}=\frac{1}{N} \sum_{i=1}^{N} \log \left(1+\sum_{j}^{K} \exp \left(f_{i}^{a \top} f_{j}^{n}-f_{i}^{a \top} f_{i}^{p}\right)\right)$</p><p>可以看到论文提出的损失函数可以获得更好的聚类效果：</p><p><img src="度量学习之损失函数⭐️/1664185677581.png" alt="image.png"></p><h2 id="NCE"><a href="#NCE" class="headerlink" title="NCE"></a>NCE</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/334772391">Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE</a>（一堆公式推导….）<br><a href="https://zhuanlan.zhihu.com/p/133137765">Noise Constractive Estimation 噪声对比估计—知乎</a><br><a href="https://link.zhihu.com/?target=https%3A//ruder.io/word-embeddings-softmax/index.html%23fn16">On word embeddings - Part 2: Approximating the Softmax - Ruder</a><br><a href="https://www.zhihu.com/question/50043438/answer/833138812">知乎麋路的回答 - 求通俗易懂解释下nce loss？</a><br><a href="https://spaces.ac.cn/archives/5617">“噪声对比估计”杂谈：曲径通幽之妙</a>—苏剑林</p></blockquote><p>NCE，也就是 Noise Contrastive Estimation（噪声对比估计），由<a href="http://proceedings.mlr.press/v9/gutmann10a.html">Gutmann &amp; Hyvarinen</a>在 2010 年提出，将概率估计问题转化为二分类问题，用二分类的最大似然估计替代原始问题。</p><p>NCE 的<strong>核心思想就是通过学习数据分布样本和噪声分布样本之间的区别，从而发现数据中的一些特性</strong>，因为这个方法需要依靠与噪声数据进行对比，所以称为“噪声对比估计（Noise Contrastive Estimation）”。更具体来说，NCE 通过<strong>逻辑回归</strong>将问题转换成了一个二分类问题，分类器能够对数据样本和噪声样本进行二分类，利用<strong>已知的</strong>噪声概率分布，来估计<strong>未知的</strong>经验概率分布。</p><blockquote><p>（以下内容主要参考：<a href="https://mp.weixin.qq.com/s/QlrxIZ8wNjmcFVoB78l9ag">https://mp.weixin.qq.com/s/QlrxIZ8wNjmcFVoB78l9ag</a>）</p></blockquote><p>以语言模型举例。现在假设一个特定上下文$c$的数据分布为$\hat{p}(w|c)$—未知，我们称从它里面取出的样本为正样本，令类别$D=1$; 而另一个与$c$无关的噪声分布为$q(w)$, 我们称从里面取出的样本为负样本,令类别为$D=0$。遵循 Gutmann and Hyvrinen (2012) 中的设置, 假设现在取出了$k_d$个正样本和$k_n$个负样本, 将这些正负样本混合形成一个混合分布$p(w|c)$。</p><p>我们得到下面这些概率:</p><p><img src="度量学习之损失函数⭐️/1673427906232.png" alt="image.png"><br>所以可以计算后验概率:</p><p><img src="度量学习之损失函数⭐️/1673427918475.png" alt="image.png"></p><p>我们<strong>令负样本和正样本的比例为</strong>$k=\frac{k_n}{k_d}$，则有：</p><p><img src="度量学习之损失函数⭐️/1673427954652.png" alt="image.png"></p><p>现在我们观察 (12) 式, NCE 所做的事情就是将式中的经验分布$\hat{p}(w|c)$替换成概率模型$p_\theta(w|c)$, 使后验概率成为参数为$\theta$的函数。……我们直接令$p_\theta(w|c)=u_\theta(w|c)$，那么 (12) 式可以写成如下形式：</p><p><img src="度量学习之损失函数⭐️/1673428432184.jpeg" alt="640.jpeg"></p><p>现在我们有了参数为$\theta$的二元分类问题, 假设标签$D_t$为伯努利分布, 那么很容易写出他的条件对数似然$L_{NCE}^c$如下, 实际上在它前面加上负号后,  $-L_{NCE}^c$也就等价于 logistics 分类里的 log loss，或者说交叉嫡损失函数：</p><p><img src="度量学习之损失函数⭐️/1673428524765.png" alt="image.png"></p><p>而 <strong>NCE 的目标函数</strong>还需要在 (14) 式的基础上除以正样本的数量$k_d$，即</p><p><img src="度量学习之损失函数⭐️/1673428567722.png" alt="image.png"></p><p>当数据数量很大时，根据大数定律，上式也可以写成：</p><p><img src="度量学习之损失函数⭐️/1673428577748.png" alt="image.png"></p><p>要最大化上述对数似然函数，也就是最大化如下目标函数：</p><p><img src="度量学习之损失函数⭐️/1673428594681.png" alt="image.png"></p><p>通过 NCE 转换后的优化目标，本质上就是对极大似然估计方法的一种近似，并且随着负样本和正样本数量比$k$的增大，这种近似越精确，这也解释了为什么作者建议我们将$k$设置的越大越好。</p><h2 id="InfoNCE"><a href="#InfoNCE" class="headerlink" title="InfoNCE"></a><strong>InfoNCE</strong></h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/506544456?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=30098367447040">对比学习损失（InfoNCE loss）与交叉熵损失的联系，以及温度系数的作用</a></p></blockquote><p><strong>InfoNCE</strong> 也被称为 <strong>NTXentLoss</strong>，是 NPairs Loss 的一般形式，被广泛用于自监督学习中。IInfoNCE 继承了 NCE 的基本思想，升级为使用分类交叉熵损失来识别一组不相关的噪声样本中的正样本，并且证明了减小这个损失函数相当于增大互信息 (mutual information) 的下界，这也是名字infoNCE的由来。具体细节这里不再赘述，感兴趣的读者可以参考这篇文章：<a href="http://karlstratos.com/notes/nce.pdf">http://karlstratos.com/notes/nce.pdf</a>，里面有比较清晰的介绍与推导。</p><p><img src="度量学习之损失函数⭐️/1654703845131.png" alt="InfoNCE loss"></p><p>其中 $u$、$v^+$、$u^-$ 分别为原样例、正样例、负样例归一化后的表示，$t$ 为温度超参。</p><p>显而易见，infoNCE最后的形式就是多元分类任务常见的交叉熵损失（Cross Entropy Loss for N-way softmax classifier)，使用分类交叉熵损失在一组负样本中识别正样本。因为表示已经归一化，据前所述，向量内积等价于向量间的距离度量。故由softmax的性质，上述损失就可以理解为，我们希望在拉近原样例与正样例距离的同时，拉远其与负样例间的距离，这正是对比学习的思想。</p><h3 id="1、InfoNCE和交叉熵损失的关系？"><a href="#1、InfoNCE和交叉熵损失的关系？" class="headerlink" title="1、InfoNCE和交叉熵损失的关系？"></a><a href="https://zhuanlan.zhihu.com/p/506544456?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=30098367447040"><strong>1、InfoNCE和交叉熵损失的关系？</strong></a></h3><p>我们先从softmax说起，下面是softmax公式：</p><p><img src="度量学习之损失函数⭐️/1654704667033-2a22f4c5-ac55-4e2c-82a6-a326aa9e14f7.svg" alt=""></p><p>交叉熵损失函数如下：</p><p><img src="度量学习之损失函数⭐️/1654704667128-c331f528-468d-44f2-8ecc-c9144ec2024b.svg" alt=""></p><p>在有监督学习下，ground truth是一个one-hot向量，softmax的结果$\hat{y}_+$取$-log$，再与ground truth相乘之后，即得到如下交叉熵损失：</p><p><img src="度量学习之损失函数⭐️/1654704667038-25ea9038-4faf-477e-b88b-7b6104ff520e.svg" alt=""></p><p>上式中的 k 在有监督学习里指的是这个数据集一共有多少类别，比如CV的ImageNet数据集有1000类，k就是1000。</p><p><strong>对于对比学习来说，理论上也是可以用上式去计算loss，但是实际上是行不通的。为什么呢？</strong></p><p>还是拿ImageNet数据集来举例，该数据集一共有128万张图片，我们使用数据增强手段（例如，随机裁剪、随机颜色失真、随机高斯模糊）来产生对比学习正样本对，每张图片就是单独一类，那k就是128万类，而不是1000类了，有多少张图就有多少类。但是softmax操作在如此多类别上进行计算是非常耗时的，再加上有指数运算的操作，当向量的维度是几百万的时候，计算复杂度是相当高的。所以对比学习用上式去计算loss是行不通的。</p><p><strong>怎么办呢？NCE loss可以解决这个问题。</strong></p><p>NCE（noise contrastive estimation）核心思想是将多分类问题转化成二分类问题，一个类是数据类别 data sample，另一个类是噪声类别 noisy sample，通过学习数据样本和噪声样本之间的区别，将数据样本去和噪声样本做对比，也就是“噪声对比（noise contrastive）”，从而发现数据中的一些特性。但是，如果把整个数据集剩下的数据都当作负样本（即噪声样本），虽然解决了类别多的问题，计算复杂度还是没有降下来，解决办法就是做负样本采样来计算loss，这就是estimation的含义，也就是说它只是估计和近似。一般来说，负样本选取的越多，就越接近整个数据集，效果自然会更好。</p><p><strong>有了NCE loss，为什么还要用Info NCE loss呢？</strong></p><p>Info NCE loss是NCE的一个简单变体，它认为如果你只把问题看作是一个二分类，只有数据样本和噪声样本的话，可能对模型学习不友好，因为很多噪声样本可能本就不是一个类，因此还是把它看成一个多分类问题比较合理（但这里的多分类 k 指代的是负采样之后负样本的数量，下面会解释）。于是就有了InfoNCE loss，公式如下：</p><p><img src="度量学习之损失函数⭐️/1654704667637-a3cf5d01-7812-4c21-b823-183bb03e458c-20230424151358274.svg" alt=""></p><p>上式中，$q·k$是模型出来的logits，相当于上文softmax公式中的$z$，$τ$是一个温度超参数，是个标量，假设我们忽略$τ$，那么infoNCE loss其实就是cross entropy loss。唯一的区别是，在cross entropy loss里，k指代的是数据集里类别的数量，而在对比学习InfoNCE loss里，这个k指的是负样本的数量。上式分母中的sum是在1个正样本和k个负样本上做的，从0到k，所以共k+1个样本，也就是字典里所有的key。恺明大佬在MoCo里提到，InfoNCE loss其实就是一个cross entropy loss，做的是一个k+1类的分类任务，目的就是想把 q 这个图片分到$k_+$这个类。</p><p>另外，我们看下图中MoCo的伪代码，MoCo这个loss的实现就是基于cross entropy loss。</p><p><img src="度量学习之损失函数⭐️/1654704669092.png" alt="image.png"></p><h3 id="2、温度系数的作用"><a href="#2、温度系数的作用" class="headerlink" title="2、温度系数的作用"></a><a href="https://zhuanlan.zhihu.com/p/506544456?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=30098367447040"><strong>2、温度系数的作用</strong></a></h3><p>温度系数$τ$虽然只是一个超参数，但它的设置是非常讲究的，直接影响了模型的效果。 上式Info NCE loss中的$q·k$相当于是logits，温度系数可以用来控制logits的分布形状。对于既定的logits分布的形状，当$τ$值变大，则$1 / τ$就变小，$q·k / τ$则会使得原来logits分布里的数值都变小，且经过指数运算之后，就变得更小了，导致原来的logits分布变得更平滑。相反，如果$τ$取得值小，$1 / τ$就变大，原来的logits分布里的数值就相应的变大，经过指数运算之后，就变得更大，使得这个分布变得更集中，更peak。</p><p>如果温度系数设的越大，logits分布变得越平滑，那么对比损失会对所有的负样本一视同仁，导致模型学习没有轻重。如果温度系数设的过小，则模型会越关注特别困难的负样本，但其实那些负样本很可能是潜在的正样本，这样会导致模型很难收敛或者泛化能力差。</p><p>总之，温度系数的作用就是它控制了模型对负样本的区分度。</p><h2 id="Decoupled-Contrastive-Learning"><a href="#Decoupled-Contrastive-Learning" class="headerlink" title="Decoupled Contrastive Learning"></a>Decoupled Contrastive Learning</h2><p>Decoupled Contrastive Learning (Yann LeCun 2021) 通过分析对比学习中广泛采用的InfoNCE损失，在梯度中确定了一个负-正耦合(NPC)乘数qB，该系数导致模型训练的梯度产生了放缩，使得目前的自监督方法严重依赖于大 Batch Size</p><p>由此修改了InfoNCE的公式消除qB的影响，去除分母中的正例对，得到解耦对比学习损失：</p><p>$\begin{aligned}<br>\mathcal{L}_{\text {DC }}&amp;=\mathbb{E}[\mathcal{L}_{DC,i}] \\<br>\mathcal{L}_{DC,i}&amp;=-\log \frac{ f(\mathbf{x}, \mathbf{c})}{\cancel{ {f(\mathbf{x}, \mathbf{c})}} + \sum_{\mathbf{x}^{\prime} \in X,x≠x^{\prime}} f\left(\mathbf{x}^{\prime}, \mathbf{c}\right)} \\<br>&amp;=-\log f(\mathbf{x}, \mathbf{c})+\log \sum_{\mathbf{x}^{\prime} \in X,x≠x^{\prime}} f\left(\mathbf{x}^{\prime}, \mathbf{c}\right) \\<br>\mathcal{L}_{DC,i}^{(k)}&amp;=-\log \frac{\exp \left(\left\langle\mathbf{z}_{i}^{(1)}, \mathbf{z}_{i}^{(2)}\right\rangle / \tau\right)}{\cancel{\exp \left(\left\langle\mathbf{z}_{i}^{(1)}, \mathbf{z}_{i}^{(2)}\right\rangle / \tau\right)}+\sum_{l \in\{1,2\}, j \in[1, N], j \neq i} \exp \left(\left\langle\mathbf{z}_{i}^{(k)}, \mathbf{z}_{j}^{(l)}\right\rangle / \tau\right)} \\<br>&amp;=-\left\langle\mathbf{z}_{i}^{(1)}, \mathbf{z}_{i}^{(2)}\right\rangle / \tau+\log \sum_{l \in\{1,2\}, j \in[1, N], j \neq i} \exp \left(\left\langle\mathbf{z}_{i}^{(k)}, \mathbf{z}_{j}^{(l)}\right\rangle / \tau\right)<br>\end{aligned}$</p><h2 id="Angular-Loss-√"><a href="#Angular-Loss-√" class="headerlink" title="Angular Loss √"></a><strong>Angular Loss √</strong></h2><p><strong>Angular loss解决了三元组损失的两个限制</strong>。首先，三元组损失假设在不同类别之间有固定的margin _m_。固定的margin是不可取的，因为不同的类有不同的类内变化，如下图所示：</p><p><img src="度量学习之损失函数⭐️/9ab16fdee73b871d2789ff87cbe7dcfd.jpeg" alt=""></p><p>第二个限制是三元组损失是如何产生负样本的梯度的。下图显示了为什么<strong>负梯度的方向可能不是最佳的</strong>，也就是说，不能保证远离正样本的类中心。</p><p><img src="度量学习之损失函数⭐️/cee0aa68e0234cf7984ef3693ec48598.png" alt=""></p><p>为了解决这两个限制，作者建议使用n的角度代替margin m，并在负样本点$x_n$处纠正梯度。<strong>不是基于距离把点往远处推，目标是最小化角度n</strong>，即，使三角形a-n-b在n点处的角度更小。下一个图说明angular loss的公式将负样本点$x_n$推离$x_c$，$x_c$为由$x_a$和$x_p$定义的局部簇的中心。另外，锚点$x_a$和正样本点$x_p$被彼此拖向对方。</p><p><img src="度量学习之损失函数⭐️/ecccb2781dc9b2553d5d893ca607c88c.png" alt=""></p><p>与原来的三元组损耗只依赖于两点(例如 $grad = x_a - x_n$)相比，angular loss的梯度要稳健得多，因为它们同时考虑了所有三点。另外，请注意，与基于距离的度量相比，操纵角度$n ‘$不仅是旋转不变的，而且本质上也是尺度不变的。<strong>我的一些建议: N-pairs和Angular loss通常优于原始的三元组损失。然而，在比较这些方法时，需要考虑一些重要的参数。</strong></p><ol><li>用于训练三元组损失的采样策略会导致显著的性能差异。如果避免了模型崩溃，困难样本挖掘是有效的，并且收敛速度更快。</li><li>训练数据集的性质是另一个重要因素。当进行行人重识别或人脸聚类时，我们可以假设每个类由单个簇表示，即具有小的类内变化的单一模式。然而，一些检索数据集，如CUB-200-2011和Stanford Online Products有很多类内变化。根据经验，hard triplet loss在人/人脸再识别任务中工作得更好，而N-pairs和 Angular losses在CUB-200和Stanford Online Product数据集上工作得更好。</li><li>当使用一个新的检索任务和调整一个新的训练数据集的超参数(学习率和batch_size)时，我发现semi-hard三元组损失是最稳定的。它没有达到最好的性能，但它是最不可能退化的。</li></ol><h2 id="SupConLoss"><a href="#SupConLoss" class="headerlink" title="SupConLoss"></a>SupConLoss</h2><blockquote><p><a href="https://arxiv.org/abs/2004.11362">Supervised Contrastive Learning</a>.<br><a href="https://zhuanlan.zhihu.com/p/143443691">https://zhuanlan.zhihu.com/p/143443691</a></p></blockquote><ul><li><p>文章借鉴并改进<a href="https://www.zhihu.com/search?q=self-supervised+learning&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A143443691%7D">self-supervised learning</a>的方法来解supervised 的问题。相较于传统的cross entropy损失函数提升了一个点。并且模型更加robustness和stable。</p></li><li><p>文章主要的创新点在于利用已有的label信息来将自监督的损失函数（如公式1所示）改造成支持<a href="https://www.zhihu.com/search?q=multiple+positives&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A143443691%7D">multiple positives</a> 和 multiple negatives（如公式2所示）。</p></li></ul><p><img src="度量学习之损失函数⭐️/1639464948149-56dfe4e1-8489-479d-9610-9d18bfc1d1f7.svg" alt=""> </p><p><img src="度量学习之损失函数⭐️/1639464948204-3a037890-59f9-408a-ab43-0c1791ca56de.svg" alt=""> </p><ul><li>作者通过梯度计算的角度说明了文中提出的loss可以更好地关注于 hard positives and negatives，从而获得更好的效果。</li></ul><p><img src="度量学习之损失函数⭐️/1639464948717.png" alt="image.png"><br>Figure 1: Cross entropy, self-supervised contrastive loss and supervised contrastive loss.</p><ul><li>如图1所示，对每一幅图像使用两种随机的不同的augmentations，这样就有了2N幅图像作为一个batch。Supervised Contrastive的训练过程包括以下两步：<ul><li>首先，<a href="https://www.zhihu.com/search?q=%E9%9A%8F%E6%9C%BAsample&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A143443691%7D">随机sample</a>训练样本，使用文中提出的Supervised Contrastive Learning训练；</li><li>第二步，固定representation部分的参数，使用cross-entropy训练分类器部分。（如果只需要获取embedding，不需要做分类的话，不需要执行这一步。）</li></ul></li></ul><h1 id="Classification-based-Loss"><a href="#Classification-based-Loss" class="headerlink" title="Classification-based Loss"></a><strong>Classification-based Loss</strong></h1><blockquote><p>将距离度量问题作为分类问题和验证问题——&gt; softmax及其变种损失函数Margin-based Softmax</p></blockquote><p>上一部分介绍了metric learning loss function中的 Contrastive loss。除此之外，还有以softmax多分类为代表的一系列classification-based loss。</p><p>softmax 主要有两个缺点：</p><ul><li>第一点是softmax函数在训练过程中无法同时做到类内相近，类间分离这一目标；</li><li>第二点是数据不平衡问题。</li></ul><p><img src="度量学习之损失函数⭐️/1639392604352.png" alt="image.png"><br>softmax函数没有类内相近，类间分离的约束条件</p><p>近年来，面部识别领域的主要技术进展集中在<strong>如何改进softmax的损失</strong>，使得既能充分利用其易于优化，收敛快的优良性质，又使得其能优化出一个具有优良泛化性的度空间。而这些技术改进主要又能被归为两大类别：</p><ul><li>引入 margin，达到了类间分离的目的，使得 Softmax Loss 能学习到更具有区分性的 metric 空间。</li><li>归一化（Normalization）。一个类别具有的样本数越大，相关的<a href="https://www.zhihu.com/search?q=%E6%9D%83%E9%87%8D%E8%8C%83%E6%95%B0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157561982%7D">权重范数</a>往往越大，通过归一化能够减少长尾数据造成的类间不平衡问题（目前主流的方法中，归一化+伸缩系数s基本成为标配）</li></ul><p><strong>先总结几个重要的点：</strong></p><div class="table-container"><table><thead><tr><th>L-Softmax</th><th>首次提出angular margin的概念，重新思考$w*x$，引入cos角，认为各类之间的夹角需要有个margin</th><th><img src="度量学习之损失函数⭐️/image-20230424153842581.png" alt="image-20230424153842581"></th></tr></thead><tbody><tr><td>A-Softmax</td><td>将weight归一化，使得特征上的点映射到单位超球面上</td><td><img src="度量学习之损失函数⭐️/image-20230424153854792.png" alt="image-20230424153854792"></td></tr><tr><td>AM-Softmax</td><td>将角度上的乘性关系改为cos值的加性关系+特征/权重归一化</td><td><img src="度量学习之损失函数⭐️/image-20230424153911064.png" alt="image-20230424153911064"></td></tr><tr><td>CosFace</td><td>同AM-Softmax</td><td><img src="度量学习之损失函数⭐️/image-20230424153925342.png" alt="image-20230424153925342"></td></tr><tr><td>Arcface</td><td>将margin由cos外移到内</td><td><img src="度量学习之损失函数⭐️/image-20230424153935636.png" alt="image-20230424153935636"></td></tr></tbody></table></div><h2 id="Normalized-Softmax-Loss"><a href="#Normalized-Softmax-Loss" class="headerlink" title="Normalized Softmax Loss"></a>Normalized Softmax Loss</h2><blockquote><p><a href="https://arxiv.org/pdf/1811.12649.pdf">Classification is a Strong Baseline for Deep Metric Learning</a></p></blockquote><p>softmax可以起到放大 $x$ 的作用，使模型训练和收敛更容易。例如 x = [4,6]，如果直接算概率，p = [0.4,0.6]；如果用<a href="https://www.zhihu.com/search?q=softmax%E5%85%AC%E5%BC%8F&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157561982%7D">softmax公式</a>计算，p = [0.12, 0.88]。通过softmax的放大，模型不需要把类别概率分的很开就可以得到比较小的loss，这样有利于模型收敛。Normalized Softmax Loss 公式如下：</p><p><img src="度量学习之损失函数⭐️/1673406919436.png" alt="image.png"></p><ul><li>Regularize embedding space to hyphersphere (通过正则化，嵌入超平面/球面)</li><li>Temperature scaling to enforce compact intra-class clusters (加入温度超参缩放，使得类内紧凑)</li></ul><h2 id="L-Softmax-Loss"><a href="#L-Softmax-Loss" class="headerlink" title="L-Softmax Loss"></a><a href="https://arxiv.org/abs/1612.02295">L-Softmax Loss</a></h2><blockquote><p><a href="https://arxiv.org/pdf/1612.02295.pdf">Large-Margin Softmax Loss for Convolutional Neural Networks</a></p></blockquote><p>2016年ICML的一篇论文 L-Softmax Loss 首次在softmax上引入了 <strong>margin</strong> 的概念，具有非常重大的意义。下面给出 L-softmax loss的定义：</p><p>$L_{i}=-\log \left(\frac{e^{\left|\boldsymbol{W}_{y_{i}}\right|\left|\boldsymbol{x}_{i}\right| \psi\left(\theta_{y_{i}}\right)}}{e^{\left|\boldsymbol{W}_{y_{i}}\right|\left|\boldsymbol{x}_{i}\right| \psi\left(\theta_{y_{i}}\right)}+\sum_{j \neq y_{i}} e^{\left|\boldsymbol{W}_{j}\right|\left|\boldsymbol{x}_{i}\right| \cos \left(\theta_{j}\right)}}\right)$</p><p>where</p><p>$\begin{array}{c}<br>\psi(\theta)=\left\{\begin{array}{cc}<br>\cos (m \theta), &amp; 0 \leq \theta \leq \frac{\pi}{m} \\<br>D(\theta), &amp; \frac{\pi}{m}&lt;\theta \leq \pi<br>\end{array}\right.<br>\end{array}$</p><p>其中 m 是整数，它决定类内的聚拢程度，能够使对应类别的夹角扩大m倍。m 值越大则分类边缘越大，同时学习目标就越难。那么为什么会有这种效果呢？个人理解，以前分类的类内角度搜索范围是 $θ∈[0,π]$，在加了 m以后，它的范围缩小到 $\frac{\theta}{m} \in\left[0, \frac{\pi}{m}\right]$。因此类内间距就变小了。</p><p>对于增加 margin 的形象解释，文章给出了一个很好的示意图（类内距离尽可能小，类间距离尽可能大）。</p><p><img src="度量学习之损失函数⭐️/1639394295007.png" alt="image.png"></p><p>$\theta_{1(2)}$ 表示特征 x 和类权重 $W_{1(2)}$ 的夹角。</p><p>先简化问题成一个二元分类的例子，假设类权重被归一化了，此时夹角就决定了样本x被分到哪一类。左边是原始的softmax loss，分界面是在两类别的中间$\theta_{1}=\theta_{2}$ ，此时（训练）样本紧贴着分界面。测试的时候，就容易混淆了。右边是L-Softmax，为了在两类中间留下空白（margin），要求分界面是 $m\theta_{1}=\theta_{2}, m&gt;1$ 。此时为了分类正确，样本特征会被压缩到一个更小的空间，两个类别的分类面也会被拉开。容易看出，此时两个类之间的 angular decision margin 是 $\frac{m-1}{m+1} \theta_{1,2}$，其中 $\theta_{1,2}$ 是类权重$W_1$和$W_2$的夹角。</p><h2 id="A-softmax-Loss"><a href="#A-softmax-Loss" class="headerlink" title="A-softmax Loss"></a><a href="https://arxiv.org/abs/1704.08063">A-softmax Loss</a></h2><blockquote><p><a href="https://www.kesci.com/mw/project/5fc1bde465710400309fcf5d">深度学习—A-softmax原理+代码</a><br><a href="https://arxiv.org/pdf/1704.08063.pdf">SphereFace: Deep Hypersphere Embedding for Face Recognition</a></p></blockquote><p>2017 年 CVPR 的 _[SphereFace]_ 在 L-Softmax 的基础上引入了<strong>权重归一化 (weight normalization)</strong>，对权值 w 进行了单位化，并将 bias 置零，即$||W||=1$ 和 $b=0$，这样判别条件 $\left|w_{1}\right||x| \cos \left(m \theta_{1}\right)&gt;\left|w_{2}\right||x| \cos \left(\theta_{2}\right)$ <strong>仅由角度(angular)距离决定</strong>。</p><blockquote><p>为什么要添加 |w|=1 的约束呢？ 作者做了两方面的解释，一个是softmax loss学习到的特征，本来就依据角度有很强的区分度，另一方面，人脸是一个比较规整的流形，将其特征映射到超平面表面也好解释。</p></blockquote><p>这样便得到了angular softmax loss，简称A-softmax loss。_[SphereFace]_ 提出的 loss 的具体形式是：</p><p>$L=\frac{1}{N} \sum_{i}-\log \left(\frac{e^{\left|\mathbf{x}_{i}\right| \cdot \cos \left(m \cdot \theta_{y_{i}, i}\right)}}{e^{\left|\mathbf{x}_{i}\right| \cdot \cos \left(m \cdot \theta_{y_{i}, i}\right)}+\sum_{j \neq y_{i}} e^{\left|\mathbf{x}_{i}\right| \cdot \cos \theta_{j, i}}}\right)$</p><p>_[SphereFace]_ 作者通过一个很形象的特征分布图，展示了引入 margin 的效果，可见，随着 margin 的增加，类内被压缩的更紧凑，类间的界限也变得更加清晰了。</p><p><img src="度量学习之损失函数⭐️/1639387025330.png" alt="image.png"></p><p>此外，在归一化方向，王峰大佬发表的 NormFace 论文提出 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.09507.pdf">L2-softmax</a>，进一步对特征向量做 L2 归一化，即feature normalization，数学上的工作非常漂亮。而且发现了feature normalization后网络难以收敛的问题，提出需要在 _L_2 超球面嵌入后引入尺度因子的办法来解决这一问题。此外，作者还解释了 _L_2 归一化在难例挖掘和处理类不均衡问题上的作用。_L_2 超球面嵌入目前已经成为业内的标准做法。 </p><p><strong>代码实现：苏剑林 </strong><a href="https://github.com/bojone/margin-softmax/blob/master/margin_softmax.py">bojone/margin-softmax</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sparse_simpler_asoftmax_loss</span>(<span class="hljs-params">y_true, y_pred, scale=<span class="hljs-number">30</span></span>):<br>    y_true = K.expand_dims(y_true[:, <span class="hljs-number">0</span>], <span class="hljs-number">1</span>) <span class="hljs-comment"># 保证y_true的shape=(None, 1)</span><br>    y_true = K.cast(y_true, <span class="hljs-string">&#x27;int32&#x27;</span>) <span class="hljs-comment"># 保证y_true的dtype=int32</span><br>    batch_idxs = K.arange(<span class="hljs-number">0</span>, K.shape(y_true)[<span class="hljs-number">0</span>])<br>    batch_idxs = K.expand_dims(batch_idxs, <span class="hljs-number">1</span>)<br>    idxs = K.concatenate([batch_idxs, y_true], <span class="hljs-number">1</span>)<br>    y_true_pred = K.tf.gather_nd(y_pred, idxs) <span class="hljs-comment"># 目标特征，用tf.gather_nd提取出来</span><br>    y_true_pred = K.expand_dims(y_true_pred, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 用到了四倍角公式进行展开</span><br>    y_true_pred_margin = <span class="hljs-number">1</span> - <span class="hljs-number">8</span> * K.square(y_true_pred) + <span class="hljs-number">8</span> * K.square(K.square(y_true_pred))<br>    <span class="hljs-comment"># 下面等效于min(y_true_pred, y_true_pred_margin)</span><br>    y_true_pred_margin = y_true_pred_margin - K.relu(y_true_pred_margin - y_true_pred)<br>    _Z = K.concatenate([y_pred, y_true_pred_margin], <span class="hljs-number">1</span>) <span class="hljs-comment"># 为计算配分函数</span><br>    _Z = _Z * scale <span class="hljs-comment"># 缩放结果，主要因为pred是cos值，范围[-1, 1]</span><br>    logZ = K.logsumexp(_Z, <span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>) <span class="hljs-comment"># 用logsumexp，保证梯度不消失</span><br>    logZ = logZ + K.log(<span class="hljs-number">1</span> - K.exp(scale * y_true_pred - logZ)) <span class="hljs-comment"># 从Z中减去exp(scale * y_true_pred)</span><br>    <span class="hljs-keyword">return</span> - y_true_pred_margin * scale + logZ<br></code></pre></td></tr></table></figure><h3 id="乘性-margin-的弊端"><a href="#乘性-margin-的弊端" class="headerlink" title="乘性 margin 的弊端"></a>乘性 margin 的弊端</h3><p>总之，A-Softmax 对分类权重进行归一化，将偏差归零，并引入可以用参数_m_控制的角边距来学习具有判别性和清晰几何解释的特征。但引入 margin 之后，有一个很大的问题，网络的训练变得非常非常困难。在  _[SphereFace]_ 中提到需要组合退火策略等极其繁琐的训练技巧。这导致这种加 margin 的方式极其不实用。而事实上，这一切的困难，都是因为引入的 margin 是<strong>乘性 margin </strong>造成的。<strong>我们来分析一下，乘性 margin 到底带来的麻烦是什么：</strong></p><ol><li>第一点，乘性 margin 把 cos 函数的单调区间压小了，导致<strong>优化困难</strong>。对 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387024312-d49336b5-c87e-4510-915b-4d8d41aff6ed.svg#clientId=ua89f5579-b5c3-4&amp;id=DSaes&amp;originHeight=24&amp;originWidth=70&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u1d4fea74-61bd-4a85-9fb1-bf8cdcfa840&amp;title=" alt=""> ，在 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387024512-85a526b5-e628-4b0b-8242-2c384baa8805.svg#clientId=ua89f5579-b5c3-4&amp;id=veMCx&amp;originHeight=23&amp;originWidth=30&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u3bd99e47-1906-410a-ba54-20cb7484e37&amp;title=" alt=""> 处在区间 [0,π] 时，是一个单调函数，也就是说 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387024382-726b501d-b92c-45a4-877d-0991ea94d124.svg#clientId=ua89f5579-b5c3-4&amp;id=qqhkA&amp;originHeight=23&amp;originWidth=30&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u1de36c29-4522-4c22-9c8c-2f0bd2b7cf6&amp;title=" alt=""> 落在这个区间里面的任何一个位置，网络都会朝着把 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387024390-0c55deed-ed78-48f5-a3b1-4afbc0dee269.svg#clientId=ua89f5579-b5c3-4&amp;id=bRsGr&amp;originHeight=23&amp;originWidth=30&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=ud29b4ddc-42df-45f7-88e2-c80d6abcadf&amp;title=" alt=""> 减小的方向优化。但加上乘性 margin m 后 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387024882-814b2218-1e86-4147-b606-a7b9f5fa742a.svg#clientId=ua89f5579-b5c3-4&amp;id=aPYGV&amp;originHeight=24&amp;originWidth=70&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u834aa2a4-c199-4a31-8eb7-7cf8a958045&amp;title=" alt=""> 的单调区间被压缩到了 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387024980-f22b8a1b-d567-420f-8112-5d7ecc156246.svg#clientId=ua89f5579-b5c3-4&amp;id=Re3M1&amp;originHeight=37&amp;originWidth=51&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=uf095645d-55d9-4521-bd48-65b5a130967&amp;title=" alt=""> ，那如果恰巧有一个 sample 的 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387025204-1a99050e-775f-4a82-8db9-3c1660e20ab7.svg#clientId=ua89f5579-b5c3-4&amp;id=sgvID&amp;originHeight=23&amp;originWidth=30&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u3b038794-8b1d-4690-b62d-24e2995b325&amp;title=" alt=""> 落在了这个单调区间外，那网络就很难优化了；</li><li>第二点，乘性 margin 所造成的 <strong>margin 实际上是不均匀</strong>的，依赖于 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387025490-a74dbcab-bf76-4a50-9394-ba83723e89c7.svg#clientId=ua89f5579-b5c3-4&amp;id=ctCGi&amp;originHeight=24&amp;originWidth=47&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u0900b080-51ff-44a6-a1de-6a1457b0dd0&amp;title=" alt=""> 的夹角。前面我们已经分析了，两个 class 之间的 angular decision margin <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387025571-dc483b7b-2a3b-46a6-975b-685f305a6427.svg#clientId=ua89f5579-b5c3-4&amp;id=vSStV&amp;originHeight=43&amp;originWidth=82&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u429b6597-f459-4e60-b650-829c970d21d&amp;title=" alt=""> ，其中 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387025806-8652436d-84a8-43a2-8dce-51b80a043ee0.svg#clientId=ua89f5579-b5c3-4&amp;id=x7s4Y&amp;originHeight=23&amp;originWidth=27&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u15df1e60-3ead-4604-a014-b828d63d63c&amp;title=" alt="">是两个 class 的 weight 的夹角。这自然带来一个问题，如果这两个 class 本身挨得很近，那么他们的 margin 就小。特别是两个难以区分的 class，可能它们的 weight 挨得特别近，也就是 <img src="https://cdn.nlark.com/yuque/0/2021/svg/8420697/1639387025804-192787ee-93cb-4478-a98a-0125aa84e6be.svg#clientId=ua89f5579-b5c3-4&amp;id=WYhn6&amp;originHeight=23&amp;originWidth=27&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u478e544d-17f6-4e0f-a5e7-cb8695e3021&amp;title=" alt=""> 几乎接近 0，那么按照乘性 margin 的方式，计算出的两个类别的间隔也是接近 0 的。换言之，乘性 margin 对易于混淆的 class 不具有可分性。</li></ol><h2 id="AM-softmax-Loss"><a href="#AM-softmax-Loss" class="headerlink" title="AM-softmax Loss"></a><a href="https://arxiv.org/abs/1801.05599">AM-softmax Loss</a></h2><blockquote><p>论文：<a href="https://arxiv.org/abs/1801.05599">Additive Margin Softmax for Face Verification</a><br>注：腾讯AI Lab的 <strong>CosFace</strong>: Large Margin Cosine Loss for Deep Face Recognition和AM-Softmax算法基本一致，工作也几乎是同时完成，两篇论文也都各自中了不同的会议。</p></blockquote><p>为了解决上述提到的两个乘性 margin 的弊端，AM-softmax（additive margin softmax loss）提出了一种加性间隔margin策略，对Softmax损失的目标logit进行特征和权重归一化。 </p><ol><li>相比于A-softmax只能施加不固定的角间距，它则施加<strong>固定的角间距</strong>。</li><li>将L-softmax和A-softmax中的乘性margin改为加性margin，即 $cos(m\theta)$ 改成 $cos(\theta)-m$，同时加上了尺度因子s，使得前向后向传播变得更加简单，性能也更好。</li></ol><p>最后，AM-Softmax 损失可以定义如下。</p><p>$L_i=-\log \left(\frac{e^{s \cdot\left(\cos \theta_{t}-m\right)}}{e^{s \cdot\left(\cos \theta_{t}-m\right)}+\sum_{i \neq t} e^{s \cdot \cos \theta_{i}}}\right)$</p><p>其中，$θ_i$代表$z,ci$的夹角。</p><p>在 AM-Softmax 原论文中，所使用的是 $s=30,m=0.35$。s的存在是必要的，因为cos函数本身是有界的，范围是[−1,1]，如果进行softmax，那么概率其实会很均衡，比如[1, 0, 0, -1]的softmax是[0.53444665, 0.19661193, 0.19661193, 0.07232949]，也就是说目标概率很难达到1，loss就降不下去，所以需要做好比例缩放，才允许pt能足够接近于1（有必要的话）。当然，s并不改变相对大小，因此这不是核心改变，核心是原来应该是$cos⁡θ_t$的地方，换成了$cosθ_t−m$加强了条件，迫使类内差距更小，类间差距更大。决策边界为：</p><p><img src="度量学习之损失函数⭐️/1645600791211-a9b31d3c-6ec3-4cf5-858f-796c638efed1.png" alt="image.png"></p><p>几何解释：<br><img src="度量学习之损失函数⭐️/1645600193929-46099e79-a355-4809-88c6-afe642732166.png" alt="image.png"></p><p>为了更好地可视化 AM-Softmax 损失的效果，使用 7 层 CNN 模型和 Fashion MNIST 数据集将其与其他损失进行了比较。CNN 模型的 3 维特征输出被归一化并绘制在一个超球体（球）中，如下所示。从可视化中，您可以看到 AM-Softmax 在对输出进行聚类时的表现与 SphereFace (A-Softmax) 相似，并且随着边距的增加，m 越大越好。<br><img src="度量学习之损失函数⭐️/1639387026193-df53b33b-3739-4163-bf85-a18f93d6f145.jpeg" alt="超球面几何解释[来源]"></p><p>总之，L-Softmax、A-Softmax 和 AM-Softmax 损失都试图通过在 Softmax 损失中引入一个边距来结合分类和度量学习，旨在最大化类之间的距离并增加相同类之间的紧凑性。在这三者中，AM-Softmax 被证明可以最大程度地提高模型性能，特别是在用于人脸验证的 LFW 和 MegaFace 数据集中。</p><p><strong>代码实现：苏剑林 </strong><a href="https://github.com/bojone/margin-softmax/blob/master/margin_softmax.py">bojone/margin-softmax</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">amsoftmax_loss</span>(<span class="hljs-params">y_true, y_pred, scale=<span class="hljs-number">30</span>, margin=<span class="hljs-number">0.35</span></span>):<br>    y_pred = y_true * (y_pred - margin) + (<span class="hljs-number">1</span> - y_true) * y_pred<br>    y_pred *= scale<br>    <span class="hljs-keyword">return</span> K.categorical_crossentropy(y_true, y_pred, from_logits=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><strong>Sparse版代码实现：苏剑林 </strong><a href="https://github.com/bojone/margin-softmax/blob/master/margin_softmax.py">bojone/margin-softmax</a></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">def sparse<span class="hljs-constructor">_amsoftmax_loss(<span class="hljs-params">y_true</span>, <span class="hljs-params">y_pred</span>, <span class="hljs-params">scale</span>=30, <span class="hljs-params">margin</span>=0.35)</span>:<br>    y_true = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>expand<span class="hljs-constructor">_dims(<span class="hljs-params">y_true</span>[:, 0], 1)</span> # 保证y_true的shape=(None, <span class="hljs-number">1</span>)<br>    y_true = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>cast(y_true, &#x27;<span class="hljs-built_in">int32</span>&#x27;) # 保证y_true的dtype=<span class="hljs-built_in">int32</span><br>    batch_idxs = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>arange(<span class="hljs-number">0</span>, <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>shape(y_true)<span class="hljs-literal">[<span class="hljs-number">0</span>]</span>)<br>    batch_idxs = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>expand<span class="hljs-constructor">_dims(<span class="hljs-params">batch_idxs</span>, 1)</span><br>    idxs = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>concatenate(<span class="hljs-literal">[<span class="hljs-identifier">batch_idxs</span>, <span class="hljs-identifier">y_true</span>]</span>, <span class="hljs-number">1</span>)<br>    y_true_pred = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>tf.gather<span class="hljs-constructor">_nd(<span class="hljs-params">y_pred</span>, <span class="hljs-params">idxs</span>)</span> # 目标特征，用tf.gather_nd提取出来<br>    y_true_pred = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>expand<span class="hljs-constructor">_dims(<span class="hljs-params">y_true_pred</span>, 1)</span><br>    y_true_pred_margin = y_true_pred - margin # 减去margin<br>    _Z = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>concatenate(<span class="hljs-literal">[<span class="hljs-identifier">y_pred</span>, <span class="hljs-identifier">y_true_pred_margin</span>]</span>, <span class="hljs-number">1</span>) # 为计算配分函数<br>    _Z = _Z<span class="hljs-operator"> * </span>scale # 缩放结果，主要因为pred是cos值，范围<span class="hljs-literal">[-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]</span><br>    logZ = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>logsumexp(_Z, <span class="hljs-number">1</span>, keepdims=True) # 用logsumexp，保证梯度不消失<br>    logZ = logZ + <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>log(<span class="hljs-number">1</span> - <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">K</span>.</span></span>exp(scale<span class="hljs-operator"> * </span>y_true_pred - logZ)) # 从Z中减去exp(scale<span class="hljs-operator"> * </span>y_true_pred)<br>    return - y_true_pred_margin<span class="hljs-operator"> * </span>scale + logZ<br></code></pre></td></tr></table></figure><p>PS：为什么改为加性 margin 依然有效果？（来自我的推导）</p><p><img src="度量学习之损失函数⭐️/1639453666077.jpeg" alt="WechatIMG57.jpeg"></p><h2 id="AAM-softmax-Loss"><a href="#AAM-softmax-Loss" class="headerlink" title="AAM-softmax Loss"></a>AAM-softmax Loss</h2><blockquote><p><a href="https://arxiv.org/abs/1801.07698">Additive Angular Margin Loss for Deep Face Recognition</a></p></blockquote><p>与AM-softmax类似，<strong>CVPR 2018 提出的 Arcface </strong>也是加性margin，差别只是 _[ArcFace]_ 的 margin 加在 Cos 算子的里面，而 _[AM-Softmax]_ 的 margin 在加性算子的外面。目前classification loss中结果最好的loss应该就是arcface了。Arcface的公式为：</p><p>$L_{3}=-\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{s\left(\cos \left(\theta_{y_{i}}+m\right)\right)}}{e^{s\left(\cos \left(\theta_{y_{i}}+m\right)\right)}+\sum_{j=1, j \neq y_{i}}^{n} e^{s \cos \theta_{j}}}$</p><p>式中，s代表比例缩放scale超参数，m代表间隔 margin。</p><p>在 _[ArcFace]_ 中，作者对集中加 margin 的方式做了很形象的对比，如下图所示。可以看出，_[ArcFace]_ 提出的 margin 更符合“角度”margin 的概念，而 _[CosFace]_ 或是 _[AM-Softmax]_ 更符合 Cosine margin 的概念。</p><p><img src="度量学习之损失函数⭐️/1639387027393.png" alt="image.png"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>最后，我们总结一下加 margin 的几种 Softmax 的几种形式：<br><img src="度量学习之损失函数⭐️/1639387028039.png" alt="image.png"></p><p>到这里问题还远远没有结束，现存的问题有：</p><ol><li>在归一化技巧下，noisy sample 对网络的负面干扰也被放大，如何削弱其影响值得进一步思索；</li><li>即使做了 weight 归一化，长尾问题也只是得到一定的缓解，不平衡的问题依然存在；</li><li>增加 margin 虽然让网络学到了更好的度量空间，但引入的超参到底怎么样才是最优的选项？</li></ol><p>这些问题依然还没有被很好解决。</p><h1 id="Circle-loss：基于统一视角泛化损失函数"><a href="#Circle-loss：基于统一视角泛化损失函数" class="headerlink" title="Circle loss：基于统一视角泛化损失函数"></a><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.10857.pdf">Circle loss</a>：基于统一视角泛化损失函数</h1><blockquote><p>1、CVPR2020的新作<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.10857.pdf">circle loss</a>结合使用 softmax loss 和 <a href="https://www.zhihu.com/search?q=contrastive+loss&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157737051%7D">contrastive loss</a>。<br><a href="https://www.bilibili.com/read/cv7441815">直播回顾 | 旷视 Circle Loss：弥合割裂、统一视角的新型深度特征学习方法</a><br>2、作者：biendata <a href="https://www.bilibili.com/read/cv7441815">https://www.bilibili.com/read/cv7441815</a> 出处：bilibili<br>3、<a href="https://spaces.ac.cn/archives/7359/comment-page-1">将“softmax+交叉熵”推广到多标签分类问题</a></p></blockquote><ul><li><strong>以往</strong><a href="https://www.zhihu.com/search?q=metric+learning+loss&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157737051%7D">metric learning loss</a><strong>的问题</strong></li></ul><p>无论是contrast loss和classification loss，它们的核心都是一个目标：<strong>类内相近，类间分离</strong>。体现到<a href="https://www.zhihu.com/search?q=loss+function&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157737051%7D">loss function</a>上，就是最大化类内相似度sp，同时最小化类间相似度sn，优化的目标是最小化二者的差值（sn-sp）。<strong>然而，这样的优化方式是不够灵活的，每个相似度应当根据其当前优化状态给予不同的优化权重。</strong>如果相似性得分偏离最佳值，则应受到严重惩罚。否则，如果<a href="https://www.zhihu.com/search?q=%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%88%86%E6%95%B0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157737051%7D">相似性分数</a>已接近最佳值，则应进行适度优化。因此，可以把优化目标改进为 $a_ns_n-a_ps_p$ ，其中αn和αp是独立的加权因子，从而允许sn和sp以不同的速度学习。对应到图b，决策边界变成了一个圆形，点A和点B的优化方向都趋近于T，这一改进解决了上述两个问题。</p><p><img src="度量学习之损失函数⭐️/1677568475138.png" alt="image.png"></p><ul><li><strong>pairwise loss和classification loss的统一形式</strong></li></ul><p>对于样本x，假设有K个类内相似度 $s_p^i(i=1,2,…K)$  以及L个类间相似度 $s_n^j(i=1,2,…L)$，则统一的优化目标为：</p><p><img src="度量学习之损失函数⭐️/1639411191116-c2ad8c62-c94b-4673-969f-9b5bf824d8f6.svg" alt=""></p><p>其中 $\gamma$ 是伸缩系数，m是margin。这个公式的本质是什么呢？我们需要知晓两个数学上的近似函数，即LogSumExp和Softplus（参考<a href="https://zhuanlan.zhihu.com/p/45014864">王峰：从最优化的角度看待Softmax损失函数</a>）：</p><p><img src="度量学习之损失函数⭐️/1639411191111-eca05b7d-e0af-43de-8524-77db52039021.svg" alt=""></p><p><img src="度量学习之损失函数⭐️/1639411191715-c21268f3-7924-4a89-98ea-46db68b55d43.svg" alt=""></p><p><img src="度量学习之损失函数⭐️/1639411191796-75a681c2-5c5c-4aa1-9bef-2889b50bfda0.svg" alt=""></p><p>把这三个近似变换公式替换到 L_uni：</p><p><img src="度量学习之损失函数⭐️/1639411191807-d06e689e-dc90-4005-a643-7e87bf1bfbca.svg" alt=""></p><p>经过近似以后，可以看出优化目标是使最小的sp比最大的sn还大margin，和我们期待的优化目标是相符的。而且，L_uni是一个通用loss，可以通过简单变换得到常见的pairwise loss和classification loss，例如AM-softmax loss和<a href="https://www.zhihu.com/search?q=triplet+loss&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157737051%7D">triplet loss</a>：</p><p>$\begin{aligned}\mathcal{L}_{a m} &amp;=\log \left[1+\sum_{j=1}^{N-1} \exp \left(\gamma\left(s_{n}^{j}+m\right)\right) \exp \left(-\gamma s_{p}\right)\right] \\&amp;=-\log \frac{\exp \left(\gamma\left(s_{p}-m\right)\right)}{\exp \left(\gamma\left(s_{p}-m\right)\right)+\sum_{j=1}^{N-1} \exp \left(\gamma s_{n}^{j}\right)}<br/>\end{aligned}$</p><p><img src="度量学习之损失函数⭐️/1639411192007-9c4ad1eb-14e3-4ee0-b356-028246ccf6b8.svg" alt=""></p><ul><li><strong>circle loss</strong></li></ul><p>准备工作已经做完了，我们已经获得了pairwise loss和classification loss的统一形式，也知道了这些loss存在的问题，接下来就在L_uni的基础上进一步改进吧。首先，暂时不考虑margin，并加入自适应的 $a_n$ 和  $a_p$：</p><p><img src="度量学习之损失函数⭐️/1639411192567-c5547d68-6df3-445a-b332-a4dd54472475.svg" alt=""></p><p>假设sp的最优值是Op，sn的最优值为On，并定义：</p><p><img src="度量学习之损失函数⭐️/1639411192735-c6c2aa50-db84-490c-a612-b831da97ccd1.svg" alt=""></p><p>这个 $a_n$ 和  $a_p$的取值也容易理解：越接近最优值，它的优化力度越小；反之，优化力度越大。接下来，想办法加入刚才忽略的margin。如果模仿之前sn与sp对称时的loss，则只需要把margin加到sn这一项就可以了，但是circle loss中sn和sp不对称，因此需要对sn和sp设置不同的margin： </p><p><img src="度量学习之损失函数⭐️/1639411193081-9fadfcc1-b05e-49ea-abe1-9f0ab99d920d.svg" alt=""></p><p>最后，关于Circle Loss 可以总结为三点：</p><ol><li>提出了一个统一的优化视角，来理解主流的损失函数；</li><li>Circle Loss使用完全相同的公式，在两种基本学习方式中都获得了极具竞争力的表现。</li><li>在一系列常见任务中，Circle Loss取得了稳定的提升。 </li></ol><h1 id="附录-1：Other-names-used-for-Ranking-Losses"><a href="#附录-1：Other-names-used-for-Ranking-Losses" class="headerlink" title="附录 1：Other names used for Ranking Losses"></a>附录 1：Other names used for Ranking Losses</h1><blockquote><p><a href="https://gombru.github.io/2019/04/03/ranking_loss/">https://gombru.github.io/2019/04/03/ranking_loss/</a></p></blockquote><p>Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.</p><ul><li><strong>Ranking loss</strong>: This name comes from the information retrieval field, where we want to train models to <strong>rank</strong> items in an specific order.</li><li><strong>Margin Loss</strong>: This name comes from the fact that these losses use a margin to compare samples representations distances.</li><li><strong>Contrastive Loss</strong>: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for Pairwise Ranking Loss, but I’ve never seen using it in a setup with triplets.</li><li><strong>Triplet Loss</strong>: Often used as loss name when triplet training pairs are employed.</li><li><strong>Hinge loss</strong>: Also known as <strong>max-margin objective</strong>. It’s used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That’s why this name is sometimes used for Ranking Losses.</li></ul><h1 id="附录-2：Ranking-Loss-Layers-in-TF-PyTorch"><a href="#附录-2：Ranking-Loss-Layers-in-TF-PyTorch" class="headerlink" title="附录 2：Ranking Loss Layers in TF/PyTorch"></a>附录 2：Ranking Loss Layers in TF/PyTorch</h1><p><strong>PyTorch</strong></p><ul><li><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CosineEmbeddingLoss">CosineEmbeddingLoss</a>. It’s a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it’s a positive or a negative pair, and the margin.</li><li><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.MarginRankingLoss">MarginRankingLoss</a>. Similar to the former, but uses euclidian distance.</li><li><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.TripletMarginLoss">TripletMarginLoss</a>. A Triplet Ranking Loss using euclidian distance.</li></ul><p><strong>TensorFlow</strong></p><ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/losses/metric_learning/contrastive_loss">contrastive_loss</a>. Pairwise Ranking Loss.</li><li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/losses/metric_learning/triplet_semihard_loss">triplet_semihard_loss</a>. Triplet loss with semi-hard negative mining.</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://jishuin.proginn.com/p/763bfbd3b598">*如何用深度学习来做检索：度量学习中关于排序损失函数的综述</a></p><p><a href="https://flashgene.com/archives/151266.html">【深度度量学习系列】Triplet-loss原理与应用</a></p><p><a href="https://ahmdtaha.medium.com/retrieval-with-deep-learning-a-ranking-loss-survey-part-1-8e88a6f8e091">Retrieval with Deep Learning: A Ranking loss Survey Part 1</a></p><p><a href="https://ahmdtaha.medium.com/retrieval-with-deep-learning-a-ranking-loss-survey-part-2-df7e7a5d584d">Retrieval with Deep Learning: A Ranking-Losses Survey Part 2</a></p><p><a href="https://zhuanlan.zhihu.com/p/82199561">深度度量学习中的损失函数</a></p><p><a href="https://zhuanlan.zhihu.com/p/94596648?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=30098367447040&amp;utm_campaign=shareopn">*策略算法工程师之路-损失函数设计</a></p><p><a href="https://kexue.fm/archives/5743">基于GRU和am-softmax的句子相似度模型</a></p><p><a href="https://zhuanlan.zhihu.com/p/358570091">一文弄懂各种loss function</a></p><p><a href="http://pelhans.com/2019/04/15/deep_learning-note5/">深度学习笔记（五）常见损失函数</a></p><p><a href="https://zhuanlan.zhihu.com/p/76391405">*人脸识别中Softmax-based Loss的演化史</a></p><p><a href="https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b">Additive Margin Softmax Loss (AM-Softmax)</a></p><p><a href="https://aclanthology.org/2020.repl4nlp-1.12.pdf">A Metric Learning Approach to Misogyny Categorization - rep4nlp2020</a></p><p><a href="https://zhuanlan.zhihu.com/p/157495428">Loss Function of Metric Learning（上）</a><br><a href="https://zhuanlan.zhihu.com/p/157561982">Loss Function of Metric Learning（中）</a><br><a href="https://zhuanlan.zhihu.com/p/157737051">Loss Function of Metric Learning（下）</a></p>]]></content>
    
    
    <categories>
      
      <category>度量学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>度量学习之采样Sampling</title>
    <link href="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%87%87%E6%A0%B7Sampling/"/>
    <url>/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%87%87%E6%A0%B7Sampling/</url>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.zhihu.com/search?q=metric+learning&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">metric learning</a>中采样方法也同样重要。甚至在<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.07567.pdf">Sampling Matters in Deep Embedding Learning</a>中，作者指出在metric learning中，采样方法比损失函数具有同等或更重要的作用。</p><h1 id="为什么要采样"><a href="#为什么要采样" class="headerlink" title="为什么要采样"></a>为什么要采样</h1><p>以triplet loss为例，它的输入为（anchor，positive，negative）。如果有一个人脸训练集，共m个人（m=10000），每个人的人脸图片有n张（n=100），那么所有可能的<a href="https://www.zhihu.com/search?q=triplet+pair&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">triplet pair</a>为 $100×99×999$ 个（假设anchor固定，positive有99个选择，negative有9999个选择）。如果这些pair全参与训练，则<a href="https://www.zhihu.com/search?q=%E5%A4%8D%E6%9D%82%E5%BA%A6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">复杂度</a>为O（mn^2），显然是不可行的。考虑<a href="https://www.zhihu.com/search?q=classification+loss&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">classification loss</a>，如果每一张人脸图片都参与训练，那么共有 $10000×100$ 张训练图片，此时复杂度是O（mn）；如果以人为单位，每个epoch随机从100张里面抽一张人脸作为这个人的训练图片，那么每个epoch的训练集为10000张，此时复杂度为O（m）。因此需要找一种对标classification loss的方法，当遍历所有图片（以图片为单位）时，为每个图片找到合适的triplet pair，此时复杂度为O（mn）；当以人为单位时，为每个人找到合适的triplet pair，此时复杂度为O（m)。为了叙述清晰，统一以后一种作为目标。</p><p>为了完成上面说的目标，我们需要进行采样，包括：</p><ul><li><strong>随机采样（Global）</strong></li><li><strong>(Semi-)Hard-negatives 采样（Local）</strong></li><li><strong>distance weighted 采样</strong></li></ul><h1 id="1-随机采样"><a href="#1-随机采样" class="headerlink" title="1.随机采样"></a>1.随机采样</h1><p>对于某一个人，先随机选一张这个人的人脸图片作为anchor，再在这个人的其它99张人脸图片里面随机选一张图片作为positive，再随机选一张其他人的人脸图片作为negative。目标达到了吗？达到了。还有其它问题吗？有，<strong>margin</strong>。</p><p>写到这里就得回顾一下metric learning中的loss function了。metric learning的所有损失函数都有一个包含margin的<a href="https://www.zhihu.com/search?q=max%E5%87%BD%E6%95%B0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">max函数</a>，用来达到类内相近，类间分离的目标。这个max函数有个特性，就是如果已经满足了公式，那么它是不参与梯度回传的。例如triplet loss：</p><p><img src="https://www.zhihu.com/equation?tex=Loss+%3D+max%28d%28a%2Cp%29-d%28a%2Cn%29%2Bmargin%2C+0%29%5C%5C#id=mN8ND&amp;originHeight=40&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>当d(a,n)小于d(a, p)+margin时，左边这一项是正数，模型通过反向传播使d(a,p)和d(a,n)分别往更小/更大的<a href="https://www.zhihu.com/search?q=%E6%96%B9%E5%90%91%E6%A2%AF%E5%BA%A6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">方向梯度</a>下降；但当d(a,n)大于d(a, p)+margin时，是公式右边的0起作用，左边这一项不再参与梯度回传，对模型训练没有帮助。然而，它们依然参与计算，使计算时间增加，影响模型的收敛速度。而且越到训练后期，模型越来越好，这种无用的pair越多。因此，需要合适的采样方法选择适当的pair，至少保证它们对训练是有帮助的。</p><h1 id="2-hard-semi-hard-采样"><a href="#2-hard-semi-hard-采样" class="headerlink" title="2.hard/semi-hard 采样"></a>2.<strong>hard/</strong><a href="https://www.zhihu.com/search?q=semi-hard&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D"><strong>semi-hard</strong></a><strong> 采样</strong></h1><p>根据直觉，既然满足了公式的pair无贡献，那么我们找到不满足<a href="https://www.zhihu.com/search?q=%E5%85%AC%E5%BC%8F&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">公式</a>的pair，用它们训练不就可以了吗？可以，又有两个新的问题来了：（1）怎么找到这些合适的pair？如果所有pair都计算，复杂度又变高了。（2）找到合适的pair之后，从这些pair里怎么进一步挑选？</p><p>问题一，<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1503.03832.pdf">FaceNet</a> 中提出了两种方法：第一种是离线计算，每隔n个epoch用当前最优模型计算；第二种是在线计算，在每个batch里面计算。如今的负采样方法基本都是选择第二种在线计算的方式。那又有问题了，如果一个batch里面都没有合适的怎么办？那只能调大batchsize了。可能一些论文就是靠调大batchsize才得到提升的，只是他们没明说：）</p><p>问题二，就是各个采样方法进一步研究的问题了。一种直觉上容易想到的方法是选择所有pair里最难的pair（即d(a,n)-d(a,p)最小），这种方式就叫做<a href="https://www.zhihu.com/search?q=hard+sampling&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D"><strong>hard sampling</strong></a>。然而，<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1503.03832.pdf">FaceNet</a> 指出：在实验中，选择最困难的负样本可能会导致训练初期收敛到不好的局部最小值，而且会导致模型崩溃。为了减轻这种情况，作者提出了<a href="https://www.zhihu.com/search?q=semi-hard+sampling&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D"><strong>semi-hard sampling</strong></a>方法，即保证d(a,p)&lt;d(a,n)，意思是我们要选择困难的负样本，但这些负样本不要太困难，即负样本和anchor不能比正样本和anchor更相似。</p><p><strong>hard/semi-hard 采样的问题</strong></p><p>hard/semi-hard采样算是比较符合直觉的采样方法了。但实际使用时，作者们发现了一个问题：“ FaceNet报告了一个一致的发现：损失的减少在某个点后急剧减慢，他们的最终系统花了80天的时间进行训练。”这是为什么呢？<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.07567.pdf">Sampling Matters</a> 给出了解释。</p><p>进一步介绍之前，需要有一个先验知识：对于n&gt;=128的n维向量，在normalize后被约束到一个n-1维的球体上。如果点在球体上分布均匀，则两个点之间距离的分布属于以下公式：</p><p><img src="https://www.zhihu.com/equation?tex=q%28d%29+%5Cpropto+d%5E%7Bn-2%7D%5B1-%5Cfrac%7B1%7D%7B4%7Dd%5E2%5D%5E%7B%5Cfrac%7Bn-3%7D%7B2%7D%7D%5C%5C#id=lAiDa&amp;originHeight=51&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>证明：<a href="https://link.zhihu.com/?target=http%3A//faculty.madisoncollege.edu/alehnen/sphere/hypers.htm">The Sphere Game in n Dimensions</a> 。在高维空间，q(d)符合$(√2,\frac{1}{2n})$ 的<a href="https://www.zhihu.com/search?q=%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">正态分布</a>。换句话说，如果样本分散均匀，随机采样时采样到 $√2$ 附近的概率很大。如果margin小于 $√2$ ，这个采样就没有帮助了。对于学习到的embeddings，分布类似：</p><p><img src="https://pic3.zhimg.com/80/v2-fe0ee5993961a21992f69d8e851fffd6_1440w.jpg#height=273&amp;id=sFzHm&amp;originHeight=329&amp;originWidth=605&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=shadow&amp;title=&amp;width=502" alt=""></p><p>选择<a href="https://www.zhihu.com/search?q=hard+negative+samples&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">hard negative samples</a>导致另一个问题。负例的梯度为：</p><p> <img src="https://www.zhihu.com/equation?tex=%5Cpartial+%3D+%5Cfrac%7Bd%28a%2Cn%29%7D%7B%5Cleft%7C+%5Cleft%7C+d%28a%2Cn%29+%5Cright%7C+%5Cright%7C%7D+w%28t%29#id=uoyaI&amp;originHeight=52&amp;originWidth=151&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>当d很小时，如果embedding有noise，则梯度的方向为：</p><p> <img src="https://www.zhihu.com/equation?tex=%5Cpartial+%3D+%5Cfrac%7Bd%28a%2Cn%29%2Bz%7D%7B%5Cleft%7C+%5Cleft%7C+d%28a%2Cn%29%2Bz+%5Cright%7C+%5Cright%7C%7D#id=QctDM&amp;originHeight=52&amp;originWidth=148&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>梯度的方向就会被改变。</p><h1 id="3-distance-weighted采样"><a href="#3-distance-weighted采样" class="headerlink" title="3.distance weighted采样"></a>3.<strong>distance weighted采样</strong></h1><p>根据以上分析，随机采样时总会采样到 $√2$ 附近的值，hard采样又会被噪声影响。那么有没有一种方法使所有区间的概率相同呢？方法很简单，乘一个概率的倒数：</p><p><img src="https://www.zhihu.com/equation?tex=Pr%28n%5E%2A+%3D+n%7Ca%29+%5Cpropto+min%28%5Clambda%2C+q%5E%7B-1%7Dd%28a%2Cn%29%29%5C%5C#id=dXI9e&amp;originHeight=43&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><br>其中 $\lambda$ 是为了避免噪声样本加入的cutoff。</p><p><img src="https://pic2.zhimg.com/80/v2-d27a8baf9f8b2f46a2ee08407467ec89_1440w.jpg#height=290&amp;id=M8Ivf&amp;originHeight=377&amp;originWidth=591&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=shadow&amp;title=&amp;width=454" alt=""></p><p>上图比较了几个采样方法的样本分布，hard sampling总是在高方差区域采样，被噪声影响。随机采样的样本都在1.4附近。semi-hard采样找到了一个狭窄的范围，尽管一开始它可能会很快收敛，但是在某些时候，该频段内没有任何实例，网络将停止更新。distance weighted采样方法对所有分布提供了相同的可能，使得训练中总能提供有用的样本。</p><h1 id="采样的实现方法"><a href="#采样的实现方法" class="headerlink" title="采样的实现方法"></a><strong>采样的实现方法</strong></h1><p>刚才我们叙述的过程中，都是以“人”为单位，为每个人找到一个合适的triplet pair。以pytorch为例，具体的实现方式是先对每一个人采样一个（anchor，positive，negative）pair，再把它们送到batch中。如果每个batch里的人数为k，则每个batch里有k个pair。把这种实现方式推到以图片为单位的情况，则对每张图片采样一个pair，再把它们送入batch中。例如batch中有2个人，每个人有3张图片。则采样到的pair为：（a1，ax，by）；（a2，ax，by）；（a3，ax，by）；（b1，bx，ay）；（b2，bx，ay）；（b3，bx，ay），其中x，y根据采样方法可能是1-3中的任意一个（positive与anchor不同）。</p><p>这种实现方法有两个问题</p><ul><li>一是计算重复，例如a1作为anchor计算了一次，也可能作为a2的positive又计算了一次，又可能作为b1的negative再计算了一次。</li><li>二是pair少，最开始的时候提到过接受不了$O(mn^2)$的复杂度，是因为m和n都很大。但当调整batchsize使m和n到合适的大小k和q时，我们在batch内已经可以接受$O(kq^2)$的复杂度了，这时我们就希望batch内能有更多的pair，使模型收敛更快。</li></ul><p>以上两个问题可以用同一种方法解决，思路来自<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1511.06452.pdf">Lifted Structured Loss</a>：即先组好batch（设置好每个batch里的人数和人脸图片个数），计算好每张图片的向量，再找到batch内所有满足margin条件的pair，并在此基础上进行采样（semihard、distanceweighted等）。这种方法现在已经是成为一种通用做法了。找到batch内所有满足margin条件的pair之后，如果使用semihard采样，则剔除掉所有hard的样本；或者可以把条件设置得更严格一些，例如设置一个epsilon，只选择比最难正例-epsilon大的<a href="https://www.zhihu.com/search?q=%E8%B4%9F%E4%BE%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A157900964%7D">负例</a>，以及比最难负例+epsilon小的正例（来自<a href="https://link.zhihu.com/?target=https%3A//openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf">Multi-Similarity Loss</a>）。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://daniel-at-world.blogspot.com/2019/07/implementing-triplet-loss-function-in.html">Implementing Triplet Loss Function in Tensorflow 2.0</a></p><p><a href="https://blog.csdn.net/qq_36387683/article/details/83583099">Tensorflow实现Triplet Loss</a></p><p><a href="https://zhuanlan.zhihu.com/p/295512971">深度学习从入门到放飞自我：完全解析triplet loss</a></p><p><a href="https://omoindrot.github.io/triplet-loss">Triplet Loss and Online Triplet Mining in TensorFlow</a></p><p><a href="https://www.zhihu.com/question/365370142">triplet loss稳定在margin附近?—hardTri &amp; l2_normalize</a></p><p><a href="https://bindog.github.io/blog/2019/10/23/why-triplet-loss-works/">为什么triplet loss有效？从直观上说明为什么triplet loss不稳定?</a></p><p><a href="https://zhuanlan.zhihu.com/p/157900964">*Sampling Methods of Metric Learning</a></p>]]></content>
    
    
    <categories>
      
      <category>度量学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>度量学习简述</title>
    <link href="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%AE%80%E8%BF%B0/"/>
    <url>/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%AE%80%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<p>一句话总结：metric learning希望使同源的向量相似度尽可能的高，而非同源的向量相似度尽可能的低，即<strong>类内相近，类间分离</strong>。经典的应用就是人脸识别。通过计算两张图片之间的相似度，使得输入图片被归入到相似度大的图片类别中去。</p><blockquote><p>Metric learning is an approach based directly on a distance metric that aims to establish similarity or dissimilarity between objects. While metric learning aims to reduce the distance between similar objects, it also aims to increase the distance between dissimilar objects.  —-Deep Metric Learning: A Survey</p></blockquote><img src="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%AE%80%E8%BF%B0/1641351150586-c880fabf-bb3b-45bc-a647-92a1504f864b.png" class="" title="image.png"><h2 id="为什么需要度量学习"><a href="#为什么需要度量学习" class="headerlink" title="为什么需要度量学习?"></a>为什么需要度量学习?</h2><p>传统的分类任务(比如MINIST和ImageNet等)，有一个很强但也很隐蔽的先验，即<strong>测试集中的类别一定在训练集类别中，此时模型已经学到了不同类之间的分界面</strong>。然而在人脸识别、指纹识别等开集分类的任务中，此先验不再成立，类别数往往很多而类内样本数比较少。在这种情况下，基于深度学习的分类方法常表现出一些局限性，如<strong>缺少类内约束、分类器优化困难</strong>等。而这些局限可以通过 Deep Metric Learning 来解决。</p><img src="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%AE%80%E8%BF%B0/1654684539718-99bd9642-d2e1-40a0-b433-11d26e1fa6f0.png" class="" title="image.png"><h2 id="度量学习的目标？"><a href="#度量学习的目标？" class="headerlink" title="度量学习的目标？"></a>度量学习的目标？</h2><p>除了保证可分性外，<strong>还要做到特征向量类内尽可能紧凑，类间尽可能分离</strong>。相当于<strong>引导分类器可以学习到能区分不同类的特征组合</strong>。Metric learning的关键在于distance metric，可以用下图去描述了：</p><img src="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%AE%80%E8%BF%B0/1639387136839-e9ff995d-cc52-4e78-ad94-569ac724f06b.png" class="" title="image.png"><h2 id="DML-Deep-Metric-amp-Representation-Learning"><a href="#DML-Deep-Metric-amp-Representation-Learning" class="headerlink" title="DML:Deep Metric &amp; Representation Learning"></a>DML:Deep Metric &amp; Representation Learning</h2><img src="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%AE%80%E8%BF%B0/1673404769396-11499924-ca91-4784-8580-645cd6260e35.png" class="" title="image.png"><p>DML 的流程</p><img src="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%AE%80%E8%BF%B0/1673404749683-add07e24-3a5d-4427-9c1a-6496a4a1d5a8.png" class="" title="image.png">]]></content>
    
    
    <categories>
      
      <category>度量学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>度量学习之 AMSoftmax 理解</title>
    <link href="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8BAMSoftmax/"/>
    <url>/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8BAMSoftmax/</url>
    
    <content type="html"><![CDATA[<h2 id="从最优化的角度来推导出Softmax交叉熵损失函数"><a href="#从最优化的角度来推导出Softmax交叉熵损失函数" class="headerlink" title="从最优化的角度来推导出Softmax交叉熵损失函数"></a>从最优化的角度来推导出Softmax交叉熵损失函数</h2><p>一般而言，最优化的问题通常需要构造一个目标函数。使用神经网络进行多分类（假设为 $C$ 类)时的目标函数是什么？可以将各个类别的输出独立开来，每个类别占据一个维度。那么如果让一个样本的真值标签（ground-truth label）所对应的分数比其他分数更大，就可以通过比较 $C$ 个分数的大小来判断样本的类别了。</p><p>多分类优化目标：</p><blockquote><p>输出C个分数，使目标分数比非目标分数更大。</p></blockquote><p>换成数学描述，设 $z=f(x)∈R^c、y$ 为真值标签的序号，那优化目标即为：</p><p>$∀_{j≠y}, z_y&gt;z_j $</p><p>利用max函数，让$ z_y $刚刚超过$ z_j $时就停止，防止 z 无限地上升或下降，控制整个神经网络的幅度</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D++%3D%5Csum_%7Bi%3D1%2Ci%5Cneq+y%7D%5E%7BC%7D+%5Cmax%28+z_i+-+z_y%2C+0%29#id=VMd31&amp;originHeight=63&amp;originWidth=208&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>然而这样做往往会使模型的泛化性能比较差，添加一个参数m，让 $z_y$ 比 $z_j$ 大过一定的数值才停止：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Bhinge%7D+%3D+%5Csum_%7Bi%3D1%2Ci%5Cneq+y%7D%5E%7BC%7D+%5Cmax%28z_i+-+z_y+%2B+m%2C+0%29#id=ri7LM&amp;originHeight=63&amp;originWidth=282&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>这个 loss 也即hinge loss…</p><p>多分类改进的优化目标（大量的非目标分数得到优化，导致的梯度爆炸）：</p><blockquote><p>改进：输出C个分数，使目标分数比<strong>最大的</strong>非目标分数更大。</p></blockquote><p>这样我们的损失函数就变成了：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D+%5Cmax%28+%5Cmax_%7Bi%5Cneq+y%7D%5C%7Bz_i%5C%7D+-+z_y%2C+0%29#id=oTsRi&amp;originHeight=36&amp;originWidth=212&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>问题：每次优化的分数过少，会使得网络收敛极其缓慢，</p><p>解决：<strong>smooth</strong>。</p><p>max函数的smooth版是LogSumExp函数：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7Blse%7D+%3D+%5Cmax%5Cleft%28+%5Clog%5Cleft%28+%5Csum_%7Bi%3D1%2Ci%5Cneq+y%7D%5E%7BC%7D%7Be%5E%7Bz_i%7D%7D+%5Cright%29+-+z_y%2C+0%5Cright%29#id=BqApN&amp;originHeight=63&amp;originWidth=304&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>LogSumExp函数的导数恰好为softmax函数：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Clog%5Cleft%28+%5Csum_%7Bi%3D1%2Ci%5Cneq+y%7D%5E%7BC%7D%7Be%5E%7Bz_i%7D%7D+%5Cright%29%7D%7B%5Cpartial+z_j%7D+%3D+%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_%7Bi%3D1%2Ci%5Cneq+y%7D%5Ec%7Be%5E%7Bz_i%7D%7D%7D#id=LytbJ&amp;originHeight=71&amp;originWidth=283&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>LogSumExp函数值是大于等于max函数值的，而且等于取到的条件也是非常苛刻的，所以使用LogSumExp函数相当于变相地加了一定的 $m$。</p><p>继续smooth：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D_%7Bsoftmax%7D++%26%3D+%5Clog%5Cleft%281+%2B+e%5E%7B%5Clog%5Cleft%28+%5Csum_%7Bi%3D1%2Ci%5Cneq+y%7D%5E%7BC%7D%7Be%5E%7Bz_i%7D%7D+%5Cright%29+-+z_y%7D%5Cright%29%5C%5C+%26%3D+%5Clog%5Cleft%281+%2B+%5Cfrac%7B%5Csum_%7Bi%3D1%2Ci%5Cneq+y%7D%5E%7BC%7D%7Be%5E%7Bz_i%7D%7D%7D%7Be%5E%7Bz_y%7D%7D%5Cright%29%5C%5C+%26%3D+%5Clog%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BC%7D%7Be%5E%7Bz_i%7D%7D%7D%7Be%5E%7Bz_y%7D%7D%5C%5C+%26%3D+-%5Clog%5Cfrac%7Be%5E%7Bz_y%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7BC%7D%7Be%5E%7Bz_i%7D%7D%7D+%5Cend%7Baligned%7D#id=rmp2h&amp;originHeight=223&amp;originWidth=318&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>在经过两步smooth化之后，我们将一个难以收敛的函数逐步改造成了大家所熟知的softmax交叉熵损失函数。从这个推导过程中我们可以看出smooth化不仅可以让优化更畅通，而且还变相地在类间引入了一定的间隔，从而提升了泛化性能。</p><img src="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8BAMSoftmax/1654705593746-3eed78fd-f45d-4bbc-9e7f-18040f9b168a.png" class="" title="image.png"><h2 id="Softmax理解之二分类与多分类"><a href="#Softmax理解之二分类与多分类" class="headerlink" title="Softmax理解之二分类与多分类"></a>Softmax理解之二分类与多分类</h2><p>总结一下，Softmax交叉熵损失函数在进行多分类时可以理解为是在训练多个二分类器的组合，只不过因为Softmax训练的是类别向量而不是分界面，所以其训练效率得到了很大的提升。Softmax交叉熵损失函数的多条优良的性质以及它在多个二分类器之间的权重分配方式有助于我们设计其他的损失函数，同时也能启发一下多个损失函数之间加权方式的研究，毕竟Softmax看似是多分类，实际上是有着巧妙设计的权重的多个二分类器，那么其他的多损失函数说不定也能利用一下类似的权重分配方式呢？</p><h2 id="Softmax理解之Smooth程度控制"><a href="#Softmax理解之Smooth程度控制" class="headerlink" title="Softmax理解之Smooth程度控制"></a>Softmax理解之Smooth程度控制</h2><p>总结一下，softmax 函数中的 $z$  幅度既不能过大、也不能过小，过小会导致近对目标函数近似效果不佳的问题；过大则会使类间的间隔趋近于0，影响泛化性能。</p><p>那么如何去控制 $z$ 的幅度呢？那就是 Softmax 交叉熵损失的温度项 $T$  ：</p><p>$\begin{aligned}<br>\mathcal{L}_{softmax}&amp;=-\log \left(\frac{e^{z_{y}}}{\sum_{i=1}^{C} e^{z_{i}}}\right) \\<br>&amp;=-\log \left(\frac{e^{\frac{z_{y}}{T}}}{\sum_{i=1}^{C} e^{\frac{z_{i}}{T}}}\right) \\<br>\end{aligned}$</p><p>温度项控制着 Softmax 的 smooth 程度， $T$  越小，则 Softmax 越接近one-hot max， $T$  越大，则近似效果越差。注意这个 $T$  是施加在所有的分数 $z$ 上的，所以这是对分数的一个线性变换，如何让温度项能够生效呢？</p><img src="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8BAMSoftmax/1654705999652.png" class="" title="image.png"><p>答案是将特征和权重全部归一化，这样 $z$ 就从特征与权重的内积变成了特征与权重的余弦，由于余弦值的范围是 $[-1,1]$ ，所以 $z$ 的幅度就大致被定下来了，然后我们再乘上一个尺度因子 $s$ 来拉长 $z$ 的幅度，来保证输入到 Softmax 里的分数能在一个合适的范围：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D+-%5Clog%5Cleft%28+%7B%5Cfrac%7Be%5E%7Bs%5Ctilde%7Bz_y%7D%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7BC%7De%5E%7Bs%5Ctilde%7Bz_i%7D%7D%7D%7D+%5Cright%29+#id=nWRQv&amp;originHeight=60&amp;originWidth=189&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>这里的 $\hat{z}$ 表示是由归一化后的特征与权重的内积（即余弦)得到的分数。</p><h2 id="Softmax理解之margin"><a href="#Softmax理解之margin" class="headerlink" title="Softmax理解之margin"></a>Softmax理解之margin</h2><p>分类模型通过学得的分界线可以表现很不错，但如果要进行特征比对任务，必须将类间的间隔拉得更大，才能保证“类间距离大于类内距离”这一目标（分类与排序的不等价性）。如何提高间隔呢？Margin-based Softmax度量学习可以了解一下，目标是：1️⃣ 除了好的分类概率，一个好的度量空间更加重要；2️⃣ 增大类间差异并且减小类内差异。</p><blockquote><p>输出C个分数，使目标分数比<strong>最大的</strong>非目标分数更大。</p></blockquote><p>其对应的损失函数为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D+%5Cmax%28+%5Cmax_%7Bi%5Cneq+y%7D%5C%7Bz_i%5C%7D+-+z_y%2C+0%29#id=C7LWo&amp;originHeight=36&amp;originWidth=212&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>参考 hinge loss，引入间隔项：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D+%5Cmax%28+%5Cmax_%7Bi%5Cneq+y%7D%5C%7Bz_i%5C%7D+-+z_y+%2B+m%2C+0%29#id=lwqHt&amp;originHeight=36&amp;originWidth=251&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>这个损失函数的意义是：</p><blockquote><p>输出C个分数，使目标分数比最大的非目标分数还要大 m。</p></blockquote><p>这里有两个需要注意的问题：一是需要限制分数 $z$ 的取值范围，否则参数 m 设置没有意义，采用一些归一化方法；二是需要控制分数 $z$ 的幅度，保证对目标函数的近似效果和泛化性能，采用尺度因子 s。</p><p>于是损失函数变为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D+%5Cmax%28+%5Cmax_%7Bi%5Cneq+y%7D%5C%7Bsz_i%5C%7D+-+sz_y+%2B+sm%2C+0%29#id=wrStj&amp;originHeight=36&amp;originWidth=278&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>注意 $max(x,0)$ 的smooth版即softplus函数 $log(1+e^x)$  ，替换后：</p><p>$\begin{aligned}<br>\mathcal{L} &amp;=\max \left(\max _{i \neq y}\left\{s \widetilde{z_{i}}\right\}-s \widetilde{z_{y}}+s m, 0\right) \\<br>&amp; \approx \operatorname{Softplus}\left(L S E\left(s \widetilde{z_{i}} ; i \neq y\right)-s\left(\widetilde{z_{y}}-m\right)\right) \\<br>&amp;=-\log \left(\frac{e^{s\left(\widetilde{z_{y}}-m\right)}}{e^{s\left(\widetilde{z_{y}}-m\right)}+\sum_{i \neq y} e^{s \widetilde{z_{i}}}}\right)<br>\end{aligned}$</p><p>这个 loss 也即AM-Softmax loss，“带有加性间隔的 Softmax 交叉熵损失函数”。</p><img src="/2021/11/26/2021-11-26-%E5%BA%A6%E9%87%8F%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8BAMSoftmax/1.png" class="" title="AMSoftmax loss.png"><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/45014864">Softmax理解的第一篇-从最优化的角度看待Softmax损失函数</a></p><p><a href="https://zhuanlan.zhihu.com/p/45368976">Softmax理解的第二篇-Softmax理解之二分类与多分类</a></p><p><a href="https://zhuanlan.zhihu.com/p/49939159">Softmax理解的第三篇-Softmax理解之Smooth程度控制</a></p><p><a href="https://zhuanlan.zhihu.com/p/52108088">Softmax理解的第四篇-Softmax理解之margin</a></p>]]></content>
    
    
    <categories>
      
      <category>度量学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keyword-BERT</title>
    <link href="/2021/11/15/2021-11-15-Keyword-BERT/"/>
    <url>/2021/11/15/2021-11-15-Keyword-BERT/</url>
    
    <content type="html"><![CDATA[<p>腾讯微信团队于2020年提出的一种深度语义匹配方法的论文。在QA检索问题中，新输入的一个问法，就需要与语料库中的所有问题-答案对（QA对）进行语义相关性匹配。但是在开放领域的场景下，由于在“问法-问题”对中会存在各式各样不同表达的词汇，导致衡量新问法与候选QA对的相似性就变的富有挑战性。</p><p>Keyword-BERT提出了一种“关键词-注意力机制”的方法来改进深度语义匹配任务。首先从海量的语料中按领域划分来生成领域相关的关键词字典。在基于BERT原有架构的基础之上，再堆叠一层“关键词注意力”层来强调在“问法-问题”对中出现的关键词。在模型的训练过程中，提出了一种新的，基于输入问法对中的关键词重合度的负样本采样方法。最终在中文QA语料上利用多种评估指标对模型进行了验证，包括召回候选数据的精确率，语义匹配的准确率。实验表明，Keyword-BERT超过了现有的其他基线模型。</p><p>论文链接：<a href="https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fpdf%2F2003.11516.pdf">https://arxiv.org/pdf/2003.11516.pdf</a></p><p>github地址：<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FDataTerminatorX%2FKeyword-BERT">https://github.com/DataTerminatorX/Keyword-BERT</a></p><p>论文相关博客：<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F_QY2EhB-TiBcb5q0379McQ">https://mp.weixin.qq.com/s/_QY2EhB-TiBcb5q0379McQ</a></p> <span id="more"></span><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>检索式问答系统大致的流程如下：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9uV1dwaWFHY29IRTcwTzNPSFFGaWFxakFQWjEzM1M4ZVNha3I5RGljS2VlbVhtVHZQN2dXR092VzhRVnpwRkZ0TGx4c1VzU1ZYczQ3Nkhqd3JFeDZzeDkxdy82NDA?x-oss-process=image/format,png" alt="img"></p><p>首先我们需要维护一个海量且高质量的问答库。然后对于用户的问题(Query)，我们从问答库里先粗略地检索出比较相似的问题 (Questions)，对于这些候选问题，再进一步进行『语义匹配』，找出最匹配的那个问题，然后将它所对应的答案，返回给用户。我们可以看到，粗略检索出来的 Question，里面噪音很多，跟我们的 Query 相比，很多都是 <strong>形似</strong>而<strong>神不似</strong>。所以最最最核心的模块，便是<strong>Query-Question 的语义匹配</strong>，用来从一堆<strong>形似</strong>的候选问题中，找出跟 Query<strong>神似</strong>的 Question。</p><h1 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h1><h2 id="系统鲁棒性不⾜"><a href="#系统鲁棒性不⾜" class="headerlink" title="系统鲁棒性不⾜"></a>系统鲁棒性不⾜</h2><ol><li>字面相似case不同义（False Positive）</li><li>同义case表述不同（False Negative）</li></ol><img src="/2021/11/15/2021-11-15-Keyword-BERT/format,png-20230424105522203.png" class="" title="img"><p>蓝色加粗的词代表模型自以为匹配上的关键信息，红色代表实际要匹配的关键信息、但模型失配了。</p><p>此外，不同的term对于搜索的意义也不同，例如“桃子味的牙膏”，这里的桃子是修饰牙膏的，核心词为“牙膏”，核心词应该就有更高的查询分值。在意图类目识别时也应该根据核心词来确认。所以提前对query做<strong>同义词转化、词性分析POS，命名实体识别NER，计算词语权重Term Weight</strong>等将会帮助检索。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>Keyword-BERT对传统语义匹配模型的框架做了两处改进，一处是加入了 <strong><em>关键词系统</em></strong>，从海量的开放域中提取关键词/词组，然后给训练样本/预测样本中出现的关键词，额外添加一个标注。另一处，是对模型做相应改进，去增强模型对这种关键信息的捕获。这两处改动的核心，是为数据和模型 显式地引入关键信息，这样我们便能从根本上解决我们所面临的数据和模型的痛点，不再只是隔靴搔痒。</p><img src="/2021/11/15/2021-11-15-Keyword-BERT/format,png-20230424105516412.png" class="" title="img"><h3 id="1-改进的模型：强化模型对关键信息的捕获"><a href="#1-改进的模型：强化模型对关键信息的捕获" class="headerlink" title="1. 改进的模型：强化模型对关键信息的捕获"></a>1. 改进的模型：强化模型对关键信息的捕获</h3><p>这一点很好理解，我们在模型中，额外增加了对关键词词对儿的处理，相当于增加了额外的 feature，给模型提供更多信息，加强模型对问题对儿的区分能力。至于具体的改进细节，我们将会在下节提到，这里先不表。</p><h3 id="2-带关键词的样本：减少对标注数据依赖"><a href="#2-带关键词的样本：减少对标注数据依赖" class="headerlink" title="2. 带关键词的样本：减少对标注数据依赖"></a>2. 带关键词的样本：减少对标注数据依赖</h3><p>我们举个例子，也是我们在引子部分提到的一个负样本：<strong>怎么扫码加微信</strong>和<strong>怎么扫码进微信群</strong>。这两个问题不相似的根源，在于<strong>微信和微信群</strong>的含义不同。但模型一开始学出来的可能是<strong>加和进</strong>这两个动词的差异（因为微信和微信群的embedding可能非常接近），只有我们提供了额外的样本，比如告诉模型<strong>怎么加豆瓣小组</strong>和<strong>怎么进豆瓣小组</strong>这两个问题是相似的，模型才可能学出<strong>进和加</strong>不是关键，继而学到真正的关键信息。所以如果我们一开始就标注出关键词，相当于告诉模型，这些是候选的、可能的关键信息，模型（经过我们改进后的）就会有意识地针对这部分进行学习，而不需要自行通过更多的样本去判别，从而从根本上解决对标组数据的依赖。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="如何构造一个-关键词系统"><a href="#如何构造一个-关键词系统" class="headerlink" title="如何构造一个 关键词系统?"></a>如何构造一个 <strong><em>关键词系统</em></strong>?</h2><p>如上面所说，一个好的关键词系统，要能抽取出<strong>多又好的关键词</strong>——即：数量多、质量高。</p><p>为了达成这个目标，采用新词发现/PMI方法从里面提取出候选的关键词。PMI的计算公式如下，</p><p>然后设计了一个 <strong><em>diff-idf</em></strong> 分值，去衡量这个关键词的领域特性，直观来说，就是这个关键词在自己领域出现的文档频次，远高于其他领域。计算方法如下所示，</p><p>其中 ^domain 表示“其他领域”的语料。举个例子，比如我们计算，“杠杆”这个词在金融领域中的重要性，那么“其他领域”就指，非金融类的语料。这里，我们用“df”来替代“tf”，是因为我们认为评估一个词汇在这个领域的重要性，在篇章（document-level）中的频次，比在词组（term-level）中的频次更为重要。而减去“其他领域”中的idf值能够确保计算出的值，是具有区分度的，能够真正代表该词在目标领域的重要性。从直觉上来说，如果一个词，在每一个领域的文章中都比较平均的出现，那么它的“diff-idf”分值会变小，因为需要同时减去，“其他领域”的idf值。我们最终就利用，“diff-idf”值来排序候选词并剔除那些低于阈值的噪声词汇。</p><p>通过这个分值排序截断后，再进行后处理，去除噪音、实体归一化等等，最后与一些公开词条一起，构成一个庞大的关键词词典。</p><img src="/2021/11/15/2021-11-15-Keyword-BERT/webp" class=""><p>具体的流程如下(比较细碎 但缺一不可)。</p><img src="/2021/11/15/2021-11-15-Keyword-BERT/format,png-20230424105507051.png" class="" title="img"><p>这个流程每天都在运行和更新，我们目前的关键词数量达到数百万级，人工评测的质量也不错。下面是一些 case 展示：</p><img src="/2021/11/15/2021-11-15-Keyword-BERT/format,png-20230424105501255.png" class="" title="img"><h2 id="如何-改进模型"><a href="#如何-改进模型" class="headerlink" title="如何 改进模型?"></a>如何 <strong><em>改进模型</em></strong>?</h2><p>BERT 相比其他已知的深度模型，是核弹级别的改进，所以我们理所当然地选择了它 (事实上我们也做了线下实验，结果都在意料之中)鉴于 BERT 的结构已家喻户晓，我们就不细述了，我们重点思考的，是如何给 BERT 增加额外的关键信息捕捉模块？我们的思路跟 Fastpair 的改进一脉相承，只不过将这种 pair-wise 的交互，变成了 attention 机制，具体细节如下：</p><img src="/2021/11/15/2021-11-15-Keyword-BERT/webp-20230424105400222" class="" title="img"><p>注意力机制对于语义匹配来说是非常重要的，然而，因为缺少额外的监督信息，深度模型往往不能够准确捕获在句子对中的关键信息，从而对这两句话做准确的相似度区分。从pair2vec受到的启发，我们提出的模型更加会关注到那些包含关键词的词组对。具体如下图3所示，在句子A中的每一个字符，只会与句子B中的关键词的字符做点乘。</p><img src="/2021/11/15/2021-11-15-Keyword-BERT/webp-20230424105432683" class="" title="img"><p>一方面，我们在最上层引入一个额外的 keyword layer，通过 attention 和 mask ，专门对两个文本之间的关键词信息进行互相之间的 attention， 增强他们之间的<strong>互信息</strong>，另一方面，对于输出的两个文本的表示，我们借鉴了机器阅读理解里 fusion 的思想进行融合，然后将融合后的结果和 CLS 一起，输出到分类层通过这样的改造，Keyword-BERT 在不同 layer 数目下的指标都优于原始 BERT。</p><img src="/2021/11/15/2021-11-15-Keyword-BERT/format,png.png" class="" title="img"><p>我们发现， layer 数越少，Keyword-BERT 相比原始 BERT 提升越明显。这也很好理解，因为 layer 数越少， BERT 所能学到的句子级别的信息越少，而关键词相当于对这种句子级别信息进行了补充。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://blog.csdn.net/xixiaoyaoww/article/details/105182946">Keyword-BERT——问答系统中语义匹配的杀手锏</a></p><p><a href="https://www.jianshu.com/p/5345f56d76c2">【NLP论文笔记】Keyword-Attentive Deep Semantic Matching（Keyword-BERT详解）</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Keyword-BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>元学习入门（转载）</title>
    <link href="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"/>
    <url>/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="元学习入门"><a href="#元学习入门" class="headerlink" title="元学习入门"></a>元学习入门</h1><p>转载自<strong><a href="https://zhuanlan.zhihu.com/p/136975128">https://zhuanlan.zhihu.com/p/136975128</a></strong></p><p>以下是本文的主要框架：</p><ol><li>Introduction</li><li>Meta Learning实施——以MAML为例</li><li>Reptile</li><li><p>What’s more</p><span id="more"></span></li></ol><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>通常在机器学习里，我们会使用某个场景的大量数据来训练模型；然而当场景发生改变，模型就需要重新训练。但是对于人类而言，一个小朋友成长过程中会见过许多物体的照片，某一天，当Ta（第一次）仅仅看了几张狗的照片，就可以很好地对狗和其他物体进行区分。</p><p>元学习Meta Learning，含义为学会学习，即learn to learn，就是带着这种对人类这种“学习能力”的期望诞生的。Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务，如：</p><ul><li>让Alphago迅速学会下象棋</li><li>让一个猫咪图片分类器，迅速具有分类其他物体的能力</li></ul><p>需要注意的是，虽然同样有“预训练”的意思在里面，但是元学习的内核区别于迁移学习（Transfer Learning），关于他们的区别，我会在下文进行阐述。</p><p>接下来，我们通过对比机器学习和元学习这两个概念的要素来加深对元学习这个概念的理解。</p><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640" class="" title="图片"><p>在机器学习中，训练单位是一条数据，通过数据来对模型进行优化；数据可以分为训练集、测试集和验证集。在元学习中，训练单位分层级了，第一层训练单位是任务，也就是说，元学习中要准备许多任务来进行学习，第二层训练单位才是每个任务对应的数据。</p><p>二者的目的都是找一个Function，只是两个Function的功能不同，要做的事情不一样。机器学习中的Function直接作用于特征和标签，去寻找特征与标签之间的关联；而元学习中的Function是用于寻找新的f，新的f才会应用于具体的任务。有种不同阶导数的感觉。又有种老千层饼的感觉，你看到我在第二层，你把我想象成第一层，而其实我在第五层。。。</p><h2 id="2-Meta-Learning实施——以MAML为例"><a href="#2-Meta-Learning实施——以MAML为例" class="headerlink" title="2. Meta Learning实施——以MAML为例"></a>2. Meta Learning实施——以MAML为例</h2><p>我们先对比机器学习的过程来进一步理解元学习。如下图所示，机器学习的一般过程如下：</p><ul><li>设计网络网络结构，如CNN、RNN等；</li><li>选定某个分布来初始化参数；（以上其实决定了初始的f的长相，选择不同的网络结构或参数相当于定义了不同的f）；</li><li>喂训练数据，根据选定的Loss Function计算Loss；</li><li>梯度下降，逐步更新 ；</li><li>得到最终的f</li></ul><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105635062" class="" title="机器学习过程，引自李宏毅《深度学习》"><p>其中，红色方框里的“配置”都是由人为设计的，我们又叫做“超参数“。Meta Learning中希望把这些配置，如网络结构，参数初始化，优化器等由机器自行设计（注：此处区别于AutoML，迁移学习（Transfer Learning）和终身学习（Life Long Learning） ），使网络有更强的学习能力和表现。</p><p>上文已经提到，【元学习中要准备许多任务来进行学习，而每个任务又有各自的训练集和测试集】。我们结合一个具体的任务，来介绍元学习和MAML的实施过程。</p><p>有一个图像数据集叫Omniglot：github.com/brendenlake/。Omniglot包含1623个不同的火星文字符，每个字符包含20个手写的case。这个任务是判断每个手写的case属于哪一个火星文字符。</p><p>如果我们要进行N-ways，K-shot（数据中包含N个字符类别，每个字符有K张图像）的一个图像分类任务。比如20-ways，1-shot分类的意思是说，要做一个20分类，但是每个分类下只有1张图像的任务。我们可以依据Omniglot构建很多N-ways，K-shot任务，这些任务将作为元学习的任务来源。构建的任务分为训练任务（Train Task），测试任务（Test Task）。特别地，每个任务包含自己的训练数据、测试数据，在元学习里，分别称为Support Set和Query Set。</p><p>MAML的目的是获取一组更好的模型初始化参数（即让模型自己学会初始化）。我们通过（许多）N-ways，K-shot的任务（训练任务）进行元学习的训练，使得模型学习到“先验知识”（初始化的参数）。这个“先验知识”在新的N-ways，K-shot任务上可以表现的更好。</p><p>接下来介绍MAML的算法流程：</p><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105712832" class="" title="图片"><p><strong>MAML算法流程</strong></p><blockquote><p>当然，在“预训练”阶段，也可以sample出1个batch的几个任务，那么在更新meta网络时，要使用sample出所有任务的梯度之和。<br>注意：在MAML中，meta网络与子任务的网络结构必须完全相同。</p></blockquote><p>这里面有几个小问题：</p><ol><li>MAML的执行过程与model pretraining &amp; transfer learning的区别是什么？</li><li>为何在meta网络赋值给具体训练任务（如任务m）后，要先更训练任务的参数，再计算梯度，更新meta网络？</li><li>在更新训练任务的网络时，只走了一步，然后更新meta网络。为什么是一步，可以是多步吗？</li></ol><p>这三个问题是MAML中很核心的问题，大家可以先思考一下，我们将在后文进行解答。我们先看一下MAML的实现代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 网络构建部分: refer: https://github.com/dragen1860/MAML-TensorFlow</span><br><br><span class="hljs-comment">#################################################</span><br><span class="hljs-comment"># 任务描述：5-ways，1-shot图像分类任务，图像统一处理成 84 * 84 * 3 = 21168的尺寸。</span><br><span class="hljs-comment"># support set：5 * 1</span><br><span class="hljs-comment"># query set：5 * 15</span><br><span class="hljs-comment"># 训练取1个batch的任务：batch size：4</span><br><span class="hljs-comment"># 对训练任务进行训练时，更新5次：K = 5</span><br><span class="hljs-comment">#################################################</span><br><br><span class="hljs-built_in">print</span>(support_x) <span class="hljs-comment"># (4, 5, 21168) </span><br><span class="hljs-built_in">print</span>(query_x) <span class="hljs-comment"># (4, 75, 21168)</span><br><span class="hljs-built_in">print</span>(support_y) <span class="hljs-comment"># (4, 5, 5)</span><br><span class="hljs-built_in">print</span>(query_y) <span class="hljs-comment"># (4, 75, 5)</span><br><span class="hljs-built_in">print</span>(meta_batchsz) <span class="hljs-comment"># 4</span><br><span class="hljs-built_in">print</span>(K) <span class="hljs-comment"># 5</span><br><br>model = MAML()<br>model.build(support_x, support_y, query_x, query_y, K, meta_batchsz, mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MAML</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self, support_xb, support_yb, query_xb, query_yb, K, meta_batchsz, mode=<span class="hljs-string">&#x27;train&#x27;</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param support_xb: [4, 5, 84*84*3] </span><br><span class="hljs-string">        :param support_yb: [4, 5, n-way]</span><br><span class="hljs-string">        :param query_xb:  [4, 75, 84*84*3]</span><br><span class="hljs-string">        :param query_yb: [4, 75, n-way]</span><br><span class="hljs-string">        :param K:  训练任务的网络更新步数</span><br><span class="hljs-string">        :param meta_batchsz: 任务数，4</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        self.weights = self.conv_weights() <span class="hljs-comment"># 创建或者复用网络参数；训练任务对应的网络复用meta网络的参数</span><br>        training = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> mode <span class="hljs-keyword">is</span> <span class="hljs-string">&#x27;train&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>      <br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">meta_task</span>(<span class="hljs-params"><span class="hljs-built_in">input</span></span>):<br>            <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            :param support_x:   [setsz, 84*84*3] (5, 21168)</span><br><span class="hljs-string">            :param support_y:   [setsz, n-way] (5, 5)</span><br><span class="hljs-string">            :param query_x:     [querysz, 84*84*3] (75, 21168)</span><br><span class="hljs-string">            :param query_y:     [querysz, n-way] (75, 5)</span><br><span class="hljs-string">            :param training:    training or not, for batch_norm</span><br><span class="hljs-string">            :return:</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br><br>            support_x, support_y, query_x, query_y = <span class="hljs-built_in">input</span><br>            query_preds, query_losses, query_accs = [], [], [] <span class="hljs-comment"># 子网络更新K次，记录每一次queryset的结果</span><br> <br>            <span class="hljs-comment">## 第0次对网络进行更新</span><br>            support_pred = self.forward(support_x, self.weights, training) <span class="hljs-comment"># 前向计算support set</span><br>            support_loss = tf.nn.softmax_cross_entropy_with_logits(logits=support_pred, labels=support_y) <span class="hljs-comment"># support set loss</span><br>            support_acc = tf.contrib.metrics.accuracy(tf.argmax(tf.nn.softmax(support_pred, dim=<span class="hljs-number">1</span>), axis=<span class="hljs-number">1</span>),<br>                                                         tf.argmax(support_y, axis=<span class="hljs-number">1</span>))<br>            grads = tf.gradients(support_loss, <span class="hljs-built_in">list</span>(self.weights.values())) <span class="hljs-comment"># 计算support set的梯度</span><br>            gvs = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(self.weights.keys(), grads))<br>            <span class="hljs-comment"># 使用support set的梯度计算的梯度更新参数，theta_pi = theta - alpha * grads</span><br>            fast_weights = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(self.weights.keys(), \<br>                    [self.weights[key] - self.train_lr * gvs[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> self.weights.keys()]))<br><br>            <span class="hljs-comment"># 使用梯度更新后的参数对quert set进行前向计算</span><br>            query_pred = self.forward(query_x, fast_weights, training)<br>            query_loss = tf.nn.softmax_cross_entropy_with_logits(logits=query_pred, labels=query_y)<br>            query_preds.append(query_pred)<br>            query_losses.append(query_loss)<br> <br>            <span class="hljs-comment"># 第1到 K-1次对网络进行更新</span><br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, K):           <br>                loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.forward(support_x, fast_weights, training),<br>                                                               labels=support_y)<br>                grads = tf.gradients(loss, <span class="hljs-built_in">list</span>(fast_weights.values()))<br>                gvs = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(fast_weights.keys(), grads))<br>                fast_weights = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(fast_weights.keys(), [fast_weights[key] - self.train_lr * gvs[key]<br>                                         <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> fast_weights.keys()]))<br>                query_pred = self.forward(query_x, fast_weights, training)<br>                query_loss = tf.nn.softmax_cross_entropy_with_logits(logits=query_pred, labels=query_y)<br>                <span class="hljs-comment"># 子网络更新K次，记录每一次queryset的结果</span><br>                query_preds.append(query_pred)<br>                query_losses.append(query_loss)<br><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>                query_accs.append(tf.contrib.metrics.accuracy(tf.argmax(tf.nn.softmax(query_preds[i], dim=<span class="hljs-number">1</span>), axis=<span class="hljs-number">1</span>),<br>                                                                tf.argmax(query_y, axis=<span class="hljs-number">1</span>)))<br>            result = [support_pred, support_loss, support_acc, query_preds, query_losses, query_accs]<br>            <span class="hljs-keyword">return</span> result<br><br>        <span class="hljs-comment"># return: [support_pred, support_loss, support_acc, query_preds, query_losses, query_accs]</span><br>        out_dtype = [tf.float32, tf.float32, tf.float32, [tf.float32] * K, [tf.float32] * K, [tf.float32] * K]<br>        result = tf.map_fn(meta_task, elems=(support_xb, support_yb, query_xb, query_yb),<br>                           dtype=out_dtype, parallel_iterations=meta_batchsz, name=<span class="hljs-string">&#x27;map_fn&#x27;</span>)<br>        support_pred_tasks, support_loss_tasks, support_acc_tasks, \<br>            query_preds_tasks, query_losses_tasks, query_accs_tasks = result<br><br>        <span class="hljs-keyword">if</span> mode <span class="hljs-keyword">is</span> <span class="hljs-string">&#x27;train&#x27;</span>:<br>            self.support_loss = support_loss = tf.reduce_sum(support_loss_tasks) / meta_batchsz<br>            self.query_losses = query_losses = [tf.reduce_sum(query_losses_tasks[j]) / meta_batchsz<br>                                                    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K)]<br>            self.support_acc = support_acc = tf.reduce_sum(support_acc_tasks) / meta_batchsz<br>            self.query_accs = query_accs = [tf.reduce_sum(query_accs_tasks[j]) / meta_batchsz<br>                                                    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K)]<br><br>            <span class="hljs-comment"># 更新meta网络，只使用了第 K步的query loss。这里应该是个超参，更新几步可以调调</span><br>            optimizer = tf.train.AdamOptimizer(self.meta_lr, name=<span class="hljs-string">&#x27;meta_optim&#x27;</span>)<br>            gvs = optimizer.compute_gradients(self.query_losses[-<span class="hljs-number">1</span>])<br>   <span class="hljs-comment"># def ********</span><br></code></pre></td></tr></table></figure><p>接下来回答一下上面的三个问题：</p><p>问题1：MAML的执行过程与model pretraining &amp; transfer learning的区别是什么？</p><p>我们将meta learning与model pretraining的loss函数写出来。</p><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105730590" class="" title="meta learning与model pretraining的loss函数"><p>注意这两个loss函数的区别：</p><ul><li>meta learning的L来源于训练任务上网络的参数更新过一次后（该网络更新过一次以后，网络的参数与meta网络的参数已经有一些区别），然后使用Query Set计算的loss；</li><li>model pretraining的L来源于同一个model的参数（只有一个），使用训练数据计算的loss和梯度对model进行更新；如果有多个训练任务，我们可以将这个参数在很多任务上进行预训练，训练的所有梯度都会直接更新到model的参数上。</li></ul><p>看一下二者的更新过程简图：</p><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105741787" class="" title="图片"><p>meta learning与model pretraining训练过程，引自李宏毅《深度学习》</p><ol><li>MAML是使用子任务的参数，第二次更新的gradient的方向来更新参数（所以左图，第一个蓝色箭头的方向与第二个绿色箭头的方向平行；左图第二个蓝色箭头的方向与第二个橘色箭头的方向平行）</li><li>而model pretraining是使用子任务第一步更新的gradient的方向来更新参数(子任务的梯度往哪个方向走，model的参数就往哪个方向走)。</li></ol><p>从sense上直观理解：</p><ul><li>model pretraining最小化当前的model（只有一个）在所有任务上的loss，所以model pretraining希望找到一个在所有任务（实际情况往往是大多数任务）上都表现较好的一个初始化参数，这个参数要在多数任务上当前表现较好。</li><li>meta learning最小化每一个子任务训练一步之后，第二次计算出的loss，用第二步的gradient更新meta网络，这代表了什么呢？子任务从【状态0】，到【状态1】，我们希望状态1的loss小，说明meta learning更care的是初始化参数未来的潜力。</li></ul><p>一个关注当下，一个关注潜力。</p><ul><li>如下图所示，model pretraining找到的参数，在两个任务上当前的表现比较好（当下好，但训练之后不保证好）；</li><li>而MAML的参数在两个子任务当前的表现可能都不是很好，但是如果在两个子任务上继续训练下去，可能会达到各自任务的局部最优（潜力好)。</li></ul><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105758258" class="" title="引自李宏毅《深度学习》"><p>这里有一个toy example可以表现MAML的执行过程与model pretraining &amp; transfer learning的区别。</p><p>训练任务：给定N个函数，y = asinx + b（通过给a和b不同的取值可以得到很多sin函数），从每个函数中sample出K个点，用sample出的K个点来预估最初的函数，即求解a和b的值。</p><p>训练过程：用这N个训练任务sample出的数据点分别通过MAML与model pretraining训练网络，得到预训练的参数。</p><p>如下图，用橘黄色的sin函数作为测试任务，三角形的点是测试任务中sample出的样本点，在测试任务中，我们希望用sample出的样本点还原橘黄色的线。</p><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105814396" class="" title="example，引自李宏毅《深度学习》"><ul><li>model pretraining的结果，在测试任务上，在finetuning之前，绿色线是一条水平线，finetuning之后还原的线基本还是一条水平线。因为在预训练的时候，有很多sin函数，model pretraining希望找到一个在所有任务上都效果较好的初始化结果，但是许多sin函数波峰和波谷重叠起来，基本就是一条水平线。用这个初始化的结果取finetuning，得到的结果仍然是水平线。</li><li>MAML的初始化结果是绿色的线，和橘黄色的线有差异。但是随着finetuning的进行，结果与橘黄色的线更加接近。</li></ul><p>问题2：为何在meta网络赋值给具体训练任务（如任务m）后，要先更训练任务的参数，再计算梯度，更新meta网络？</p><p>这个问题其实在问题1中已经进行了回答，更新一步之后，避免了meta learning陷入了和model pretraining一样的训练模式，更重要的是，可以使得meta模型更关注参数的“潜力”。</p><p>问题3：在更新训练任务的网络时，只走了一步，然后更新meta网络。为什么是一步，可以是多步吗？</p><p>李宏毅老师的课程中提到：</p><ul><li>只更新一次，速度比较快；因为meta learning中，子任务有很多，都更新很多次，训练时间比较久。</li><li>MAML希望得到的初始化参数在新的任务中finetuning的时候效果好。如果只更新一次，就可以在新任务上获取很好的表现。把这件事情当成目标，可以使得meta网络参数训练是很好（目标与需求一致）。</li><li>当初始化参数应用到具体的任务中时，也可以finetuning很多次。</li><li>Few-shot learning往往数据较少。</li></ul><p>那么MAML中的训练任务的网络可以更新多次后，再更新meta网络吗？</p><p>我觉得可以。直观上感觉，更新次数决定了子任务对于meta网络的影响程度，我觉得这个步数可以作为一个参数来调。</p><p>另外，即将介绍的下一个网络——Reptile，也是对训练任务网络进行多次更新的。</p><h2 id="3-Reptile"><a href="#3-Reptile" class="headerlink" title="3. Reptile"></a>3. Reptile</h2><p>Reptile与MAML有点像，我们先看一下Reptile的训练简图：</p><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105835197-2305116." class="" title="Reptile训练过程，引自李宏毅《深度学习》"><p>Reptile的训练过程如下：</p><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105856700" class="" title="图片"><p>Reptile，每次sample出1个训练任务</p><img src="/2021/11/15/2021-11-15-%E5%85%83%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/640-20230424105906470" class="" title="图片"><p>Reptile，每次sample出1个batch训练任务</p><p>在Reptile中：</p><ul><li>训练任务的网络可以更新多次</li><li>reptile不再像MAML一样计算梯度（因此带来了工程性能的提升），而是直接用一个参数乘以meta网络与训练任务的网络参数的差来更新meta网络参数</li><li>从效果上来看，Reptile效果与MAML基本持平</li></ul><h2 id="4-What’s-more"><a href="#4-What’s-more" class="headerlink" title="4. What’s more"></a>4. What’s more</h2><p>元学习入门部分的文章基本就分享到这里了~</p><ul><li>从出发点上来看，元学习和model pretraining有点像，即，都是让网络具有一些先验知识。</li><li>从训练过程的设计来看，元学习更关注模型的潜力，而model pretraining更注重模型当下在多数情况下的表现，效果孰好孰坏很难直接判定。这大概也就是仰望天空和脚踏实地的区别hahaha</li><li>元学习除了可以初始化参数以外，还有一些设计可以帮助确定网络结构，如何更新参数等等这里有李宏毅老师的一个课程大家可以关注一下youtube.com/watch? 。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mp.weixin.qq.com/s/cF3SrrmS7ke8WIkW8qqKcA">一文入门元学习（Meta-Learning）（附代码）</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Meta-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络入门（转载）</title>
    <link href="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/"/>
    <url>/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<p>本文参照以下两篇blog，这两篇应该是目前介绍GNN和GCN最好的blog了。</p><ol><li><p><a href="https://distill.pub/2021/gnn-intro/">https://distill.pub/2021/gnn-intro/</a></p></li><li><p><a href="https://distill.pub/2021/understanding-gnns/">https://distill.pub/2021/understanding-gnns/</a></p></li></ol><p>讲图神经网络(GNN)之前，先介绍一下什么是graph，为什么需要graph，以及graph有什么问题，然后介绍一下如何用GNN处理graph问题，最后从GNN推广到GCN。</p> <span id="more"></span><hr><h3 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h3><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110010694" class="" title="图片"><p>图由Vertex(V)、Edge(E)和Global(U)三部分构成。V可以表示为node identity或者number of neighbors，E可以表示为edge identity或者edge weight，U可以表示为number of nodes或者longest path。</p><p>为了进一步描述每一个node、edge和entire graph，可以把信息存储在graph的每个部分中。其实就是把信息以embedding的方式存储。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110021086-2305222." class="" title="图片"><p>还可以通过edge的方向性(有向的、无向的)来构建特殊的图。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110028958" class="" title="图片"><h3 id="Graphs-and-where-to-find-them"><a href="#Graphs-and-where-to-find-them" class="headerlink" title="Graphs and where to find them"></a>Graphs and where to find them</h3><p>graph data在生活中无处不在，比如最常见的image和text都可以认为是graph data的一种特例，还有其他一些场景也可以用graph data表达。</p><p>Images as graphs</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110033206" class="" title="图片"><p>图片的位置可以表示成(列数-行数)的形式，将图片构建成adjacency matrix，蓝色块表示pixel和pixel之间相临，无方向性，画成graph就是右边图片的形式。</p><p>Text as graphs</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110036702" class="" title="图片"><p>文本也可以构建成adjacency matrix，跟图片不一样的是，文本是一个有向图，每个词只跟前一个词相连接，并且有方向性。</p><p>其他还有比如分子、社交网络、学术引用网络等等都可以构建成graph。</p><h3 id="What-types-of-problems-have-graph-structured-data"><a href="#What-types-of-problems-have-graph-structured-data" class="headerlink" title="What types of problems have graph structured data?"></a>What types of problems have graph structured data?</h3><p>graph可以处理graph-level、node-level和edge-level三种层面的问题。</p><p>Graph-level task</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110040218" class="" title="图片"><p>输入graph，输出整个graph的类别。在图像中，和图像分类任务类似；在文本中，和句子情感分析类似。</p><p>Node-level task</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110045601" class="" title="图片"><p>输入node不带类别的graph，输出每个node的类别。在图像中，和语义分割类似；在文本中，和词性分类类似。</p><p>Edge-level task</p><p>edge-level任务用来预测node之间的相互关系。如下图所示，先将不同部分分割出来，然后判断不同部分的相互关系。构建成graph就是对edge进行类别预测。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110049460" class="" title="图片"><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110053917" class="" title="图片"><p>The challenges of using graphs in machine learning</p><p>如何用神经网络处理graph任务呢？</p><p>第一步是考虑如何表示和神经网络相兼容的图。graph最多有4种想要预测的信息：node、edge、global-context和connectivity。前3个相对容易，比如可以用一个 表示存储了第i个node的特征矩阵N。然而connectivity的表示要复杂的多，最直接的方式是构建邻接矩阵，但是空间效率很低，可能会产生非常稀疏的邻接矩阵。</p><p>还有一个问题是，许多邻接矩阵可以编码相同的连通性，但是不能保证这些不同的矩阵在神经网络中会产生相同的结果(也就是说，它们不是置换不变的)。</p><p>一种优雅而高效表示稀疏矩阵的方法是用邻接表。edge  表示节点 和  之间的连通性，在邻接表的第k项中表示为一个元组(i,j)。</p><p>以一个graph的邻接表为例，如下图所示:</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110057856" class="" title="图片"><h3 id="Graph-Neural-Networks"><a href="#Graph-Neural-Networks" class="headerlink" title="Graph Neural Networks"></a>Graph Neural Networks</h3><p>通过上面的描述，graph可以通过置换不变的邻接表表示，那么可以设计一个graph neural networks(GNN)来解决graph的预测任务。</p><p>The simplest GNN</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110102288" class="" title="图片"><p>从最简单的GNN开始，更新所有graph的属性(nodes(V)，edges(E)，global(U))作为新的embedding，但是不使用graph的connectivity。</p><p>GNN对graph的每个组件分开使用MLP，称为GNN layer。对于每个node vetor，使用MLP后返回一个learned node-vector，同理edge会返回一个learned edge embedding，global会返回一个global embedding。</p><p>和CNN类似，GNN layer可以堆叠起来组成GNN。由于GNN layer不更新输入graph的connectivity，所有输出graph的邻接表跟输入图相同。但是通过GNN layer，node、edge和global-context的embedding已经更新。</p><p>GNN Predictions by Pooling Information</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110105961" class="" title="图片"><p>如果想用GNN做二分类任务，那么可以用一个linear classifier对graph进行分类。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110108452" class="" title="图片"><p>然而，有时候信息只储存在edge中，那么就需要从edge收集信息转移到node用于预测，这个过程称之为pooling。</p><p>Pooling过程有两个步骤：</p><p>1.对于要pooling的每一项(上图一行中的一列)，收集它们的embedding并且concat到一个矩阵中（上图中的一行）。</p><p>2.收集的embedding通过求和操作进行聚合。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110114468" class="" title="图片"><p>因此，如果只有edge-level的特征，可以通过pooling传递信息来预测node(上图虚线表示pooling传递信息给node，然后进行二分类)。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110118353" class="" title="图片"><p>同理，如果只有node-level的特征，可以通过pooling传递信息来预测edge。类似CV中的global average pooling。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110121635" class="" title="图片"><p>同理，可以通过node-level的特征预测global。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110125474" class="" title="图片"><p>和CNN类似，通过GNN blocks可以构建出一个end-to-end的GNN。</p><p>需要注意的是，在这个最简单的GNN中，没有在GNN layer使用graph的connectivity。每个node、每个edge以及global-context都是独立处理的，只在聚合信息进行预测的时候使用了connectivity。</p><p>Passing messages between parts of the graph</p><p>为了使learned embedding感知到graph的connectivity，可以在GNN layer使用pooling构建出更加复杂的GNN(和convlution类似)。可以使用message passing实现，相邻node或者edge可以交换信息，并且影响彼此的embedding更新。</p><p>Message passing过程有三个步骤：</p><p>1.对于graph的每个node，收集所有相邻node的embedding。</p><p>2.通过相加操作聚合所有message。</p><p>3.所有聚合的message都通过一个update function传递(通常使用一个可学习的神经网络)。</p><p>这些步骤是利用graph的connectivity的关键。在GNN layer中构建更精细的message passing变体，可以获得表达和能力更强的GNN模型。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110129567" class="" title="图片"><p>通过堆叠的message passing GNN layer，一个node可以引入整个graph的信息：在三层GNN layer之后，一个node可以获得3步远的node信息。</p><p>对最简单的GNN范式进行更新，增加一个message passing操作:</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110132431" class="" title="图片"><p>Learning edge representations</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110135098" class="" title="图片"><p>通过meassage passing，可以在GNN layer的node和edge之间共享信息。可以像之前使用相邻node信息一样，先将edge信息pooling，然后用update function转化并且存储，从而聚合来自相邻edge的信息。</p><p>然而，存储在graph中的node和edge信息不一定具有相同的大小或形状，因此不能立刻知道如何将它们组合起来。一种方法是学习从node空间到edge空间的线性映射，或者，可以在update function之前将它们concat在一起。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110138127" class="" title="图片"><p>在构造GNN时，需要设计更新哪些graph属性以及更新顺序。比如可以使用weave的方式进行更新，node to node(linear)，edge to edge(linear)，node to edge(edge layer)，edge to node(node layer)。</p><p>Adding global representations</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110141086" class="" title="图片"><p>上面描述的GNN模型还有一个缺陷：在graph中，距离很远的node无法有效的传递信息，对于一个node，如果有k个layer，那么信息传递的距离最多是k步，这对于依赖相距很远的node信息的预测任务来说，是比较严重的问题。一种解决办法是让所有node可以相互传递信息，但是对于大的graph来说，计算量会非常昂贵。另一种解决办法是使用graph(U)的全局表示，称为master node或者context vector。这个全局的context vector可以连接到网络的所有node和edge，可以作为它们之间信息传递的桥梁，构建出一个graph的整体表示。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110144786" class="" title="图片"><p>从这个角度看，所有graph属性都可以通过将感兴趣的属性和其他属性关联，然后在pooling过程中利用起来。比如对于一个node，可以同时考虑相邻的node、edge和global信息，然后通过concat将它们关联起来，最后通过线性映射将它们映射到相同特征空间中。</p><h3 id="The-Challenges-of-Computation-on-Graphs"><a href="#The-Challenges-of-Computation-on-Graphs" class="headerlink" title="The Challenges of Computation on Graphs"></a>The Challenges of Computation on Graphs</h3><p>graph在计算中有三个挑战：lack of consistent structure、node-order equivariance和scalability。</p><p>Lack of Consistent Structure</p><p>graph是极其灵活的数学模型，同时这意味着它们缺乏跨实例的一致结构。比如不同分子之间有不同的结构。用一种可以计算的格式来表示graph并不是一件简单的事情，graph的最终表示通常由实际问题决定。</p><p>Node-Order Equivariance</p><p>graph的node之间通常没有内在的顺序，相比之下，在图像中，每个像素都是由其在图像中的绝对位置唯一决定的。因此，我们希望我们的算法是节点顺序不变的:它们不应该依赖于graph中node的顺序。如果我们以某种方式对node进行排列，则由算法计算得到的node表示也应该以同样的方式进行排列。</p><p>Scalability</p><p>graph可以非常大，像Facebook和Twitter这样的社交网络，它们拥有超过10亿的用户，对这么大的数据进行操作并不容易。幸运的是，大多数自然出现的graph都是“稀疏的”:它们的边数往往与顶点数成线性关系。graph的稀疏性导致可以使用特殊的方法有效计算graph中node的表示。另外，和graph的数据量相比，这些方法的参数要少得多。</p><h3 id="Problem-Setting-and-Notation"><a href="#Problem-Setting-and-Notation" class="headerlink" title="Problem Setting and Notation"></a>Problem Setting and Notation</h3><p>许多问题都可以用graph来表示:</p><p>Node Classification: 对单个节点进行分类。</p><p>Graph Classification: 对整个图进行分类。</p><p>Node Clustering: 根据连接性将相似的节点分组。</p><p>Link Prediction: 预测缺失的链接。</p><p>Influence Maximization: 识别有影响的节点。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110149084" class="" title="图片"><h3 id="Extending-Convolutions-to-Graphs"><a href="#Extending-Convolutions-to-Graphs" class="headerlink" title="Extending Convolutions to Graphs"></a>Extending Convolutions to Graphs</h3><p>卷积神经网络在图像中提取特征方面是非常强大的。而图像本身可以看作是一种非常规则的网格状结构的图，其中单个像素为节点，每个像素处的RGB通道值为节点特征。</p><p>因此，将卷积推广到任意的graph是一个很自然的想法。然而，普通卷积不是节点顺序不变的，因为它们依赖于像素的绝对位置。graph可以通过执行某种填充和排序，保证每个节点的邻域结构一致性，但是还有更加普遍和强大的方法来处理这个问题。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110152226" class="" title="图片"><p>下面首先介绍一下在节点邻域上构造多项式滤波器的方法，这和CNN在相邻像素上滤波类似。然后介绍更多最新的方法如何用更强大的机制扩展这个想法。</p><h3 id="Polynomial-Filters-on-Graphs"><a href="#Polynomial-Filters-on-Graphs" class="headerlink" title="Polynomial Filters on Graphs"></a>Polynomial Filters on Graphs</h3><p>The Graph Laplacian</p><p>给定一个graph G，用A来表示数值为0或者1的邻接矩阵，用D表示对角度矩阵(矩阵对角线数值表示与行node相邻node的数量)，那么 。graph Laplacian矩阵L可以表示为：L=D - A。右图的对角线就是行node的度数，负数是减去的邻接矩阵(蓝色数字和graph中的C对应)。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110203707" class="" title="图片"><p>Polynomials of the Laplacian</p><p>Laplacian的多项式可以表示为：</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110208237" class="" title="图片"><p>这些多项式可以认为和CNN中“filters”等价，而系数w是“filters”的权重。</p><p>为了方便讨论，下面考虑节点只有一维特性的情况(每个节点是一个数值)。使用前面选择的节点顺序，我们可以将所有节点特征堆在一起得到一个x向量。</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110210556" class="" title="图片"><p>通过构造的特征向量x，可以定义它和多项式滤波器 的卷积公式为：</p><p>关于Laplacian矩阵如何作用在x上的解释，参照<a href="https://distill.pub/2021/understanding-gnns/。">https://distill.pub/2021/understanding-gnns/。</a></p><p>ChebNet</p><p>ChebNet对多项式滤波器公式进行了改进:</p><p>其中 是度数为i的第一种切比雪夫多项式， 是使用L最大特征值定义的归一化Laplacian矩阵。关于ChebNet细节参照<a href="https://distill.pub/2021/understanding-gnns/。">https://distill.pub/2021/understanding-gnns/。</a></p><p>Embedding Computation</p><p>下面介绍一下如何堆叠带非线性的ChebNet(或者任何多项式过滤器) layer构建成一个GNN，就像标准的CNN一样。假设有K个不同的多项式过滤器层， 的可学习参数表示为 ，那么计算过程可以表示成：</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110215248" class="" title="图片"><p>GNN和CNN计算方式类似，将输入作为初始features，然后计算多项式过滤器，然后和输入特征进行矩阵相乘，最后用非线性函数进行非线性变换，循环迭代K次。</p><p>需要注意的是，GNN在不同的节点上过滤器权重参数共享，和CNN类似，CNN的卷积在不同位置也是参数共享的。</p><h3 id="Modern-Graph-Neural-Networks"><a href="#Modern-Graph-Neural-Networks" class="headerlink" title="Modern Graph Neural Networks"></a>Modern Graph Neural Networks</h3><p>ChebNet是graph中学习局部过滤器的一个突破，它激发了许多人从不同的角度思考图卷积。</p><p>多项式过滤器对x卷积可以展开为：</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110218960" class="" title="图片"><p>这其实是一个1步局部卷积(也就是只跟直接相连的node卷积)，可以看成由两个步骤产生：</p><p>1.聚合直接相邻的特征 。</p><p>2.结合node自身的特征 。</p><p>通过确保聚合是node-order equivariant，来保证整个卷积是node-order equivariant。</p><p>这些卷积可以被认为是相邻节点之间的“message passing”:在每一步之后，每个节点从它的相邻节点接收信息。</p><p>通过迭代重复K次1步局部卷积，感受野为K步远的所有node。</p><p>Embedding Computation</p><p>Message-passing形成了很多GNN模型的backbone。下面介绍一些流行的GNN模型:</p><ul><li>Graph Convolutional Networks (GCN)</li><li>Graph Attention Networks (GAT)</li><li>Graph Sample and Aggregate (GraphSAGE)</li><li>Graph Isomorphism Network (GIN)</li></ul><p>GCN</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110222948" class="" title="图片"><p>GAT</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110228090" class="" title="图片"><p>GraphSAGE</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110234613" class="" title="图片"><p>GIN</p><img src="/2021/11/15/2021-11-15-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/640-20230424110238080" class="" title="图片"><p>比较一下GCN、GAT、GraphSAGE和GIN的形式，主要差别就在于如何聚合信息和如何传递信息。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文只是简单介绍了一下GNN和GCN的一些变体，但图神经网络的领域是极其广阔的。下面提一下一些可能感兴趣的点：</p><p>GNNs in Practice：如何提高GNN的效率、GNN的正则化技术</p><p>Different Kinds of Graphs：Directed graphs、Temporal graphs、Heterogeneous graphs</p><p>Pooling：SortPool、DiffPool、SAGPool</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mp.weixin.qq.com/s/oWxDVQkhL0wAJb23usLZVA">有史以来最好的图神经网络科普</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>GNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常见距离度量方法</title>
    <link href="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/"/>
    <url>/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>在NLP中，我们经常要去比较两个句子的相似度，其标准方法是想办法将句子编码为固定大小的向量（Word2Vec、BERT等），然后用某种几何距离（欧氏距离、cos距离等）作为相似度。这种方案相对来说比较简单，而且检索起来比较快速，一定程度上能满足工程需求。</p><p>此外，还可以直接<strong>比较两个变长序列的差异性</strong>，比如编辑距离，它通过动态规划找出两个字符串之间的最优映射，然后算不匹配程度；</p><p><strong>不同距离度量方法的图示</strong></p><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/distance.png" class="" title="图片"><span id="more"></span><h2 id="基于字符串💡"><a href="#基于字符串💡" class="headerlink" title="基于字符串💡"></a>基于字符串💡</h2><p>基于字符串的方法都是直接对原始文本进行比较, 主要包括编辑距离[11] (Levenshtein Distance, LD)、最长公共子序列[12] (Longest Common Sequence, LCS)、N-Gram[13] 和 Jaccard 相似度[14] 等.</p><p>基于字符串的方法原理简单、实现方便, 并且直接对原始文本进行比较, 多用于文本的快速模糊匹配, 其不足主要在于没有考虑到单词的含义及单词和单词之间的相互关系, 并且同义词、多义词等问题都无法处理.</p><h3 id="1-汉明距离"><a href="#1-汉明距离" class="headerlink" title="1. 汉明距离"></a>1. 汉明距离</h3><p>汉明距离是一个概念，它表示两个（<strong>相同长度</strong>）字对应位不同的数量，比如：1011101 与 1001001 之间的汉明距离是 2；”toned” 与 “roses” 之间的汉明距离是 3。</p><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/640.png" class=""><ul><li><p><a href="https://link.jianshu.com/?t=https%3A%2F%2Fblog.csdn.net%2Fluo123n%2Farticle%2Fdetails%2F9999481">编辑距离（Levenshtein距离）详解（附python实现）</a></p></li><li><p><a href="https://link.jianshu.com/?t=https%3A%2F%2Fwww.biaodianfu.com%2Fedit-distance.html">使用Python计算文本相似性之编辑距离</a></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> textdistance<br>textdistance.hamming(<span class="hljs-string">&#x27;text&#x27;</span>,<span class="hljs-string">&#x27;test&#x27;</span>) <span class="hljs-comment">#1 </span><br>textdistance.hamming.normalized_similarity(<span class="hljs-string">&#x27;text&#x27;</span>,<span class="hljs-string">&#x27;test&#x27;</span>) <span class="hljs-comment">#0.75 </span><br></code></pre></td></tr></table></figure><ul><li><p>缺点</p><p>正如你所预料的，当两个向量的长度不相等时，汉明距离很难使用。你会希望将相同长度的向量相互比较，以了解哪些位置不匹配。</p><p>而且，只要它们不同或相等，它就不考虑实际值。因此，当幅度是一个重要的衡量标准时，不建议使用这个距离衡量。</p></li><li><p>用例</p><p>典型的使用情况包括在计算机网络上传输数据时的纠错/检测。它可以用来确定二进制字中的失真位数，以此来估计错误。</p><p>此外，你还可以使用汉明距离来测量分类变量之间的距离。</p></li></ul><h3 id="2-Levenshtein距离"><a href="#2-Levenshtein距离" class="headerlink" title="2. Levenshtein距离"></a>2. Levenshtein距离</h3><p>莱文斯坦距离，又称<strong>编辑距离</strong>，是<a href="https://baike.baidu.com/item/编辑距离">编辑距离</a>的一种。指两个<a href="https://baike.baidu.com/item/字串">字串</a>之间，由一个转成另一个所需的最少编辑操作次数。</p><p>允许的编辑操作包括插入，删除和替换。莱文斯坦距离的应用：拼写检查、语音辨识、DNA分析、抄袭侦测。</p><ul><li><a href="https://link.jianshu.com/?t=https%3A%2F%2Fwww.biaodianfu.com%2Fedit-distance.html">使用Python计算文本相似性之编辑距离</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> textdistance<br>textdistance.levenshtein(<span class="hljs-string">&quot;this test&quot;</span>, <span class="hljs-string">&quot;that test&quot;</span>) <span class="hljs-comment"># 2</span><br>textdistance.levenshtein.normalized_similarity(<span class="hljs-string">&quot;this test&quot;</span>, <span class="hljs-string">&quot;that test&quot;</span>) <span class="hljs-comment"># 0.8</span><br></code></pre></td></tr></table></figure><h3 id="3-Jaro-Winkler距离"><a href="#3-Jaro-Winkler距离" class="headerlink" title="3. Jaro-Winkler距离"></a>3. Jaro-Winkler距离</h3><p><a href="https://en.wikipedia.org/wiki/Jaro–Winkler_distance">J-W距离（Jaro–Winkler distance）</a>是两个字符串之间的另一个相似性度量。该算法对字符串中的字符串中的差异进行了惩罚。使用此算法的动机是，错字通常更有可能在字符串的后面而不是开头出现。比较”this test 和 test this”时，即使字符串包含完全相同的单词（只是顺序不同），相似度分数也仅为2/3。</p><p>Jaro–Winkler distance 是<strong>适合于如名字这样较短的字符之间</strong>计算相似度。0分表示没有任何相似度，1分则代表完全匹配。</p><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/20161109212242924.png" class="" title="这里写图片描述"><p>其中，m是两个字符串匹配上的字符数目，t是字符中换位数目的一半，即若在字符串的第i位出现了a,b，在第j位又出现了b,a，则表示两者出现了换位。</p><p><em>PS：在该算法中，更加突出了前缀相同的重要性，即如果两个字符串在前几个字符都相同的情况下，它们会获得更高的相似性。</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> textdistance<br>textdistance.jaro_winkler(<span class="hljs-string">&quot;this test&quot;</span>, <span class="hljs-string">&quot;test this&quot;</span>) <span class="hljs-comment"># .666666666...</span><br>textdistance.jaro_winkler(“ mes”，“ messi”) <span class="hljs-comment"># 0.86 </span><br></code></pre></td></tr></table></figure><h3 id="4-Jaccard-相似度"><a href="#4-Jaccard-相似度" class="headerlink" title="4. Jaccard 相似度"></a>4. Jaccard 相似度</h3><p>Jaccard Similarity（或称交集比联合）度量两个字符串之间的共享字符，而不考虑顺序。公式是查找公用令牌的数量并将其除以唯一令牌的总数。</p><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/640-20230424170101529.png" class=""><p>总结就是一句话：<strong>集合的交集与集合的并集的比例</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> textdistance<br>tokens_1 =“ hello world”.split()<br>tokens_2 =“ world hello”.split()<br>textdistance.jaccard(tokens_1，tokens_2) <span class="hljs-comment">#1.0 </span><br><br>tokens_1 =“ hello new world”.split()<br>tokens_2 = “Hello World”.split()<br>textdistance.jaccard(tokens_1，tokens_2) <span class="hljs-comment"># 0.666</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sim_jaccard</span>(<span class="hljs-params">s1, s2</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;jaccard相似度&quot;&quot;&quot;</span><br>    s1, s2 = <span class="hljs-built_in">set</span>(s1), <span class="hljs-built_in">set</span>(s2)<br>    ret1 = s1.intersection(s2)  <span class="hljs-comment"># 交集</span><br>    ret2 = s1.union(s2)  <span class="hljs-comment"># 并集</span><br>    sim = <span class="hljs-number">1.0</span> * <span class="hljs-built_in">len</span>(ret1) / <span class="hljs-built_in">len</span>(ret2)<br>    <span class="hljs-keyword">return</span> sim<br></code></pre></td></tr></table></figure><ul><li><p>缺点</p><p>Jaccard指数的一个主要缺点是，它受数据大小的影响很大。大的数据集会对指数产生很大的影响，因为它可以在保持相似的交叉点的同时显著增加联合。</p></li><li><p>用例</p><p>Jaccard指数经常用于使用二进制或二值化数据的应用中。当你有一个深度学习模型预测图像的片段时，例如，一辆汽车，Jaccard指数就可以用来计算给定真实标签的预测片段的准确度。同样，它也可以用于文本相似性分析，以衡量文档之间的选词重叠程度。因此，它可以用来比较模式的集合。</p></li></ul><h3 id="5-Sorensen–Dice-coefficient"><a href="#5-Sorensen–Dice-coefficient" class="headerlink" title="5. Sørensen–Dice coefficient"></a>5. Sørensen–Dice coefficient</h3><p>与 Jaccard 类似，Dice 系数也是一种计算简单集合之间相似度的一种计算方式。</p><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/640-20230424170128211.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> textdistance<br>tokens_1 =“ hello world”.split()<br>tokens_2 =“ world hello”.split()<br>textdistance.sorensen(tokens_1，tokens_2) <span class="hljs-comment">#1.0 </span><br><br>tokens_1 =“ hello new world”.split()<br>tokens_2 =“ hello world”.split()<br>textdistance.sorensen(tokens_1，tokens_2) <span class="hljs-comment">#0.8</span><br></code></pre></td></tr></table></figure><ul><li><p>缺点</p><p>与Jaccard指数一样，它们都高估了集合的重要性，只有很少或没有TP（Truth Positive）值的正集合。因此，它可以求得多盘的平均分数。它将每个项目与相关集合的大小成反比加权，而不是平等对待它们。</p></li><li><p>用例</p><p>与Jaccard指数相似，通常用于图像分割任务或文本相似性分析。</p></li></ul><h3 id="6-余弦相似度"><a href="#6-余弦相似度" class="headerlink" title="6. 余弦相似度"></a>6. 余弦相似度</h3><p>余弦相似度是比较两个字符串的常用方法。该算法将字符串视为向量，并计算它们之间的余弦值。与上面的 Jaccard Similarity 相似，余弦相似度也忽略了要比较的字符串中的顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> textdistance<br>textdistance.cosine(<span class="hljs-string">&quot;apple&quot;</span>, <span class="hljs-string">&quot;ppale&quot;</span>) <span class="hljs-comment"># 1.0</span><br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/textdistance-cosine.jpg" alt=""></p><h2 id="基于向量表示💡"><a href="#基于向量表示💡" class="headerlink" title="基于向量表示💡"></a>基于向量表示💡</h2><h3 id="1-欧式距离"><a href="#1-欧式距离" class="headerlink" title="1. 欧式距离"></a>1. 欧式距离</h3><p>欧氏距离（L2范数/欧几里得度量/Euclidean Distance）指在m维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离） </p><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/640-20230424170150578.png" class=""><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/20161109201239933.png" class="" title="这里写图片描述"><ul><li><p>numpy实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">EuclideanDistance</span>(<span class="hljs-params">vec1, vec2</span>):<br>    d = <span class="hljs-number">1.0</span>/(<span class="hljs-number">1.0</span> + np.linalg.norm(vec1-vec2, <span class="hljs-built_in">ord</span>=<span class="hljs-number">2</span>))<br>    <span class="hljs-comment"># d = 1.0/(1.0 + np.sqrt(np.sum(np.square(vec1-vec2))))</span><br></code></pre></td></tr></table></figure></li><li><p>scipy实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.spatial.distance <span class="hljs-keyword">import</span> pdist<br>x=np.random.random(<span class="hljs-number">8</span>)<br>y=np.random.random(<span class="hljs-number">8</span>)<br>z=np.vstack([x,y])<br>d2=pdist(z)<br></code></pre></td></tr></table></figure></li><li><p>numpy实现（矩阵形式输入）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">eucl_sim</span>(<span class="hljs-params">self, s1_vec_ist, s2_vec_list</span>):<br>    A = np.array(s1_vec_ist)<br>    B = np.array(s2_vec_list)<br>    vecProd = np.dot(A, B.transpose())<br>    SqA = A ** <span class="hljs-number">2</span><br>    sumSqA = np.matrix(np.<span class="hljs-built_in">sum</span>(SqA, axis=<span class="hljs-number">1</span>))<br>    sumSqAEx = np.tile(sumSqA.transpose(), (<span class="hljs-number">1</span>, vecProd.shape[<span class="hljs-number">1</span>]))<br>    SqB = B ** <span class="hljs-number">2</span><br>    sumSqB = np.<span class="hljs-built_in">sum</span>(SqB, axis=<span class="hljs-number">1</span>)<br>    sumSqBEx = np.tile(sumSqB, (vecProd.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))<br>    SqED = sumSqBEx + sumSqAEx - <span class="hljs-number">2</span> * vecProd<br>    SqED[SqED &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0.0</span><br>    ED = np.sqrt(SqED)<br>    ED = <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + ED[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure></li><li><p>缺点</p><p>虽然这是一种常见的距离测量方法，但欧几里得距离并不是尺度不变的，这意味着计算出的距离可能会根据特征的单位而有所偏斜。通常情况下，在使用这种距离测量之前，需要对数据进行归一化。</p></li><li><p>用例</p><p>当你有低维数据，并且向量的大小很重要，需要测量时，欧氏距离的效果非常好。如果在低维数据上使用欧氏距离，kNN和HDBSCAN等方法就会显示出很好的效果。</p></li></ul><h3 id="2-曼哈顿距离"><a href="#2-曼哈顿距离" class="headerlink" title="2. 曼哈顿距离"></a>2. 曼哈顿距离</h3><p>曼哈顿距离（L1范数/Manhattan Distance）  在欧几里得空间的固定直角坐标系上，两点所形成的线段对轴产生的投影的距离总和（ 如棋盘 ）。它们只能移动直角，且计算距离时不涉及对角线的移动。</p><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/640-20230424170216606.png" class=""><ul><li>numpy实现</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Manhattan</span>(<span class="hljs-params">vec1, vec2</span>):<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">abs</span>(dataA - dataB))<br>    <span class="hljs-comment"># return np.linalg.norm(vec1-vec2,ord=1)</span><br>    <span class="hljs-comment"># return 1/(1+np.sqrt((np.sum(vec1-vec2)**2)))</span><br></code></pre></td></tr></table></figure><ul><li><p>缺点</p><p>虽然曼哈顿距离对于高维数据似乎还不错，但它是一个比欧几里得距离更不直观的测量方法，尤其是在高维数据中使用时。</p><p>而且，它比欧几里得距离更容易给出一个更高的距离值，因为它不可能是最短路径。这不一定会带来问题，但你应该考虑到这一点。</p></li><li><p>用例</p><p>当你的数据集有离散和/或二进制属性时，曼哈顿似乎很好用，因为它考虑到了现实中在这些属性值内可以采取的路径。以欧氏距离为例，会在两个向量之间创建一条直线，而在现实中这可能实际上是不可能的。</p></li></ul><h3 id="3-余弦相似度"><a href="#3-余弦相似度" class="headerlink" title="3. 余弦相似度"></a>3. 余弦相似度</h3><ul><li><p><strong>算法描述</strong></p><p>（1）使用词向量表示问句中的每一个单词；</p><p>（2）累加求平均词向量得句子的向量表示；</p><p>（3）最后计算<strong>两个向量之间角度的余弦</strong>作为相似度</p></li></ul><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/640-20230424170228469.png" class=""><img src="/2021/11/09/2021-11-09-%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/20161109161752179.png" class="" title="余弦公式"><p>几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。余弦取值范围为[-1,1]。求得两个向量的夹角，并得出夹角对应的余弦值，此余弦值就可以用来表征这两个向量的相似性。夹角越小，趋近于0度，余弦值越接近于1，它们的方向更加吻合，则越相似。</p><ul><li>numpy实现</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Cosine</span>(<span class="hljs-params">vec1, vec2</span>):<br>    sumData = np.dot(vec1, vec2)<span class="hljs-comment"># = np.sum(dataA*dataB)</span><br>    denom = np.linalg.norm(vec1) * np.linalg.norm(vec2)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> + <span class="hljs-number">0.5</span> * (sumData / denom)<span class="hljs-comment"># 归一化 [0,1]</span><br></code></pre></td></tr></table></figure><ul><li>numpy实现（矩阵形式输入）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Cosine</span>(<span class="hljs-params">s1_vec_ist, s2_vec_list</span>):<br>    _matrixA = np.array(s1_vec_ist)<br>    _matrixB = np.array(s2_vec_list)<br>    _matrixA_matrixB = np.dot(_matrixA, _matrixB.transpose())<br><br>    _matrixA_norm = np.sqrt(np.multiply(_matrixA, _matrixA).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>))<br>    _matrixB_norm = np.sqrt(np.multiply(_matrixB, _matrixB).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>))<br>    similar_scores = np.divide(_matrixA_matrixB, _matrixA_norm * _matrixB_norm.transpose())<br>    similar_scores = <span class="hljs-number">0.5</span> + <span class="hljs-number">0.5</span> * similar_scores[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><ul><li>Keras实现</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def consine_distance(vectors):<br>    (featsA, featsB) = vectors<br>    dis = K.sum(featsA * featsB, <span class="hljs-attribute">axis</span>=1, <span class="hljs-attribute">keepdims</span>=<span class="hljs-literal">True</span>) / (<br>        K.sum(featsA ** 2, <span class="hljs-attribute">axis</span>=1, <span class="hljs-attribute">keepdims</span>=<span class="hljs-literal">True</span>)<br>        * K.sum(featsB ** 2, <span class="hljs-attribute">axis</span>=1, <span class="hljs-attribute">keepdims</span>=<span class="hljs-literal">True</span>)<br>    )<br>    return dis<br></code></pre></td></tr></table></figure><ul><li>sklearn官方实现 <a href="https://blog.csdn.net/tszupup/article/details/107942261">https://blog.csdn.net/tszupup/article/details/107942261</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity, paired_distances<br><br>simi = cosine_similarity(x, y)<br>simi = [x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> dot_list]<br></code></pre></td></tr></table></figure><ul><li><p>缺点</p><p>余弦相似性的一个主要缺点是不考虑向量的大小，只考虑其方向。在实际应用中，这意味着值的差异没有被完全考虑。以推荐系统为例，那么余弦相似性并没有考虑到不同用户之间的评分等级差异。</p></li><li><p>用例</p><p>当我们有高维数据且向量的大小并不重要时，我们经常使用余弦相似度。对于文本分析来说，当数据用字数来表示时，这种测量方法是很常用的。</p><p>例如，当一个词在一个文档中出现的频率高于另一个文档时，这并不一定意味着一个文档与该词的关系更大。可能是文档的长度不均匀，计数的大小就不那么重要了。那么，我们最好是使用不考虑大小的余弦相似性。</p></li></ul><blockquote><p><strong>PS：欧式距离和夹角余弦的区别</strong></p><ul><li>夹角余弦更能<strong>反映两者之间的变动趋势</strong>，两者有很高的<strong>变化趋势相似度</strong>；</li><li>而欧式距离较大是因为<strong>两者数值有很大的区别</strong>，即两者拥有很高的<strong>数值差异</strong>。</li></ul></blockquote><h3 id="4-修正余弦相似度"><a href="#4-修正余弦相似度" class="headerlink" title="4. 修正余弦相似度"></a>4. 修正余弦相似度</h3><ul><li><strong>算法描述</strong></li></ul><p>修正cosine 减去的是对item i打过分的每个user u，其打分的均值</p><ul><li>numpy实现</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">data</span> = np.mat([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]])<br><span class="hljs-attribute">avg</span> = np.mean(data[:,<span class="hljs-number">0</span>]) # 下标<span class="hljs-number">0</span>表示正在打分的用户<br><br><span class="hljs-attribute">def</span> AdjustedCosine(dataA,dataB,avg):<br>    <span class="hljs-attribute">sumData</span> = (dataA - avg) * (dataB - avg).T # 若列为向量则为 dataA.T * dataB<br>    <span class="hljs-attribute">denom</span> = np.linalg.norm(dataA - avg) * np.linalg.norm(dataB - avg)<br>    <span class="hljs-attribute">return</span> <span class="hljs-number">0</span>.<span class="hljs-number">5</span> + <span class="hljs-number">0</span>.<span class="hljs-number">5</span> * (sumData / denom)<br><br><span class="hljs-attribute">print</span>(AdjustedCosine(data[<span class="hljs-number">0</span>,:],data[<span class="hljs-number">1</span>,:],avg))<br></code></pre></td></tr></table></figure><h3 id="5-皮尔森相关系数"><a href="#5-皮尔森相关系数" class="headerlink" title="5. 皮尔森相关系数"></a>5. 皮尔森相关系数</h3><p>皮尔森相关系数 (Pearson Correlation Coefficient) 是<strong>余弦相似度在维度缺失上面的一种改进方法</strong>，用于度量两个变量之间的相关程度 。实际上是在计算夹角余弦之前将两个向量<strong>减去各个样本的平均值</strong>，达到<strong>中心化</strong>的目的。</p><ul><li>numpy实现1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Pearson</span>(<span class="hljs-params">vec1, vec2</span>):<br>    _vec1 = vec1 - np.mean(vec1)<br>    _vec2 = vec2 - np.mean(vec2)<br>    sumData = np.dot(_vec1, _vec2)<br>    denom = np.linalg.norm(_vec1) * np.linalg.norm(_vec2)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> + <span class="hljs-number">0.5</span> * (sumData / denom)<br></code></pre></td></tr></table></figure><ul><li>numpy实现2</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">def</span> Pearson(vec1, vec2):<br>    <span class="hljs-attribute">X</span>=np.vstack([vec1, vec2])<br>    <span class="hljs-attribute">return</span> <span class="hljs-number">0</span>.<span class="hljs-number">5</span> + <span class="hljs-number">0</span>.<span class="hljs-number">5</span> * np.corrcoef(X)[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><ul><li>numpy实现（矩阵形式输入）</li></ul><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs sqf">def pers_sim(self, s1_vec_ist, s2_vec_list):<br>    <span class="hljs-variable">_matrixA</span> = np.array(s1_vec_ist)<br>    <span class="hljs-variable">_matrixB</span> = np.array(s2_vec_list)<br>    avgA = np.mean(<span class="hljs-variable">_matrixA</span>)<br>    avgB = np.mean(<span class="hljs-variable">_matrixB</span>)<br>    <span class="hljs-variable">_matrixA</span> = <span class="hljs-variable">_matrixA</span> - avgA<br>    <span class="hljs-variable">_matrixB</span> = <span class="hljs-variable">_matrixB</span> - avgB<br>    <span class="hljs-variable">_matrixA_matrixB</span> = np.dot(<span class="hljs-variable">_matrixA</span>, <span class="hljs-variable">_matrixB</span>.transpose())<br><br>    <span class="hljs-variable">_matrixA_norm</span> = np.<span class="hljs-built_in">sqrt</span>(np.multiply(<span class="hljs-variable">_matrixA</span>, <span class="hljs-variable">_matrixA</span>).sum(axis=<span class="hljs-number">1</span>))<br>    <span class="hljs-variable">_matrixB_norm</span> = np.<span class="hljs-built_in">sqrt</span>(np.multiply(<span class="hljs-variable">_matrixB</span>, <span class="hljs-variable">_matrixB</span>).sum(axis=<span class="hljs-number">1</span>))<br>    similar_scores = np.divide(<span class="hljs-variable">_matrixA_matrixB</span>, <span class="hljs-variable">_matrixA_norm</span> * <span class="hljs-variable">_matrixB_norm</span>.transpose())<br>    similar_scores = <span class="hljs-number">0.5</span> + <span class="hljs-number">0.5</span> * similar_scores[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><h3 id="6-Word-Mover’s-Distance"><a href="#6-Word-Mover’s-Distance" class="headerlink" title="6. Word Mover’s Distance #"></a>6. Word Mover’s Distance<a href="https://kexue.fm/archives/7388#Word Mover&#39;s Distance"> #</a></h3><p>Word Mover’s Distance（WMD）（推词机距离？），大概可以理解为将一个句子变为另一个句子的最短路径，某种意义上也可以理解为编辑距离的光滑版。实际使用的时候，通常会去掉停用词后再计算WMD。</p><p>参考实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">word_mover_distance</span>(<span class="hljs-params">x, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;WMD（Word Mover&#x27;s Distance）的参考实现</span><br><span class="hljs-string">    x.shape=[m,d], y.shape=[n,d]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    p = np.ones(x.shape[<span class="hljs-number">0</span>]) / x.shape[<span class="hljs-number">0</span>]<br>    q = np.ones(y.shape[<span class="hljs-number">0</span>]) / y.shape[<span class="hljs-number">0</span>]<br>    D = np.sqrt(np.square(x[:, <span class="hljs-literal">None</span>] - y[<span class="hljs-literal">None</span>, :]).mean(axis=<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> wasserstein_distance(p, q, D)<br></code></pre></td></tr></table></figure><h3 id="7-Word-Rotator’s-Distance"><a href="#7-Word-Rotator’s-Distance" class="headerlink" title="7. Word Rotator’s Distance #"></a>7. Word Rotator’s Distance<a href="https://kexue.fm/archives/7388#Word Rotator&#39;s Distance"> #</a></h3><p>WMD其实已经挺不错了，但非要鸡蛋里挑骨头的话，还是能挑出一些缺点来：</p><blockquote><p>1、它使用的是欧氏距离作为语义差距度量，但从Word2Vec的经验我们就知道要算词向量的相似度的话，用coscos往往比欧氏距离要好；</p><p>2、WMD理论上是一个无上界的量，这意味着我们不大好直观感知相似程度，从而不能很好调整相似与否的阈值。</p></blockquote><p>为了解决这两个问题，一个比较朴素的想法是将所有向量除以各自的模长来归一化后再算WMD，但这样就完全失去了模长信息了。最近的论文<a href="https://arxiv.org/abs/2004.15003">《Word Rotator’s Distance: Decomposing Vectors Gives Better Representations》</a>则巧妙地提出，在归一化的同时可以把模长融入到约束条件p,qp,q里边去，这就形成了WRD。</p><p>Word Rotator’s Distance（WRD）了。由于使用的度量是余弦距离，所以两个向量之间的变换更像是一种旋转（rotate）而不是移动（move），所以有了这个命名；同样由于使用了余弦距离，所以它的结果在[0,2][0,2]内，相对来说更容易去感知其相似程度。</p><p>参考实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">word_rotator_distance</span>(<span class="hljs-params">x, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;WRD（Word Rotator&#x27;s Distance）的参考实现</span><br><span class="hljs-string">    x.shape=[m,d], y.shape=[n,d]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x_norm = (x**<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)**<span class="hljs-number">0.5</span><br>    y_norm = (y**<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)**<span class="hljs-number">0.5</span><br>    p = x_norm[:, <span class="hljs-number">0</span>] / x_norm.<span class="hljs-built_in">sum</span>()<br>    q = y_norm[:, <span class="hljs-number">0</span>] / y_norm.<span class="hljs-built_in">sum</span>()<br>    D = <span class="hljs-number">1</span> - np.dot(x / x_norm, (y / y_norm).T)<br>    <span class="hljs-keyword">return</span> wasserstein_distance(p, q, D)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">word_rotator_similarity</span>(<span class="hljs-params">x, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;1 - WRD</span><br><span class="hljs-string">    x.shape=[m,d], y.shape=[n,d]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - word_rotator_distance(x, y)<br></code></pre></td></tr></table></figure><h2 id="常用文本匹配库"><a href="#常用文本匹配库" class="headerlink" title="常用文本匹配库"></a>常用文本匹配库</h2><p> <a href="https://github.com/life4/textdistance">textdistance</a> 是一个用于比较两个或多个序列之间距离的Python库。它使用30多种不同的算法计算序列的距离，提供了可用于模糊匹配算法的集合.</p><p><a href="https://github.com/ztane/python-Levenshtein">python-Levenshtein</a> 是一个快速计算Levenshtein距离和字符串相似度的模块。它能够快速计算出编辑距离以及编辑操作.</p><h3 id="textdistance"><a href="#textdistance" class="headerlink" title="textdistance"></a>textdistance</h3><p><strong>textdistance</strong>包提供了可用于模糊匹配算法的集合，可以使用如下所示的pip来安装<strong>textdistance</strong>：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> textdistance<br></code></pre></td></tr></table></figure><p>但是，如果您希望从算法中获得最快的速度，则可以调整pip install命令，如下所示：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> textdistance[extras]<br></code></pre></td></tr></table></figure><p>安装完成后，我们可以像下面那样导入<strong>textdistance</strong>：</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> textdistance<br></code></pre></td></tr></table></figure><p>All algorithms have some common methods:</p><ol><li><code>.distance(*sequences)</code> — calculate distance between sequences.</li><li><code>.similarity(*sequences)</code> — calculate similarity for sequences.</li><li><code>.maximum(*sequences)</code> — maximum possible value for distance and similarity. For any sequence: <code>distance + similarity == maximum</code>.</li><li><code>.normalized_distance(*sequences)</code> — normalized distance between sequences. The return value is a float between 0 and 1, where 0 means equal, and 1 totally different.</li><li><code>.normalized_similarity(*sequences)</code> — normalized similarity for sequences. The return value is a float between 0 and 1, where 0 means totally different, and 1 equal.</li></ol><p>For example, <a href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> textdistance<br><br>textdistance.hamming(<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>)<span class="hljs-comment"># 1</span><br>textdistance.hamming.distance(<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>)<span class="hljs-comment"># 1</span><br>textdistance.hamming.similarity(<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>)<span class="hljs-comment"># 3</span><br>textdistance.hamming.normalized_distance(<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>)<span class="hljs-comment"># 0.25</span><br>textdistance.hamming.normalized_similarity(<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>)<span class="hljs-comment"># 0.75</span><br>textdistance.Hamming(qval=<span class="hljs-number">2</span>).distance(<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>)<span class="hljs-comment"># 2</span><br></code></pre></td></tr></table></figure><p>Any other algorithms have same interface.</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://kexue.fm/archives/7388">从EMD、WMD到WRD：文本向量序列的相似度计算</a>√</p><p><a href="http://www.yglong.com/beginner-guide-to-text-vectorization.html">文本向量化方法的原理及实现详解</a></p><p><a href="https://cloud.tencent.com/developer/article/1149836">计算两个字符串相(或句子)似度的方法1 编辑距离2 余弦相似度3 FuzzyWuzzy</a></p><p><a href="http://weirping.coding.me/blog/edit-distance.html">字串相似度-编辑距离</a></p><p><a href="https://blog.csdn.net/qq_19707521/article/details/78479532">Python Numpy计算各类距离</a></p><p><a href="https://www.lizenghai.com/archives/3071.html">8种相似度度量方式的原理及实现</a></p><p><a href="https://zhuanlan.zhihu.com/p/33164335">推荐算法入门（1）相似度计算方法大全</a></p><p><a href="https://www.jianshu.com/p/5e49160735ae">相似度算法原理及python实现</a> √</p><p><a href="https://mp.weixin.qq.com/s/3-_TzRvXmqdRov1W5_-F7A">常见距离度量方法优缺点对比！Datawhale</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度语义匹配</title>
    <link href="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/"/>
    <url>/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/</url>
    
    <content type="html"><![CDATA[<p>简要介绍语义文本相似度计算的最新研究进展, 主要包括基于字符串、基于统计、基于深度学习的语义相似度计算方法。</p> <span id="more"></span><h2 id="1-基于字符串"><a href="#1-基于字符串" class="headerlink" title="1.基于字符串"></a>1.基于字符串</h2><p>基于字符串的方法都是直接对原始文本进行比较, 主要包括编辑距离[11] (Levenshtein Distance, LD)、最长公共子序列[12] (Longest Common Sequence, LCS)、N-Gram[13] 和 Jaccard 相似度[14] 等. 具体可参考《2021-05-26-常见距离度量方法》</p><p>基于字符串的方法原理简单、实现方便, 并且直接对原始文本进行比较, 多用于文本的快速模糊 匹配, 其不足主要在于没有考虑到单词的含义及单词和单词之间的相互关系, 并且同义词、多义词等问题都无法处理.</p><h2 id="2-基于统计"><a href="#2-基于统计" class="headerlink" title="2.基于统计"></a>2.基于统计</h2><p>基于统计的方法源于一种分布假设, 该假设认为上下文相似的单词具有相似的语义, 这类计算方 法先通过某种策略将文本转换成一个向量, 然后通过各种向量空间的转换, 最后计算表征文本的向量间距离, 通过向量空间中的度量来衡量文本间的相似度. </p><p>主流的基于统计的方法包括向量空间模型[17] (Vector Space Model, VSM) 和主题模型 (Topic Model), 而主题模型又可分为潜在语义分析模型[18] (Latent Semantic Analysis, LSA)、概率潜在语义分析模型[19] (Probabilistic Latent Semantic Analysis, PLSA) 和隐含狄利克雷分布模型[20] (Latent Dirichlet Allocation, LDA) 等.</p><h3 id="2-1-基于向量空间模型"><a href="#2-1-基于向量空间模型" class="headerlink" title="2.1　基于向量空间模型"></a>2.1　基于向量空间模型</h3><p>Salton 等[17] 在 1975 年首次提出向量空间模型 (VSM), 主要思想就是假设一个文本的语义只与该文本中的单词有关, 而忽略其语序和单词之间的相互关系, 然后通过<strong>基于词频统计</strong>的方法, 将文本映射成向量, 最后通过向量间的距离计算以表征文本间的相似度. </p><p>目前 VSM 中最常用的是基于 TF-IDF 的权重计算法（详细介绍可参考<a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html">TF-IDF与余弦相似性的应用（二）：找出相似文章</a>）, TF-IDF的分数代表了词语在当前文档和整个语料库中的相对重要性。TF-IDF 分数由两部分组成：第一部分是<strong>词语频率</strong>（Term Frequency），第二部分是<strong>逆文档频率</strong>（Inverse Document Frequency），计算方式如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">TF</span><span class="hljs-params">(t)</span></span>= 该词语在当前文档出现的次数 / 当前文档中词语的总数<br><span class="hljs-function"><span class="hljs-title">IDF</span><span class="hljs-params">(t)</span></span>= log_e（文档总数 / 出现该词语的文档总数+α）<br>TF-IDF=TF*IDF<br></code></pre></td></tr></table></figure><p>TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。</p><ul><li>代码示例如下（借助<code>Sklearn</code>库）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer<br> <br>texts = [<br>    <span class="hljs-string">&#x27;There used to be Stone Age&#x27;</span>,<br>    <span class="hljs-string">&#x27;There used to be Bronze Age bronze&#x27;</span>,<br>]<br><br><span class="hljs-comment"># word level tf-idf</span><br>vectorizer = TfidfVectorizer(analyzer=<span class="hljs-string">&#x27;word&#x27;</span>)<br>vec = vectorizer.fit_transform(texts)<br><span class="hljs-built_in">print</span>(vectorizer.vocabulary_)<br><span class="hljs-built_in">print</span>(vec.toarray())<br><br><span class="hljs-comment"># ngram level tf-idf </span><br>tfidf_vect_ngram = TfidfVectorizer(analyzer=<span class="hljs-string">&#x27;word&#x27;</span>, token_pattern=<span class="hljs-string">r&#x27;\w&#123;1,&#125;&#x27;</span>, ngram_range=(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>), max_features=<span class="hljs-number">5000</span>)<br>tfidf_vect_ngram.fit(texts)<br>xtrain_tfidf = tfidf_vect.transform(texts)<br></code></pre></td></tr></table></figure><p>在利用 TF-IDF 权重计算法计算出各个特征项的权重之后, 就得到了可以表征文本的向量, 接下来只要计算向量之间的距离即可, 一般来说, 距离越近则两文本越相似. </p><h3 id="2-2-基于主题模型"><a href="#2-2-基于主题模型" class="headerlink" title="2.2　基于主题模型"></a>2.2　基于主题模型</h3><p>主题模型的基本假设是每个文档包含 多个主题, 而每个主题又包含多个单词. 换句话说, 文档的语义由一些隐变量表示, 这里的隐变量是指主题, 而这些主题代表了文档的语义信息. 而主题模型的目的就是揭示这些隐变量和它们之间的相互关系. 主题模型主要包括: ① 潜在语义分析 (LSA) 模型; ② 概率潜在语义分析 (PLSA) 模型; ③ 潜在狄 利克雷分布 (LDA) 模型.</p><p><strong>潜在语义分析（LSA, Latent Semantic Analysis）</strong><a href="https://leovan.me/cn/2020/10/text-similarity/#fn:deerwester1990indexing">5</a> 的核心思想是将文本的高维词空间映射到一个低维的向量空间，我们称之为隐含语义空间。降维可以通过<a href="https://leovan.me/cn/2017/12/evd-svd-and-pca/">奇异值分解（SVD）</a>实现.</p><h3 id="2-3-基于概率模型"><a href="#2-3-基于概率模型" class="headerlink" title="2.3　基于概率模型"></a>2.3　基于概率模型</h3><p>BM25 算法的全称为 Okapi BM25，是一种搜索引擎用于评估查询和文档之间相关程度的排序算法，其中 BM 是 Best Match 的缩写. BM25 算法是对 TF-IDF 算法的优化，在词频的计算上，BM25 限制了文档 D 中关键词 qi 的词频对评分的影响。为了防止词频过大，BM25 将这个值的上限设置为 k1+1。</p><script type="math/tex; mode=display">bm25=A(TF)*B(IDF)</script><p>首先我们来看一下bm25算法的计算公式：</p><script type="math/tex; mode=display">\operatorname{Score}(Q, d)=\sum_{i}^{n} W_{i} \cdot R\left(q_{i}, d\right)</script><p>我们来看看这个公式，首先Wi表示第i个词的权重，这里我们一般会使用TF-IDF算法来计算词语的权重，我在之前的博文<a href="https://blog.csdn.net/oscar6280868/article/details/80884470">对TF-IDF的理解与数学推导</a>中，对TF-IDF算法有详细地分析与介绍，大家可以阅读参考。这个公式第二项R(qi,d)表示我们查询query中的每一个词和文章d的相关度，这一项就涉及到复杂的运算，我们慢慢来看。一般来说Wi的计算我们一般用逆项文本频率IDF的计算公式：</p><script type="math/tex; mode=display">\operatorname{IDF}\left(q_{i}\right)=\log \frac{N+0.5}{n\left(q_{i}\right)+0.5}</script><p>在这个公式中，N表示文档的总数，n(qi)表示包含这个词的文章数，为了避免对数里面分母项等于0，我们给分子分母同时加上0.5，这个0.5被称作调教系数，所以当n(qi)越小的时候IDF值就越大，表示词的权重就越大。我们来举个栗子：“bm25”这个词只在很少一部分的文章中出现，n(qi)就会很小，那么“bm25”的IDF值就很大；“我们”，“是”，“的”这样的词，基本上在每一篇文章中都会出现，那么n(qi)就很接近N，所以IDF值就很接近于0，接着我们来看公式中的第二项R(qi,d)，我们首先来看看第二项的计算公式：</p><script type="math/tex; mode=display">R\left(q_{i}, d\right)=\frac{f_{i}\left(k_{1}+1\right)}{f_{i}+K} \cdot \frac{q f_{i}\left(k_{2}+1\right)}{q f_{i}+k_{2}}</script><p>在这个公式中，一般来说，k1、k2和b都是调节因子，k1=1、k2=1、b = 0.75,qfi表示qi在查询query中出现的频率，fi表示qi在文档d中出现的频率，因为在一般的情况下，qi在查询query中只会出现一次，因此把qfi=1和k2=1代入上述公式中，后面一项就等于1，最终可以得到：</p><script type="math/tex; mode=display">R\left(q_{i}, d\right)=\frac{f_{i}\left(k_{1}+1\right)}{f_{i}+K}</script><p>我们再来看看K，在这里其实K的值也是一个公式的缩写，我们把K展开来看：</p><script type="math/tex; mode=display">K=\mathrm{k}_{1} \cdot\left(1-b+b \cdot \frac{d l}{a v g(d l)}\right)</script><p>在K的展开式中dl表示文档的长度，avg(dl)表示文档的平均长度，b是前面提到的调节因子，从公式中可以看出在文章长度比平均文章长度固定的情况下，调节因子b越大，文章长度占有的影响权重就越大，反之则越小。在调节因子b固定的时候，当文章的长度比文章的平均长度越大，则K越大，R(qi,d)就越小。我们把K的展开式带入到bm25计算公式中去：</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{Score}(Q, d)=\sum_{i}^{n} W_{i} \cdot R\left(q_{i}, d\right) \\=\sum_{i}^{n} W_{i} \cdot \frac{f_{i}\left(k_{1}+1\right)}{f_{i}+K} \\=\sum_{i}^{n} \operatorname{IDF}\left(q_{i}\right) \cdot \frac{f_{i}\left(k_{i}+1\right)}{f_{i}+k_{1} \cdot\left(1-b+b \frac{d l}{\operatorname{avg}(d l)}\right)} \\=\sum_{i}\left(\log \frac{N+0.5}{n\left(q_{i}\right)+0.5}\right) \cdot \frac{f_{i}\left(k_{i}+1\right)}{f_{i}+k_{1} \cdot\left(1-b+b \frac{d l}{\operatorname{avg}(d l)}\right)}\end{aligned}</script><p>在我们了解了bm25算法的原理了之后，我们再一起学习一下用Python来实现它，参考<a href="https://gist.github.com/koreyou/f3a8a0470d32aa56b32f198f49a9f2b8">使用sklearn的TfidfVectorizer实现OKapi BM25</a></p><ul><li>个人代码实现</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sim_bm25</span>(<span class="hljs-params">s1, s2, s_avg=<span class="hljs-number">6.05</span>, k1=<span class="hljs-number">2.0</span>, b=<span class="hljs-number">0.75</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;BM25算法计算文本相似度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">score_w</span>(<span class="hljs-params">idf_w, fid, len_d</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;计算query中某词与候选文档的相关度得分&quot;&quot;&quot;</span><br>        ra = fid * (k1 + <span class="hljs-number">1</span>)<br>        rb = fid + k1 * (<span class="hljs-number">1</span> - b + b * len_d / s_avg)<br>        <span class="hljs-keyword">return</span> idf_w * (ra / rb)<br>    sim = <span class="hljs-number">0</span><br>    len_d = <span class="hljs-built_in">len</span>(s2)<br>    <span class="hljs-comment"># 1. 分词</span><br>    s1_list, s2_list = jieba.lcut(s1), jieba.lcut(s2)<br>    <span class="hljs-comment"># 2. 计算s1中每个词与s2的相关度得分并累加得s1与s2的相关度</span><br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> s1_list:<br>        idf_w = idf.get(w, <span class="hljs-number">1</span>)<br>        fid = s2_list.count(w)<br>        score = score_w(idf_w, fid, len_d)<br>        sim += score<br>    <span class="hljs-keyword">return</span> sim<br></code></pre></td></tr></table></figure><p>如果想更深入的了解BM25的理论基础，推荐阅读<a href="https://leovan.me/cn/2020/10/text-similarity/；想要了解基本实现代码，可以参考">https://leovan.me/cn/2020/10/text-similarity/；想要了解基本实现代码，可以参考</a> <a href="https://gist.github.com/koreyou/f3a8a0470d32aa56b32f198f49a9f2b8">Implementation of OKapi BM25 with sklearn’s TfidfVectorizer</a></p><p><strong>小结</strong></p><p>传统的文本匹配技术主要有Jaccard、Levenshtein、Simhash、TF-idf、Bm25、VSM等算法，其主要是基于统计学方法通过词汇重合度来计算两段文本的字面相似度。然而，仅通过字面相似度是衡量文本的匹配度是远远不够的，因为同一语义的文本在形式上千变万化，两段文本可以表现为字面相似但词序不同而导致语义完全相反；可以表现为字面相似但个别字词不同而导致意思大相径庭；更可以表现为字面完全不相似而语义相同等问题。所以，传统的匹配算法存在着词义局限、结构局限等问题。</p><h2 id="3-基于深度学习"><a href="#3-基于深度学习" class="headerlink" title="3.基于深度学习"></a>3.基于深度学习</h2><h3 id="3-1-分布式词向量"><a href="#3-1-分布式词向量" class="headerlink" title="3.1　分布式词向量"></a>3.1　分布式词向量</h3><p>简单地说, 词向量技术就是将单词映射成向量, 最早出现的 one-hot 编码和 TF-IDF 方法都可以 将单词映射为向量. 但是, 这两种方法都面临<strong>维度灾难</strong>和<strong>语义鸿沟</strong>问题. 分布式词向量可以在保存更多语义信息的同时降低向量维度, 在一定程度上可以克服维度灾难和语义鸿沟问题. </p><p>Mikolov 等[55] 提出的 <strong>word2vec</strong> 是最早生成分布式词向量的方法, 它包含两个模型, 分别为 CBOW 和 Skip-gram, 基本思路是确定中心词和上下文窗口大小, CBOW 是通过上下文来预测中心词, Skipgram 是通过中心词来预测上下文, 整体来说是通过自监督训练的模型生成词向量. word2vec 的主要 问题在于它只能考虑局部信息, 局部信息的大小取决于上下文窗口的大小. </p><p>Pennington 等[56] 提出 <strong>Glove</strong> 模型, 该模型通过语料库构建单词的共现矩阵, 然后通过该共现矩阵 用概率的思想得到最终的词向量, 综合了全局语料, 在一定程度上考虑了全局信息. </p><p>Joulin 等[57] 则是提出了一种快速文本分类方法 <strong>FastText</strong>, 其同样可以用于生成词向量, 模型架构 与 CBOW 类似, 但赋予模型的任务不同. CBOW预测上下文的中间词，fasttext预测文本标签。与word2vec算法的衍生物相同，稠密词向量也是在训练神经网络的过程中得到的。fasttext的<a href="https://link.zhihu.com/?target=https%3A//github.com/salestock/fastText.py">python实现</a></p><p>以上 3 种词向量为静态词向量, 即当它们应用于下游任务时词向量始终是固定的, 无法解决一词 多义问题, 同时无法满足不同语境下语义不同的场景需求. </p><p>动态词向量则是为了解决上述问题所提出的, 这类词向量首先在大型语料库上进行预训练, 然后在面对具体下游任务时微调所有参数, 那么在上下文输入不同时, 生成的词向量也不同, 因此可以解 决一词多义问题. </p><p>Peters 等[58] 提出 <strong>ELMO</strong> 模型, 其使用双向语言模型和两个分开的双层 LSTM 作为编码器, 通过 在大型语料库上进行预训练得到动态词向量. </p><p>Radford 等[59] 提出 <strong>GPT</strong> 模型, 通过将单向语言模型与编码能力更强大的 Transformer[60] 架构的 decoder 部分相结合, 从而生成词向量. </p><p>Devlin 等[61] 提出的 <strong>BERT</strong> 模型, 则是应用 Transformer[60] 的 encoder 部分, 结合 mask 机制, 并且对模型增加了预测“next sentence prediction”任务, 从而生成了更加优质的动态词向量, 该模型也是目前最常用的词向量预训练方法之一. 要是想使用BERT作为服务，具体实现过程可以参考博客<a href="[https://ningshixian.github.io/2019/11/08/bertasservice%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/](https://ningshixian.github.io/2019/11/08/bertasservice使用指南/">《<strong>bert_as_service使用指南</strong>》</a>)</p><h3 id="3-2-基于无监督学习方法的语义文本相似度计算"><a href="#3-2-基于无监督学习方法的语义文本相似度计算" class="headerlink" title="3.2　基于无监督学习方法的语义文本相似度计算"></a>3.2　基于无监督学习方法的语义文本相似度计算</h3><p>无监督学习方法不需要带有标签的数据集就可以计算文本间的语义相似度, 减少了人工打标签的成本, 并且更加通用</p><p><a href="https://cloud.tencent.com/developer/article/1155863">https://cloud.tencent.com/developer/article/1155863</a></p><p>Le 等[62] 在 word2vec[55] 的启发下提出了 <strong>doc2vec</strong>. 该模型在训练词向量的同时, 加入表征段落的向 量和词向量共同训练. doc2vec 与 word2vec 相同, 有两种训练方式, 一种是通过段落向量和上下文单 词向量预测中心词; 另一种则是通过段落向量预测文本中包含的单词. 这样就可以通过计算训练好的 段落向量之间的余弦值表征其语义文本相似度. gensim库提供了doc2vec的实现.</p><p>Kusner 等[67] 将句子相似度问题转换为运输问题, 引入词移距离, 将文本间距离转化为约束条件下 的最优化问题, 巧妙地将运输问题中的 EMD 算法应用到相似度计算当中, 得到了 <strong>WMD</strong> 算法. 简单来说，用句子A走到句子B的最短距离来衡量两者的相似程度。表示在下图中就是非停用词的向量转移总距离：</p><p><img src="https://pic1.zhimg.com/80/v2-4b9262114340fdf55afc1fd30404d5d0_1440w.jpg" alt="img" style="zoom: 67%;" /></p><p>Arora 等[68] 提出的 <strong>SIF</strong> 算法则是将主成分分析 (PCA) 用于句向量的生成, 首先用通过改进的 TFIDF 方法对词向量进行加权平均得到句向量, 然后减去所有句子向量组成的矩阵的第一个主成分上的投影, 得到最终的句子嵌入. 实验表明该方法具有不错的竞争力, 在大部分数据集上都比平均词向量或者使用 TFIDF 加权平均的效果要好.</p><h3 id="3-3-基于监督学习方法"><a href="#3-3-基于监督学习方法" class="headerlink" title="3.3　基于监督学习方法"></a>3.3　基于监督学习方法</h3><p>监督学习方法是一类需要带有标签的训练集对模型进行训练后才能进行语义文本相似度计算的 方法, 这类方法由于有标签对模型进行指导, 在多数有训练集的任务上, 其性能要优于无监督学习方 法. 监督学习方法从模型架构上可以分为两种, 一种是<strong>孪生网络</strong> (Siamese Network) 架构; 另一种是<strong>交互 (interaction-based) 模型</strong>架构, 两种模型的典型架构如图所示.</p><img src="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/image-20230424110804738.png" class="" title="image-20230424110804738"><ol><li><p><strong>孪生网络架构</strong>主要体现在句子对中$S_A$和$S_B$同时输入左右两个网络当中, 这两个网络的输入层和编码层模型架构完全相同并共享权重，通过余弦相似度等方式来计算query间的相似度，不断最大化正样本之间的相关性，抑制负样本之间的相关性；双塔式更侧重对表示层的构建</p></li><li><p><strong>交互模型</strong>如图 2b) 和图 2c) 所示, 整体与孪生网络类似, 但在编码层利用 Attention 机制或其他技术增加两个网络间的交互，交互式模型通常使用二分类的任务来进行训练，当模型输入的两个query语义一致，label为“1”，反之，label为“0”。在预测时，可通过logits来作为置信度判断。</p></li></ol><p>两种框架比较的话，</p><ul><li>基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量速度会快很多；缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。</li><li>基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。</li><li><p>二者在Similarity measure layer的匹配度函数f(x,y)计算，通常有两种方法：一种是通过相似度度量函数进行计算，实际使用过程中最常用的就是 cosine 函数，这种方式简单高效，并且得分区间可控意义明确；另一种方法是将两个向量再接一个多层感知器网络（MLP），通过数据去训练拟合出一个匹配度得分，更加灵活拟合能力更强，但对训练的要求也更高。</p></li><li><p><strong>语义表征模型常用于海量query召回，交互式模型更多使用于语义排序阶段</strong></p></li></ul><h4 id="3-3-1-基于孪生网络的方法"><a href="#3-3-1-基于孪生网络的方法" class="headerlink" title="3.3.1 基于孪生网络的方法"></a>3.3.1 基于孪生网络的方法</h4><p>它的优势是结构简单、解释性强，且易于实现，是深度学习出现之后应用最广泛的深度文本匹配方法。整个学习过程可以分为两步：① 表示层：计算 query 和 doc 各自的 representation；② 匹配层：根据上一步得到的 representation，计算 query-doc 的匹配分数。</p><h5 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h5><p>DSSM 算法是最先将孪生网络架构用于语义文本相似度计算的算法之一, DSSM 架构主要分为输入层、表示层、匹配层, 这种三层架构也是后来基于孪生网络的算法最常用的 架构. 输入层主要将原始文本映射为向量, 由于本文在 2013 年被提出, 当时的分布式词向量刚刚问世, 因此本文并没有使用词向量, 而是使用字符级的 <strong>trigram</strong> 方法将文本映射为高维向量, 更具体地说, trigram 是将 3 个字符为一组映射为 one-hot 向量, 最后将句子中的 trigram 向量相加得到高维句子向 量表示. 表示层是将高维的句子向量映射为低维向量, DSSM 表示层就是简单的几个全连接层, 最终 将句子映射为 128 维的低维向量. 匹配层是将两个低维句子向量表示之间求<strong>余弦相似度</strong>来表征两个句子的语义相似度.  DSSM模型在文本匹配任务上取得了突出的成绩, 但忽略了语序信息和上下文信息.</p><p><img src="https://pic1.zhimg.com/80/v2-774b04defa9ab870c7e8d9fad6cde190_1440w.jpg" alt="img" style="zoom: 50%;" /></p><h5 id="ARC-I"><a href="#ARC-I" class="headerlink" title="ARC-I"></a>ARC-I</h5><p>针对上述讲到的 DSSM 模型对 query 和 doc 序列和上下文信息捕捉能力的不足，华为诺亚方舟在 2015 年在 DSSM 的模型基础上加入了 <strong>CNN 模块</strong>，通过卷积层不同的 feature map 来得到相邻 term 之间的多种组合关系，通过池化层 max pooling 来提取这些组合关系中最重要的部分，进而得到 query 和 doc 各自的表示。卷积层虽然提取到了 word-ngram 的信息，但是池化层依然是在局部窗口进行 pooling，因此一定程度上无法得到全局的信息。</p><img src="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/image-5a59670f2113499d95fc165ca56b7065.webp" class=""><h5 id="Siam-CNN"><a href="#Siam-CNN" class="headerlink" title="Siam-CNN"></a><strong>Siam-CNN</strong></h5><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">题目：Applying Deep Learning to Answer Selection: A Study and An Open Task<br>论文：https://arxiv.org/pdf/1508.01585.pdf<br></code></pre></td></tr></table></figure><p>在2015年IBM提出的Siam-CNN架构中，作者尝试了<strong>多种孪生架构</strong>，使用CNN作为基础编码器，hinge loss作为损失函数. 该方法对DSSM的改进主要发生在表示层, 该模型表示层中添加了卷积层和池化层, 使得上下文信息得到了有效保留, 但是由于卷积核的限制, 距离较远的上下文信息仍会丢失. 最后实验发现第二种是最好的. </p><p>PS： During training, for each training question Q there is a positive answer A +(the ground truth). A training instance is constructed by pairing this A + with a negative answer A −(a wrong answer) sampled from the whole answer space</p><p><img src="https://pic1.zhimg.com/80/v2-f8a949b01adbaa20a974ad41901215cc_1440w.jpg" alt="img" style="zoom:67%;" /></p><h5 id="Siam-LSTM"><a href="#Siam-LSTM" class="headerlink" title="Siam-LSTM"></a><strong>Siam-LSTM</strong></h5><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">题目：Siamese Recurrent Architectures for Learning Sentence Similarity<br>论文：http://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf<br></code></pre></td></tr></table></figure><p>2016年的Siam-LSTM在结构上比较简单，就是直接用共享权重的LSTM编码，取最后一步的输出作为表示。有个改进点是作者使用了<strong>Manhattan距离</strong>计算句子对的语义相似度. 实验证明将该方法和SVM结合用于情感分类(Entailment Classification), 效果提升明显.</p><p><img src="https://pic3.zhimg.com/80/v2-012618cc9e422b78fc0c57972fa687f2_1440w.jpg" alt="img" style="zoom:67%;" /></p><h5 id="BiLSTM-Self-Attention"><a href="#BiLSTM-Self-Attention" class="headerlink" title="BiLSTM+Self-Attention"></a>BiLSTM+Self-Attention</h5><p>Lin等[A structured self-attentive sentence embedding] 将<strong>双向LSTM(BiLSTM)和Self-Attention</strong>技术相结合得到句子向量表示, 具体来说就是, 首先将句子通过BiLSTM模型, 将得到的每一时刻的两个方向的向量拼接成一个二维矩阵, 然后通过自注意力机制(Self-Attention)[<a href="javascript:;">60</a>]得到句中每个词向量对应的权重, 最终通过词向量的加权求和得到句向量. 在训练网络时同样是使用Siamese架构, 在得到句向量后进行简单的特征提取, 如拼接、点积、对位相减等, 然后输入一个多层感知机, 得到最终的语义文本相似度.</p><img src="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/image-20230424111128904.png" class="" title="image-20230424111128904"><h5 id="Sentence-BERT"><a href="#Sentence-BERT" class="headerlink" title="Sentence-BERT"></a>Sentence-BERT</h5><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">题目：Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks<br>论文：https://arxiv.org/pdf/1908.10084<br>代码：https://github.com/UKPLab/sentence-transformers<br></code></pre></td></tr></table></figure><p>EMNLP2019的Sentence-BERT是目前最常用的BERT式双塔模型，一是效果真的好，二是作者的开源工具做的很方便，用的人越来越多。</p><p>Sentence BERT(Sbert) 网络是通过 SNLI 数据集（标注了一对句子之间的关系，可能是蕴含、矛盾或者中立）进行预训练。首先将第一个句子输入到BERT，通过不同的Pooling方法获得句子的Embedding表示，第二个句子同样如此，然后将这两个Embedding变换后通过Softmax输出这对句子之间关系的概率进行训练（类似分类问题）。最后，获取pooling层的输出用于句子之间的余弦相似度计算和推理。SBert的网络结构：</p><img src="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/webp-20230424111047089" class=""><ul><li><strong>在评估测试阶段，SBert直接使用余弦相似度来比较两个句向量之间的相似度</strong>，极大提升了推理速度；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer, util<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># code adapted from  https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic_search.py</span><br><br>model = SentenceTransformer(<span class="hljs-string">&#x27;D:/#Pre-trained_Language_Model/weights/distilbert-multilingual-nli-stsb-quora-ranking&#x27;</span>)<br><br><span class="hljs-comment"># Corpus with example sentences</span><br>sentences = [<span class="hljs-string">&quot;我想应聘。&quot;</span>, <span class="hljs-string">&quot;阿斯顿噶似的噶似的盾构法施工大苏打&quot;</span>, <br>        <span class="hljs-string">&quot;我想租冠寓的房子，如何操作？&quot;</span>,<br>        <span class="hljs-string">&quot;我约的保洁想取消，怎么操作&quot;</span>,<br>        <span class="hljs-string">&quot;我约的搬家想取消，怎么操作&quot;</span>,<br>        <span class="hljs-string">&quot;如果我有一处闲置物业，想租给冠寓怎么操作？&quot;</span>,<br>        <span class="hljs-string">&quot;我约的保洁想更改下时间，怎么办&quot;</span>,<br>        <span class="hljs-string">&quot;我是企业客户，我的员工有自租房需求，如何能使员工享受优惠价入住？&quot;</span>,<br>        <span class="hljs-string">&quot;我约的搬家想更改下时间，怎么办&quot;</span>,<br>        <span class="hljs-string">&quot;如果租住过程中有问题想投诉怎么办？&quot;</span>,<br>        <span class="hljs-string">&quot;如果我想和冠寓的门店合作提供一些增值性服务，怎么操作？&quot;</span>]<br><br><span class="hljs-comment"># Each sentence is encoded as a 1-D vector with 78 columns</span><br>sentence_embeddings = model.encode(sentences, convert_to_tensor=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Query sentences:</span><br>queries = [<span class="hljs-string">&#x27;我想租房&#x27;</span>, <span class="hljs-string">&#x27;约保洁&#x27;</span>]<br>query_embeddings = model.encode(queries)<br><br><br><span class="hljs-comment"># Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity</span><br>top_k = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> query, query_embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(queries, query_embeddings):<br>    cos_scores = util.pytorch_cos_sim(query_embedding, sentence_embeddings)[<span class="hljs-number">0</span>]<br>    cos_scores = cos_scores.cpu()<br><br>    <span class="hljs-comment">#We use np.argpartition, to only partially sort the top_k results</span><br>    top_results = np.argpartition(-cos_scores, <span class="hljs-built_in">range</span>(top_k))[<span class="hljs-number">0</span>:top_k]<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n\n======================\n\n&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Query:&quot;</span>, query)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTop 5 most similar sentences in corpus:&quot;</span>)<br><br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> top_results[<span class="hljs-number">0</span>:top_k]:<br>        <span class="hljs-built_in">print</span>(sentences[idx].strip(), <span class="hljs-string">&quot;(Score: %.4f)&quot;</span> % (cos_scores[idx]))<br></code></pre></td></tr></table></figure><p><strong>SBert开源地址</strong>：<a href="https://github.com/UKPLab/sentence-transformers">https://github.com/UKPLab/sentence-transformers</a></p><p><strong>SBert多语预训练模型下载地址</strong>：<a href="https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/">https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/</a></p><h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>纵览上述方法, 可以清晰地看到基于孪生网络的计算方法的发展进程, 对于模型的改进主要集中在使用不同的编码器, 如从最开始的多层感知机发展为 CNN、LSTM, 再到 Transformer 等.</p><p><strong>对速度要求高的召回场景可以用BOW+MLP、CNN，精度要求高的排序场景可以用LSTM、Transformer</strong>。同时两个向量的融合方法以及loss也都可以优化，比如做一些轻微的交互、像Deformer一样前面双塔接后面的多层交互，或者根据需要选择pointwise、pairwise（排序场景）损失。</p><p>但真要想做句间关系SOTA的话，比如刷榜，光靠双塔模型还是不行的，它有两个问题比较大：</p><ol><li><strong>位置信息</strong>。如果用BOW的话“我很不开心”和“我不很开心”两句的意思就变成一样了，虽然用RNN、BERT引入位置编码可以减缓一些，但不去让两个句子相互比较的话对于最后的分类层还是很难的</li><li><strong>细粒度语义</strong>。比如“我开心”和“我不开心”这两句话只有一个字的区别，但BOW模型很可能给出较高的相似度，交互式模型则可以稍有缓解</li></ol><h4 id="3-3-2-基于交互模型的方法"><a href="#3-3-2-基于交互模型的方法" class="headerlink" title="3.3.2 基于交互模型的方法"></a>3.3.2 基于交互模型的方法</h4><p>基于孪生网络的方法在编码层对句子编码时是相互独立的, 句子对之间没有交互, 这样对于计算句子对的语义相似度会造成一定影响, 基于交互模型的方法就是为了解决这个问题而产生的, 它是在孪生网络的基础上增加两个平行网络之间的交互作用, 从而提取到句子对之间更加丰富的交互信息.</p><h5 id="MatchPyramid"><a href="#MatchPyramid" class="headerlink" title="MatchPyramid"></a>MatchPyramid</h5><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">题目：Text Matching as Image Recognition<br>论文：https://arxiv.org/pdf/1602.06359.pdf<br>代码：https://github.com/pl8787/MatchPyramid-TensorFlow<br></code></pre></td></tr></table></figure><p>MatchPyramid 模型的提出灵感源于 CV 领域里的图像识别。在此之前，文本匹配的维度是一维的 ( TextCNN 为代表的 word embedding )；而图像是二维平面的，显然信息更加丰富。2016 年的 AAAI 上中科院提出了基于 word-level 级别交互的<strong>矩阵匹配</strong>思想，用一个二维矩阵来代表 query 和 doc 中每个 word 的交互。模型关键，在于如何构建 query 和 doc 中 word 级别的匹配信息，文章提出了匹配矩阵的 3 种构造方法，indicator、cosine 以及 dot product，分别如下：</p><ol><li><p><strong>indicator match</strong></p><p>0-1 匹配矩阵，只有两个 word 完全相同才等于 1，否则为 0，相当于是精确匹配</p></li><li><p><strong>cosine match</strong></p><p>将每个 word 用 glove 预训练得到的词向量，两两计算 cosine 相似度</p></li><li><p><strong>dot product match</strong></p><p>将每个 word 用 glove 预训练得到的词向量，两两计算向量点积</p></li></ol><p><img src="https://pic2.zhimg.com/80/v2-9752b8ccbc1da96344e0925f16b735cd_1440w.jpg" alt="img" style="zoom:80%;" /></p><h5 id="MV-LSTM"><a href="#MV-LSTM" class="headerlink" title="MV-LSTM"></a>MV-LSTM</h5><p>AAAI2016 年提出的 MV-LSTM 模型，用双向 LSTM 来对两段文本 query 和 doc 进行编码，然后将 LSTM 的每个隐层输出进行拼接作为 query 和 doc 每个 term 的输出，并对这些 term 两两计算匹配分数，得到不同维度的匹配度矩阵。最后，先用 k-max pooling 压缩特征，然后用 MLP 后输出最终的匹配分数。</p><p>整个模型可以分为 3 部分：① query 和 doc 各自的 LSTM 编码；② query 和 doc 的匹配矩阵；③ 匹配信号融合。文中提到了 cosine、双线性以及 tensor layer 这 3 种计算方法，由于网络参数的不断加大，拟合精确度和复杂度也依次提升：</p><ol><li><p><strong>cosine 相似度</strong></p><p>query 中的 term u 以及 doc 中的 term v，计算 cosine 相似度 $s(u, v)=\frac{u^{T} v}{|u| \cdot|v|}$</p></li><li><p><strong>双线性 bilinear</strong></p><p>通过引入待训练参数 M 进行映射 $s(u, v)=u^{T} M v+b$</p></li><li><p><strong>Tensor layer</strong></p><p> $s(u, v)=\frac{u^{T} v}{|u| \cdot|v|}$</p></li></ol><img src="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/image-16dce761a5f648d3af92d94195ea1e16.webp" class=""><h5 id="ABCNN"><a href="#ABCNN" class="headerlink" title="ABCNN"></a>ABCNN</h5><p>Yin等[<a href="javascript:;">79</a>]提出了ABCNN模型, 是在词向量基础上通过CNN网络对句子进行处理, 在分别对句子对中的句子进行卷积和池化操作的同时, 使用Attention机制对两个中间步骤进行交互, 文中一共尝试了3种添加Attention的策略, 主要区别是作用于模型的不同阶段, 如第一种Attention是直接作用在词向量组成的矩阵上, 而第二种Attention是作用在经过卷积和池化操作后产生的输出矩阵上, 第三种则是将前两种方法相结合. 可以将经过卷积和池化操作后得到的结果看作短语向量, 而该短语向量的长度取决于卷积核的大小, 从这个角度理解, 第一种和第二种Attention方法的区别实质上是在不同粒度上对模型进行了处理.</p><h5 id="PWIM"><a href="#PWIM" class="headerlink" title="PWIM"></a>PWIM</h5><p>He等[<a href="javascript:;">80</a>]通过BiLSTM对句子进行建模提出PWIM算法, 该方法共分为4个部分. 第一部分将BiLSTM网络每个时刻的输出, 即该时刻的双向hidden state做拼接获得的结果, 作为对应时刻word的representation. 第二部分通过计算两个句子的每个时刻的向量表示进行余弦相似度、欧式距离和点积计算, 然后根据计算结果来确定不同向量对的权重, 该方法认为句子内部不同的词的重要性是不一样的, 两个句子间重要的单词对, 对于句子相似度的计算贡献更大, 这些单词对应该得到更多的重视, 最后通过多层CNN网络得到最终结果.</p><h5 id="ESIM"><a href="#ESIM" class="headerlink" title="*ESIM"></a>*ESIM</h5><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">题目：Enhanced LSTM for Natural Language Inference<br>论文：https://www.aclweb.org/anthology/P17-1152.pdf<br>代码：https://github.com/coetaur0/ESIM<br></code></pre></td></tr></table></figure><p>ACL2017中的ESIM 模型在众多短文本匹配比赛中大杀四方，表现十分出色。ESIM 模型通过精细的序列式网络设计，以及同时考虑局部推断和全局推断的方法来达到更好的效果。它的计算过程有一下四步：</p><ol><li>用BiLSTM对embedding编码，得到表示 $a,b$</li><li>将两句话的BiLSTM输出进行<strong>soft attention</strong>，得到融合的句子表示 $\alpha, \beta$</li><li>将 $[a, \alpha, a-\alpha, a \cdot \alpha]$​ 拼接作为新表示，用BiLSTM再次编码，分别得到 $v_a,v_b$</li><li>max+avg池化后过进行最终分类 $v=softmax(\left[v_{a, a v e} ; v_{a, m a x} ; v_{b, a v e} ; v_{b, m a x}\right])$​</li></ol><p><strong>局部推理层是 ESIM 的点睛之笔， 主要就是用 attention 机制计算出 a 句某一单词和 b 句中各个单词的权重，然后再将权重赋予 b 句各个单词，用来表征 a 句中的该单词，形成一个新的序列，b 句亦然。</strong></p><p>具体来说，使用<strong>点积</strong>的方式计算 a 句中第 i 个词和 b 句中第 j 个词之间的 attention 权重:</p><script type="math/tex; mode=display">e_{i j}=\overline{\mathbf{a}}_{i}^{T} \overline{\mathbf{b}}_{j}</script><p>然后对两句各单词之间进行交互性计算，该词与另一句子联系越大，则计算出的值也会越大：</p><script type="math/tex; mode=display">\begin{aligned}\tilde{\mathbf{a}}_{i} &=\sum_{j=1}^{\ell_{b}} \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{\ell_{b}} \exp \left(e_{i k}\right)} \overline{\mathbf{b}}_{j}, \forall i \in\left[1, \ldots, \ell_{a}\right] \\\tilde{\mathbf{b}}_{j} &=\sum_{i=1}^{\ell_{a}} \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{\ell_{a}} \exp \left(e_{k j}\right)} \overline{\mathbf{a}}_{i}, \forall j \in\left[1, \ldots, \ell_{b}\right]\end{aligned}</script><p>得到局部推断后，为了增强信息，分别使用了向量相减、向量点积。最终 4 个向量 concat 起来，$m_a$和$m_b$分别表示增强后的 premise 和 hypothesis。</p><script type="math/tex; mode=display">\begin{array}{l}m_{a}=[\bar{a} ; \tilde{a} ; (\bar{a}-\tilde{a}) ; (\bar{a} \tilde{a})] \\m_{b}=[\bar{b} ; \tilde{b} ; (\bar{b}-\tilde{b}) ; (\bar{b} \bigodot \widetilde{b})]\end{array}</script><p>PS：局部推理层中对于匹配矩阵的做法区别于使用 MLP 去提取 query 和 doc 两个句子的做法；而是用两种 LSTM 提取编码后计算加权向量表达。得到加权向量表达后，和原始向量进行乘积和相减后，总共 4 个向量 concat 起来，相当于是丰富了提取到的信息。同时，相比BIMPM，ESIM训练速度快，效果也并没有逊色太多</p><img src="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/image-2ceda1d52b3340fe9692e7aa6d438412.webp" class=""><h5 id="BIMPM"><a href="#BIMPM" class="headerlink" title="BIMPM"></a>BIMPM</h5><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">题目：Bilateral Multi-perspective Matching<br>论文：https://arxiv.org/pdf/1702.03814.pdf<br>代码：https://github.com/terrifyzhao/text_matching<br></code></pre></td></tr></table></figure><p>IJCAI2017 年提出的同样基于BiLSTM网络提出<strong>双向多角度匹配模型BiMPM</strong>, 。该模型最大的创新点在于，对于给定的 query 和 doc，作者认为在匹配过程中，不仅需要考虑 query 到 doc，也应该考虑从 doc 到 query 的倒推关系，因此这是个双边 ( Bilateral ) 的关系。对于多角度，则是在考虑两个句子 query 和 doc 的关系的时候，用了 4 种不同的方法，体现了多角度 ( Multi-Perspective ) 的思想。BIMPM有着复杂的网络结构，计算复杂度高，很多计算之间又是串行的，可想而知，<strong>对于大规模的文本匹配计算速度很慢</strong></p><img src="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/image-252e4dd7a23942ac938472e78e498de7.png" class=""><p>整个模型可以分为 5 层：① 词表示层；② 上下文表示层（使用词向量与字符向量的拼接）；③ 匹配层；④ 聚合层（使用双向 LSTM将两个句子 P 和 Q 匹配得到的 16 个匹配向量（P-&gt;Q,Q-&gt;P 各 8 个），聚合成一个固定长度的匹配向量）；⑤ 输出层（两层 MLP+softmax），其中匹配层为模型的核心，共提出了四种匹配策略，分别是：Full-Matching、Maxpooling-Matching、Attentive-Matching和 Max-Attentive-Matching：</p><img src="/2021/11/09/2021-11-09-%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/640-20230424111225396" class=""><ol><li><strong>full-matching</strong>：在这种匹配策略中，用 P 中每一个 time step 的上下文向量（包含前向和后向）分别与句子 Q 中最后一个 time step 的上下文向量（包含前向和后向）计算匹配值，即</li></ol><script type="math/tex; mode=display">\begin{array}{c}\overrightarrow{\boldsymbol{m}}_{i}^{\text {full }}=f_{m}\left(\overrightarrow{\boldsymbol{h}}_{i}^{p}, \overrightarrow{\boldsymbol{h}}_{N}^{q} ; \boldsymbol{W}^{1}\right) \\\overleftarrow{\boldsymbol{m}}_{i}^{\text {full }}=f_{m}\left(\overleftarrow{\boldsymbol{h}}_{i}^{p}, \overleftarrow{\boldsymbol{h}}_{1}^{q} ; \boldsymbol{W}^{2}\right)\end{array}</script><ol><li><strong>maxpooling-matching</strong>：在第一种匹配方法中选取最大的匹配值，即</li></ol><script type="math/tex; mode=display">\begin{array}{l}\overrightarrow{\boldsymbol{m}}_{i}^{\max }=\max _{j \in(1 \ldots N)} f_{m}\left(\overrightarrow{\boldsymbol{h}}_{i}^{p}, \overrightarrow{\boldsymbol{h}}_{j}^{q} ; \boldsymbol{W}^{3}\right) \\\overleftarrow{\boldsymbol{m}}_{i}^{\max }=\max _{j \in(1 \ldots N)} f_{m}\left(\overleftarrow{\boldsymbol{h}}_{i}^{p}, \overleftarrow{\boldsymbol{h}}_{j}^{q} ; \boldsymbol{W}^{4}\right)\end{array}</script><p>​    where $\max _{j \in(1 \ldots N)}$ is element-wise maximum.</p><ol><li><strong>attentive-matching</strong>：先对 P 和 Q 中每一个 time step 的上下文向量（包含前向和后向）计算余弦相似度，得到相似度矩阵</li></ol><script type="math/tex; mode=display">\begin{array}{l}\vec{\alpha}_{i, j}=\operatorname{cosine}\left(\overrightarrow{\boldsymbol{h}}_{i}^{p}, \overrightarrow{\boldsymbol{h}}_{j}^{q}\right) \\\overleftarrow{\alpha}_{i, j}=\operatorname{cosine}\left(\overleftarrow{\boldsymbol{h}}_{i}^{p}, \overleftarrow{\boldsymbol{h}}_{j}^{q}\right)\end{array}</script><p>​    然后将相似度矩阵作为 Q 中每一个 time step 权重，通过对 Q 的所有上下文向量（包含前向和后向）加权求和，计算出整个句子 Q 的注意力向量</p><script type="math/tex; mode=display">\begin{aligned}\overrightarrow{\boldsymbol{h}}_{i}^{\text {mean }} &=\frac{\sum_{j=1}^{N} \vec{\alpha}_{i, j} \cdot \overrightarrow{\boldsymbol{h}}_{j}^{q}}{\sum_{j=1}^{N} \vec{\alpha}_{i, j}} \\\overleftarrow{\boldsymbol{h}}_{i}^{\text {mean }} &=\frac{\sum_{j=1}^{N} \overleftarrow{\alpha}_{i, j} \cdot \overleftarrow{\boldsymbol{h}}_{j}^{q}}{\sum_{j=1}^{N} \overleftarrow{\alpha}_{i, j}}\end{aligned}</script><p>​    最后，将 P 中每一个 time step 的上下文向量（包含前向和后向）分别与句子 Q 的注意力向量计算匹配值</p><script type="math/tex; mode=display">\begin{array}{l}\overrightarrow{\boldsymbol{m}}_{i}^{a t t}=f_{m}\left(\overrightarrow{\boldsymbol{h}}_{i}^{p}, \overrightarrow{\boldsymbol{h}}_{i}^{\text {mean }} ; \boldsymbol{W}^{5}\right) \\\overleftarrow{\boldsymbol{m}}_{i}^{a t t}=f_{m}\left(\overleftarrow{\boldsymbol{h}}_{i}^{p}, \overleftarrow{\boldsymbol{h}}_{i}^{\text {mean }} ; \boldsymbol{W}^{6}\right)\end{array}</script><ol><li><strong>max-attentive-matching</strong>：与 attentive-matching 的匹配策略相似，不同之处在于选择句子 Q 所有上下文向量中余弦相似度最大的向量作为句子 Q 的注意力向量。</li></ol><h5 id="DIIN"><a href="#DIIN" class="headerlink" title="DIIN"></a>DIIN</h5><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">题目：Natural Language Inference over Interaction Space<br>论文：https://arxiv.org/pdf/1709.04348.pdf<br>代码：https://github.com/YerevaNN/DIIN-in-Keras<br></code></pre></td></tr></table></figure><p>DIIN 模型是在 ICLR2018 上提出的, 输入层使用单词嵌入、字符特征和句法特征的串联, 编码层使用自注意力<a href="self-attention"><a href="javascript:;">60</a></a>机制, 交互层采用点积操作得到交互矩阵, 然后使用DenseNet[<a href="javascript:;">84</a>]进行特征抽取, 之后将抽取的特征传入多层感知机模型得到最终的结果, 本方法简单有效, 在NLI任务上表现出很好的性能. </p><p><img src="https://pic2.zhimg.com/80/v2-c44b0398dc3e5372e8b91959b8052081_1440w.jpg" alt="img" style="zoom: 67%;" /></p><p>模型可以分为 5 个部分，分别是嵌入层、编码层、交互层、特征抽取层以及输出层：</p><ul><li>Embedding Layer</li></ul><p>在 embedding 层中，论文中同时将单词级别 embedding、字符级别特征的 embedding （OOV）以及句法级别的特征 embedding 信息 （一个是词性特征 ( POS )，另一个是二进制精确匹配特征 ( EM )），全部concat 起来作为每个词的 embedding。</p><ul><li>Encoding Layer</li></ul><p>Encoding Layer的主要作用是将上一层的特征进行融合并进行encode。论文中作者采用的是<strong>self-attention机制</strong>，同时考虑到了词序和上下文信息。以P作为例子，首先计算attetion matrix</p><script type="math/tex; mode=display">A_{i j}=\alpha\left(\hat{P}_{i}\right), \hat{P}_{j} \in R</script><p>这里的attention matrix的 计算方法和transformer的还不太一样，作者取了三个维度的值并<strong>拼接</strong>起来，如下面的公式所示，我的理解是a与b应该是一样的，因为都是针对的P，唯一多了个a ∘ b其中的∘是对位相乘，如果P的维度是d，那么拼接后的向量维度是3d</p><script type="math/tex; mode=display">\alpha(a, b)=w_{a}^{T}[a ; b ; a \circ b]</script><p>然后加上softmax计算self-attention的值</p><script type="math/tex; mode=display">\bar{P}=\sum_{j=1}^{p} \frac{\exp \left(A_{i j}\right)}{\sum_{k=1}^{p} \exp \left(A_{k j}\right)} \hat{P}_{j}</script><p>接下来作为引入了LSTM中门的概念semantic composite fuse gate，其计算公式如下</p><script type="math/tex; mode=display">\begin{array}{c}z_{i}=\tanh \left(W^{1 t}\left[\hat{P}_{i} ; \bar{P}_{i}\right]+b^{1}\right) \\r_{i}=\sigma\left(W^{2 T}\left[\hat{P}_{i} ; \bar{P}_{i}\right]+b^{2}\right) \\f_{i}=\sigma\left(W^{3 T}\left[\hat{P}_{i} ; \bar{P}_{i}\right]+b^{3}\right) \\\tilde{P}_{i}=r_{i} \circ \hat{P}_{i}+f_{i} \circ z_{i}\end{array}</script><p>其中 W 的维度均是$[ 2 d , d ]$，b的维度是d，σ表示的是sigmoid函数</p><p>H的操作和P的操作一样，就不再赘述了，有一点需要注意，论文中认为P和H是有细微差距的，所以attention的权重和gate的权重是没有共享的，不过论文中的任务五是NLI，如果是相似度匹配的任务我觉得此处是可以共享的，必须相似度匹配两个句子本身就是很接近的，此观点仅代表个人想法没有验证过，代码中我们还是以论文为主。</p><ul><li>Interaction Layer</li></ul><p>Interaction Layer的主要目的是把P与H做一个相似度的计算，提取出其中的相关性，可以采用余弦相似度、欧氏距离等等，这里作者发现对位相乘（向量点积）的效果很好，所以公式中的 $\beta(a, b)=a \circ b$</p><script type="math/tex; mode=display">I_{i j}=\beta\left(\tilde{P}_{i}, \tilde{H}_{i}\right) \in R^{d}</script><ul><li>Feature Extraction Layer</li></ul><p>Feature Extraction Layer的任务正如其名，做特征提取。这一层论文主要采用的是CNN的结构，作者实验发现ResNet效果会好一些，但是最终还是选择了DenseNet，因为DenseNet能更好的保存参数，并且作者观察到ResNet如果把skip connection移除了模型就没法收敛了（ResNet的关键就在于skip connection不知道作者为什么要说明这一点）BN还会导致收敛变慢（这里应该是指针对当前这个模型来说）所以作者并没有采用ResNet。卷积采用的是relu激活函数，都是1×1的卷积核来对上文提到的相关性tensor进行缩放，并且这里引入了一个超参数η，例如输入的channel是k那么输出的channel就是k × η，然后把结果送到3层Dense block中，Dense block包含了n个3×3的卷积核，成长率是g，transition layer采用了1×1的卷积核来做channel的缩减，然后跟上一个步长为2的最大池化层，transition layer的缩减率用θ表示。这一层的关键就在于DenseNet，对DenseNet不清楚的小伙伴一定要先去了解该网络的结构原理，这也是为啥作者会把该模型取名为Densely Interactive Inference Network</p><ul><li>Output Layer</li></ul><p>输出层使用简单的一个线性层以及 softmax 多分类进行分类。</p><h5 id="HCAN"><a href="#HCAN" class="headerlink" title="HCAN"></a>HCAN</h5><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">题目：Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling<br>论文：https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_EMNLP2019.pdf<br>代码：https://github.com/jinfengr/hcan<br></code></pre></td></tr></table></figure><p>HCAN是Facebook在EMNLP2019提出的模型，虽然比上文的ESIM、PWIM等模型高上4+个点，但还是被BERT甩了很远。不过这个模型的核心也不只是做语义匹配，而是同时做检索相关性（Relevance Matching），也就是搜索中query-doc的匹配。</p><p>模型计算如下：</p><ol><li>Encoding层作者提出了三种方法，堆叠的CNN作为Deep Encoder，不同尺寸卷积核作为Wide Encoder，BiLSTM作为Contextual Encoder编码更长距离的上下文</li><li>先把两句话交互得到attention score矩阵，之后对于query中每个词，求得doc中最相似的词的分数，作为向量Max(S)，按照同样的方法求出Mean(S)，长度都为|Q|，再分别乘上query中每个词的tfidf统计，得到相关性匹配向量 <img src="https://www.zhihu.com/equation?tex=O_%7BRM%7D" alt="[公式]"></li><li>用加性attention对query和doc进行交互，得到新的表示，再花式拼接过BiLSTM，得到语义匹配向量 <img src="https://www.zhihu.com/equation?tex=O_%7BSM%7D" alt="[公式]"></li><li>将 <img src="https://www.zhihu.com/equation?tex=O_%7BSM%7D%2C+O_%7BRM%7D" alt="[公式]"> 拼接起来过MLP，最后分类</li></ol><p>通过实验结果来看，Deep Encoder的表现是最好的，在7/8个评估上都超过另外两个。</p><p><img src="https://pic1.zhimg.com/80/v2-9e3b3f27fc2de53bc27c52baceca84c0_1440w.jpg" alt="img"></p><p>模型只是工具，数据才是天花板，数据质不好/数量不够，模型再花哨也没用。像BiMPM和ESIM，小数据集无法发挥它们的效果，所以训练调参的过程会很耗时。像是quora question pairs数据集提供了40W对训练样本，不平衡程度也不大(正负比 3:5)，这时两个模型效果都不错，这也印证了数据驱动的重要性。</p><h5 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h5><p>基于交互模型的方法实质就是在孪生网络架构的基础上, 通过某种策略对两个孪生网络的中间环节进行交互. 目前最普遍的策略是各种不同的Attention机制. 具有交互能力的模型结构普遍更为复杂, 包含更多的模型参数, 这就导致了这类模型的计算成本较高.</p><h4 id="孪生-vs-交互"><a href="#孪生-vs-交互" class="headerlink" title="孪生 vs. 交互"></a>孪生 vs. 交互</h4><ul><li>基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量速度会快很多；缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。</li><li>基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。</li></ul><p><img src="https://picb.zhimg.com/v2-a22a3a5c809591e6c3226e4ef1ea2732_r.jpg" alt="preview" style="zoom:50%;" /></p><h2 id="4-开源文本匹配工具"><a href="#4-开源文本匹配工具" class="headerlink" title="4.开源文本匹配工具"></a>4.开源文本匹配工具</h2><h3 id="5-1-MatchZoo"><a href="#5-1-MatchZoo" class="headerlink" title="5.1 MatchZoo"></a>5.1 MatchZoo</h3><blockquote><p><a href="https://blog.csdn.net/qq_34182808/article/details/103027879">MatchZoo简单使用</a></p><p><a href="https://zhuanlan.zhihu.com/p/98180757">莫冉-MatchZoo使用的常规流程</a></p><p><a href="https://zhuanlan.zhihu.com/p/94085483">莫冉-[文本匹配] 关于文本匹配工具MatchZoo</a></p></blockquote><p><a href="https://github.com/NTMC-Community/MatchZoo">MatchZoo</a> 是一个Python环境下基于TensorFlow/keras开发的开源文本匹配工具，让大家更加直观地了解深度文本匹配模型的设计、更加便利地比较不同模型的性能差异、更加快捷地开发新型的深度匹配模型。</p><p>安装</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> matchzoo<br></code></pre></td></tr></table></figure><p>使用示例</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import matchzoo as mz<br><span class="hljs-built_in">print</span>(mz.__version__)<br><br><span class="hljs-comment">### 定义任务，包含两种，一个是Ranking，一个是classification</span><br>task = mz.tasks.Ranking()<br><span class="hljs-comment"># task = mz.tasks.Ranking(loss=mz.losses.RankCrossEntropyLoss(num_neg=1))</span><br><span class="hljs-built_in">print</span>(task)<br><br><span class="hljs-comment">### 准备数据，数据在源码中有，不确定在pip安装的是否存在</span><br><span class="hljs-comment">### train_raw是matchzoo中自定的数据格式matchzoo.data_pack.data_pack.DataPack</span><br>train_raw = mz.datasets.toy.load_data(<span class="hljs-attribute">stage</span>=<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-attribute">task</span>=task)<br>test_raw = mz.datasets.toy.load_data(<span class="hljs-attribute">stage</span>=<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-attribute">task</span>=task)<br><span class="hljs-built_in">print</span>(train_raw.left.head())<br><span class="hljs-built_in">print</span>(train_raw.frame().head())<br><br><span class="hljs-comment">### 数据预处理，BasicPreprocessor为指定预处理的方式，在预处理中包含了两步：fit,transform</span><br><span class="hljs-comment">### fit将收集一些有用的信息到preprocessor.context中，不会对输入DataPack进行处理</span><br><span class="hljs-comment">### transformer 不会改变context、DataPack,他将重新生成转变后的DataPack.</span><br><span class="hljs-comment">### 在transformer过程中，包含了Tokenize =&gt; Lowercase =&gt; PuncRemoval等过程，这个过程在方法中应该是可以自定义的</span><br>preprocessor = mz.preprocessors.BasicPreprocessor()<br>preprocessor.fit(train_raw)  ## init preprocessor inner state.<br>train_processed = preprocessor.transform(train_raw)<br>test_processed = preprocessor.transform(test_raw)<br><br><span class="hljs-comment">### 创建模型以及修改参数（可以使用mz.models.list_available()查看可用的模型列表）</span><br><span class="hljs-built_in">print</span>(model.params.to_frame()[[<span class="hljs-string">&#x27;Name&#x27;</span>, <span class="hljs-string">&#x27;Description&#x27;</span>, <span class="hljs-string">&#x27;Value&#x27;</span>]])<br>model = mz.models.DenseBaseline()<br>model.params[<span class="hljs-string">&#x27;task&#x27;</span>] = task<br>model.params[<span class="hljs-string">&#x27;input_shapes&#x27;</span>] = preprocessor.context[<span class="hljs-string">&#x27;input_shapes&#x27;</span>]<br>model.params[<span class="hljs-string">&#x27;mlp_num_units&#x27;</span>] = 3<br>model.params.update(preprocessor.context)<br>model.guess_and_fill_missing_params(<span class="hljs-attribute">verbose</span>=0)<br>model.params.completed()<br>model.build()<br>model.compile()<br>model.backend.summary()<br><br><span class="hljs-comment">### 训练, 评估, 预测</span><br>x, y = train_processed.unpack()<br>test_x, test_y = test_processed.unpack()<br>evaluate = mz.callbacks.EvaluateAllMetrics(model, <span class="hljs-attribute">x</span>=test_x, <span class="hljs-attribute">y</span>=test_y, <span class="hljs-attribute">batch_size</span>=batch_size)<br>model.fit(x,y,<span class="hljs-attribute">batch_size</span>=32, <span class="hljs-attribute">epochs</span>=5, callbacks=[evaluate])<br>model.evaluate(test_x,test_y)<br>model.predict(test_x)<br><br><span class="hljs-comment">### 保存模型</span><br>model.save(<span class="hljs-string">&#x27;my-model&#x27;</span>)<br>loaded_model = mz.load_model(<span class="hljs-string">&#x27;my-model&#x27;</span>)<br></code></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><ol><li>需要安装nltk中的词库（根据提示，缺啥装啥）</li><li>由于matchzoo的预处理模块是为英文设计的，需要进行中文适配，参考<a href="https://blog.csdn.net/m0_38103546/article/details/104667975">《matchzoo中文适配笔记》</a> <a href="https://blog.csdn.net/wkh7717/article/details/89886713">《matchzoo中文支持研究笔记》</a></li><li>数据定义。需要将本地数据转换成<code>DataPack</code>这种数据结构进行组织 </li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">df</span> = pd.DataFrame(<br>  &#123;<br>    <span class="hljs-string">&quot;text_left&quot;</span>: table[<span class="hljs-string">&quot;sentence1&quot;</span>],<br>    <span class="hljs-string">&quot;text_right&quot;</span>: table[<span class="hljs-string">&quot;sentence2&quot;</span>],<br>    <span class="hljs-string">&quot;label&quot;</span>: table[<span class="hljs-string">&quot;label&quot;</span>],<br>  &#125;<br>)<br><span class="hljs-comment"># 将读取之后的DataFrame类型转换成DataPack类型</span><br>dp = mz.pack(<span class="hljs-built_in">df</span>, task=<span class="hljs-string">&#x27;ranking&#x27;</span>)<br><span class="hljs-comment">## 这里的task参数务必要指定，否则由于label的类型问题可能报错</span><br></code></pre></td></tr></table></figure><ul><li><p>DataPack——&gt;DataFrame：可以通过<code>df.frame()</code>返回<code>DataFrame</code>类型进行查看；</p></li><li><p>DataFrame——&gt;DataPack：通过顶级函数<code>pack</code>将<code>DataFrame</code>数据直接转换为<code>DataPack</code></p></li><li><p>DataPack——&gt;<code>(X, y)</code>元组：通过<code>unpack</code>函数进行拆包，其中<code>X</code>为<code>&#123;str: np.array&#125;</code>的键值对，拆包后的数据可直接用于模型的<code>fit</code>或<code>fit_generator</code>函数进行；<code>y</code>是二维列表，对应实际的标签值。</p></li></ul><ol><li><p>对上面的<code>DataPack</code>类型进行转换为<code>Dataset</code>，定义这个类的时候，<strong>参数中有<code>mode</code>，可以选择<code>point</code>和<code>pair</code>这两种模式</strong>。</p></li><li><p>pair模式下的采样过程：</p></li></ol><ul><li><p>以<code>id_left</code>为标准进行分组，并按照<code>label</code>进行降序排序</p></li><li><p>对于不同的组（group）：</p><ul><li>获取当前组的所有不同的<code>labels</code></li><li><p>对于除了最后一个之外的<code>labels</code>（即<code>labels[:-1]</code>）：</p></li><li><ul><li>获取等于当前<code>label</code>的所有样本，重复<code>num_dup</code>的次数；</li><li>将小于当前<code>label</code>的所有样本作为负例；</li><li>对于每一个正样本，采样<code>num_neg</code>个负样本；</li><li>按照<code>(pos_sample, neg_sample)</code>的形式添加到<code>pairs</code>列表中。</li></ul></li></ul></li><li><p>将所有的<code>pairs</code>进行<code>concat</code>。</p></li></ul><h3 id="5-2-Deep-text-matching"><a href="#5-2-Deep-text-matching" class="headerlink" title="5.2 Deep text matching"></a>5.2 Deep text matching</h3><blockquote><p><a href="https://github.com/wangle1218/deep_text_matching">https://github.com/wangle1218/deep_text_matching</a></p></blockquote><p>借鉴了 <a href="https://github.com/NTMC-Community/MatchZoo">MatchZoo </a>和 <a href="https://github.com/terrifyzhao/text_matching">text_matching</a> ，使用 keras 深度学习框架进行复现cdssm, arc-ii,match_pyramid, mvlstm ,esim, drcn ,bimpm 等模型，借鉴了 <a href="https://github.com/bojone/bert4keras">bert4keras </a>项目的代码测试了几个 bert 系列模型（bert 及其变体）</p><h3 id="5-3-shenweichen-DeepMatch"><a href="#5-3-shenweichen-DeepMatch" class="headerlink" title="5.3 shenweichen/DeepMatch"></a>5.3 shenweichen/DeepMatch</h3><p><a href="https://github.com/shenweichen/DeepMatch">DeepMatch</a> is a deep matching model library for recommendations &amp; advertising. It’s easy to <strong>train models</strong> and to <strong>export representation vectors</strong> for user and item which can be used for <strong>ANN search</strong>.You can use any complex model with <code>model.fit()</code>and <code>model.predict()</code> .</p><h3 id="5-4-58的qa-match"><a href="#5-4-58的qa-match" class="headerlink" title="5.4 58的qa_match"></a>5.4 58的qa_match</h3><h2 id="我的文本匹配方案"><a href="#我的文本匹配方案" class="headerlink" title="我的文本匹配方案"></a>我的文本匹配方案</h2><p> <a href="NLU文本匹配调研报告20210628.pptx">NLU文本匹配调研报告20210628.pptx</a> </p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/111769969">谈谈文本匹配和多轮检索</a></p><p><a href="https://zhuanlan.zhihu.com/p/357864974">21个经典深度学习句间关系模型｜代码&amp;技巧</a></p><p><a href="https://www.6aiq.com/article/1589474365961">贝壳找房【深度语义匹配模型 】原理篇一：表示型</a></p><p><a href="https://www.6aiq.com/article/1589798723495">贝壳找房【深度语义匹配模型】原理篇二：交互篇</a></p><p><a href="https://www.6aiq.com/article/1590190626464">【深度语义匹配模型】实践篇：语义匹配在贝壳找房智能客服中的应用</a></p><p><a href="https://blog.csdn.net/u012526436/article/details/90179466">文本相似度，文本匹配模型归纳总结</a></p><p><a href="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/CQA%E8%B0%83%E7%A0%94-%E5%B7%A5%E4%B8%9A%E7%95%8C.md#221-%E5%9F%BA%E4%BA%8E%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%E7%9A%84%E7%AE%97%E6%B3%95">QA-Survey/CQA调研-工业界.md</a></p><p><a href="https://www.6aiq.com/article/1587873440587">干货! 搜索系统中的深度匹配模型</a></p><p><a href="http://hdsfdxzkb.xml-journal.net/cn/article/doi/10.3969/j.issn.1000-5641.202091011?viewType=HTML">韩程程, 李磊, 刘婷婷, 高明. 语义文本相似度计算方法[J]. 华东师范大学学报（自然科学版）</a></p>]]></content>
    
    
    <categories>
      
      <category>语义匹配</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>     🍊句子表征优化小结</title>
    <link href="/2021/11/09/2021-11-09-%F0%9F%8D%8A%E5%8F%A5%E5%AD%90%E8%A1%A8%E5%BE%81%E4%BC%98%E5%8C%96%E5%B0%8F%E7%BB%93/"/>
    <url>/2021/11/09/2021-11-09-%F0%9F%8D%8A%E5%8F%A5%E5%AD%90%E8%A1%A8%E5%BE%81%E4%BC%98%E5%8C%96%E5%B0%8F%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<blockquote><p>转载自《<a href="https://mp.weixin.qq.com/s/0_XBOgwkFvxr-i54_yyntw">前沿重器[28] | 前沿的向量召回都是怎么做的</a>》</p></blockquote><p>向量召回本身是个新潮的东西，这次给大家小小的总结一下，工业界和科研界中常见的向量召回方案，希望能给大家一些启示吧。</p><h2 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h2><p>首先，什么是向量召回，核心的流程是什么样的，先介绍下。首先，所谓向量召回，就是把物料和用户query都进行向量表征，物料可以构造索引供query向量进行检索，本质还是在库里检索和query最接近的那些物料，相当于推荐中的一路召回。</p><p>由于深度学习的逐渐深入，这种泛化的向量表征方式也成为了大家尝试的方向，更有比较激进的在尝试直接跳过字面的召回而直接选择向量召回的方式，可见该方式的热门程度，大家所关注的，就是他对特征的要求低，而且泛化能力强，所以也就成为了“香饽饽”。</p><p>那么，工业界和科研界到底是怎么做的，有没有什么先进的思路可以考虑，本文会分模块地详细总结，并给出一些大家一些思路吧。当然，我对图深度学习、图谱等不太熟悉，本文先不聊，主要就是聊双塔。</p><p>文章整体思路是这样的（懒人目录）：</p><ul><li><strong>NLP技术下的QQ匹配</strong></li><li><strong>工业界Query塔的表征</strong></li><li><strong>工业界物料塔的表征</strong></li><li><strong>难负例的挖掘</strong></li></ul><p>本文是综述，不会把每篇文章都展开讲，也不会列举出这个角度的所有文章，而是大概描述文章的特定亮点，并进行总结，说到的参考资料都会放到文末，并标上序号，大家后续按需深入阅读。</p><h2 id="NLP技术下的QQ匹配"><a href="#NLP技术下的QQ匹配" class="headerlink" title="NLP技术下的QQ匹配"></a>NLP技术下的QQ匹配</h2><p>QQ匹配是最简单而又经典的方案了，在对话系统和传统的搜索系统中非常常见，而且日渐成熟，其本质就是把query和物料放在同一个语义空间下进行训练和推理，专业说法称为语义表征。这方面的研究，我分为3个模块：</p><ul><li><strong>表征层。</strong></li><li><strong>预训练模型引入后的新问题。</strong></li><li><strong>损失函数的优化思路。</strong></li></ul><h3 id="表征层"><a href="#表征层" class="headerlink" title="表征层"></a>表征层</h3><p>表征成的优化相信大家都比较熟悉的，始于DSSM[1]，后续里面的全连接层可以换成CNN，成为CLSM[2]，又可以换成LSTM，成为DSSM-LSTM[3]，而后续又对这块进行了一些总结性的归纳，有了infersent[4]，统一了整个框架，并给出了一些较好的训练方法。（我也在以前有聊过这里的细节，见：<a href="http://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247486225&amp;idx=1&amp;sn=4aca79d060480c301607e3ba722622f3&amp;chksm=e882538fdff5da99ef1440fb2d2c2eff1934e5dab5c8c1d90dcbd9a7171d22f657f87f05bd96&amp;scene=21#wechat_redirect">心法利器[7] | 漫谈语义相似度与语义向量表征</a>）</p><p>而后，就是预训练模型的时代了，最有代表性的是Sentence-BERT[5]和SimBERT[6]，这俩在我看来都是在预训练模型用于语义表征的代表之作，虽然说后续对比学习的引入让预训练在此方面的应用更加优秀了，但这两篇工作，仍旧是很有学习和吸收的价值。</p><blockquote><p>SimBERT = BERT + UniLM + 对比学习</p><p>SimBERT属于有监督训练，训练语料是自行收集到的相似句对（文本1，文本2，label），通过一句来预测另一句的相似句生成任务来构建Seq2Seq部分，然后前面也提到过[CLS]的向量事实上就代表着输入的句向量，所以可以同时用它来训练一个检索任务。换句话说，对于检索任务在努力训练模型“[CLS]的向量事实上就代表着输入的句向量”，如果仅仅从相似度判断来看（相似度判断等价于检索），强制A生成B一定程度上也是在使A的表征和B更逼近。</p><p>我在中文数据集上测试过不少场景，不得不说，simBERT确实是融合了检索和生成的功能，尤其适合于相关性检索，生成的质量还有待提升</p></blockquote><h3 id="预训练模型引入后的问题"><a href="#预训练模型引入后的问题" class="headerlink" title="预训练模型引入后的问题"></a>预训练模型引入后的问题</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/497926743">https://zhuanlan.zhihu.com/p/497926743</a></p></blockquote><p>预训练模型直接做语义表征是有问题的。如Google开源的bert模型，Next Sentence作为BERT的Pretrain任务，天然提供了文本匹配的范式。一般取【cls】作为文本的feature extraction，然而，这个【cls】具备语义特征信息的前提是微调，也就是被有监督train过之后才能很好的embedding化句子。</p><p>若是讲仅通过无监督得到句子的【cls】用于匹配任务，尤其是用cos计算相似度时，实验证明即便两条相关度较低的句子依旧可以获得一定相似度得分。在论文REPRESENTATION DEGENERATION PROBLEM IN TRAINING NATURAL LANGUAGE GENERATION MODELS 中提出了了表达退化问题 (Representation degeneration problem)，一定程度上解释了为什么直接用BERT是不可取的。</p><p>第一阶段是直接在预训练的表征后增加一些转化的层，比较有代表性的是bert-flow[8]和bert-whitening[9, 10]，通过映射的方式把预测的结果从有问题的空间转化到正确的空间里去，所谓的正确理解可能会有所不同，个人是比较喜欢bert-whitening对标准正态分布是各向同性的这一理解，用线性的变换的方式就能轻松解决该问题。</p><p>第二阶段，是考虑直接去解决词频和训练的问题，某种程度上说，其实和数据增强类似的思路，从样本或者从训练层面进行增强，从而直接解决预测层间的空间问题，以样本增强为代表的是美团的ConSERT[11]，而在模型层面或训练策略层面的增强，代表之作就是现在大家都比较熟悉的SIMCSE[12]，SIMCSE 在数据集没有标注的情况下，要增强这一数据集总体的语义特性，直接利用模型的Dropout来完成这种目标，又简单又实用。流程：将数据集放入对比训练，无需标签，自监督增强语义信息。以SIMCSE为例，在STS等标准数据集下的效果，是要比作为极限的BERT要好一大截，算是继预训练模型引入之后的又一质变，甚至可以从数据中看到，无监督的方案甚至是能够逼近有监督方案的，这点其实强的蛮离谱的。</p><p>提一下，二阶段的这套思路，其实也很大程度上让NLPer意识到，<strong>对比学习可能是进一步优化语义表征的一大突破口。</strong></p><h3 id="损失函数的优化思路"><a href="#损失函数的优化思路" class="headerlink" title="损失函数的优化思路"></a>损失函数的优化思路</h3><p>与其说对比学习，其实我更想说是损失函数的应用。这块的NLP论文我接触的其实不多，然而是看到了这个东西在CV领域方兴未艾。</p><p>这个关注点，我其实在很久之前的文章就有聊到了，就是这篇：<a href="http://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247486612&amp;idx=1&amp;sn=08e4da8576ac7f3e1be155051ccb3604&amp;chksm=e882540adff5dd1cea793f10f7d9fcaa36299455c7e4f59d6ef683e1f5522d662c59d785c84e&amp;scene=21#wechat_redirect">前沿重器[8] | CV研究启发语义相似和表征</a>，在人脸识别领域，表征这个事情和我们现在聊的非常接近，相似度表征的训练内部其实有大量可以用来进行优化的方式，例如margin的思想能把不同类的尽可能拉开，类间的壁垒得到提高，从而提升预测性能。这块论文不在提了，直接前面我说的那篇文章吧。</p><p>人脸说完，NLP有在用吗，有的，直接给个工业界的例子吧，OPPO的小布助手在这块其实已经有应用并且落了地了，可以参考下[13]，我也写过讲解：<a href="http://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247486580&amp;idx=1&amp;sn=cc108a7c340c20e4a947971cd191503e&amp;chksm=e88254eadff5ddfcecfb7afc36c1f52ce9e15438eca31bb930a2bfd3acdb0257dfb225f2c2d0&amp;scene=21#wechat_redirect">前沿重器[7] | 小布助手登顶百度千言短文本相似度的秘诀</a>。</p><h2 id="工业界Query塔的匹配"><a href="#工业界Query塔的匹配" class="headerlink" title="工业界Query塔的匹配"></a>工业界Query塔的匹配</h2><p>有关Query塔的匹配，我比较想聊的是两篇文章吧，都是KDD21的，淘宝[14]和FB[15]的，这两篇都值得稍微的展开，淘宝那篇文章我专门聊过：<a href="http://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247487695&amp;idx=1&amp;sn=370ac19afbbb2c27a07ed6c3fa8b9b70&amp;chksm=e8824851dff5c1474f522ef37a17661900fd1494c9043ebc1b94426d219fc0053f098538b4b9&amp;scene=21#wechat_redirect">前沿重器[18] | KDD21-淘宝向量检索</a>。<br>略……</p><h2 id="HARD-CASE挖掘"><a href="#HARD-CASE挖掘" class="headerlink" title="HARD CASE挖掘"></a>HARD CASE挖掘</h2><p>HARS CASE挖掘可以说是向量召回领域的一个特色问题，难负例本身对预测的准确性影响巨大，借此机会，分享几个挖掘方案吧。</p><ul><li><strong>用字面检索（如BM25粗排的召回）召回与query相似的样本，赋为负例。</strong></li><li><strong>BATCH内，通过两两相似度矩阵进行粗排挖掘[14]。</strong></li><li><strong>正负例表征加权求和构造负类embedding[13]。</strong></li><li><strong>R-dropout[17]等增强方式。</strong></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>上面综述了不少方法，让大家能够了解到前沿的向量召回都是用的什么做表征的，划几个重点：</p><ul><li><strong>工业界和科研界好像尚未有些不同，科研界更多关注语义的表征，预训练模型更多，工业界则更在乎多粒度的语言理解，还有用户行为。</strong></li><li><strong>科研界，在语义表征上，NLP层面预训练模型还是有很大优势。</strong></li><li><strong>多信息的组合，以及日常任务的协作，导致了工业界模型虽然都还偏大，但其实都有分多个模块，然后最终组装的模式。</strong></li><li><strong>attention，因为多模块组合的原因，成了工业界的宠儿。</strong></li></ul><p>当然了，这里也给大家在向量召回上一些使用建议吧。</p><ul><li><strong>预训练模型并不是必须得，可以先考虑多粒度的语义理解并进行融合，这个优先级更高。</strong></li><li><strong>搜索不止有语义信息，无论是query塔还是物料塔，还会有很多信息和特征，前者有用户历史行为，后者有多模态和其他物料特征。</strong></li><li><strong>向量召回不太能是唯一的召回方案，字面召回还是建议保留，这点在[13]的架构图里有体现，我上一篇文章也有聊这个问题：</strong><a href="http://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247488276&amp;idx=1&amp;sn=a2d04351456cec3e6ca471cf91235e90&amp;chksm=e8824b8adff5c29cc7cbbb0cddf2f90857fc93cb6c4f7c6661e8f9f0cd04f13520525997178d&amp;scene=21#wechat_redirect">心法利器[62] | 向量召回和字面召回的选择与权衡</a><strong>。</strong></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>句子 embedding</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>语义匹配前言：对比学习</title>
    <link href="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    <url>/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a><strong>1.引言</strong></h2><p>近两年，对比学习（Contrastive Learning）在计算机视觉领域（CV）掀起了一波浪潮，<strong>MoCo</strong>[1]、<strong>SimCLR</strong>[2]、<strong>BYOL</strong>[3]、<strong>SimSiam</strong>[4]等基于对比学习思想的模型方法层出不穷，作为一种无监督表示学习方法，在CV的一些任务上的表现已经超过了有监督学习。同时，自然语言处理（NLP）领域近来也有了一些跟进的工作，例如<strong>ConSERT</strong>[5]、<strong>SimCSE</strong>[6]等模型利用对比学习思想进行句表示学习，在语义文本相似度匹配（STS）等任务上超过了SOTA。</p><p>这篇笔记将带大家梳理一下对比学习的基本思想与方法，回顾一下CV领域对比学习的发展历程，并介绍几篇对比学习应用在NLP领域文本表示学习中的工作。</p><h2 id="2-对比学习简介"><a href="#2-对比学习简介" class="headerlink" title="2.对比学习简介"></a><strong>2.对比学习简介</strong></h2><h3 id="2-1-基本思想"><a href="#2-1-基本思想" class="headerlink" title="2.1 基本思想"></a><strong>2.1 基本思想</strong></h3><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1650448996786.png" class="" title="image.png"><p>在介绍对比学习的具体方法之前，让我们先了解下什么是对比学习？对比学习它最大的技术源泉来自于<strong>度量学习（Metric Learning）</strong>,你要看的话,会发现它的运作流程,基本就是度量学习的流程。什么意思呢?度量学习的优化目标就是说：比如我有正例和负例，它要将实体映射到一个空间里面去。<strong>它的目标是让正例在空间中近一些，负例在空间中远一些，这是度量学习的一个主体思想。</strong>其实对比学习从框架上来讲，基本就是度量学习上述思想。</p><p>另外一方面，对比学习的提出，包括这两年的兴起，我觉得很大的刺激因素是来自于BERT。因为我们知道BERT在NLP里面效果特别好，它是通过自监督的方式，在BERT预训练模型的时候，是把输入的句子随机扣掉一定比例的单词，然后让模型去准确地预测这些词。这是典型的自监督的模式，就是说不用人工来构造训练数据，可以根据一些规则自动构造训练数据。因为BERT效果特别好，我的理解是很多做图像领域的学者受到这件事的启发，所以就拿度量学习的框架再加上自监督学习的思路，来构造对比学习。所以说，<strong>你可以把对比学习理解为自监督版本的度量学习。</strong><br><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1654685124195.png" class="" title="image.png"></p><h3 id="2-2-自监督学习分类"><a href="#2-2-自监督学习分类" class="headerlink" title="2.2 自监督学习分类"></a>2.2 自监督学习分类</h3><p>自监督可以分为两种类型：生成式自监督学习，判别式自监督学习。VAE和GAN是生成式自监督学习的两类典型方法，即它要求模型重建图像或者图像的一部分，这类型的任务难度相对比较高，要求像素级的重构，中间的图像编码必须包含很多细节信息。<strong>对比学习则是典型的判别式自监督学习</strong>，相对生成式自监督学习，对比学习的任务难度要低一些。</p><h3 id="2-3-对比学习的指导原则以及构建模型的三个关键问题"><a href="#2-3-对比学习的指导原则以及构建模型的三个关键问题" class="headerlink" title="2.3 对比学习的指导原则以及构建模型的三个关键问题"></a>2.3 对比学习的指导原则以及构建模型的三个关键问题</h3><p>目前，对比学习貌似处于“无明确定义、有指导原则”的状态，它的<strong>指导原则</strong>是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。</p><p>明确了对比学习的指导原则，要构建对比学习模型就需要解决三个关键问题：<strong>第一个问题是：正例怎么构造？</strong>对于对比学习来说，原则上正例应该是自动构造出的，也就是自监督的方式构造的。负例怎么构造？一般来说负例好选，通常就是随机选的。<strong>第二个关键问题是Encoder映射函数</strong>，也即如何构造能够遵循上述指导原则的表示学习模型结构？<strong>第三个问题是Loss function怎么设计？</strong>即如何防止模型坍塌(Modal Collapse)。至于这个防止模型坍塌就是要让相似的实例在投影的空间中尽可能接近，不相似的尽可能远离。如果模型坍塌了，那么可能所有的实例都映射到了一个点，也就是一个常数。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1654685199301.png" class="" title="image.png"><h3 id="2-4-正负样例"><a href="#2-4-正负样例" class="headerlink" title="2.4 正负样例"></a>2.4 正负样例</h3><p>回顾2.3节说到的对比学习最关键的是三个问题：正负样例、对比损失以及模型结构。模型结构将在第三章介绍具体工作时详解介绍，先来聊一聊正负样例和对比损失。</p><p>一般对比学习的负例大多可以通过随机抽取构造，所以对比学习中最核心的问题是如何构造正例。</p><ul><li>数据增强：目前正例构造是对样例进行数据增强/扰动来得到更多正样本，而不是通过人工标注的数据（也就是有监督的方式）。正确有效的数据增强技术对于学习好的表征至关重要。比如SimCLR的实验表明，图片的随机裁剪和颜色失真是最有效的两种方式。而对于句子来说，删除或替换可能会导致语义的改变。</li><li>负样本构造：一般对比学习中使用in-batch negatives，将一个batch内的不相关数据看作负样本。<ul><li>注意，使用大的batch size是许多对比学习方法成功的一个关键因素。当batch size足够大时，能够提供大量的负样本，使得模型学习更好表征来区别不同样本。</li></ul></li></ul><p>图像领域中的扰动大致可分为两类：空间/几何扰动和外观/色彩扰动。空间/几何扰动的方式包括但不限于图片翻转（flip）、图片旋转（rotation）、图片挖剪（cutout）、图片剪切并放大（crop and resize）。外观扰动包括但不限于色彩失真、加高斯噪声等，见图4的例子。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424111722982" class=""><p>自然语言领域的扰动也大致可分为两类：词级别（token-level）和表示级别（embedding-level）。词级别的扰动大致有句子剪裁（crop）、删除词/词块（span）、换序、同义词替换等。表示级别的扰动包括加高斯噪声、<strong>dropout</strong>等。见图5。<br><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424111727781" class=""></p><h3 id="2-5-对比损失"><a href="#2-5-对比损失" class="headerlink" title="2.5 对比损失"></a>2.5 <a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247538505&amp;idx=3&amp;sn=b8257395c4b50e47f01d78696172cf85&amp;chksm=ebb76f9ddcc0e68b24e08aba4ed59a16e8ce95ca289fc9cb74a5e92124db16bfc249d868a22e#rd">对比损失</a></h3><p>在有了正负例之后，我们需要给模型信号，激励他们拉近正样例间的距离，拉远负样例间的距离，这就要通过设计对比损失来完成。给定一个样例 $x$ 和它对应的正样例 $x^+$ 以及负样例 $x_1^-,x_2^-,…,x_N^-$ (N可以为1，也可以很大)，我们需要一个表示函数 $f$，以及一个距离的度量函数 $D$。表示函数即由我们定义的模型得到，至于度量函数，最简单的自然是欧几里得距离：$D(x_1)=||x_1·x_2||_2$。不过在对比学习中一般会将表示归一化为长度为1的向量，故欧几里得距离与向量内积就相差一常数项，为计算方便，通常就用内积作为距离度量，即：$D(x_1)=x_1·x_2$，也可叫做余弦距离。有了这些准备工作，我们就可以定义对比损失。</p><ul><li><strong>原始对比损失</strong></li></ul><p>其实对比思想并不是近来才兴起，05、06年就有工作用对比思想做度量学习和数据降维。给定一个样例对 $(x_1,x_2)$，我们有标签 $y∈{0,1}$， $y=1$ 代表样例对互为正例，$y=0$ 代表样例对互为负例。进而定义了对比损失：</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424111731909" class=""><p>其中，$D()$ 为距离度量函数，$L_p()$ 为一递增函数，$L_n()$ 为一递减函数。当互为正例时，距离越远损失越高；互为负例时，距离越近损失越高。文献<strong>[8]</strong>中，$L_p(x)=1/2x^2$ ，$L_n(x)=1/2(max\{0,m-x\})^2$ ；文献<strong>[11]</strong>中，$L_p(x)=x^2$ ，$L_n(x)=(max\{0,m-x\})^2$，$m$ 是超参，控制负例的范围。用平方是因为他们的距离度量为欧几里得距离，平方可以规避开方操作。</p><p>$L=yD(·)^2+(1-y)(max(0,m-D(·)))^2$</p><ul><li><strong>三元组损失（triplet loss）</strong></li></ul><p>相信很多人对triplet loss 很熟悉了，最初是<strong>FaceNet</strong>[12]做人脸表示学习时提出的：给定一个三元组 $(x,x^+,x^-)$， $x$ 被称做锚点(Anchor), $x^+$ 为正例，$x^-$ 为负例。triplet loss设计的初衷也是希望通过对比使得锚点与正例的距离更近，与负例更远，具体形式为:</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424111739561" class=""><p>其中 $D()$ 为欧几里得距离，$m$ 是用来控制正例负例距离间的偏离量，使模型不需要考虑优化过于简单的负例。论文还指出，挑选真正有挑战的负例对提升模型表现非常重要。</p><p>虽然triplet loss已经满足了对比学习的要求，但是他把一个样例限定在了一个三元组中，一个正例只与一个负例对比。实际操作中，对一个样例，我们能得到的负样例个数远远多于正样例，为了利用这些资源，近年来对比学习多用InfoNCE损失（还有其它类型的对比损失，这里只介绍主流做法）。</p><ul><li><strong>N-pair Loss</strong></li></ul><p><strong>N-pair Loss（</strong>Multi-Class N-pair loss）拓展了Triplet Loss，泛化到与多个负样本进行对比。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1654685386863.png" class="" title="image.png"><ul><li><strong>NCE 损失</strong></li></ul><p><strong>NCE</strong>（Noise Contrastive Estimation）全称是噪声对比估计，是估计统计模型参数的一种方法，主要通过学习数据分布和噪声分布之间的区别。</p><p> (NCE) 将概率估计问题（多分类）转化为二分类问题，判别给定样例是来源于原始分布还是噪声分布，用二分类的最大似然估计替代原始问题。核心思想是通过逻辑回归（logistic regression）对数据与噪声进行二分类，利用已知的噪声概率分布，来估计未知的经验概率分布。听起来好像跟对比学习没什么关系，但如果把噪音分布的样本想成负样例，那这个二元分类问题就可以理解为让模型对比正负样例作出区分进而学习到正样例(原始分布)的分布特征，这是不是就跟对比学习的思想很像了？</p><p>假设目标样本分布 $𝒑_𝒎 (θ)$，采样的噪声分布 $𝒑_𝒏$  ，通过估计 $𝒑_𝒎/𝒑_𝒏$来最终估计出 $𝒑_𝒎 (θ)$的参数 θ ，并让这些数据的最大似然概率最大。最终的对数似然函数：（x 表示数据，y 表示噪声）</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1646805723899-a243ec56-be5e-4b02-af7c-9faef6015e9d.png" class=""><p>NCE 常被用于解决多分类问题下 softmax 分母归一化中类别太多计算量太大，难以求值的问题</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1654686214719.png" class="" title="image.png"><ul><li><strong>InfoNCE损失</strong></li></ul><p>InfoNCE继承了NCE的基本思想，从一个新的分布引入负样例，构造了一个新的多元分类问题，并且证明了减小这个损失函数相当于增大互信息 (mutual information) 的下界，这也是名字infoNCE的由来。具体细节这里不再赘述，感兴趣的读者可以参考<a href="http://karlstratos.com/notes/nce.pdf">「这篇文章」</a>，里面有比较清晰的介绍与推导。</p><p>我们直接看一下目前<strong>对比学习中常见的infoNCE loss形式：</strong></p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1654703845131.png" class="" title="image.png"><p>其中 $u$、$v^+$、$u^-$ 分别为原样例、正样例、负样例归一化后的表示，$t$ 为温度超参。</p><p>显而易见，infoNCE最后的形式就是多元分类任务常见的交叉熵损失（Cross Entropy Loss for N-way softmax classifier)，使用分类交叉熵损失在一组负样本中识别正样本。因为表示已经归一化，据前所述，向量内积等价于向量间的距离度量。故由softmax的性质，上述损失就可以理解为，我们希望在拉近原样例与正样例距离的同时，拉远其与负样例间的距离，这正是对比学习的思想。</p><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg3NzY2ODIzOA==&amp;mid=2247495099&amp;idx=1&amp;sn=d5f57b9cc9f797de603f6a8ed1a9b17e&amp;chksm=cf1ddf5af86a564caffe60ca4f6071db4055da3741a13f9dff0d0d619e5a934540bf96120a93&amp;scene=21#wechat_redirect"><strong>温度超参τ用什么作用？</strong></a></p><p>τ为softmax的温度超参，并不是原始InfoNCE损失的组成部分，它被引入的一个重要前提假设就是“不完全信任用户的点击标签”，意在控制模型对标签的信任程度，越小（趋向于0），则越信任，反之则越不信任。τ越小，softmax越接近真实的max函数，越大越接近一个均匀分布。因此，当很小时，只有难区分的负样例才会对损失函数产生影响，同时，对错分的样例（即与原样例距离比正样例与原样例距离近）有更大的惩罚。实验结果表明，对比学习对$t$ 很敏感。下文对比损失若不特意提及，则默认为infoNCE loss。</p><h3 id="2-6-衡量标准"><a href="#2-6-衡量标准" class="headerlink" title="2.6 衡量标准"></a>2.6 衡量标准</h3><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/v2-1e089e04890db88c8239ae2bbdf78616_1440w.jpg" class=""><p>好的对比学习系统应该满足什么条件呢？或者说好的对比学习的效果如何？（可以参考上图所示论文）它应兼顾两个要素：<strong>Alignment和Uniformity</strong>。</p><p>Alignment代表我们希望对比学习把相似的正例在投影空间里面有相近的编码，一般来说，对比学习获得的表示向量会被投影到<strong>单位超球面</strong>上进行相互比较。如果模型的表示能力足够好，能够把相似的例子（比如带有相同类标号的数据）在超球面上聚集到较近区域，那么很容易使用线性分类器把某类和其它类区分开。</p><p>关键的是第二点，就是这个uniformity。Uniformity代表什么含义呢? Uniformity直观上来说就是：当所有实例映射到投影空间之后，我们希望它在投影空间内的分布是尽可能均匀的。这里有个点不好理解：为什么我们希望分布是均匀的呢？其实，追求分布均匀性不是Uniformity的目的，而是手段。追求分布的Uniformity实际想达成的是什么目标呢？它实际想达成的是:我们希望实例映射到投影空间后，在对应的Embedding包含的信息里，可以更多保留自己个性化的与众不同的信息。</p><p>那么，Embedding里能够保留更多个性化的信息，这又代表什么呢？举个例子，比如有两张图片，都是关于狗的，但是一张是在草地上跑的黑狗，一张是在水里游泳的白狗。如果在投影成Embedding后，模型能各自保留各自的个性化信息，那么两张图片在投影空间里面是有一定距离的，以此表征两者的不同。而这就代表了分布的均匀性，指的是在投影球面上比较均匀，而不会说因为都是关于狗的图片，而聚到球面的同一个点中去，那就会忽略掉很多个性化的信息。这就是说为什么Uniformity分布均匀代表了编码和投影函数f保留了更多的个性化信息。</p><p>Uniformity特性的极端反例，是所有数据映射到单位超球面同一个点上，这极度违背了Uniformity原则，因为这代表所有数据的信息都被丢掉了，体现为数据极度不均匀得分布到了超球面同一个点上。也就是说，所有数据经过特征表示映射过程 $g_{\theta}(f_{\theta}(x))$ 后，都收敛到了同一个常数解，一般将这种异常情况称为<strong>模型坍塌（Collapse)</strong>。</p><p>一个好的对比学习系统，要兼顾这两者。既要考虑Alignment，相似实例在投影空间里距离越近越好。也要考虑Uniformity，也就是不同实例在投影空间里面分布要均匀一些，让实例映射成embedding之后尽可能多的保留自己的个性化信息。</p><h2 id="3-对比学习在CV领域的应用"><a href="#3-对比学习在CV领域的应用" class="headerlink" title="3.对比学习在CV领域的应用"></a><strong>3.对比学习在CV领域的应用</strong></h2><p>这一章将介绍近几年图像领域对比学习有代表性的几篇工作：</p><ul><li>基于负例的对比学习：SimCLR系列</li><li>基于负例的对比学习：MoCo系列</li><li>基于负例的对比学习：SwAV系列</li><li>基于Stop-gradient的对比学习</li><li>其他对比学习方法：Barlow Twins</li></ul><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1654743199309.png" class="" title="image.png"><h3 id="3-1-SimCLR-Batch内负例"><a href="#3-1-SimCLR-Batch内负例" class="headerlink" title="3.1 SimCLR-Batch内负例"></a>3.1 <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>-Batch内负例</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/345474252">基于In-Batch负例的对比学习-SimCLR</a><br><a href="https://yugeten.github.io/posts/2021/12/ssl/">An incomplete and slightly outdated literature review on augmentation based self-supervise learning</a></p></blockquote><p>作为Google的大作，SimCLR提出了一种简单的对比学习框架，通过对同一个图像进行增强，得到两个不同版本，随后通过编码器$f=ResNet$对图像编码，再使用一个映射层$g$将其映射到特征空间，使用  <a href="https://arxiv.org/pdf/1807.03748.pdf%5D">infoNCE</a> 损失进行训练</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1650448865906.png" class="" title="image.png"><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1659513097461.png" class="" title="image.png"><ol><li><strong>随机数据增强模块</strong><br>本文使用了三种方法进行数据增强：<strong>随机裁剪和调整</strong>（裁剪后调整图像尺寸为原图大小)、<strong>随机色彩失真</strong>和<strong>随机高斯模糊</strong>。对于某张图片，我们从可能的增强操作集合 T 中，随机抽取两种分别作用在原始图像上，形成两张经过增强的新图像 $<x_A,x_B>$ ，两者互为正例；而负例使用 In-Batch Negatives</li></ol><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1650449619418.png" class="" title="image.png"><ol><li><strong>基编码器(base encoder)</strong><br>SimCLR采用的是典型的<strong>双塔</strong>结构，其中基编码器定义为$f(·)$，其作用在于从增强后的数据集中提取表征向量 $h_i$：</li></ol><p><img src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bh%7D_i%3Df%28%5Cwidetilde%7Bx%7D_i%29%3DResNet%28%5Cwidetilde%7Bx%7D_i%29+%5C%5C#id=JLMB5&amp;originHeight=40&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><ol><li><strong>小型神经网络投影头(projection head)</strong><br>非线性变换结构 Projector，这里以函数$g(·)$代表，它的作用是将编码后的表征 $h_i$映射到潜在特征空间中，本文使用的是一个两层 MLP [FC-&gt;BN-&gt;ReLU-&gt;FC]，具体计算方法如下：</li></ol><p><img src="https://www.zhihu.com/equation?tex=z_i%3Dg%28h_i%29%3DW%5E%7B%282%29%7D%28%5Csigma%28W%5E%7B%281%29%7D%28h_i%29%29%29+%5C%5C#id=gtjC1&amp;originHeight=44&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><ol><li><strong>对比损失</strong><br>The model θ is learned through minimising the following <a href="https://arxiv.org/pdf/1807.03748.pdf%5D">infoNCE</a> objective:</li></ol><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1650450111794.png" class="" title="image.png"><p>其中sim表示两个向量之间的余弦相似度，即 $\operatorname{sim}(u, v)=\frac{u^{T} v}{|u| \cdot|v|}$.</p><p>通过最小化上述 objective，我们最大化同一图像的两个视图view的表示之间的相似性，并最小化不同图像之间的相似性。</p><hr><p><strong>SimCLR 伪代码流程：</strong></p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1027447-20210305190712586-1714272228.png" class=""><p><strong>SimCLR怎么防止坍塌：</strong></p><blockquote><p>SimCLR本质上是通过引入负例来防止模型坍塌的</p></blockquote><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424111911991" class=""><p>上图展示的InfoNCE损失函数。InfoNCE的分子 $S(z_i,z_i^+)$ 部分体现了Alignment这个要素，因为它期望正例在投影空间里面越近越好，也就是相似性越大越好。它防止坍塌是靠分母里的  $S(z_i,z_j)$ 负例：也就是说，如果图片和负例越不相似，则相似性得分越低，代表投影空间里距离越远，则损失函数就越小。InfoNCE通过强迫图片和众多负例之间，在投影球面相互推开，以此实现分布的均匀性（防止模型坍塌)，也就兼顾了Uniformity这一要素。所以你可以理解为SimCLR是通过随机负例来防止模型坍塌的。</p><p>图像领域对比学习，目前有两个很明确的结论，是这样的：第一是：在Batch内随机选取负例，选取的负例数量越多，对比学习模型的效果越好；第二个是在InfoNCE的公式里有个τ，这个叫<strong>温度系数</strong>，温度系数对于对比学习模型效果的影响非常之大，你设置成不同的参数，可能效果会差百分之几十，一般来说，这个τ经验上应该取比较小的值，从0.01到0.1之间。问题是：将这个超参设大或设小，它是如何影响模型优化过程的呢？目前的研究结果表明，InfoNCE是个能够感知负例难度的损失函数，而之所以能做到这点，主要依赖超参。</p><p>什么是有难度的负例？什么是容易的负例呢？我们知道，对比学习里，对于某个数据 $x_i$，除了它的唯一的正例 $x_i^+$外，所有其它数据都是负例。但是，这些负例，有些和 $x_i$ 比较像，有些则差异比较大，比如假设 $x_i$是张关于狗的图片，那么另外一张狗的图片，或者一张狼的图片，就是有难度的负例，而如果是一张关于人的或者树的图片，则比较好和 $x_i$ 区分开，是容易例子。如果经过 $g_θ(f_θ(x))$ 将数据映射到单位超球面后，根据Alignment原则，一般来说，比较像的、有难度的负例在超球面上距离 $x_i$ 比较近，而比较容易区分的负例，则在超球面上距离 $x_i$ 比较远。所以说，对于例子 $x_i$ 来说，在超球面上距离 $x_i$ 越近，则这个负例越难和 $x_i$ 区分，距离 $x_i$ 越远，则这个负例越容易和 $x_i$ 区分。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/v2-8468c5a6b359716e13d8da063a09e5c0_1440w.jpg" class=""><p><strong>总体而言，温度参数 $\tau$ 起到如下作用：温度参数会将模型更新的重点，聚焦到有难度的负例，并对它们做相应的惩罚，难度越大，也即是与 $x_i$  </strong>距离越近，则分配到的惩罚越多。所谓惩罚，就是在模型优化过程中，将这些负例从 $x_i$ 身边推开，是一种斥力。也就是说，距离 $x_i$ 越近的负例，温度超参会赋予更多的排斥力，将它从 $x_i$ 推远。而如果温度超参数 $\tau$ 设置得越小，则InfoNCE分配惩罚项的范围越窄，更聚焦在距离 $x_i$ 比较近的较小范围内的负例里。同时，这些被覆盖到的负例，因为数量减少了，所以，每个负例，会承担更大的斥力（参考上图左边子图）。极端情况下，假设温度系数趋近于0，那么InfoNCE基本退化为Triplet，也就是说，有效负例只会聚焦在距离 $x_i$ 最近的一到两个最难的实例。从上述分析，可以看出：温度超参越小，则更倾向把超球面上的局部密集结构推开打散，使得超球面上的数据整体分布更均匀（参考上图右边子图)。</p><p>那么，是不是温度超参 $\tau$ 设置的越小越好呢？因为这个数值越小，意味着超球面上的数据分布越均匀，越符合Uniformity原则。其实，并不是这样的。因为在对比学习这种场景下，对于某个数据 $x_i$ ，只有一对正例 $<x_i,x_i^+>$ ，可能会发生如下的情形：距离 $x_i$ 比较近的所谓“负例”，其实本来应该是正例，比如 $x_i$ 是一张狗的照片，而 $x_i^-$ 其实也是一张狗的照片。只是因为对比学习是无监督的，我们没有先验知识知晓这一点，所以也会把这张狗的照片当作负例。而如果温度超参越小，则可能越会倾向把这些本来是潜在正例的数据在超球面上推远，而这并不是我们想要看到的。要想容忍这种误判，理论上应该把温度超参设置大一些。所以，温度超参需要在鼓励Uniformity和容忍这种误判之间找到一个平衡点，而调节这个参数大小，其实就是在寻找这两者的平衡点。</p><h3 id="3-2-Moco-Batch外负例"><a href="#3-2-Moco-Batch外负例" class="headerlink" title="3.2 Moco-Batch外负例"></a>3.2 Moco-Batch外负例</h3><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424111920308" class=""><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1659513555363.png" class="" title="image.png"><p>刚才讲的是SimCLR系统，现在介绍另外一个代表系统Moco（Facebook提出）。前面说过一个已有结论：负例用的越多，模型效果越好。我们知道SimCLR是在Batch里面随机选的负例。但是在Batch里采负例有现实问题：Batch size不能无限放大，因为batch size增大对计算资源要求比较高，所以总有个限度。现有结论是负例越多越好，但是batch size制约了负例的采样个数，我们希望能采取一些技术手段，能采样大量的负例，但是又不受Batch size大小限制的约束。MocoV2是典型的解决这种矛盾的例子，也就是说，我们如何能够解除batch size的约束，来大幅增加负例的数量。</p><p>上图是Moco V2的模型结构图。其实它和SimCLR的结构基本是类似的，只有一点小差异。它也是上下两个双塔结构，网络结构也由两个映射子结构组成，和SimCLR完全一样。下面这个分支结构本身是和上分支的网络结构是完全一样的。</p><p><strong>上下两个分支的区别有两点：第一点是下分支的网络参数更新机制和上分支的更新机制不一样，采用动量更新机制；第二点是说Moco维护了一个负例队列。</strong>细节在这里不展开说了。它实际是想解决一个问题，就是负例受batch size大小制约的影响，怎么解决的呢？通过维护的负例队列来解决。也就是说下分支的正例通过下分支打成embedding,然后会把它放入队列里面（入队），在队列待了太久的会让它出队。Moco的负例采集方法和SimCLR一样，是随机抽取负例。但它不是在batch里面取，而是从负例队列里取。你可以在负例队列里面取任意大小的负例数量来作为模型的负例，这样就避免了负例大小受batch size的影响。这就是典型的Moco做对比学习的思路。<strong>Moco主要是两点：一个是下分支的动量更新，一个就是负例队列。</strong>这两点是比较新的。</p><ul><li><strong>代码流程</strong></li></ul><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424111928619" class=""><p>如果归纳一下，现在采用负例的方法主要有三个共识：第一，网络结构现在大家基本用的都是双塔结构。第二，映射函数一般由两部分组成，首先是Resnet，用来对图像编码，还有一个是projector。第三，在Moco v3和SimCLRv2 版本升级时都有体现，就是encoder越来越复杂。如果用Resnet它会越来越宽，越来越深，也可以用更复杂的transformer来做这件事。这三点应该说是目前用负例作对比学习的三个比较典型的特点。</p><h3 id="3-3-SwAV-对比聚类"><a href="#3-3-SwAV-对比聚类" class="headerlink" title="3.3 SwAV-对比聚类"></a>3.3 SwAV-对比聚类</h3><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/v2-205b29c398f2c40a9bf92940ed851f1b_1440w.jpg" class=""><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1659513763697.png" class="" title="image.png"><p>最后再介绍下SwAV，它是个典型的对比聚类的方法，SwAV是图像对比学习众多模型中效果最好的方法之一。关于正负例构造方法，SwAV和刚才提到的方法一样，这里不再多提。关于模型结构，SwAV的双塔结构和两个映射函数，和SimCLR等模型一样，是上下对称的。</p><p><strong>SwAV的主要特点在这里：在模型训练过程中引入了聚类。</strong>我们以一个例子阐述其工作流程。对于Batch内某张图像 $x$ 来说，假设其经过图像增强后，在Aug1和Aug2里的对应的增强后图像分别是 $x_i$ 和 $x_j$ ，数据对 $<x_i,x_j>$ 互为正例。增强视图 $x_i$ 走上分枝，经过 $g_θ(f_θ(x))$ 投影到单位超球面中某点 $z_i$ ，增强视图 $x_j$ 走下分枝，经过  $g_θ(f_θ(x))$ 投影到单位超球面中某点 $z_j$。之后，SwAV对Aug1和Aug2中的表示向量，根据Sinkhorn-Knopp算法，在线对Batch内数据进行聚类。假设走下分枝的 $x_j$ 聚类到了 $q_j$ 类，则SwAV要求表示学习模型根据预 $x_i$ 测 $x_j$ 所在的类，也就是说，要将 $z_i$ 分到第 $q_j$ 类，具体损失函数采用 $z_i$ 和Prototype中每个类中心向量的交叉熵：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7Baug1%7D+%28z_i%2Cq_j+%29%3D-%E2%88%91_kq_j%5Ek%E2%8B%85log%28p_i%5Ek+%29++#id=R4Rr9&amp;originHeight=31&amp;originWidth=263&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>其中，</p><p><img src="https://www.zhihu.com/equation?tex=p_i%5Ek%3Dexp%28z_i+c_k%2F%CF%84%29%2F%E2%88%91_%7Bk%5E%22%7Dexp%28z_i+c_%7Bk%5E%22+%7D%2F%CF%84%29+#id=PCjMY&amp;originHeight=31&amp;originWidth=294&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>$c_k$ 为第k个聚类的类中心向量，$\tau$ 为温度超参数。</p><p>因为是对称结构，同样的，SwAV要求模型根据 $x_j$ 预测 $x_i$ 所在的类，也就是说，要将 $z_j$ 分到第 $q_i$ 类。所以，SwAV的总体损失函数是两个分枝损失之和：</p><p><img src="https://www.zhihu.com/equation?tex=+L_%7BSwAV%7D%3DL_%7Baug1%7D+%28z_i%2Cq_j+%29%2BL_%7Baug2+%7D%28z_j%2Cq_i+%29#id=os7Zh&amp;originHeight=24&amp;originWidth=290&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>这被称为Swapped Prediction。</p><p>SwAV也会面临模型坍塌的问题，具体表现形式为：Batch内所有实例都聚类到同一个类里。所以为了防止模型坍塌，SwAV对聚类增加了约束条件，要求Batch内实例比较均匀地聚类到不同的类别中。</p><p>这种对比聚类方法，看上去貌似只用了正例，未使用负例。但本质上，它与直接采用负例的对比学习模型，在防止模型坍塌方面作用机制是类似的，是一种隐形的负例。我们可以再仔细观察下它的损失函数，从中不难看出，在单位球面中，它要求某个投影点 $z_i$ 向另外一个投影点 $z_j$ 所属的聚类中心靠近，这体现了Alignment原则；而分母中的投影点 $z_j$ 所不属于的那些类中心，则充当了负例，它要求投影点 $z_i$ 在超球面上，和其它聚类中心越远越好，这体现了Uniformity属性，也是防止模型坍塌的关键。我们也可以换个角度，从聚类的角度来看SimCLR中的正例和负例，我们可以把SimCLR看成是：每两个正例组成了一个聚类中心。如果从这个角度看，其实SimCLR这种正负例方法，是种极端情况下的聚类模型。我们在上文说过，SimCLR这种模式，当温度超参设的比较小的时候，容易出现误判的负例，而聚类模型无疑在容忍负例误判方面，天然有很好的包容力，这也许是聚类方法效果好的原因之一。</p><h3 id="3-4-非对称结构：模型不坍塌之谜"><a href="#3-4-非对称结构：模型不坍塌之谜" class="headerlink" title="3.4 非对称结构：模型不坍塌之谜"></a>3.4 非对称结构：模型不坍塌之谜</h3><p>上文有述，在常见的基于负例的对比学习方法中，负例有着举足轻重的作用，它起到了将投影到超球体平面的各个实例对应的表示向量相互推开，使得图像对应的表示向量在超球体表面分布均匀的作用，以此来避免表示学习方法模型坍塌问题。尽管对比聚类方法看似没有明确使用负例，但如果深究，会发现仍然是负例在避免模型坍塌方面起作用。</p><p>那么，问题来了：并不是所有机构都有像谷歌一样的算力能够让<strong>batch-size=8192，用128核的TPU</strong>进行运算，同时越大的batch size，就有越大的可能包含错误负例（False Negative）…如果我们只使用正例，不使用负例来训练对比学习模型，这种思路是可行的吗？乍一看，这几乎是不可能的：假设只有正例，模型推动正例在表示空间内相互靠近。如果只有这一优化目标，很明显，理论上，模型会很快收敛到常数解，也就是所有数据会被映射到表示空间里同一个点上。就是说，很容易出现模型坍塌的结局。</p><p>但是，<strong>BYOL</strong>模型就是这么做的，关键是，它还做成功了，更关键的是，不仅做成功了，它还是目前效果最好的对比学习模型之一。BYOL的作者想：如何不用负例，也能学到好的表示呢？如果共用encoder，用MSE作为损失，缩小相同图像不同view的距离，肯定会坍缩。而作者发现如果把其中一个encoder变成随机初始化的固定下来（stop gradient），就能达到18.8%的准确率</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/v2-8f156f1561606a94b4edc19ebcb909f3_1440w.jpg" class=""><p>BYOL的模型结构如上图所示。对于Batch内任意图像，类似SimCLR采取随机图像增强，产生两组增强图像视图Aug1和Aug2，彼此互为正例，分别走上下两个模型分枝，上分枝被称为Online，下分枝被称为Target。Online分枝的Encoder和Projector和其它对比学习模型是一样的，但是，在Projector之后，新增了一个非线性变换模块Predictor，Predictor的结构和Projector类似（[FC-&gt;BN-&gt;ReLU-&gt;FC]构成的MLP映射网络），产生表示向量 $v_i$ 并对 $v_i$ 做L2正则化，将向量映射到单位超球面上。Target分枝结构类似Moco V2对应下分枝的动量更新结构，即由自有参数的Encoder和Projector构成，且模型参数不参与梯度更新，采用Online分枝对应结构参数的Moving Average动量更新方式。以此方式，产生增强图像Aug2的向量 $z_j$ ，同样地，会对 $z_j$ 做L2正则化操作，将表示向量映射到单位超球面上。但是，因为BYOL不用负例，所以并不需要维护Moco V2中的负例队列，下分枝只是对Aug2中的正例进行投影。</p><p>对于BYOL来说，它的优化目标要求Online部分的正例，在表示空间中向Target侧对应的正例靠近，也即拉近两组图像增强正例之间的距离，对应Loss 函数为：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7Baug1%7D%3D%E2%80%96v_i-z_j+%E2%80%96_2%5E2%3D2-2%5Ccdot%E2%8C%A9v_i%EF%BC%8Cz_j+%E2%8C%AA%2F%28%E2%80%96v_i+%E2%80%96_2%5Ccdot%E2%80%96z_j+%E2%80%96_2+%29#id=JaFCL&amp;originHeight=29&amp;originWidth=417&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>可见，经过改写，$L_{aug1}$ 也是Cosine相似性的一个变体，它的最小值对应两个表示向量的Cosine最大值，也即优化目标是在单位超球面上，正例之间的距离越近越好。由于online和Target分枝是不对称的，所以BYOL会交换两批增强图像，要求Aug2的图像也走一遍Online网络，并向Aug1图像对应的Target分枝表示向量靠近。也就是说，BYOL的损失函数为：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7Bbyol%7D%3DL_%7Baug1%7D%2BL_%7Baug2%7D#id=EILnN&amp;originHeight=23&amp;originWidth=168&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>我们知道，Moco V2在下分枝也采用了动量更新结构，如果我们把Moco V2的负例队列抛掉，并在它的上分枝加入类似BYOL的Predictor模块，则BYOL和Moco V2在结构上就保持一致。如果这么改动，两者的差异主要体现在损失函数带来的优化目标不一样：两者都试图将正例在表示空间拉近，但是Moco V2会在InfoNCE损失函数里用负例来防止模型坍塌，而BYOL对应的损失函数里，则没有对应的负例子项。</p><p>问题是：既然BYOL只用正例，它是如何防止模型坍塌的呢？背后的原因，目前仍然是未解之谜，不过对此也有些研究进展。BYOL的论文里首先指明了：之所以它没有坍塌到常数解，是由于online和Target两者结构的不对称造成的。具体而言，是动量更新的target结构和Online中的Predictor共同协作发生作用的。如果拿掉Predictor，或者把Target结构中的模型参数改成近乎实时和Online对应结构保持一致（就是说，每个Batch反向传播后，将Online部分最新的参数完全赋予给Target对应结构参数。或者理解为，动量更新公式中权重m取值为0），无论是哪种情况，模型都会发生坍塌。BYOL在论文里进一步实验，表明了最关键的因素在于新加入的Predictor结构：即使Target结构参数和Online部分保持一致，只要把Predictor部分的学习率调大，那么BYOL同样也不会坍塌。这说明Predictor的存在，是BYOL模型不坍塌的最关键因素，但是要配置大的学习率。此外，有其它研究[参考：Understanding self-supervised and contrastive learning with bootstrap your own latent (BYOL).]指出，Predictor中的BN在其中起到了主要原因，因为BN中采用的Batch内统计量，起到了类似负例的作用。但是很快，BYOL的作者在另外一篇文章里[参考：BYOL works even without batch statistics]对此进行了反驳，把Predictor中的BN替换成Group Norm+Weight standard，这样使得Predictor看不到Batch内的信息，同样可以达到采用BN类似的效果，这说明并非BN在起作用。</p><p><strong>所以说，为何BYOL这种只用正例的对比学习模型不会发生期望中的模型坍塌，目前还未有定论，但是我们可以定位到主要由于Predictor结构的存在造成的。</strong>当然，说是模型结构的不对称带来的效果，原则上是没有问题的，因为这是一种相对粗略的说法。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/v2-a22db42a88ceab75019e40178cac66d7_1440w.jpg" class=""><p><strong>SimSiam</strong>进一步对BYOL进行了简化，我们可以大致将SimSiam看作是：把BYOL的动量更新机制移除，下分枝的Encoder及Projector和上分枝对应构件参数共享版本的BYOL（参考上图），类似前面介绍BYOL里说的Predictor加大学习率的版本。但是，从后续文献的实验对比来看，SimSiam效果是不及BYOL的，这说明动量更新机制尽管可能不是防止模型坍塌的关键因素，但是对于提升对比学习模型效果是很重要的。</p><h3 id="3-5-Decouple-Contrastive-Learning"><a href="#3-5-Decouple-Contrastive-Learning" class="headerlink" title="3.5 Decouple Contrastive Learning"></a>3.5 Decouple Contrastive Learning</h3><blockquote><p><a href="https://mp.weixin.qq.com/s/LjF6r-BjpNZfwwYkzNXPTA">图灵奖大佬 Lecun 发表对比学习新作，比 SimCLR 更好用！</a></p></blockquote><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1638951979921-4a8476be-6689-4af0-99b3-79b6bf64b66b.png" class="" title="image.png"><p>SimCLR对于模型效果的提升必须基于<strong>大Batch Size</strong>才会有效果。</p><p>而在近期，由 Yann Lecun 等人发表了一篇题为《Decouple Contrastive Learning》的论文，其中仔细分析了SimCLR和其他自监督学习模型所使用的InfoNCE损失函数，<strong>仅仅对InfoNCE的表达式进行了一处修改，就大大缓解了InfoNCE对于大Batch Size的需求问题</strong>，并在不同规模的Vision Benchmarks上均取得优于SimCLR的结果。</p><p>具体来说，通过将导数中的NPC乘数移除，作者推导出了下面的损失函数。在这个损失函数中，<strong>正负样本的耦合带来的梯度放缩被消去</strong>，作者将该损失称为<strong>Decoupled Contrastive Learning (DCL) Loss</strong>，即<strong>解耦对比损失函数</strong>：<br><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1638952096918-649a528a-3f53-41aa-ad9f-ac4283692021.webp" class=""></p><p>可见，Decoupled Constrive Learning中的损失直接<strong>去掉了SimCLR损失函数分母中两个正样本对之间的相似度</strong>，从而直接计算正样本对的相似度同所有负样本对相似度之和的比值。</p><p>总结来说，DCL损失仅在SimCLR所采用的损失函数基础上采取了一些小的改动，使得模型能够在训练过程中也不要求大Batch Size，同时对正负样本对进行解耦。<strong>在不同的Batch Size上，DCL损失的效果均优于SimCLR</strong>。同时，<strong>Batch Size越小，DCL损失提供的性能提升越大</strong>，这与先前的理论推导一致。</p><p>介绍了这么多对比学习在图像领域的工作，接下来我们一起看一下自然语言处理领域的对比学习工作，篇幅有限，这里主要介绍两篇对比学习做句子表示学习的文章。</p><h2 id="4-对比学习在NLP领域的应用"><a href="#4-对比学习在NLP领域的应用" class="headerlink" title="4.对比学习在NLP领域的应用"></a><strong>4.对比学习在NLP领域的应用</strong></h2><h3 id="4-0-Bert句向量不能直接用于相似度问题的分析"><a href="#4-0-Bert句向量不能直接用于相似度问题的分析" class="headerlink" title="4.0 Bert句向量不能直接用于相似度问题的分析"></a>4.0 <a href="https://toutiao.io/posts/sjxo3i1/preview">Bert句向量不能直接用于相似度问题的分析</a></h3><p>通常，我们使用Bert模型 [CLS] token的embedding向量或者最后几层的token embedding向量的平均来作为句子的sentence embedding。然而BERT的原生句子表示被证明是低质量的，甚至比不上Glove的结果。如下图所示，当应用BERT的句子表示到STS（semantic textual similarity）任务上时，几乎所有句子对都达到了0.6~1.0的相似度得分，这就是BERT的原生句子表示的<strong>坍塌问题（collapse issue），这意味着所有的句子都倾向于编码到一个较小的局部区域内</strong>，因此大多数的句子对都具有较高的相似度分数，即使是那些语义上完全无关的句子对。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1649088043659.png" class="" title="image.png"><p>图1 左：BERT表示空间的坍缩问题（横坐标是人工标注的相似度分数，纵坐标是模型预测的余弦相似度)；右：经过ConSERT方法Fine-tune之后</p><p>类似的现象在先前的研究中也有被观察到，研究人员发现BERT的词表示空间是<strong>「</strong><a href="http://www.360doc.com/content/21/1110/14/7673502_1003580952.shtml"><strong>各向异性</strong></a><strong>」</strong>的（即，用不同的方式去衡量它，他表现出不同的语义差别很大），高频词聚集到一起并且靠近坐标原点，而低频词则稀疏分散。当使用token embedding的平均作为句子表示时，高频词的就会起到主导地位，这就导致产生了对句子真实语义的偏置。也就是说，BERT句向量表示的<strong>坍缩和句子中的高频词有关</strong>。因此，在下游任务中直接应用BERT的原生句子表示是不合适的。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1649088521465.png" class="" title="image.png"><h3 id="4-1-无监督句子表征学习"><a href="#4-1-无监督句子表征学习" class="headerlink" title="4.1 无监督句子表征学习"></a>4.1 无监督句子表征学习</h3><p>针对表示学习存在的各向异性/表示崩塌现象，对比学习并不是唯一解决该问题的研究方向，对BERT的后处理操作也是一种解决方法。</p><h4 id="Bert-flow"><a href="#Bert-flow" class="headerlink" title="Bert-flow"></a>Bert-flow</h4><p>BERT-flow 在文中详细总结并分析了以上两个问题之后，思想也比较直接，既然 BERT 出来的 embedding 向量存在各向异性，进而也产生了分布不均匀问题，那我就采用一个变换，将 BERT encode 的句子表达转换到一个各向同性且分布较均匀的空间。而标准的高斯分布刚好是一个各向同性的空间，且是一个凸函数，语义分布也更平滑均匀。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424112026125" class=""><p>BERT-flow 采用一种流式可逆变换，记为：</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424112034435" class=""><p>，其中 u 就是 BERT 空间向量（observe space），z 即是高斯空间向量（latent space)，$f:z→u$ 则是一个可逆变换，这个变换也是模型需要学出来的。<strong>最后，通过无监督的方式 maximize 这个…优化目标，得到可逆的映射变换 f，这其实就是在 Bert pre-train model 后接了一个 flow 变换的模型，让其继续 pre-train，学出 flow 变换，从而完成向量空间的 transform。</strong></p><p>小结：BERT-flow 完整分析了BERT 句子 embedding 里存在的向量各向异性及分布不均匀问题，并列出了完整的实验结果佐证了这一现象。同时提出了一种 flow 变换，将各向异性的 BERT 向量转换到一个标准的高斯分布空间，从而有效的提升了无监督领域的文本表达效果</p><h4 id="Bert-Whitening"><a href="#Bert-Whitening" class="headerlink" title="Bert-Whitening"></a>Bert-Whitening</h4><p>既然是要做一个向量的转换，那有没有简单一点的方法，直接校正句向量？BERT-whitening 提出通过一个白化的操作直接校正句向量的协方差矩阵，简单粗暴，也达到了与 BERT-flow 差不多的效果。此外，whitening技术还能够降低句子表征的维数，降低存储成本，加快模型检索速度。</p><p>方法很简单，思路也很直接，就是将现存的 BERT 向量空间分布记 $x_i-(u,\sum)$，其中均值为 $u$，协方差 $\sum$，进行白化操作，转换成$u=0,\sum$为单位阵的分布，也就是进行如下表达转换：$x_i^`=(x_i-u)W$。所以问题就变成计算 BERT 向量分布的均值 $u$，协方差 $\sum$。<br><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1639019964448-76631a76-df31-4fec-a072-fdadabdc332c.webp" class=""><br>上图为步骤，把当前任务的语料，分别一句句地输入到预训练模型中得到各自的embedding，然后对embeddings做特征值分解，得到变换矩阵，然后存起来。应用时，输入新的句子，把它们输入预训练模型，得到句子embedding，再用存起来的变换矩阵u和W做变换，这时候得到的embedding就是标准正交基表示的embedding。</p><blockquote><p>怎么求解均值 $u$和协方差 $\sum$，可以参考博客看完整推导：</p></blockquote><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424112044575" class=""><p>小结：BERT-whitening 确实在 BERT-flow的基础上，更简单更直接的对原 BERT 向量进行空间分布转换，也达到 BERT-flow 差不多甚至更好的效果，加上作者的完整推导分享，思路也很清晰易懂，同时在 SVD 矩阵分解那一步，可以针对对角阵 Λ 进行一个降维操作，只保留特征值大的前 n 个维度，剔除冗余维度，这无疑在工程上应用节省了内存，也提升了性能。</p><p>上面说到，直接用BERT句向量做无监督语义相似度计算效果会很差，任意两个句子的BERT句向量的相似度都相当高，其中一个原因是向量分布的非线性和奇异性，<strong>BERT-flow通过normalizing flow将向量分布映射到规整的高斯分布上，更近一点的BERT-whitening对向量分布做了PCA降维消除冗余信</strong>息，但是标准化流的表达能力太差，而whitening操作又没法解决非线性的问题。因此研究转向了对比学习。<a href="https://www.zhihu.com/question/480187938/answer/2103245373">基于对比学习(Contrastive Learning)的文本表示模型【为什么】能学到语义【相似】度？</a>这篇文章说的挺好，梳理了对比学习应用于语义相似度任务的思路。</p><h3 id="4-2-ConSERT"><a href="#4-2-ConSERT" class="headerlink" title="4.2 ConSERT"></a><strong>4.2 ConSERT</strong></h3><blockquote><p>论文：《ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer》<br>下载链接：<a href="https://arxiv.org/abs/2105.11741">https://arxiv.org/abs/2105.11741</a><br>开源代码：<a href="https://github.com/yym6472/ConSERT">https://github.com/yym6472/ConSERT</a><br>博客地址：<a href="https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html">ACL 2021｜美团提出基于对比学习的文本表示模型，效果相比BERT-flow提升8%</a></p></blockquote><p>为解决BERT原生句子表示的<strong>坍塌问题</strong>，美团NLP中心知识图谱团队提出了基于对比学习的句子表示迁移方法——ConSERT，模型结构非常简单，基本与SimCLR相同，只是把ResNet换成了Bert，并且去掉了映射头，见下图。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424112050370" class="" title="ConSERT模型结构 ConSERT模型结构"><p>ConSERT主要包含三个部分：</p><ul><li>一个数据增强模块（详见后文），作用于Embedding层，为同一个句子生成两个不同的增强版本（View）。</li><li>一个共享的BERT编码器，为输入的句子生成句向量。</li><li>一个对比损失层，用于在一个Batch的样本中计算对比损失，采用和SimCLR一致的<strong>NT-Xent损失</strong>对模型进行Fine-tune，实验中温度超参取0.1</li></ul><p>训练时，先从数据集D中采样一个Batch的文本，设Batch size为N。通过数据增强模块，每一个样本都通过两种预设的数据增强方法生成两个版本，得到总共2N条样本。这2N条样本均会通过共享的BERT编码器进行编码，然后通过一个平均池化层，得到2N个句向量。我们采用和SimCLR一致的NT-Xent损失对模型进行Fine-tune：</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1649089505762.png" class="" title="image.png"><p>这里的sim()函数为余弦相似度函数；r表示对应的句向量；τ表示temperature，是一个超参数，实验中取0.1。该损失从直观上理解，是让Batch内的每个样本都找到其对应的另一个增强版本，而Batch内的其他2N−2个样本将充当负样本。优化的结果就是让同一个样本的两个增强版本在表示空间中具有尽可能大的一致性，同时和其他的Batch内负样本相距尽可能远。</p><p><strong>数据增强方法</strong><br><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/49a779ab260430f5e5d71c3d31385964174369.png" class="" title="四种高效的数据增强方法：Adversarial Attack、Token Shuffling、Cutoff、Dropout，均作用于Embedding层 四种高效的数据增强方法：Adversarial Attack、Token Shuffling、Cutoff、Dropout，均作用于Embedding层"></p><p>ConSERT考虑了<strong>在Embedding层隐式生成增强样本</strong>的方法，如上图所示：</p><ol><li><strong>对抗攻击（Adversarial Attack）</strong>：这一方法通过梯度反传生成对抗扰动，将该扰动加到原本的Embedding矩阵上，就能得到增强后的样本。由于生成对抗扰动需要梯度反传，因此这一数据增强方法仅适用于有监督训练的场景。</li><li><strong>打乱词序（Token Shuffling）</strong>：这一方法扰乱输入样本的词序。由于Transformer结构没有“位置”的概念，模型对Token位置的感知全靠Embedding中的Position Ids得到。因此在实现上，我们只需要将Position Ids进行Shuffle即可。</li><li><strong>裁剪（Cutoff）</strong>：又可以进一步分为两种：<ul><li>Token Cutoff：随机选取Token，将对应Token的Embedding整行置为零。</li><li>Feature Cutoff：随机选取Embedding的Feature，将选取的Feature维度整列置为零。</li></ul></li><li><strong>Dropout：Embedding</strong>中的每一个元素都以一定概率置为零，与Cutoff不同的是，该方法并没有按行或者按列的约束。</li></ol><p>除了无监督训练以外，作者还提出了几种进一步<strong>融合监督信号的策略</strong>：</p><ol><li>联合训练（joint）：在NLI数据集上，通过权重联合训练监督与无监督目标</li><li>先有监督再无监督（sup-unsup）：先使用有监督损失训练模型，再使用无监督的方法进行表示迁移。</li><li>联合训练再无监督（joint-unsup）：先使用联合损失训练模型，再使用无监督的方法进行表示迁移。</li></ol><p><strong>无监督实验结果</strong></p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1649090098784.png" class="" title="image.png"><p>在无监督实验中，我们直接基于预训练的BERT在无标注的STS数据上进行Fine-tune。结果显示，我们的方法在完全一致的设置下大幅度超过之前的SOTA—BERT-flow，达到了8%的相对性能提升。</p><p><strong>有监督实验结果</strong></p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1649090198794.png" class="" title="image.png"><p>在有监督实验中，我们额外使用了来自SNLI和MNLI的训练数据，使用上面提到的融合额外监督信号的三种方法进行了实验。实验结果显示，我们的方法在“仅使用NLI有标注数据”和“使用NLI有标注数据 + STS无标注数据”的两种实验设置下均超过了基线。在三种融合监督信号的实验设置中，我们发现joint-unsup方法取得了最好的效果。</p><p><strong>不同的数据增强方法分析</strong></p><p>就单种数据增强方法而言，Token Shuffle &gt; Token Cutoff &gt;&gt; Feature Cutoff ≈ Dropout &gt;&gt; None。温度超参τ值在0.08到0.12之间时会得到最优结果。不同Batch size下模型的表现相差不大。</p><p><a href="https://zhuanlan.zhihu.com/p/481992147">ConSERT和simCSE的区别</a></p><h3 id="4-3-SimCSE"><a href="#4-3-SimCSE" class="headerlink" title="4.3 SimCSE"></a>4.3 SimCSE</h3><blockquote><p>论文：SimCSE: Simple Contrastive Learning of Sentence Embeddings<br>论文地址：<a href="https://arxiv.org/abs/2104.08821">https://arxiv.org/abs/2104.08821</a><br>论文代码：<a href="https://github.com/princeton-nlp/SimCSE">https://github.com/princeton-nlp/SimCSE</a><br>博客讲解：①<a href="https://kexue.fm/archives/8348">中文任务还是SOTA吗？我们给SimCSE补充了一些实验</a>、 ②<a href="https://zhuanlan.zhihu.com/p/390523965">SimCSE loss理解</a>、③<a href="https://mp.weixin.qq.com/s/GpVpYGLGC-hAfC37fO5ypw">NLP与对比学习的巧妙融合，简单暴力效果显著！</a></p></blockquote><p>SimCSE 是陈丹琦组的文章，有点重剑无锋，大巧不工的意思。SimCSE 在模型结构和对比损失上，与ConSERT基本相同，只是把映射头又加上了，最大创新点是<strong>采用简单的 dropout 正例构造方法+对比学习</strong>在STS任务就达到了SOTA！</p><p>SimCSE 分为无监督与有监督版本，但基本思想上没有什么区别，只是在正负样本数据构造上不同。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/640-20230424112122792" class=""><p><strong>无监督版本</strong>上，就是将一个句子输入 encoder 两次，由于在标准的 Transformer 中，dropout mask是随机生成的，所以将同一个样本分两次输入到同一个编码器中，可以获得两次不同 dropout masks 下的句子向量 $h_i^{z_i},h_i^{z^`_i}$，$z$ 表示一次随机的 dropout mask。给定一个包含 N 个句子的 batch 输入，对于句子 i，使用 BERT 预训练模型通过随机 dropout mask 编码两次得到的相似样本对作为正例 ，batch 内除句子 i 的其他样本 embedding 作为负例，训练的目标函数为：</p><p>$l_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{i}^{z_{i}^{\prime}}\right) / \tau}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{j}^{z_{j}^{\prime}}\right) / \tau}}$</p><p>其中 $t$ 为温度超参，sim 表示余弦相似度：$h_{1}^{T} h_{2} /\left(\left|h_{1}\right| \cdot\left|h_{2}\right|\right)$。在实际训练时，根据上面的对比学习目标函数，微调 BERT/RoBERTa 的模型参数。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1645167974927-7b789405-d793-4677-a24f-1aadde8ce515.png" class="" title="image.png"><p>第一行是无监督的SimCSE，sample指的是对数据集采样训练，采样大小为134k，而full使用整个数据集作为训练集，图中的分数为斯皮尔曼相关系数 。而最后一栏中使用entailment作为正例，以contradiction作为hard neg，这也是有监督模型的最终模型。</p><p><strong>有监督版本</strong>上，因为有监督数据了，就不是重复输入一个文本两次。选取文本蕴涵任务对应的数据集，有三个标签，entailment（蕴涵），neutral（中立），contradiction（相反），</p><ol><li><p>有监督对比学习最终选择了利用文本蕴涵任务 NLI (SNLI+MNLI) 数据集，与无监督方法类似，将每一个premise和与其相对的entailment作为正样本对；负例包括两部分，第一部分是 batch 内其他样本作为负例，第二部分是 premise 对应的contradiction 作为 hard negatives。</p></li><li><p>loss 上与无监督是一致的，只是数据构造上的区别。训练目标函数为（<strong>infoNCE loss</strong>）：</p></li></ol><p>$l_{i}=-\log \frac{e^{\operatorname{sim}\left(h_{i}, h_{i}^{+}\right) / \tau}}{\sum_{j=1}^{N}\left(e^{\operatorname{sim}\left(h_{i}, h_{j}^{+}\right) / \tau}+e^{\operatorname{sim}\left(h_{i}, h_{j}^{-}\right) / \tau}\right)}$</p><ol><li>作者对比了在不同数据集上进行训练后在STS-B上的验证表现，并且实验结果表明引入hard neg能提高模型的效果， spearman 相关系数从84.9提升到了86.2。并且对于有多个contradiction的premise，作者只随机抽取了一个作为hard neg，使用多个hard neg对结果并没有提升。</li></ol><p><strong>实验结果：</strong><br>下表展示了无监督 SimCSE 和 有监督 SimCSE 模型在多个英文 STS 数据集上的效果：<img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1639040121672-e3793aa8-8112-4b22-becd-94593d2e62fc.webp" class="">可以看出，在 7 个 STS 数据集上，无监督 SimCSE 比之前 state-of-the-art 的无监督方法效果提升显著。当加入 NLI 数据集进行监督训练时，SimCSE 模型的效果进一步得到提升，SimCSE-BERT-base 的平均 spearman 相关系数达到 0.8157，SimCSE-RoBERTa-large 的平均 spearman 相关系数达到 0.8376。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1639040121760-febe9188-16d3-4ba6-bb82-a2fbfb9046de.webp" class=""><p>上图为基于 BERT-base 的不同模型的可视化，模型效果越接近左下角越好。圆点的数字代表不同模型在多个 STS 数据集上的平均 spearman 相关系数。</p><p><strong>小结：</strong>SimCSE 方法上大道至简，在解决像 BERT 这种 Transformer结构出来的 embedding 的各向异性及分布不均匀问题上，提出了一个更简单易行的方案，在有监督及无监督任务上都达到了 SOTA，证明了通过 dropout 构造的样本进行对比学习有效性可行性。</p><h3 id="4-4-ESimCSE"><a href="#4-4-ESimCSE" class="headerlink" title="4.4 ESimCSE"></a>4.4 ESimCSE</h3><blockquote><p>ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding</p></blockquote><p>ESimCSE是对上述SimCSE构建正负样本方法的改进，主要出发点如下：</p><ul><li>句子的长度信息通常会被编码，因此无监督的SimCSE中的每个正对长度是相同的。故用这些正对训练的无监督SimCSE 往往会认为长度相同或相似的句子在语义上更相似。</li><li>Momentum Contrast（动量对比）最早是在MoCo提出，是一种能够有效的扩展负例对并同时缓解内存限制的一种方法。ESimCSE借鉴了这一思想来扩展负例。</li></ul><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1646806888736-cbc9c018-198a-418b-8af3-a5ec3ab0ca10.png" class=""><ul><li>正例：作者先探究了句子对的长度差对SimCSE的影响，当长度差大于3时无监督SimCSE模型的效果大幅度降低。为了降低句子长度差异的影响，作者尝试了随机插入、随机删除和词重复三种方法构建正例，发现前两者导致语义相似度下降明显，而词重复可以保持较高的相似度，同时缓解了句子长度带来的问题。故使用word repetition进行正例构造。</li><li>负例：① in-batch negatives ② 动量更新队列中的样本</li></ul><p>损失函数如下（<strong>infoNCE loss</strong>）：</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1646806888746-879a6179-8a33-4f47-aa85-f37e8c2638ef.png" class=""><p>实验表明，ESimCSE整体效果优于无监督的SimCSE，在语义文本相似性（STS)任务上效果优于BERTbase版的SimCSE 2%。</p><h3 id="4-5-R-Drop"><a href="#4-5-R-Drop" class="headerlink" title="4.5 R-Drop"></a>4.5 R-Drop</h3><blockquote><p><a href="https://kexue.fm/archives/8496">又是Dropout两次！这次它做到了有监督任务的SOTA</a></p></blockquote><p>SimCSE通过简单的“Dropout两次”来构造正样本进行对比学习，达到了无监督语义相似度任务的全面SOTA。SimCSE 利用Dropout本身的随机性，让同一个样本通过得到的结果视为正样本对，而batch内的所有其他样本视为负样本，然后就是通过<strong>infoNCE loss</strong>来缩小正样本的距离、拉大负样本的距离了。</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1650424478191-0964b5f3-04bd-4971-b393-4bf6fd0e62c9.png" class=""><p>而 R-Drop（Regularized Dropout） 将“Dropout两次”的思想用到了有监督任务中，每个实验结果几乎都取得了明显的提升。<br><a href="https://kexue.fm/usr/uploads/2021/07/2898424596.png"><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1650424402519-027b0664-3c62-407f-ae04-3d9ca92aff30.png" class="" title="R-Drop示意图"></a></p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1650424574497-2ef6c02a-29dd-4aeb-b1d2-1b7e6ef3a7c1.png" class="" title="image.png"><p>在苏神的R-Drop具体实现中，</p><ul><li><a href="https://github.com/bojone/r-drop/blob/main/sentiment_ssl.py">sentiment_ssl.py</a>采用半监督训练。先采样一个batch的标签数据，正常CE训练，然后采样一个batch的无标签数据，KL训练，两者交替训练；</li><li><a href="https://github.com/bojone/r-drop/blob/main/iflytek.py">iflytek.py</a>是有监督训练，二者同时训练； </li></ul><h3 id="4-6-Supervised-Contrastive-Learning"><a href="#4-6-Supervised-Contrastive-Learning" class="headerlink" title="4.6 Supervised Contrastive Learning"></a>4.6 Supervised Contrastive Learning</h3><ul><li><p>文章借鉴并改进self-supervised learning的方法来解supervised 的问题。相较于传统的cross entropy损失函数提升了一个点。并且模型更加robustness和stable。</p></li><li><p>文章主要的创新点在于利用已有的label信息来将自监督的损失函数（如公式1所示）改造成支持multiple positives 和 multiple negatives（如公式2所示）。<br><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1637231836581-90bb711d-de21-4f3a-9c00-d2f48054ea36.svg" class=""> （公式1）</p><p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1637231836544-bd2c66c5-787f-4b3c-abf3-8f435c77b7cc.svg" class=""> （公式2)</p></li><li><p>作者通过梯度计算的角度说明了文中提出的loss可以更好地关注于 hard positives and negatives，从而获得更好的效果。</p></li></ul><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1637231837186.png" class="" title="Figure 1: Cross entropy, self-supervised contrastive loss and supervised contrastive loss."><ul><li>如图1所示，对每一幅图像使用两种随机的不同的augmentations，这样就有了2N幅图像作为一个batch。Supervised Contrastive的训练过程包括以下两步：<ul><li>首先，随机sample训练样本，使用文中提出的Supervised Contrastive Learning训练；</li><li>第二步，固定representation部分的参数，使用cross-entropy训练分类器部分。（如果只需要获取embedding，不需要做分类的话，不需要执行这一步。）</li></ul></li><li>实验效果</li></ul><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1637231837214.png" class="" title="image.png"><p>Table 2: Top-1/Top-5 accuracy results on ImageNet on ResNet-50 and ResNet-200 with AutoAugment being used as the augmentation for Supervised Contrastive learning.</p><ul><li>大力出奇迹的参数设置：batch size设置为8192，训练了700个epoch。作者指出每一步的训练要比cross entropy慢50%。当然，作者也说明减小 batch size和epoch不会掉太多点。</li></ul><h2 id="5-对比学习的思考和疑问"><a href="#5-对比学习的思考和疑问" class="headerlink" title="5.对比学习的思考和疑问"></a>5.对比学习的思考和疑问</h2><p><strong>0、</strong><a href="https://mp.weixin.qq.com/s/xHFsPrW6IFsCMszKn0myBA"><strong>向量化召回 vs. 对比学习</strong></a></p><ul><li>向量化召回，属于Supervised Learning，无论是U2I, U2U, I2I, <strong>哪两个向量应该是相似的（正例）是根据用户反馈（标注）得到的</strong>。<ul><li>因此，在召回算法中，正样本从来就不是问题。大家从来不为找不到正样本而发愁，反而要考虑如何严格正样本的定义，将一些用户意愿较弱的信号（i.e., 噪声）从正样本中删除出去，顺便降低一下样本量，节省训练时间。</li><li>召回的主要研究目标是负样本，如何构建easy/hard negative，降低Sample Selection Bias。</li></ul></li><li>对比学习，属于<strong>Self-Supervised Learning</strong> (<strong>SSL</strong>)的一种实现方式，产生的背景是为了解决”<strong>标注少或无标注</strong>“的问题。<ul><li>我之前说“召回是负样本的艺术”，那么CL更注重的应该是如何构建正样本。</li><li><strong>Data Augmentation是CL的核心</strong>，研究如何将一条样本经过变化，构建出与其相似的变体。</li><li>Data Augmentation在CV领域比较成熟了（翻转、旋转、缩放、裁剪、移位等）。而推荐场景下，数据由大量高维稀疏ID组成，特征之间又相互关联，如何变化才能构建出合情合理的相似正样本，仍然是一个值得研究的课题。</li></ul></li></ul><p><strong>1、对比学习 vs. 度量学习 vs. 表征学习</strong></p><ul><li><p>表征学习的目的是对复杂的原始数据化繁为简，把原始数据的无效的或者冗余的信息剔除，把有效信息进行提炼，形成特征（feature）。特征提取可以人为地手工处理，也可以借助特定的算法自动提取。Roughly Speaking， 前者为特征工程，后者为表征学习（Representation Learning）。</p></li><li><p>而度量学习 (Deep metric learning) 是把这个representation的学习看作是 distance calculation 的一部分了，因此是一种implicit representation learning。学习好的representation不是最终的目的？最终目的而是为了度量相似的物体离得近,不相似的离得远？</p><ul><li>度量学习多为二元组或三元组的形式，如常见的Siamese network和Triplet network </li><li>Hard Negative 的挖掘对度量学习的最终效果有较大的影响</li><li>metric learning提出了这么多数学上优雅的loss function</li></ul></li><li><p>对比学习属于度量学习，本质都是去拉近相似的样本，推开不相似的样本，从而学习一个具有极强表达能力的表征空间。但是<strong>对比学习是无监督或者自监督学习方法，而度量学习一般为有监督学习方法</strong>。而且对比学习在 loss 设计时，为单正例多负例的形式，因为是无监督，数据是充足的，也就可以找到无穷的负例，但如何构造有效正例才是重点。</p></li></ul><p><strong>2、对比学习中一般选择一个 batch 中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？</strong></p><p>答：加大 batch size，以降低 batch 训练中采样到伪负例的概率，减少它的影响</p><p><strong>3、infoNCE loss 如何去理解，和 CE loss有什么区别？</strong></p><p>答：<strong>infoNCE loss</strong> 全称 info Noise Contrastive Estimation loss，对于一个 batch 中的样本 i，它的 loss 为：<br><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1637214492254-c34e7473-31aa-45f7-9a0d-b481ff610ade.png" class=""><br><strong>CE loss</strong>，Cross Entropy loss：<br><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/1637214530701-9fe8cbc7-199c-45b1-8e2d-fba05bfd6479.png" class=""></p><p>看的出来，info NCE loss 和在一定条件下简化后的 CE loss 是非常相似的，但有一个区别要注意的是：infoNCE loss 中的 K 是 batch 的大小，是可变的，是第 i 个样本要和 batch 中的每个样本计算相似度，而 batch 里的每一个样本都会如此计算，因此上面公式只是样本 i 的 loss。CE loss 中的 K 是分类类别数的大小，任务确定时是不变的，i 位置对应标签为 1 的位置。</p><p><strong>4、对比学习的 infoNCE loss 中的温度常数的作用是什么？</strong></p><p>答：温度系数的作用是调节对困难样本的关注程度：<strong>越小的温度系数越关注于将本样本和最相似的困难样本分开</strong>，去得到更均匀的表示。然而困难样本往往是与本样本相似程度较高的，很多困难负样本其实是潜在的正样本，过分强迫与困难样本分开会破坏学到的潜在语义结构，因此，温度系数不能过小。</p><p>考虑两个极端情况，温度系数趋向于 0 时，对比损失退化为只关注最困难的负样本的损失函数；当温度系数趋向于无穷大时，对比损失对所有负样本都一视同仁，失去了困难样本关注的特性。</p><p><strong>5、SimCSE 中的 dropout mask 指的是什么，dropout rate 的大小影响的是什么？</strong></p><p>答：SimCSE 中的 dropout mask，对于 BERT 模型本身，是一种网络模型的随机，是对网络参数 W 的 mask，起到防止过拟合的作用。</p><p>而 SimCSE 巧妙的把它作为了一种 noise，起到数据增强的作用，因为同一句话，经过带 dropout 的模型两次，得到的句向量是不一样的，但是因为是相同的句子输入，最后句向量的语义期望是相同的，因此作为正例对，让模型去拉近它们之间的距离。</p><p><strong>6、对比学习 和 一般的预训练 如何结合起来使用？  </strong></p><p>由于对比学习是对<strong>相对空间中的向量表示</strong>，单纯地运算相对关系算力要求很高【Batch size 巨大】【SimCLR暴力美学证明可以纯算，但一般做不起】，一般作为其他模型绝对空间相对准确后的对任务的相对微调。</p><p>比如说，Bert能使空间<strong>词向量绝对空间的位置</strong>，相对准确，但是针对某些任务，它的聚类效果不够好，我们使用对比学习调整它们间的相对关系，从而适应我们的任务。</p><p><strong>7、对比学习 和 下游任务 怎么结合？</strong></p><p>两者联合训练 还是 分阶段训练？从现在的实验来看，先与下游任务进行有监督的联合训练（CL+任务），然后再在下游任务上无监督的训练（CL），效果最好。</p><h2 id="6-大厂关于对比学习实践"><a href="#6-大厂关于对比学习实践" class="headerlink" title="6.大厂关于对比学习实践"></a>6.大厂关于对比学习实践</h2><blockquote><p><a href="https://mp.weixin.qq.com/s/9N2tk6QTCTuBkrU5Xwb2ow">如何利用噪音数据：对比学习在微博场景的应用</a></p></blockquote><p>微博在自然语言处理场景中利用对比学习构建了CD-TOM模型(Contrastive Document-Topic Model)。将同一微博中的&lt;文档,话题&gt;数据对作为训练数据通过对比学习构建表示学习模型，将微博正文和话题映射到同一表示空间中，从而使得语义相近的正文和话题距离拉近，语义不相近的距离拉远。<strong>通过这种方式可以获得微博正文和话题的高质量embedding用于下游任务中</strong>。CD-TOM模型结构如下图所示：</p><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/2b818ae4f31d8e9ab7c9ff3a81d3fabd.png" class=""><p><a href="https://mp.weixin.qq.com/s/UDG5z4lcOiRquRN0H6ELCQ"><strong>用对比学习解释经验做法</strong></a>：（张俊林总结的非常好）</p><p>做召回有三个做法：in-batch负例，要带温度超参，embedding要做Norm。因为是经验做法，现在并没有人给出解释，为什么要这么做。如果把目前的召回模型看做是对比学习的一种变体，你就可以用对比学习的理论来解释这三种做法背后的道理。</p><ul><li><strong>① 双塔模型随机负例的作用是什么</strong></li><li><strong>② 温度超参的作用是什么</strong></li><li><strong>③ Embedding为什么要做Norm</strong></li></ul><h2 id="7-对比学习个人实践"><a href="#7-对比学习个人实践" class="headerlink" title="7.对比学习个人实践"></a>7.对比学习个人实践</h2><img src="/2021/11/09/2021-11-09-%F0%9F%86%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E4%B8%AA%E4%BA%BA%E5%AE%9E%E8%B7%B5.png" class=""><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文粗略的介绍了一下对比学习的思想与方法，以及一系列在图像、文本上的工作。对比学习在语义表示学习上的能力有目共睹，但还有一些问题，比如在自然语言领域，复杂的数据增强方式反而不如简单的dropout，这有没有可能是由于自然语言对扰动语义鲁棒性不强导致的呢？同时，CV领域的对比学习已经可以不用负例，那在NLP领域是不是也会有相似的发展轨迹呢？让我们拭目以待！</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mp.weixin.qq.com/s/c09FUv9UBcj0W8jfkPn6xA">赛尔笔记 | 对比学习</a>√<br><a href="https://zhuanlan.zhihu.com/p/367290573">对比学习（Contrastive Learning）:研究进展精要 | 张俊林</a><br><a href="https://mp.weixin.qq.com/s/UDG5z4lcOiRquRN0H6ELCQ">张俊林：从对比学习视角，重新审视推荐系统的召回粗排模型</a>√<br><a href="https://mp.weixin.qq.com/s/QMGXFt5CdT9_vdwuvBde_g">对比学习（Contrastive Learning）综述</a><br><a href="https://www.cnblogs.com/kailugaji/p/14488034.html">从对比学习(Contrastive Learning)到对比聚类(Contrastive Clustering)</a><br><a href="https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html">ACL 2021｜美团提出基于对比学习的文本表示模型，效果相比BERT-flow提升8%</a>√<br><a href="https://mp.weixin.qq.com/s/93y1wyv76KHn_fzERR7NHQ">进入BERT时代，向量语义检索我们关注什么-丁香园</a><br><a href="https://mp.weixin.qq.com/s/SV7nytaOpVfz-7qyhL78ww">文本表达进击：从BERT-flow到BERT-whitening、SimCSE</a>√<br>有关对比学习的综述，看看这个挺好的：<a href="https://zhuanlan.zhihu.com/p/346686467">https://zhuanlan.zhihu.com/p/346686467</a><br>而有关NLP的对比学习，可以看看这篇文章：<a href="https://zhuanlan.zhihu.com/p/334732028">https://zhuanlan.zhihu.com/p/334732028</a><br>美团的ConSert：<a href="https://mp.weixin.qq.com/s/C4KaIXO9Lp8tlqhS3b0VCw">https://mp.weixin.qq.com/s/C4KaIXO9Lp8tlqhS3b0VCw</a><br><a href="https://mp.weixin.qq.com/s/9iHZqWGjJLz7Sw7JSnpmWQ">2021最新对比学习（Contrastive Learning）在各大顶会上的经典必读论文解读</a><br><a href="https://mp.weixin.qq.com/s/mX12zl5KTmcZDHPlVl8NZg">基于对比学习(Contrastive Learning)的文本表示模型为什么能学到语义相似度？</a><br><a href="https://mp.weixin.qq.com/s/0tSR1s64-LRGWdtPy5x_Yw">业界总结 | 如何改进双塔模型，才能更好的提升你的算法效果？</a><br><a href="https://zhuanlan.zhihu.com/p/367290573">对比学习（Contrastive Learning）:研究进展精要 | 张俊林</a><br><a href="https://zhuanlan.zhihu.com/p/370782081 NLP 语义匹配：经典前沿方案整理">利用Contrastive Learning对抗数据噪声：对比学习在微博场景的实践 | 张俊林</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247538505&amp;idx=3&amp;sn=b8257395c4b50e47f01d78696172cf85&amp;chksm=ebb76f9ddcc0e68b24e08aba4ed59a16e8ce95ca289fc9cb74a5e92124db16bfc249d868a22e#rd">谈一谈对比学习-哈工大 SCIR-介绍了一下对比学习的思想与方法，以及一系列在图像、文本上的工作</a><br><a href="https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html">🍑Contrastive Representation Learning-Lil’Log</a><br><a href="https://mp.weixin.qq.com/s/LyjZ8AVQk9INOEhAKif39A">对比学习在NLP和多模态领域的应用</a></p>]]></content>
    
    
    <categories>
      
      <category>度量学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>对比学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>     语义文本相似度方案</title>
    <link href="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/"/>
    <url>/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<p>文本匹配是自然语言理解中的一个核心问题，它可以应用于大量的自然语言处理任务中，例如信息检索、问答系统、复述问题、对话系统、机器翻译等等。这些自然语言处理任务在很大程度上都可以抽象成文本匹配问题，比如<strong>信息检索可以归结为搜索词和文档资源的匹配，问答系统可以归结为问题和候选答案的匹配，复述问题可以归结为两个同义句的匹配，对话系统可以归结为前一句对话和回复的匹配，机器翻译则可以归结为两种语言的匹配</strong>。</p><p>本文简要介绍语义文本相似度计算的最新研究进展, 主要包括基于字符串、基于统计、基于深度学习的语义相似度计算方法。</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1662531637155-47f14b20-6370-4236-b3b1-44b5e39af2e1.png" class="" title="文本相似模型概览.drawio.png"><h2 id="1-基于字符串"><a href="#1-基于字符串" class="headerlink" title="1.基于字符串"></a>1.基于字符串</h2><p>基于字符串的方法都是直接对原始文本进行比较, 主要包括编辑距离[11] (Levenshtein Distance, LD)、最长公共子序列[12] (Longest Common Sequence, LCS)、N-Gram[13] 和 Jaccard 相似度[14] 等. 具体可参考《2021-05-26-常见距离度量方法》</p><p>基于字符串的方法原理简单、实现方便, 并且直接对原始文本进行比较, 多用于文本的快速模糊 匹配, 其不足主要在于没有考虑到单词的含义及单词和单词之间的相互关系, 并且同义词、多义词等问题都无法处理.</p><h2 id="2-基于统计"><a href="#2-基于统计" class="headerlink" title="2.基于统计"></a>2.基于统计</h2><p>基于统计的方法源于一种分布假设, 该假设认为上下文相似的单词具有相似的语义, 这类计算方 法先通过某种策略将文本转换成一个向量, 然后通过各种向量空间的转换, 最后计算表征文本的向量间距离, 通过向量空间中的度量来衡量文本间的相似度.</p><p>主流的基于统计的方法包括向量空间模型[17] (Vector Space Model, VSM) 和主题模型 (Topic Model), 而主题模型又可分为潜在语义分析模型[18] (Latent Semantic Analysis, LSA)、概率潜在语义分析模型[19] (Probabilistic Latent Semantic Analysis, PLSA) 和隐含狄利克雷分布模型[20] (Latent Dirichlet Allocation, LDA) 等.</p><h3 id="2-1-基于向量空间模型"><a href="#2-1-基于向量空间模型" class="headerlink" title="2.1 基于向量空间模型"></a>2.1 基于向量空间模型</h3><p>Salton 等[17] 在 1975 年首次提出向量空间模型 (VSM), 主要思想就是假设一个文本的语义只与该文本中的单词有关, 而忽略其语序和单词之间的相互关系, 然后通过<strong>基于词频统计</strong>的方法, 将文本映射成向量, 最后通过向量间的距离计算以表征文本间的相似度.</p><h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>目前 VSM 中最常用的是基于 TF-IDF 的权重计算法（详细介绍可参考<a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html">TF-IDF与余弦相似性的应用（二）：找出相似文章</a>）, TF-IDF的分数代表了词语在当前文档和整个语料库中的相对重要性。TF-IDF 分数由两部分组成：第一部分是<strong>词语频率</strong>（Term Frequency），第二部分是<strong>逆文档频率</strong>（Inverse Document Frequency），计算方式如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">TF</span><span class="hljs-params">(t)</span></span>= 该词语在当前文档出现的次数 / 当前文档中词语的总数<br><span class="hljs-function"><span class="hljs-title">IDF</span><span class="hljs-params">(t)</span></span>= log_e（文档总数 / 出现该词语的文档总数）+σ<br>平滑版 <span class="hljs-built_in">IDF</span>(t)= log_e（文档总数+<span class="hljs-number">1</span> / 出现该词语的文档总数+<span class="hljs-number">1</span>）+σ<br>TF-IDF=TF*IDF<br></code></pre></td></tr></table></figure><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1660292604860-10c0729a-b8da-4d03-b34b-b2c4c29fac6e.png" class="" title="image.png"><p>在利用 TF-IDF 权重计算法计算出各个特征项的权重之后, 就得到了可以表征文本的向量, 接下来只要计算向量之间的距离即可, 一般来说, 距离越近则两文本越相似.</p><p>TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。</p><ul><li>代码示例如下（借助<code>Sklearn</code>库）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br>texts = [<br>    <span class="hljs-string">&#x27;今天 天气 很好 ， 出去 玩&#x27;</span>,<br>    <span class="hljs-string">&#x27;我 来到 北京 清华大学&#x27;</span>,<br>    <span class="hljs-string">&quot;小明 硕士 毕业 与 中国 科学院&quot;</span><br>]<span class="hljs-comment">#提前分好词</span><br><br><span class="hljs-comment"># word level tf-idf</span><br>vectorizer = TfidfVectorizer(analyzer=<span class="hljs-string">&#x27;word&#x27;</span>,use_idf=<span class="hljs-literal">True</span>, smooth_idf=<span class="hljs-literal">True</span>, token_pattern=<span class="hljs-string">r&#x27;\w&#123;1,&#125;&#x27;</span>)<br>vec = vectorizer.fit_transform(texts)<span class="hljs-comment">#训练&amp;#将文本转为词频矩阵</span><br>xtest_vec = vectorizer.transform(xtest)<span class="hljs-comment"># 仅向量转换：直接用训练集学习到的idf去计算测试集里面每一个tf-idf</span><br><br><span class="hljs-built_in">print</span>(vectorizer.vocabulary_)<span class="hljs-comment">#&#123;word:idx&#125;</span><br><span class="hljs-built_in">print</span>(vectorizer.get_feature_names())<span class="hljs-comment">#获取词袋模型中的所有词语</span><br><span class="hljs-built_in">print</span>(idx_w=vectorizer.vocabulary_.get(<span class="hljs-string">&quot;今天&quot;</span>))<span class="hljs-comment">#获取&quot;今天&quot;在词表中的索引</span><br><span class="hljs-built_in">print</span>(vectorizer.idf_[idx_w] <span class="hljs-keyword">if</span> idx_w <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>)<span class="hljs-comment">#根据词索引，获取对应的 idf 值</span><br><span class="hljs-built_in">print</span>(vec.toarray())<span class="hljs-comment">#显示词频矩阵</span><br><br>sns.heatmap(vec.toarray(), annot=<span class="hljs-literal">True</span>, cbar = <span class="hljs-literal">False</span>, xticklabels = vocab)<br></code></pre></td></tr></table></figure><h3 id="2-2-基于主题模型"><a href="#2-2-基于主题模型" class="headerlink" title="2.2 基于主题模型"></a>2.2 基于主题模型</h3><p>主题模型的基本假设是每个文档包含 多个主题, 而每个主题又包含多个单词. 换句话说, 文档的语义由一些隐变量表示, 这里的隐变量是指主题, 而这些主题代表了文档的语义信息. 而主题模型的目的就是揭示这些隐变量和它们之间的相互关系. 主题模型主要包括: ① 潜在语义分析 (LSA) 模型; ② 概率潜在语义分析 (PLSA) 模型; ③ 潜在狄 利克雷分布 (LDA) 模型.</p><p><strong>潜在语义分析（LSA, Latent Semantic Analysis）</strong><a href="https://leovan.me/cn/2020/10/text-similarity/#fn:deerwester1990indexing">5</a> 的核心思想是将文本的高维词空间映射到一个低维的向量空间，我们称之为隐含语义空间。降维可以通过<a href="https://leovan.me/cn/2017/12/evd-svd-and-pca/">奇异值分解（SVD）</a>实现.</p><h3 id="2-3-基于概率模型"><a href="#2-3-基于概率模型" class="headerlink" title="2.3 基于概率模型"></a>2.3 基于概率模型</h3><h4 id="BM25"><a href="#BM25" class="headerlink" title="BM25"></a>BM25</h4><blockquote><p><a href="https://www.pinecone.io/learn/semantic-search/">https://www.pinecone.io/learn/semantic-search/</a></p></blockquote><p>BM25 算法的全称为 Okapi BM25，是一种搜索引擎用于评估查询和文档之间相关程度的排序算法，其中 BM 是 Best Match 的缩写。 <strong>Okapi BM25 是 TF-IDF 的优化版，主要是根据文档长度对结果进行归一化。</strong></p><p>首先我们来看一下bm25算法的计算公式：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1654770741231-89f9f602-7eb2-4035-ad36-69414047319d.png" class="" title="image.png"><p>这是一个看起来很讨厌的方程——但它只不过是我们的 TF-IDF 公式带了一些新参数！让我们比较两个 TF 部分：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1654770794149-37a4b119-8a4c-441d-ac9b-76369376e104.png" class="" title="image.png"><p>BM25 算法的一个重要的设计思想是，<strong>它认为词频和相关性的关系并不是线性的</strong>。也就是说，随着词频的增加，相关性的增加会越来越不明显，并且还会有一个阈值上限。当词频达到阈值以后，那相关性就不会再增长了。因此，BM25 对于 TF 的使用，设立了一个词项在文档中的权重计算公式如下：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1660293788206-8d19f848-ba85-461e-b0c7-468c546c9cf6.png" class="" title="image.png"><p>在这个公式中，随着 tf 的值逐步变大，权重会趋向于 k1 + 1 这个固定的阈值上限（将公式的分子分母同时除以 tf，就能看出这个上限）。其中，k1 是可以人工调整的参数。k1 越大，权重上限越大，收敛速度越慢，表示 tf 越重要。在极端情况下，也就是当 k1 = 0 时，就表示 tf 不重要。</p><p>$权重=1$</p><p>除了考虑词频，<strong>BM25 算法还考虑了文档长度的影响</strong>，也就是同样一个词项，_如果在两篇文档中出现了相同的次数，但一篇文档比较长，而另一篇文档比较短，那一般来说，短文档中这个词项会更重要。_这个时候，我们需要在上面的公式中，加入文档长度相关的因子。那么，整个公式就会被改造成如下的样子：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1660295364438-e6ec07d3-2e44-4760-8e0e-54a7dde52301.png" class="" title="image.png"><p>你会看到，分母中的 k1 部分被乘上了文档长度的权重。其中，l 表示当前文档的长度，而 L 表示全部文档的平均长度。l 越长，分母中的 k1 就会越大，整体的相关性权重就会越小。b是人工调整的参数，它的取值范围是 0 到 1，代表了文档长度的重要性，一般设置为 0.75。</p><p>除此以外，如果查询词比较复杂，比如说一个词项会重复出现，那我们也可以把它看作是一个短文档，用类似的方法计算词项在查询词中的权重。举个例子，如果我们的查询词是“极客们的极客时间课程”，那么“极客”这个词项，其实在查询词中就出现了两次，它的权重应该比“时间”“课程”这些只出现一次的词项更重要。因此，BM25 对词项在查询词中的权重计算公式如下：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1660300150186-f18b64ee-ebad-4507-9abb-1a6d69131712.png" class="" title="image.png"><p>其中 tfq 表示词项在查询词 q 中的词频，而 k2 是可以人工调整的参数，它和 k1 的参数作用是类似的。由于查询词一般不会太长，所以词频也不会很大，因此，我们没必要像对待文档一下，用 k1 = 1.2 这么小的范围对它进行控制。我们可以放大词频的作用，把 k2 设置在 0~10 之间。极端情况下，也就是当 k2 取 0 时，表示我们可以完全不考虑查询词中的词项权重。</p><p>然后是 IDF 部分，它甚至没有引入任何新参数——它只是从 TF-IDF 重新排列我们的旧 IDF。为了避免对数里面分母项等于0，我们给分子分母同时加上0.5，这个0.5被称作平滑系数。</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1654770794622-2164f4f4-81d3-43e0-93d3-3025eefd176e.png" class="" title="image.png"><p>这样，我们就得到了 BM25 算法计算一个词项和指定文档相关性的打分公式，公式如下：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1660300245507-88638bb1-cbd2-4095-a502-0f6fed8912e2.png" class="" title="image.png"><p>如果一个查询词 q 中有多个词项 t，那我们就要把每一个词项 t 和文档 d 的相关性都计算出来，最后累加。这样，我们就得到了这个查询词 q 和文档 d 的相关性打分结果。</p><p>现在，这个修改的结果是什么？如果我们取一个包含 12 个标记的序列，并逐渐向它提供越来越多的“匹配”标记——我们会产生以下分数：</p><div class="table-container"><table><thead><tr><th><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1654770787813-f9e61573-cca3-4240-83be-6d16e91f6c0f.png" class="" title="image.png"></th><th><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1654770789119-19a31114-19a3-4987-abe0-c0c51be7f1a0.png" class="" title="image.png"></th></tr></thead><tbody><tr><td></td></tr></tbody></table></div><p>TF-IDF（顶部）和 BM25（底部）算法使用 12 个标记的句子和递增数量的相关标记（x 轴）的比较。<br>BM25 算法的效果比 TF-IDF 更好，应用也更广泛。比如说，在 Lucene 和 Elastic Search 这些搜索框架，以及 Google 这类常见的搜索引擎中，就都支持 BM25 排序。不过要用好它，你需要结合我们今天讲的内容，更清楚地理解它的原理。这样才能根据不同的场景，去调整相应的参数，从而取得更好的效果。</p><p>来整体看一下bm25计算公式的推导过程：</p><p>$\begin{aligned}<br>\operatorname{Score}(Q, d)=\sum_{i}^{n} W_{i} \cdot R\left(q_{i}, d\right) \\=\sum_{i}^{n} W_{i} \cdot \frac{f_{i}\left(k_{1}+1\right)}{f_{i}+K} \\<br>=\sum_{i}^{n} \operatorname{IDF}\left(q_{i}\right) \cdot \frac{f_{i}\left(k_{i}+1\right)}{f_{i}+k_{1} \cdot\left(1-b+b \frac{d l}{\operatorname{avg}(d l)}\right)} \\=\sum_{i}\left(\log \frac{N+0.5}{n\left(q_{i}\right)+0.5}\right) \cdot \frac{f_{i}\left(k_{i}+1\right)}{f_{i}+k_{1} \cdot\left(1-b+b \frac{d l}{\operatorname{avg}(d l)}\right)}<br>\end{aligned}$</p><p>在我们了解了bm25算法的原理了之后，我们再一起学习一下用Python来实现它，参考<a href="https://gist.github.com/koreyou/f3a8a0470d32aa56b32f198f49a9f2b8">使用sklearn的TfidfVectorizer实现OKapi BM25</a></p><ul><li>个人代码实现</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sim_bm25</span>(<span class="hljs-params">s1, s2, s_avg=<span class="hljs-number">6.05</span>, k1=<span class="hljs-number">2.0</span>, b=<span class="hljs-number">0.75</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;BM25算法计算文本相似度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">score_w</span>(<span class="hljs-params">idf_w, fid, len_d</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;计算query中某词与候选文档的相关度得分&quot;&quot;&quot;</span><br>        ra = fid * (k1 + <span class="hljs-number">1</span>)<br>        rb = fid + k1 * (<span class="hljs-number">1</span> - b + b * len_d / s_avg)<br>        <span class="hljs-keyword">return</span> idf_w * (ra / rb)<br>    sim = <span class="hljs-number">0</span><br>    len_d = <span class="hljs-built_in">len</span>(s2)<br>    <span class="hljs-comment"># 1. 分词</span><br>    s1_list, s2_list = jieba.lcut(s1), jieba.lcut(s2)<br>    <span class="hljs-comment"># 2. 计算s1中每个词与s2的相关度得分并累加得s1与s2的相关度</span><br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> s1_list:<br>        idf_w = idf.get(w, <span class="hljs-number">1</span>)<br>        fid = s2_list.count(w)<br>        score = score_w(idf_w, fid, len_d)<br>        sim += score<br>    <span class="hljs-keyword">return</span> sim<br></code></pre></td></tr></table></figure><p>如果想更深入的了解BM25的理论基础，推荐阅读<a href="https://leovan.me/cn/2020/10/text-similarity/；想要了解基本实现代码，可以参考">https://leovan.me/cn/2020/10/text-similarity/；想要了解基本实现代码，可以参考</a> <a href="https://gist.github.com/koreyou/f3a8a0470d32aa56b32f198f49a9f2b8">Implementation of OKapi BM25 with sklearn’s TfidfVectorizer</a></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>传统的文本匹配技术主要有Jaccard、Levenshtein、Simhash、TF-idf、Bm25、VSM等算法，其主要是基于统计学方法通过词汇重合度来计算两段文本的字面相似度。然而，仅通过字面相似度是衡量文本的匹配度是远远不够的，因为同一语义的文本在形式上千变万化，两段文本可以表现为字面相似但词序不同而导致语义完全相反；可以表现为字面相似但个别字词不同而导致意思大相径庭；更可以表现为字面完全不相似而语义相同等问题。所以，传统的匹配算法存在着词义局限、结构局限等问题。</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1655793230594-23183af8-1889-4368-a24e-20c28f7a4e8b.png" class="" title="image.png"><h2 id="3-基于深度学习"><a href="#3-基于深度学习" class="headerlink" title="3.基于深度学习"></a>3.基于深度学习</h2><h3 id="3-1-分布式词向量"><a href="#3-1-分布式词向量" class="headerlink" title="3.1　分布式词向量"></a>3.1　分布式词向量</h3><p>简单地说, 词向量技术就是将单词映射成向量, 最早出现的 one-hot 编码和 TF-IDF 方法都可以 将单词映射为向量. 但是, 这两种方法都面临<strong>维度灾难</strong>和<strong>语义鸿沟</strong>问题. 分布式词向量可以在保存更多语义信息的同时降低向量维度, 在一定程度上可以克服维度灾难和语义鸿沟问题.</p><p>Mikolov 等[55] 提出的 <strong>word2vec</strong> 是最早生成分布式词向量的方法, 它包含两个模型, 分别为 CBOW 和 Skip-gram, 基本思路是确定中心词和上下文窗口大小, CBOW 是通过上下文来预测中心词, Skipgram 是通过中心词来预测上下文, 整体来说是通过自监督训练的模型生成词向量. word2vec 的主要 问题在于它只能考虑局部信息, 局部信息的大小取决于上下文窗口的大小.</p><p>Pennington 等[56] 提出 <strong>Glove</strong> 模型, 该模型通过语料库构建单词的共现矩阵, 然后通过该共现矩阵 用概率的思想得到最终的词向量, 综合了全局语料, 在一定程度上考虑了全局信息.</p><p>Joulin 等[57] 则是提出了一种快速文本分类方法 <strong>FastText</strong>, 其同样可以用于生成词向量, 模型架构 与 CBOW 类似, 但赋予模型的任务不同. CBOW预测上下文的中间词，fasttext预测文本标签。与word2vec算法的衍生物相同，稠密词向量也是在训练神经网络的过程中得到的。fasttext的<a href="https://link.zhihu.com/?target=https%3A//github.com/salestock/fastText.py">python实现</a></p><p>以上 3 种词向量为静态词向量, 即当它们应用于下游任务时词向量始终是固定的, 无法解决一词 多义问题, 同时无法满足不同语境下语义不同的场景需求.</p><p>动态词向量则是为了解决上述问题所提出的, 这类词向量首先在大型语料库上进行预训练, 然后在面对具体下游任务时微调所有参数, 那么在上下文输入不同时, 生成的词向量也不同, 因此可以解 决一词多义问题.</p><p>Peters 等[58] 提出 <strong>ELMO</strong> 模型, 其使用双向语言模型和两个分开的双层 LSTM 作为编码器, 通过 在大型语料库上进行预训练得到动态词向量.</p><p>Radford 等[59] 提出 <strong>GPT</strong> 模型, 通过将单向语言模型与编码能力更强大的 Transformer[60] 架构的 decoder 部分相结合, 从而生成词向量.</p><p>Devlin 等[61] 提出的 <strong>BERT</strong> 模型, 则是应用 Transformer[60] 的 encoder 部分, 结合 mask 机制, 并且对模型增加了预测“next sentence prediction”任务, 从而生成了更加优质的动态词向量, 该模型也是目前最常用的词向量预训练方法之一. 要是想使用BERT作为服务，具体实现过程可以参考博客<a href="">《<strong>bert_as_service使用指南</strong>》</a></p><h3 id="3-2-无监督文本匹配"><a href="#3-2-无监督文本匹配" class="headerlink" title="3.2　无监督文本匹配"></a>3.2　无监督文本匹配</h3><blockquote><p>参考 《<a href="https://mp.weixin.qq.com/s/dIO0imsB1VdKobf5RWLnaw">五千字全面梳理文本匹配</a>》</p></blockquote><p>首先我们来谈一下无监督的方法。</p><p>对于无监督的文本匹配，我们需要实时把握两个重点：文本表征和相似函数的度量。文本表征指的是我们将文本表示为计算机可以处理的形式，更准确了来说是数字化文本。而这个数字化文本，必须能够表征文本信息，这样才说的通。相似函数的度量就是你选择何种函数对文本相似度进行一个判定，比如欧氏距离，余弦距离，Jacard相似度，海明距离等等。</p><h4 id="WMD-Word-Mover’s-Distance"><a href="#WMD-Word-Mover’s-Distance" class="headerlink" title="WMD (Word Mover’s Distance)"></a>WMD (Word Mover’s Distance)</h4><blockquote><p><a href="https://medium.com/@nihitextra/word-movers-distance-for-text-similarity-7492aeca71b0">https://medium.com/@nihitextra/word-movers-distance-for-text-similarity-7492aeca71b0</a></p></blockquote><p>WMD Word Mover’s Distance的缩写，翻译为词移距离，WMD距离越大相似度越小，WMD距离越小文本相似度越大。理解这个概念，分为两个步骤。</p><p>首先第一步，有两个文档A和B，A中的每个词遍历和B中每个词的距离，挑出A中每个词和B中每个词中最小的距离，最后相加，得到A到B的WMD。</p><p>这个时候，需要明白第一步是存在巨大问题的。什么问题呢？A，B，C三个文档。A全部词和音乐相关。B中一个词和音乐相关，C中一个词和音乐相关，其余词和音乐有点关系，而且定义B和C中和音乐相关的词是同一个词。</p><p>根据我们的直觉，A和B相关性肯定小于A和C的相关性，但是如果按照第一步的算法去做，会得到相关性相等的结果，这是不对的。</p><p>这是因为A中的所有词匹配到的B中的那个和音乐相关的词（遍历取最小），A中所有的词匹配到C中和音乐相关的词（遍历取最小），B和C中和音乐相关的词一样，就导致相关性相等。</p><p>怎么解决这个问题，这就是第二步，让A中的所有词在匹配的时候，不是遍历取最小，而是让每个词匹配到C中的所有词，只不过匹配的权重不同。</p><p>这个权重从两个部分去看：一个是从A看，要符合分配出去的权重等于自身这个词在本文档的权重，一个是从B看，分配到B的某个词的权重要等于这个词在B文档的权重。</p><p>大概理解到这里。之后关于WMD加速（因为复杂度比较高）的内容就没有进一步的去了解</p><h4 id="SIF-Smooth-Inverse-Frequency"><a href="#SIF-Smooth-Inverse-Frequency" class="headerlink" title="SIF (Smooth Inverse Frequency)"></a>SIF (Smooth Inverse Frequency)</h4><p>从语义上来讲，求一句话中词嵌入的平均值似乎给与不相关的单词太多权重了。而Smooth Inverse Frequency试着用两种方法解决这一问题：</p><ul><li>加权：就像上文用的TF-IDF，SIF取句中词嵌入的平均权重。每个词嵌入都由a/(a + p(w))进行加权，其中a的值经常被设置为0.01，而p(w)是词语在语料中预计出现的频率。用大白话去描述这个公式就是词在语料中出现的越多，对应的权重越小。这么做就减少了高频词的作用，其实本质上和IDF是很类似的。</li><li>常见元素删除：接下来，SIF计算了句子嵌入中最重要的元素。然后它减去这些句子嵌入中的主要成分。这就可以删除与频率和句法有关的变量，他们和语义的联系不大。比如原始句子是”我爱吃红烧肉“和”我超不爱吃红烧肉“。减去共有信息之后，剩下的词向量矩阵就会对不相似的信息比较敏感。</li></ul><p>最后，SIF使一些不重要的词语的权重下降，例如but、just等，同时保留对语义贡献较大的信息。总体来说。SIF这个算法还是很不错，很适合做个基线初期上线的。</p><p>代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">sentences = [[<span class="hljs-string">&quot;token1&quot;</span>, <span class="hljs-string">&quot;token2&quot;</span>, <span class="hljs-string">&quot;...&quot;</span>], ...]<br>vector = [[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [...]], ...]<br><span class="hljs-keyword">from</span> sim.sif_usif <span class="hljs-keyword">import</span> SIF<br><span class="hljs-keyword">from</span> sim.sif_usif <span class="hljs-keyword">import</span> uSIF<br><br>sif = SIF(n_components=<span class="hljs-number">5</span>, component_type=<span class="hljs-string">&quot;svd&quot;</span>)<br>sif.fit(tokens_list=sentences, vector_list=vector)<br><br>usif = uSIF(n_components=<span class="hljs-number">5</span>, n=<span class="hljs-number">1</span>, component_type=<span class="hljs-string">&quot;svd&quot;</span>)<br>usif.fit(tokens_list=sentences, vector_list=vector)<br></code></pre></td></tr></table></figure></p><h3 id="3-3-有监督文本匹配"><a href="#3-3-有监督文本匹配" class="headerlink" title="3.3　有监督文本匹配"></a>3.3　有监督文本匹配</h3><p>监督学习方法是一类需要带有标签的训练集对模型进行训练后才能进行语义文本相似度计算的 方法, 这类方法由于有标签对模型进行指导, 在多数有训练集的任务上, 其性能要优于无监督学习方 法. 监督学习方法从模型架构上可以分为两种, 一种是<strong>孪生网络</strong> (Siamese Network) 架构; 另一种是<strong>交互 (interaction-based) 模型</strong>架构, 两种模型的典型架构如图所示.</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/008i3skNly1guktjoweehj61740o20vv02.jpg" class=""><ol><li><strong>孪生网络架构（</strong>Simese dual encoder，SDE<strong>）</strong>主要体现在句子对中$S_A$和$S_B$同时输入左右两个网络当中, 这两个网络的输入层和编码层模型架构完全相同并共享权重，通过余弦相似度等方式来计算query间的相似度，不断最大化正样本之间的相关性，抑制负样本之间的相关性；此外，还有一种伪孪生网络，也叫非对称双塔结构（Asymmetric dual encoder，ADE）。ADE只是部分参数共享或者完全不共享，是独立的两套参数网络。<ul><li>它们的共同点是都不会进行深层交互！</li><li>ADE在多个任务上的表现一致地明显逊色于SDE，但是ADE-SPL的表现可以媲美SDE。SPL 指的是shared projection layer，即当最后的顶层参数共用一套全连接层的时候， ADE可以取得和SDE接近的效果（参考自《<a href="https://mp.weixin.qq.com/s/MF3NVyLBh0xIVCEMltzBgw">双塔模型的最强出装，谷歌又开始玩起“老古董”了？</a>》）</li><li>双塔结构一个最典型的应用是召回or粗排，对计算速度要求严格的场景。</li></ul></li><li><strong>交互模型</strong>如图 2b) 和图 2c) 所示, 整体与孪生网络类似, 但在编码层利用 Attention 机制或其他技术增加两个网络间的交互，交互式模型通常使用二分类的任务来进行训练，当模型输入的两个query语义一致，label为“1”，反之，label为“0”。在预测时，可通过logits来作为置信度判断。 <ul><li>典型应用是精排，要求是准确性！</li><li></li></ul></li></ol><p>两种框架比较的话，</p><ul><li>基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量速度会快很多；缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。 </li><li>基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。 </li><li><strong>语义表征模型常用于海量query召回，交互式模型更多使用于语义排序阶段</strong> </li></ul><h4 id="3-3-1-基于孪生网络的方法"><a href="#3-3-1-基于孪生网络的方法" class="headerlink" title="3.3.1 基于孪生网络的方法"></a>3.3.1 基于孪生网络的方法</h4><p>它的优势是结构简单、解释性强，且易于实现，是深度学习出现之后应用最广泛的深度文本匹配方法。整个学习过程可以分为两步：① 表示层：计算 query 和 doc 各自的 representation；② 匹配层：根据上一步得到的 representation，计算 query-doc 的匹配分数。</p><h5 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h5><blockquote><p>DSSM第一篇深度学习领域文本匹配文章<br>Learning Deep Structured Semantic Models for Web Search usin0g Clickthrough Data</p></blockquote><p>DSSM 算法是最先将孪生网络架构用于语义文本相似度计算的算法之一, DSSM 架构主要分为输入层、表示层、匹配层, 这种三层架构也是后来基于孪生网络的算法最常用的 架构. 输入层主要将原始文本映射为向量, 由于本文在 2013 年被提出, 当时的分布式词向量刚刚问世, 因此本文并没有使用词向量, 而是使用字符级的 <strong>trigram</strong> 方法将文本映射为高维向量, 更具体地说, trigram 是将 3 个字符为一组映射为 one-hot 向量, 最后将句子中的 trigram 向量相加得到高维句子向 量表示. 表示层是将高维的句子向量映射为低维向量, DSSM 表示层就是简单的几个全连接层, 最终 将句子映射为 128 维的低维向量. 匹配层是将两个低维句子向量表示之间求<strong>余弦相似度</strong>来表征两个句子的语义相似度.  DSSM模型在文本匹配任务上取得了突出的成绩, 但忽略了语序信息和上下文信息.</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/v2-774b04defa9ab870c7e8d9fad6cde190_1440w.jpg" class=""><h5 id="ARC-I"><a href="#ARC-I" class="headerlink" title="ARC-I"></a>ARC-I</h5><p>针对上述讲到的 DSSM 模型对 query 和 doc 序列和上下文信息捕捉能力的不足，华为诺亚方舟在 2015 年在 DSSM 的模型基础上加入了 <strong>CNN 模块</strong>，通过卷积层不同的 feature map 来得到相邻 term 之间的多种组合关系，通过池化层 max pooling 来提取这些组合关系中最重要的部分，进而得到 query 和 doc 各自的表示。卷积层虽然提取到了 word-ngram 的信息，但是池化层依然是在局部窗口进行 pooling，因此一定程度上无法得到全局的信息。</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/image-5a59670f2113499d95fc165ca56b7065-20230424200729876.webp" class=""><h5 id="Siam-CNN"><a href="#Siam-CNN" class="headerlink" title="Siam-CNN"></a><strong>Siam-CNN</strong></h5><blockquote><p>题目：Applying Deep Learning to Answer Selection: A Study and An Open Task<br>论文：<a href="https://arxiv.org/pdf/1508.01585.pdf">https://arxiv.org/pdf/1508.01585.pdf</a></p></blockquote><p>在2015年IBM提出的Siam-CNN架构中，作者尝试了<strong>多种孪生架构</strong>，使用CNN作为基础编码器，hinge loss作为损失函数. 该方法对DSSM的改进主要发生在表示层, 该模型表示层中添加了卷积层和池化层, 使得上下文信息得到了有效保留, 但是由于卷积核的限制, 距离较远的上下文信息仍会丢失. 最后实验发现第二种是最好的.</p><p>PS： During training, for each training question Q there is a positive answer A +(the ground truth). A training instance is constructed by pairing this A + with a negative answer A −(a wrong answer) sampled from the whole answer space</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/v2-f8a949b01adbaa20a974ad41901215cc_1440w.jpg" class=""><h5 id="Siam-LSTM"><a href="#Siam-LSTM" class="headerlink" title="Siam-LSTM"></a><strong>Siam-LSTM</strong></h5><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs latex">题目：Siamese Recurrent Architectures for Learning Sentence Similarity<br>论文：http://www.mit.edu/~jonasm/info/MuellerThyagarajan<span class="hljs-built_in">_</span>AAAI16.pdf<br></code></pre></td></tr></table></figure><p>2016年的Siam-LSTM在结构上比较简单，就是直接用共享权重的LSTM编码，取最后一步的输出作为表示。有个改进点是作者使用了<strong>Manhattan距离</strong>计算句子对的语义相似度. 实验证明将该方法和SVM结合用于情感分类(Entailment Classification), 效果提升明显.</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/v2-012618cc9e422b78fc0c57972fa687f2_1440w.jpg" class=""><h5 id="BiLSTM-Self-Attention"><a href="#BiLSTM-Self-Attention" class="headerlink" title="BiLSTM+Self-Attention"></a>BiLSTM+Self-Attention</h5><p>Lin等[A structured self-attentive sentence embedding] 将<strong>双向LSTM(BiLSTM)和Self-Attention</strong>技术相结合得到句子向量表示, 具体来说就是, 首先将句子通过BiLSTM模型, 将得到的每一时刻的两个方向的向量拼接成一个二维矩阵, 然后通过自注意力机制(Self-Attention)[<a href="javascript:;">60</a>]得到句中每个词向量对应的权重, 最终通过词向量的加权求和得到句向量. 在训练网络时同样是使用Siamese架构, 在得到句向量后进行简单的特征提取, 如拼接、点积、对位相减等, 然后输入一个多层感知机, 得到最终的语义文本相似度.</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/008i3skNly1gtyxtrjvl3j60ns0igwgd02.jpg" class=""><h5 id="Sentence-BERT"><a href="#Sentence-BERT" class="headerlink" title="Sentence-BERT"></a>Sentence-BERT</h5><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex">题目：Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks<br>论文：https://arxiv.org/pdf/1908.10084<br>代码：https://github.com/UKPLab/sentence-transformers<br></code></pre></td></tr></table></figure><p>EMNLP2019的Sentence-BERT是目前最常用的BERT式双塔模型，一是效果真的好，二是作者的开源工具做的很方便，用的人越来越多。</p><p>Sentence BERT(Sbert) 网络是通过 SNLI 数据集（标注了一对句子之间的关系，可能是蕴含、矛盾或者中立）进行预训练。首先将第一个句子输入到BERT，通过不同的Pooling方法获得句子的Embedding表示，第二个句子同样如此，然后将这两个Embedding变换后通过Softmax输出这对句子之间关系的概率进行训练（类似分类问题）。最后，获取pooling层的输出用于句子之间的余弦相似度计算和推理。SBert的网络结构：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1599653-ac9cc602bc22d081.png" class=""><ul><li><strong>在评估测试阶段，SBert直接使用余弦相似度来比较两个句向量之间的相似度</strong>，极大提升了推理速度；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer, util<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># code adapted from  https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic_search.py</span><br><br>model = SentenceTransformer(<span class="hljs-string">&#x27;D:/#Pre-trained_Language_Model/weights/distilbert-multilingual-nli-stsb-quora-ranking&#x27;</span>)<br><br><span class="hljs-comment"># Corpus with example sentences</span><br>sentences = [<span class="hljs-string">&quot;我想应聘。&quot;</span>, <span class="hljs-string">&quot;阿斯顿噶似的噶似的盾构法施工大苏打&quot;</span>, <br>        <span class="hljs-string">&quot;我想租冠寓的房子，如何操作？&quot;</span>,<br>        <span class="hljs-string">&quot;我约的保洁想取消，怎么操作&quot;</span>,<br>        <span class="hljs-string">&quot;我约的搬家想取消，怎么操作&quot;</span>,<br>        <span class="hljs-string">&quot;如果我有一处闲置物业，想租给冠寓怎么操作？&quot;</span>,<br>        <span class="hljs-string">&quot;我约的保洁想更改下时间，怎么办&quot;</span>,<br>        <span class="hljs-string">&quot;我是企业客户，我的员工有自租房需求，如何能使员工享受优惠价入住？&quot;</span>,<br>        <span class="hljs-string">&quot;我约的搬家想更改下时间，怎么办&quot;</span>,<br>        <span class="hljs-string">&quot;如果租住过程中有问题想投诉怎么办？&quot;</span>,<br>        <span class="hljs-string">&quot;如果我想和冠寓的门店合作提供一些增值性服务，怎么操作？&quot;</span>]<br><br><span class="hljs-comment"># Each sentence is encoded as a 1-D vector with 78 columns</span><br>sentence_embeddings = model.encode(sentences, convert_to_tensor=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Query sentences:</span><br>queries = [<span class="hljs-string">&#x27;我想租房&#x27;</span>, <span class="hljs-string">&#x27;约保洁&#x27;</span>]<br>query_embeddings = model.encode(queries)<br><br><br><span class="hljs-comment"># Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity</span><br>top_k = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> query, query_embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(queries, query_embeddings):<br>    cos_scores = util.pytorch_cos_sim(query_embedding, sentence_embeddings)[<span class="hljs-number">0</span>]<br>    cos_scores = cos_scores.cpu()<br><br>    <span class="hljs-comment">#We use np.argpartition, to only partially sort the top_k results</span><br>    top_results = np.argpartition(-cos_scores, <span class="hljs-built_in">range</span>(top_k))[<span class="hljs-number">0</span>:top_k]<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n\n======================\n\n&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Query:&quot;</span>, query)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTop 5 most similar sentences in corpus:&quot;</span>)<br><br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> top_results[<span class="hljs-number">0</span>:top_k]:<br>        <span class="hljs-built_in">print</span>(sentences[idx].strip(), <span class="hljs-string">&quot;(Score: %.4f)&quot;</span> % (cos_scores[idx]))<br></code></pre></td></tr></table></figure><p><strong>SBert开源地址</strong>：<a href="https://github.com/UKPLab/sentence-transformers">https://github.com/UKPLab/sentence-transformers</a></p><p><strong>SBert多语预训练模型下载地址</strong>：<a href="https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/">https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/</a></p><h5 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h5><p>纵览上述方法, 可以清晰地看到基于孪生网络的计算方法的发展进程, 对于模型的改进主要集中在使用不同的编码器, 如从最开始的多层感知机发展为 CNN、LSTM, 再到 Transformer 等.</p><p><strong>对速度要求高的召回场景可以用BOW+MLP、CNN，精度要求高的排序场景可以用LSTM、Transformer</strong>。同时两个向量的融合方法以及loss也都可以优化，比如做一些轻微的交互、像Deformer一样前面双塔接后面的多层交互，或者根据需要选择pointwise、pairwise（排序场景）损失。</p><p>但真要想做句间关系SOTA的话，比如刷榜，光靠双塔模型还是不行的，它有两个问题比较大：</p><ol><li><strong>位置信息</strong>。如果用BOW的话“我很不开心”和“我不很开心”两句的意思就变成一样了，虽然用RNN、BERT引入位置编码可以减缓一些，但不去让两个句子相互比较的话对于最后的分类层还是很难的</li><li><strong>细粒度语义</strong>。比如“我开心”和“我不开心”这两句话只有一个字的区别，但BOW模型很可能给出较高的相似度，交互式模型则可以稍有缓解</li></ol><h4 id="3-3-2-基于交互模型的方法"><a href="#3-3-2-基于交互模型的方法" class="headerlink" title="3.3.2 基于交互模型的方法"></a>3.3.2 基于交互模型的方法</h4><p>基于孪生网络的方法在编码层对句子编码时是相互独立的, 句子对之间没有交互, 这样对于计算句子对的语义相似度会造成一定影响, 基于交互模型的方法就是为了解决这个问题而产生的, 它是在孪生网络的基础上增加两个平行网络之间的交互作用, 从而提取到句子对之间更加丰富的交互信息.</p><h5 id="MatchPyramid"><a href="#MatchPyramid" class="headerlink" title="MatchPyramid"></a>MatchPyramid</h5><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex">题目：Text Matching as Image Recognition<br>论文：https://arxiv.org/pdf/1602.06359.pdf<br>代码：https://github.com/pl8787/MatchPyramid-TensorFlow<br></code></pre></td></tr></table></figure><p>MatchPyramid 模型的提出灵感源于 CV 领域里的图像识别。在此之前，文本匹配的维度是一维的 ( TextCNN 为代表的 word embedding )；而图像是二维平面的，显然信息更加丰富。2016 年的 AAAI 上中科院提出了基于 word-level 级别交互的<strong>矩阵匹配</strong>思想，用一个二维矩阵来代表 query 和 doc 中每个 word 的交互。模型关键，在于如何构建 query 和 doc 中 word 级别的匹配信息，文章提出了匹配矩阵的 3 种构造方法，indicator、cosine 以及 dot product，分别如下：</p><ol><li><strong>indicator match</strong><br>0-1 匹配矩阵，只有两个 word 完全相同才等于 1，否则为 0，相当于是精确匹配 </li><li><strong>cosine match</strong><br>将每个 word 用 glove 预训练得到的词向量，两两计算 cosine 相似度 </li><li><strong>dot product match</strong><br>将每个 word 用 glove 预训练得到的词向量，两两计算向量点积 </li></ol><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/v2-9752b8ccbc1da96344e0925f16b735cd_1440w.jpg" class=""><h5 id="MV-LSTM"><a href="#MV-LSTM" class="headerlink" title="MV-LSTM"></a>MV-LSTM</h5><p>AAAI2016 年提出的 MV-LSTM 模型，用双向 LSTM 来对两段文本 query 和 doc 进行编码，然后将 LSTM 的每个隐层输出进行拼接作为 query 和 doc 每个 term 的输出，并对这些 term 两两计算匹配分数，得到不同维度的匹配度矩阵。最后，先用 k-max pooling 压缩特征，然后用 MLP 后输出最终的匹配分数。</p><p>整个模型可以分为 3 部分：① query 和 doc 各自的 LSTM 编码；② query 和 doc 的匹配矩阵；③ 匹配信号融合。文中提到了 cosine、双线性以及 tensor layer 这 3 种计算方法，由于网络参数的不断加大，拟合精确度和复杂度也依次提升：</p><ol><li><strong>cosine 相似度</strong><br>query 中的 term u 以及 doc 中的 term v，计算 cosine 相似度 $s(u, v)=\frac{u^{T} v}{|u| \cdot|v|}$ </li><li><strong>双线性 bilinear</strong><br>通过引入待训练参数 M 进行映射 $s(u, v)=u^{T} M v+b$ </li><li><strong>Tensor layer</strong><br>$s(u, v)=\frac{u^{T} v}{|u| \cdot|v|}$ </li></ol><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/image-16dce761a5f648d3af92d94195ea1e16-20230424200833973.webp" class=""><h5 id="ABCNN"><a href="#ABCNN" class="headerlink" title="ABCNN"></a>ABCNN</h5><p>Yin等[<a href="javascript:;">79</a>]提出了ABCNN模型, 是在词向量基础上通过CNN网络对句子进行处理, 在分别对句子对中的句子进行卷积和池化操作的同时, 使用Attention机制对两个中间步骤进行交互, 文中一共尝试了3种添加Attention的策略, 主要区别是作用于模型的不同阶段, 如第一种Attention是直接作用在词向量组成的矩阵上, 而第二种Attention是作用在经过卷积和池化操作后产生的输出矩阵上, 第三种则是将前两种方法相结合. 可以将经过卷积和池化操作后得到的结果看作短语向量, 而该短语向量的长度取决于卷积核的大小, 从这个角度理解, 第一种和第二种Attention方法的区别实质上是在不同粒度上对模型进行了处理.</p><h5 id="PWIM"><a href="#PWIM" class="headerlink" title="PWIM"></a>PWIM</h5><p>He等[<a href="javascript:;">80</a>]通过BiLSTM对句子进行建模提出PWIM算法, 该方法共分为4个部分. 第一部分将BiLSTM网络每个时刻的输出, 即该时刻的双向hidden state做拼接获得的结果, 作为对应时刻word的representation. 第二部分通过计算两个句子的每个时刻的向量表示进行余弦相似度、欧式距离和点积计算, 然后根据计算结果来确定不同向量对的权重, 该方法认为句子内部不同的词的重要性是不一样的, 两个句子间重要的单词对, 对于句子相似度的计算贡献更大, 这些单词对应该得到更多的重视, 最后通过多层CNN网络得到最终结果.</p><h5 id="ESIM"><a href="#ESIM" class="headerlink" title="*ESIM"></a>*ESIM</h5><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex">题目：Enhanced LSTM for Natural Language Inference<br>论文：https://www.aclweb.org/anthology/P17-1152.pdf<br>代码：https://github.com/coetaur0/ESIM<br></code></pre></td></tr></table></figure><p>ACL2017中的ESIM 模型在众多短文本匹配比赛中大杀四方，表现十分出色。ESIM 模型通过精细的序列式网络设计，以及同时考虑局部推断和全局推断的方法来达到更好的效果。它的计算过程有一下四步：</p><ol><li>用BiLSTM对embedding编码，得到表示 $a,b$</li><li>将两句话的BiLSTM输出进行<strong>soft attention</strong>，得到融合的句子表示 $\alpha, \beta$</li><li>将 $[a, \alpha, a-\alpha, a \cdot \alpha]$ 拼接作为新表示，用BiLSTM再次编码，分别得到 $v_a,v_b$</li><li>max+avg池化后过进行最终分类 $v=softmax(\left[v_{a, a v e} ; v_{a, m a x} ; v_{b, a v e} ; v_{b, m a x}\right])$</li></ol><p><strong>局部推理层是 ESIM 的点睛之笔， 主要就是用 attention 机制计算出 a 句某一单词和 b 句中各个单词的权重，然后再将权重赋予 b 句各个单词，用来表征 a 句中的该单词，形成一个新的序列，b 句亦然。</strong></p><p>具体来说，使用<strong>点积</strong>的方式计算 a 句中第 i 个词和 b 句中第 j 个词之间的 attention 权重:</p><p>$e_{i j}=\overline{\mathbf{a}}_{i}^{T} \overline{\mathbf{b}}_{j}$</p><p>然后对两句各单词之间进行交互性计算，该词与另一句子联系越大，则计算出的值也会越大：</p><p>$\begin{aligned}<br>\tilde{\mathbf{a}}_{i} &amp;=\sum_{j=1}^{\ell_{b}} \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{\ell_{b}} \exp \left(e_{i k}\right)} \overline{\mathbf{b}}_{j}, \forall i \in\left[1, \ldots, \ell_{a}\right] \\<br>\tilde{\mathbf{b}}_{j} &amp;=\sum_{i=1}^{\ell_{a}} \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{\ell_{a}} \exp \left(e_{k j}\right)} \overline{\mathbf{a}}_{i}, \forall j \in\left[1, \ldots, \ell_{b}\right]<br>\end{aligned}$</p><p>得到局部推断后，为了增强信息，分别使用了向量相减、向量点积。最终 4 个向量 concat 起来，$m_a$和$m_b$分别表示增强后的 premise 和 hypothesis。</p><p>$\begin{array}{l}<br>m_{a}=[\bar{a} ; \tilde{a} ; (\bar{a}-\tilde{a}) ; (\bar{a} \tilde{a})] \\<br>m_{b}=[\bar{b} ; \tilde{b} ; (\bar{b}-\tilde{b}) ; (\bar{b} \bigodot \widetilde{b})]<br>\end{array}$</p><p>PS：局部推理层中对于匹配矩阵的做法区别于使用 MLP 去提取 query 和 doc 两个句子的做法；而是用两种 LSTM 提取编码后计算加权向量表达。得到加权向量表达后，和原始向量进行乘积和相减后，总共 4 个向量 concat 起来，相当于是丰富了提取到的信息。同时，相比BIMPM，ESIM训练速度快，效果也并没有逊色太多</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/image-2ceda1d52b3340fe9692e7aa6d438412-20230424200854159.webp" class=""><h5 id="BIMPM"><a href="#BIMPM" class="headerlink" title="BIMPM"></a>BIMPM</h5><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex">题目：Bilateral Multi-perspective Matching<br>论文：https://arxiv.org/pdf/1702.03814.pdf<br>代码：https://github.com/terrifyzhao/text<span class="hljs-built_in">_</span>matching<br></code></pre></td></tr></table></figure><p>IJCAI2017 年提出的同样基于BiLSTM网络提出<strong>双向多角度匹配模型BiMPM</strong>, 。该模型最大的创新点在于，对于给定的 query 和 doc，作者认为在匹配过程中，不仅需要考虑 query 到 doc，也应该考虑从 doc 到 query 的倒推关系，因此这是个双边 ( Bilateral ) 的关系。对于多角度，则是在考虑两个句子 query 和 doc 的关系的时候，用了 4 种不同的方法，体现了多角度 ( Multi-Perspective ) 的思想。BIMPM有着复杂的网络结构，计算复杂度高，很多计算之间又是串行的，可想而知，<strong>对于大规模的文本匹配计算速度很慢</strong></p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/image-252e4dd7a23942ac938472e78e498de7-20230424200902497.png" class=""><p>整个模型可以分为 5 层：① 词表示层；② 上下文表示层（使用词向量与字符向量的拼接）；③ 匹配层；④ 聚合层（使用双向 LSTM将两个句子 P 和 Q 匹配得到的 16 个匹配向量（P-&gt;Q,Q-&gt;P 各 8 个），聚合成一个固定长度的匹配向量）；⑤ 输出层（两层 MLP+softmax），其中匹配层为模型的核心，共提出了四种匹配策略，分别是：Full-Matching、Maxpooling-Matching、Attentive-Matching和 Max-Attentive-Matching：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/640-20230424200915421.jpeg" class=""><ol><li><strong>full-matching</strong>：在这种匹配策略中，用 P 中每一个 time step 的上下文向量（包含前向和后向）分别与句子 Q 中最后一个 time step 的上下文向量（包含前向和后向）计算匹配值，即</li></ol><p>$\begin{array}{c}<br>\overrightarrow{\boldsymbol{m}}_{i}^{\text {full }}=f_{m}\left(\overrightarrow{\boldsymbol{h}}_{i}^{p}, \overrightarrow{\boldsymbol{h}}_{N}^{q} ; \boldsymbol{W}^{1}\right) \\<br>\overleftarrow{\boldsymbol{m}}_{i}^{\text {full }}=f_{m}\left(\overleftarrow{\boldsymbol{h}}_{i}^{p}, \overleftarrow{\boldsymbol{h}}_{1}^{q} ; \boldsymbol{W}^{2}\right)<br>\end{array}$</p><ol><li><strong>maxpooling-matching</strong>：在第一种匹配方法中选取最大的匹配值，即</li></ol><p>$\begin{array}{l}<br>\overrightarrow{\boldsymbol{m}}_{i}^{\max }=\max _{j \in(1 \ldots N)} f_{m}\left(\overrightarrow{\boldsymbol{h}}_{i}^{p}, \overrightarrow{\boldsymbol{h}}_{j}^{q} ; \boldsymbol{W}^{3}\right) \\<br>\overleftarrow{\boldsymbol{m}}_{i}^{\max }=\max _{j \in(1 \ldots N)} f_{m}\left(\overleftarrow{\boldsymbol{h}}_{i}^{p}, \overleftarrow{\boldsymbol{h}}_{j}^{q} ; \boldsymbol{W}^{4}\right)<br>\end{array}$</p><p>where $\max _{j \in(1 \ldots N)}$ is element-wise maximum.</p><ol><li><strong>attentive-matching</strong>：先对 P 和 Q 中每一个 time step 的上下文向量（包含前向和后向）计算余弦相似度，得到相似度矩阵</li></ol><p>$\begin{array}{l}<br>\vec{\alpha}_{i, j}=\operatorname{cosine}\left(\overrightarrow{\boldsymbol{h}}_{i}^{p}, \overrightarrow{\boldsymbol{h}}_{j}^{q}\right) \\<br>\overleftarrow{\alpha}_{i, j}=\operatorname{cosine}\left(\overleftarrow{\boldsymbol{h}}_{i}^{p}, \overleftarrow{\boldsymbol{h}}_{j}^{q}\right)<br>\end{array}$</p><p>然后将相似度矩阵作为 Q 中每一个 time step 权重，通过对 Q 的所有上下文向量（包含前向和后向）加权求和，计算出整个句子 Q 的注意力向量</p><p>$\begin{aligned}<br>\overrightarrow{\boldsymbol{h}}_{i}^{\text {mean }} &amp;=\frac{\sum_{j=1}^{N} \vec{\alpha}_{i, j} \cdot \overrightarrow{\boldsymbol{h}}_{j}^{q}}{\sum_{j=1}^{N} \vec{\alpha}_{i, j}} \\<br>\overleftarrow{\boldsymbol{h}}_{i}^{\text {mean }} &amp;=\frac{\sum_{j=1}^{N} \overleftarrow{\alpha}_{i, j} \cdot \overleftarrow{\boldsymbol{h}}_{j}^{q}}{\sum_{j=1}^{N} \overleftarrow{\alpha}_{i, j}}<br>\end{aligned}$</p><p>最后，将 P 中每一个 time step 的上下文向量（包含前向和后向）分别与句子 Q 的注意力向量计算匹配值</p><p>$\begin{array}{l}<br>\overrightarrow{\boldsymbol{m}}_{i}^{a t t}=f_{m}\left(\overrightarrow{\boldsymbol{h}}_{i}^{p}, \overrightarrow{\boldsymbol{h}}_{i}^{\text {mean }} ; \boldsymbol{W}^{5}\right) \\<br>\overleftarrow{\boldsymbol{m}}_{i}^{a t t}=f_{m}\left(\overleftarrow{\boldsymbol{h}}_{i}^{p}, \overleftarrow{\boldsymbol{h}}_{i}^{\text {mean }} ; \boldsymbol{W}^{6}\right)<br>\end{array}$</p><ol><li><strong>max-attentive-matching</strong>：与 attentive-matching 的匹配策略相似，不同之处在于选择句子 Q 所有上下文向量中余弦相似度最大的向量作为句子 Q 的注意力向量。</li></ol><h5 id="DIIN"><a href="#DIIN" class="headerlink" title="DIIN"></a>DIIN</h5><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex">题目：Natural Language Inference over Interaction Space<br>论文：https://arxiv.org/pdf/1709.04348.pdf<br>代码：https://github.com/YerevaNN/DIIN-in-Keras<br></code></pre></td></tr></table></figure><p>DIIN 模型是在 ICLR2018 上提出的, 输入层使用单词嵌入、字符特征和句法特征的串联, 编码层使用自注意力<a href="self-attention"><a href="javascript:;">60</a></a>机制, 交互层采用点积操作得到交互矩阵, 然后使用DenseNet[<a href="javascript:;">84</a>]进行特征抽取, 之后将抽取的特征传入多层感知机模型得到最终的结果, 本方法简单有效, 在NLI任务上表现出很好的性能.</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/v2-c44b0398dc3e5372e8b91959b8052081_1440w.jpg" class=""><p>模型可以分为 5 个部分，分别是嵌入层、编码层、交互层、特征抽取层以及输出层：</p><ul><li>Embedding Layer</li></ul><p>在 embedding 层中，论文中同时将单词级别 embedding、字符级别特征的 embedding （OOV）以及句法级别的特征 embedding 信息 （一个是词性特征 ( POS )，另一个是二进制精确匹配特征 ( EM )），全部concat 起来作为每个词的 embedding。</p><ul><li>Encoding Layer</li></ul><p>Encoding Layer的主要作用是将上一层的特征进行融合并进行encode。论文中作者采用的是<strong>self-attention机制</strong>，同时考虑到了词序和上下文信息。以P作为例子，首先计算attetion matrix</p><p>$A_{i j}=\alpha\left(\hat{P}_{i}\right), \hat{P}_{j} \in R$</p><p>这里的attention matrix的 计算方法和transformer的还不太一样，作者取了三个维度的值并<strong>拼接</strong>起来，如下面的公式所示，我的理解是a与b应该是一样的，因为都是针对的P，唯一多了个a ∘ b其中的∘是对位相乘，如果P的维度是d，那么拼接后的向量维度是3d</p><p>$\alpha(a, b)=w_{a}^{T}[a ; b ; a \circ b]$</p><p>然后加上softmax计算self-attention的值</p><p>$\bar{P}=\sum_{j=1}^{p} \frac{\exp \left(A_{i j}\right)}{\sum_{k=1}^{p} \exp \left(A_{k j}\right)} \hat{P}_{j}$</p><p>接下来作为引入了LSTM中门的概念semantic composite fuse gate，其计算公式如下</p><p>$\begin{array}{c}<br>z_{i}=\tanh \left(W^{1 t}\left[\hat{P}_{i} ; \bar{P}_{i}\right]+b^{1}\right) \\<br>r_{i}=\sigma\left(W^{2 T}\left[\hat{P}_{i} ; \bar{P}_{i}\right]+b^{2}\right) \\<br>f_{i}=\sigma\left(W^{3 T}\left[\hat{P}_{i} ; \bar{P}_{i}\right]+b^{3}\right) \\<br>\tilde{P}_{i}=r_{i} \circ \hat{P}_{i}+f_{i} \circ z_{i}<br>\end{array}$</p><p>其中 W 的维度均是$[ 2 d , d ]$，b的维度是d，σ表示的是sigmoid函数</p><p>H的操作和P的操作一样，就不再赘述了，有一点需要注意，论文中认为P和H是有细微差距的，所以attention的权重和gate的权重是没有共享的，不过论文中的任务五是NLI，如果是相似度匹配的任务我觉得此处是可以共享的，必须相似度匹配两个句子本身就是很接近的，此观点仅代表个人想法没有验证过，代码中我们还是以论文为主。</p><ul><li>Interaction Layer</li></ul><p>Interaction Layer的主要目的是把P与H做一个相似度的计算，提取出其中的相关性，可以采用余弦相似度、欧氏距离等等，这里作者发现对位相乘（向量点积）的效果很好，所以公式中的 $\beta(a, b)=a \circ b$</p><p>$I_{i j}=\beta\left(\tilde{P}_{i}, \tilde{H}_{i}\right) \in R^{d}$</p><ul><li>Feature Extraction Layer</li></ul><p>Feature Extraction Layer的任务正如其名，做特征提取。这一层论文主要采用的是CNN的结构，作者实验发现ResNet效果会好一些，但是最终还是选择了DenseNet，因为DenseNet能更好的保存参数，并且作者观察到ResNet如果把skip connection移除了模型就没法收敛了（ResNet的关键就在于skip connection不知道作者为什么要说明这一点）BN还会导致收敛变慢（这里应该是指针对当前这个模型来说）所以作者并没有采用ResNet。卷积采用的是relu激活函数，都是1×1的卷积核来对上文提到的相关性tensor进行缩放，并且这里引入了一个超参数η，例如输入的channel是k那么输出的channel就是k × η，然后把结果送到3层Dense block中，Dense block包含了n个3×3的卷积核，成长率是g，transition layer采用了1×1的卷积核来做channel的缩减，然后跟上一个步长为2的最大池化层，transition layer的缩减率用θ表示。这一层的关键就在于DenseNet，对DenseNet不清楚的小伙伴一定要先去了解该网络的结构原理，这也是为啥作者会把该模型取名为Densely Interactive Inference Network</p><ul><li>Output Layer</li></ul><p>输出层使用简单的一个线性层以及 softmax 多分类进行分类。</p><h5 id="HCAN"><a href="#HCAN" class="headerlink" title="HCAN"></a>HCAN</h5><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex">题目：Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling<br>论文：https://cs.uwaterloo.ca/~jimmylin/publications/Rao<span class="hljs-built_in">_</span>etal<span class="hljs-built_in">_</span>EMNLP2019.pdf<br>代码：https://github.com/jinfengr/hcan<br></code></pre></td></tr></table></figure><p>HCAN是Facebook在EMNLP2019提出的模型，虽然比上文的ESIM、PWIM等模型高上4+个点，但还是被BERT甩了很远。不过这个模型的核心也不只是做语义匹配，而是同时做检索相关性（Relevance Matching），也就是搜索中query-doc的匹配。</p><p>模型计算如下：</p><ol><li>Encoding层作者提出了三种方法，堆叠的CNN作为Deep Encoder，不同尺寸卷积核作为Wide Encoder，BiLSTM作为Contextual Encoder编码更长距离的上下文</li><li>先把两句话交互得到attention score矩阵，之后对于query中每个词，求得doc中最相似的词的分数，作为向量Max(S)，按照同样的方法求出Mean(S)，长度都为|Q|，再分别乘上query中每个词的tfidf统计，得到相关性匹配向量 <img src="https://www.zhihu.com/equation?tex=O_%7BRM%7D#id=XMhh8&amp;originHeight=20&amp;originWidth=40&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=shadow&amp;title=" alt=""></li><li>用加性attention对query和doc进行交互，得到新的表示，再花式拼接过BiLSTM，得到语义匹配向量 <img src="https://www.zhihu.com/equation?tex=O_%7BSM%7D#id=wbp78&amp;originHeight=20&amp;originWidth=38&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=shadow&amp;title=" alt=""></li><li>将 <img src="https://www.zhihu.com/equation?tex=O_%7BSM%7D%2C+O_%7BRM%7D#id=T0BDo&amp;originHeight=20&amp;originWidth=86&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=shadow&amp;title=" alt=""> 拼接起来过MLP，最后分类</li></ol><p>通过实验结果来看，Deep Encoder的表现是最好的，在7/8个评估上都超过另外两个。</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/v2-9e3b3f27fc2de53bc27c52baceca84c0_1440w.jpg" class=""><p>模型只是工具，数据才是天花板，数据质不好/数量不够，模型再花哨也没用。像BiMPM和ESIM，小数据集无法发挥它们的效果，所以训练调参的过程会很耗时。像是quora question pairs数据集提供了40W对训练样本，不平衡程度也不大(正负比 3:5)，这时两个模型效果都不错，这也印证了数据驱动的重要性。</p><h5 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h5><p>基于交互模型的方法实质就是在孪生网络架构的基础上, 通过某种策略对两个孪生网络的中间环节进行交互. 目前最普遍的策略是各种不同的Attention机制. 具有交互能力的模型结构普遍更为复杂, 包含更多的模型参数, 这就导致了这类模型的计算成本较高.</p><h4 id="孪生-vs-交互"><a href="#孪生-vs-交互" class="headerlink" title="孪生 vs. 交互"></a>孪生 vs. 交互</h4><ul><li>基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量速度会快很多；缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。</li><li>基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。</li></ul><h2 id="4-『评价指标』"><a href="#4-『评价指标』" class="headerlink" title="4.『评价指标』"></a>4.『评价指标』</h2><p>首先，对于每一个文本对，采用余弦相似度对其打分。打分完成后，采用所有余弦相似度分数和所有 gold label 计算 Spearman Correlation。</p><p>其中，<strong>Pearson</strong> Correlation 与 <strong>Spearman</strong> Correlation 都是用来计算两个分布之间相关程度的指标。Pearson Correlation 计算的是两个变量是否线性相关，而 Spearman Correlation 关注的是两个序列的单调性是否一致。并且论文《Task-Oriented Intrinsic Evaluation of Semantic Textual Similarity》证明，采用 Spearman Correlation 更适合评判语义相似度任务。Pearson Correlation 与 Spearman Correlation 的公式如下：</p><img src="/2021/11/09/2021-11-09-%F0%9F%90%98%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/1661236959872-b5e9133c-cac2-4e6d-a948-f4aa4499ba4b.png" class=""><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/111769969">谈谈文本匹配和多轮检索</a></p><p><a href="https://zhuanlan.zhihu.com/p/357864974">21个经典深度学习句间关系模型｜代码&amp;技巧</a></p><p><a href="https://www.6aiq.com/article/1589474365961">贝壳找房【深度语义匹配模型 】原理篇一：表示型</a></p><p><a href="https://www.6aiq.com/article/1589798723495">贝壳找房【深度语义匹配模型】原理篇二：交互篇</a></p><p><a href="https://www.6aiq.com/article/1590190626464">【深度语义匹配模型】实践篇：语义匹配在贝壳找房智能客服中的应用</a></p><p><a href="https://blog.csdn.net/u012526436/article/details/90179466">文本相似度，文本匹配模型归纳总结</a></p><p><a href="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/CQA%E8%B0%83%E7%A0%94-%E5%B7%A5%E4%B8%9A%E7%95%8C.md#221-%E5%9F%BA%E4%BA%8E%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%E7%9A%84%E7%AE%97%E6%B3%95">QA-Survey/CQA调研-工业界.md</a></p><p><a href="https://www.6aiq.com/article/1587873440587">干货! 搜索系统中的深度匹配模型</a></p><p><a href="http://hdsfdxzkb.xml-journal.net/cn/article/doi/10.3969/j.issn.1000-5641.202091011?viewType=HTML">韩程程, 李磊, 刘婷婷, 高明. 语义文本相似度计算方法[J]. 华东师范大学学报（自然科学版）</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>语义匹配</tag>
      
      <tag>语义相似度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>对话系统之NLU</title>
    <link href="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8BNLU/"/>
    <url>/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8BNLU/</url>
    
    <content type="html"><![CDATA[<h2 id="核心思路"><a href="#核心思路" class="headerlink" title="核心思路"></a>核心思路</h2><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8BNLU/640.jpeg" class="" title="图片"><p>意图识别NLU，一般作为多分类问题，需求是对给定query进行分类，找到最可能的主问题类别。</p><p>对话系统之NLU2.0，核心思路是以搜代分（检索+排序）：</p><p>离线：</p><ul><li>训练一个语义相似度表征模型。</li><li>用表征模型预测每一个知识库内标准问的语义向量。</li><li>将语义向量入库，即存入检索es库中</li></ul><p>在线（来了一个query请求）：</p><ul><li>用上面训练的表征模型预测query的向量。</li><li>通过传统检索方法召回top n候选集。</li><li><p>根据相似度完成对top n候选的排序。</p><span id="more"></span></li></ul><h2 id="离线"><a href="#离线" class="headerlink" title="离线"></a>离线</h2><p>离线负责数据预处理、数据集构建、模型构建、模型训练、物料/索引问的向量预测、索引构建入库的核心操作，当然一些<strong>服务的基本配件</strong>，比如一键打包/部署的脚本、定时任务、日志、监控、校验等任务也会有</p><p>索引构建入库的2种方式：</p><ol><li><p>切词，然后做倒排索引，即传统的search切词倒排索引模式</p></li><li><p>现在比较潮流的做法就是把文本转化为向量然后做成向量索引，即泛化程度较高的语义向量索引</p></li></ol><h3 id="语义相似度表征模型方案1："><a href="#语义相似度表征模型方案1：" class="headerlink" title="语义相似度表征模型方案1："></a>语义相似度表征模型方案1：</h3><ul><li>采用 AM-Softmax，它是一种带margin的softmax，通常用于<a href="https://kexue.fm/archives/8656">用分类做检索的场景</a></li><li>分类任务的目标是“最靠近所属类的中心”，而排序的目标是“类内差距小于类间差距”</li><li>为了保证分类模型的特征可以用于排序，那么每个样本不仅仅要最靠近类中心，而且是距离加上 $m$​ 之后还要最靠近类中心，这便是带加性margin的Softmax的重要性！</li><li>最后，取CLS层输出，直接进行余弦相似度计算</li></ul><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu2b37hzw5j61fk0huadg02.jpg" alt="图片1"></p><h3 id="语义相似度表征模型方案2："><a href="#语义相似度表征模型方案2：" class="headerlink" title="语义相似度表征模型方案2："></a>语义相似度表征模型方案2：</h3><ul><li>分类相似性学习-Pointwise<br>即将句子相似度视为二分类模型（sigmoid接近1更相似）；</li><li>度量学习<br>即将句子相似度视为排序关系学习问题；</li><li>采用Listwise的训练方式，如：ListNet<br>正负例采样<br>主问题+相似问+负采样多个<br>取度量模型的隐层进行ranking</li></ul><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu2b3kh545j60wy0iq0w702.jpg" alt="图片2"></p><h3 id="语义相似度模型选择："><a href="#语义相似度模型选择：" class="headerlink" title="语义相似度模型选择："></a>语义相似度模型选择：</h3><p><strong>1、Poly-encoders</strong></p><p><strong>论文名称</strong>：ICLR 2020 | Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring</p><p><strong>arxiv地址</strong>：<br><em><a href="https://arxiv.org/pdf/1905.01969.pdf">https://arxiv.org/pdf/1905.01969.pdf</a></em></p><p>论文解读：<br><a href="https://jishuin.proginn.com/p/763bfbd2cb83">https://jishuin.proginn.com/p/763bfbd2cb83</a></p><p>解决双塔式交互不足以及交互式速度慢的问题！</p><p><strong>2、Keyword-BERT</strong></p><p><strong>论文名称</strong>：Keyword-Attentive Deep Semantic Matching</p><p><strong>arxiv地址</strong>：<a href="https://arxiv.org/pdf/2003.11516.pdf">https://arxiv.org/pdf/2003.11516.pdf</a></p><p>中文解读：<a href="https://webcache.googleusercontent.com/search?q=cache:cRZ2vGcvgnQJ:https://www.cnblogs.com/xxBryce/p/14393513.html+&amp;cd=7&amp;hl=zh-CN&amp;ct=clnk">基于关键字注意的深层语义匹配模型</a>、<a href="https://mp.weixin.qq.com/s/_QY2EhB-TiBcb5q0379McQ">远离送命题: 问答系统中语义匹配的『杀手锏』</a></p><p>Code: <a href="https://github.com/DataTerminatorX/Keyword-BERT/tree/master/keyword-bert">https://github.com/DataTerminatorX/Keyword-BERT/tree/master/keyword-bert</a></p><p>注意：如果我们不能提供足够的训练样本，去教会模型分辨出『关键信息』，光凭模型自身的花式 CNN/RNN/Attention，纵使使出浑身解数，在一些很难分辨的 case 上也未必work！！可以考虑 <a href="https://blog.csdn.net/xixiaoyaoww/article/details/105182946">Keyword-BERT</a></p><p>本文的主要贡献如下：</p><ol><li>Keyword-attentive BERT：在 BERT 的最后一层引入一个额外的 keyword-attentive 层，目标是在注意力机制中强调关键词与非关键词之间的交互。通过明确地 “告诉” 模型哪些是重要的词，实验表明 Keyword-attentive BERT 优于原始 BERT。</li><li>更好的负采样训练更鲁棒的模型：提出了一种新的负采样方法，该方法采用 keyword 重叠分数来选择信息更丰富的负样本。此外，应用实体替换技巧来生成更多种类的负样本，比如将实体 “China” 替换成 “America”，经过数据增强，模型训练得更加鲁棒。</li><li>Keyword 抽取：提出一种利用领域信息的简单有效的关键字提取算法，抽取出来的关键字可以在三个方面使用：a. 建立 keyword-attentive 深层语义匹配模型，b. 提高 QA 搜索引擎的召回质量，c. 改进负采样训练一个更好的语义匹配模型。</li></ol><p><img src="https://pic4.zhimg.com/80/v2-56383a400aa3fa7ba5415c86e67fa84b_1440w.png" alt="img"></p><p><img src="https://pic3.zhimg.com/80/v2-c5ee562b42172fd3a9c62fcbf8da2ea6_1440w.jpg" alt=""></p><p><strong>3、SimNet</strong></p><p>服务地址：<a href="https://www.paddlepaddle.org.cn/modelbasedetail/SimNet">Paddle短文本语义匹配—SimNet</a></p><p>短文本语义匹配(SimilarityNet, SimNet)是一个计算短文本相似度的框架，可以根据用户输入的两个文本，计算出相似度得分。SimNet框架在百度各产品上广泛应用，主要包括BOW、CNN、RNN、MMDNN等核心网络结构形式，提供语义相似度计算训练和预测框架，适用于信息检索、新闻推荐、智能客服等多个应用场景，帮助企业解决语义匹配问题。可通过<a href="https://ai.baidu.com/tech/nlp_basic/simnet">AI开放平台-短文本相似度</a>线上体验。</p><p>同时推荐用户参考<a href="https://aistudio.baidu.com/aistudio/projectDetail/124373"> IPython Notebook demo</a></p><h2 id="在线"><a href="#在线" class="headerlink" title="在线"></a>在线</h2><p>在线则有query/用户的向量预测、向量召回的工作（facebook开源的向量召回工具faiss等）。用户请求query输入，把它用离线训练好的模型转化为向量。之后通过传统检索召回一批和query字面相似的结果，经过一些规则并根据相似度进行排序，即可得到与之最接近的top主问题。除此之外，当然也有日志、监控、热更新、服务等其他算法服务也会有的工作。</p><h2 id="NLU2-0-优缺点"><a href="#NLU2-0-优缺点" class="headerlink" title="NLU2.0 优缺点"></a>NLU2.0 优缺点</h2><p>优点是：</p><ul><li><p>可控性强，准确率高</p><p>可以通过备选query的上下线来进行干预</p></li><li><p>把文本分类问题实质上转化为一个比较通用的相似度任务</p><p>语义相似度模型的构建相对简单，可以用外部数据进行训练，不需要领域内标注数据。</p></li><li><p>可以通过加备选query（别名）完成很多个分类的配置，从而实现多分类</p><p>对于样例数较少的类别比较友好，能够解决长尾问题？</p></li></ul><p>但是也有缺点：</p><ul><li>需要足量的备选query才能够实现较为精准的预测，缺少的话会影响召回。</li></ul><p>Tips:</p><ul><li>对于类目比较多的文本分类，我们可以把问题拆分，部分高频的可以用普通的文本分类来处理</li><li>对于长尾的，文本分类多分类其实很难处理，很容易学不到，因此就可以用文本检索的方式来处理</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247487022&amp;idx=1&amp;sn=8d4637014107c76c895cbed2e3d6f81b&amp;chksm=e88256b0dff5dfa61c3b8fe5b7041b7b2a172ca5df4aca02decec6abfcff2b61b48e7687d1df&amp;scene=21#wechat_redirect">心法利器[26] | 以搜代分：文本多分类新思路</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247486586&amp;idx=1&amp;sn=ddd1716c5e5860361df3e45de0f14de9&amp;chksm=e88254e4dff5ddf289bdf02e5e6d18baf42bda78415b61e620c567a749fafb00d2dacd50232a&amp;scene=21#wechat_redirect">心法利器[16] | 向量表征和向量召回</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247486856&amp;idx=1&amp;sn=19531ad972ff63af69dcc521ad654e63&amp;chksm=e8825516dff5dc008c1a3cf8c74ddb1ec04057f3ebb32dd4d54498f9f88f791ef412edef6037&amp;scene=21#wechat_redirect">心法利器[20] | NLU落地场景-智能对话交互</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>NLU</tag>
      
      <tag>以搜代分</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>对话系统之槽位提取</title>
    <link href="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/"/>
    <url>/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/</url>
    
    <content type="html"><![CDATA[<p>槽位提取是NER（Named Entity Recognition）技术在对话系统的应用，要想了解槽位提取的细节，本篇文章会先对NER进行系统介绍，然后再对槽位提取在龙小湖对话系统中的一些应用进行汇报。</p><span id="more"></span><h1 id="NER简述"><a href="#NER简述" class="headerlink" title="NER简述"></a>NER简述</h1><h2 id="一、什么是NER"><a href="#一、什么是NER" class="headerlink" title="一、什么是NER"></a>一、什么是NER</h2><p>Named Entity Recognition(NER)，指识别文本中具有特定意义的实体，主要包括人名、地名、机构名等专有名词。在对话机器人中，我们通过NER技术来提取对话中的实体词，即槽位词，以便用于控制对话逻辑跳转和标识对话关键词等。</p><h2 id="二、NER主要方法"><a href="#二、NER主要方法" class="headerlink" title="二、NER主要方法"></a>二、NER主要方法</h2><h3 id="2-1-基于规则和词典的方法"><a href="#2-1-基于规则和词典的方法" class="headerlink" title="2.1 基于规则和词典的方法"></a>2.1 基于规则和词典的方法</h3><p>一般来说，我们在做命名实体的时候，可以首先考虑一下可否使用正则。假如命名实体的名称规律比较简单，我们可以找出模式，然后设计相应的正则表达式或者规则，然后把符合模式的字符串匹配出来，作为命名实体识别的结果。</p><p>这种NER系统的特点是高精确率与低召回率；然而难以迁移应用到别的领域中去，基于领域的规则往往不通用，对新的领域而言，需要重新制定规则且不同领域字典不同。</p><h3 id="2-2-无监督学习方法"><a href="#2-2-无监督学习方法" class="headerlink" title="2.2 无监督学习方法"></a>2.2 无监督学习方法</h3><p>主要是基于聚类的方法，根据文本相似度得到不同的簇，表示不同的实体类别组。常用到的特征或者辅助信息有词汇资源、语料统计信息（TF-IDF）、浅层语义信息（分块NP-chunking）等。</p><h3 id="2-3-基于特征的监督学习方法"><a href="#2-3-基于特征的监督学习方法" class="headerlink" title="2.3 基于特征的监督学习方法"></a>2.3 基于特征的监督学习方法</h3><p>NER可以被转换为一个分类问题或序列标记问题。分类问题就是判断一个词语是不是命名实体、是哪一种命名实体。常见的做法就是，基于一个词语或者字的上下文构造特征，来判断这个词语或者字是否为命名实体；序列标注问题就是给句子当中的每一个词打上标签，而后提取出我们所需要的关键词；标签的格式通常有IOB2和IOBES两种标准。</p><p>所以就会涉及到特征工程和模型选择上。</p><p>特征工程：word级别特征（词法特征、词性标注等），词汇特征（维基百科、DBpdia知识），文档及语料级别特征。</p><p>模型选择：隐马尔可夫模型、决策树、最大熵模型、最大熵马尔科夫模型、支持向量机、条件随机场。</p><h3 id="2-4-深度学习方法"><a href="#2-4-深度学习方法" class="headerlink" title="2.4 深度学习方法"></a>2.4 深度学习方法</h3><p>近年来，基于DL的NER模型占据了主导地位并取得了最先进的成果。与基于特征的方法相比，深度学习有利于自动发现隐藏的特征。NN把语言看做是序列数据，然后用自身极强的拟合能力，把这种序列转换为标签序列。BiLSTM+CRF方案结合了神经网络的拟合能力和CRF的全局视野，是非常经典、有效的一种NER模型结构。</p><h4 id="1）BiLSTM-CRF"><a href="#1）BiLSTM-CRF" class="headerlink" title="1）BiLSTM+CRF"></a>1）BiLSTM+CRF</h4><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/640-20230424170714211" class="" title="图片"><p>BiLSTM的输出作为CRF的发射概率矩阵，而CRF层可以加入一些约束来保证最终预测结果是有效的。这些约束可以在训练数据时被CRF层自动学习得到。</p><h4 id="2）IDCNN-CRF"><a href="#2）IDCNN-CRF" class="headerlink" title="2）IDCNN+CRF"></a>2）IDCNN+CRF</h4><p>尽管BILSTM在NER任务中有很好的表现，但是却不能充分利用GPU的并行性，导致该模型的想能较差，因此出现了一种新的NER模型方案IDCNN+CRF。</p><p>在IDCNN+CRF模型结构中，待识别query先经过Embedding层获取向量表示；然后经过空洞卷积层（IDCNN），IDCNN通过空洞卷积增大模型的感受野， 相较于传统的CNN，IDCNN能够捕捉更长的上下文信息，更适合序列标注这类需要全局信息的任务；在IDCNN之后经过一层全连接神经网络（FF层）后引入CRF，同样CRF的目的在于防止非法槽位标记（BIO）的出现。</p><blockquote><p>补充：尽管传统的CNN有明显的计算优势，但是传统的CNN在经过卷积之后，末梢神经元只能得到输入文本的一小部分信息，为了获取上下文信息，需要加入更多的卷积层，导致网络越来越深，参数越来越多，容易发生过拟合。</p></blockquote><p>文本空洞卷积的示意图如下：</p><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/v2-63f77c2c69c5055f1bacc11429afa7a5_1440w.jpg" class="" title="img"><h4 id="3）Bert-BiLSTM-CRF"><a href="#3）Bert-BiLSTM-CRF" class="headerlink" title="3）Bert+BiLSTM+CRF"></a>3）Bert+BiLSTM+CRF</h4><p>Bert由谷歌大佬与2018年提出来，刚出来的时候横扫了11项NLP任务。BERT通过微调的方法可以灵活的应用到下游业务，所以这里我们也可以考虑使用Bert作为embedding层，将特征输入到Bilstm+CRF中，以谋求更好的效果。</p><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/640-20230424170735937" class="" title="图片"><h4 id="NER模型之CRF的作用"><a href="#NER模型之CRF的作用" class="headerlink" title="NER模型之CRF的作用"></a>NER模型之CRF的作用</h4><p>在上述模型中，在NER任务上，我们看到很多深度学习之后都会接上一层CRF，那么CRF在整个过程中到底发挥着什么样的作用呢？通常我们直接使用逐帧softmax时，是将序列标注过程作为n个k分类问题，相当于每个token相互独立的进行分类（假设深度模型内部交互不明显的话），而采用CRF实质上是在进行一个k^n分类，相当于直接从所有的序列空间里找出转移概率最大的那条序列。其实质上是局部最优（token最优）与全局最优（序列最优）的区别，因而采用CRF能够有效避免出现非法的序列标记，从而确保序列有效。下图展示了一个softmax导致序列标注异常的案例。</p><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/640-20230424170740084" class="" title="图片"><h2 id="三、NER模型效果优化"><a href="#三、NER模型效果优化" class="headerlink" title="三、NER模型效果优化"></a>三、NER模型效果优化</h2><h3 id="3-1-模型优化之数据增强"><a href="#3-1-模型优化之数据增强" class="headerlink" title="3.1 模型优化之数据增强"></a>3.1 模型优化之数据增强</h3><p>针对启动阶段存在的数据不足问题，可以采用数据增强的方式来补充训练数据，NER做数据增强，和别的任务有啥不一样呢？很明显，NER是一个token-level的分类任务，在进行全局结构化预测时，一些增强方式产生的数据噪音可能会让NER模型变得敏感脆弱，导致指标下降、最终奔溃。</p><p>参考论文《<strong>An Analysis of Simple Data Augmentation for Named Entity Recognition</strong>》主要是将传统的数据增强方法应用于NER中、并进行全面分析与对比。效果如何？</p><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/640-20230424170756424" class="" title="图片"><p>作者借鉴sentence-level的传统数据增强方法，将其应用于NER中，共有4种方式（如上图所示）：</p><ul><li><strong>Label-wise token replacement (LwTR)</strong> ：即同标签token替换，采用二项分布概率对句子进行采样，概率替换某位置的token为同标签其它token，如果token长度不一致，则进行延展，句子长度发生变化。</li><li><strong>Synonym replacement (SR)</strong> ：即同义词替换，利用WordNet查询同义词，然后根据二项分布随机替换。如果替换的同义词大于1个token，那就依次延展BIO标签。</li><li><strong>*Mention replacement (MR)</strong> ：即实体提及替换，与同义词方法类似，利用训练集中的相同实体类型进行替换，如果替换的mention大于1个token，那就依次延展BIO标签，如上图：「headache」替换为「neuropathic pain syndrome」，依次延展BIO标签。</li><li><strong>Shuffle within segments (SiS)</strong> ：按照mention来切分句子，然后再对每个切分后的片段进行shuffle。如上图，共分为5个片段： [She did not complain of], [headache], [or], [any other neurological symptoms], [.]. 。也是通过二项分布判断是否被shuffle（mention片段不会被shuffle），如果shuffle，则打乱片段中的token顺序。</li><li><strong>总结规则模板，直接生成数据</strong>。（收益不小）</li></ul><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/640-20230424170805807" class="" title="图片"><p>由上图得出以下结论：</p><ul><li>各种数据增强方法都超过不使用任何增强时的baseline效果。</li><li>对于RNN网络，<strong>实体提及替换优于其他方法</strong>；对于Transformer网络，<strong>同义词替换最优。</strong></li><li>总体上看，所有增强方法一起使用（<strong>ALL</strong>）会由于单独的增强方法。</li><li>低资源条件下，数据增强效果增益更加明显；</li><li>充分数据条件下，数据增强可能会带来噪声，甚至导致指标下降；</li></ul><h3 id="3-2-模型优化之词汇增强"><a href="#3-2-模型优化之词汇增强" class="headerlink" title="3.2 模型优化之词汇增强"></a>3.2 模型优化之词汇增强</h3><p><strong>有的学者开始另辟蹊径，利用外部词汇信息力求与BERT一战；</strong></p><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/640-20230424170811645" class="" title="图片"><ul><li>Lattice LSTM：<a href="https://arxiv.org/abs/1805.02023">Chinese NER Using Lattice LSTM</a></li></ul><p>引入词汇信息，在原有的输入序列的基础上添加匹配到的词汇作为额外的链路，整体看起来有点像<code>ResNet</code>的短路链接，两端分别连接原始输入序列的词首尾，称之为<code>Latttice-LSTM</code>。事实也证明词典带来的提升是明显的，一举超越<code>BERT</code>，重回武林宝座。缺点： 计算性能低下，不能batch并行化；信息损失：每个字符只能 获取以它为结尾的词汇信息；可迁移性差；</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1guw6jz6h53j61dk0pwdla02.jpg" alt="image-20210928112728953"></p><ul><li>LR-CNN：<a href="https://pdfs.semanticscholar.org/1698/d96c6fffee9ec969e07a58bab62cb4836614.pdf&#39;">CNN-Based Chinese NER with Lexicon Rethinking</a></li></ul><p>该篇指出<code>Latttice-LSTM</code>第一：速度太慢，第二：无法进行词汇匹配的选择。为了解决这两个问题，将原始输入序列按照词典匹配的词汇信息进行<code>Bigram,Trigram</code>合并然后<code>CNN</code>特征提取，然后将匹配到词汇信息，进行时间维度上attention计算后，利用<code>Rethinking</code>机制，反馈到原始<code>Bigram,Trigram</code>层，进行词汇匹配的选择，以解决词汇冲突的问题。</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1guw6kg95zej61e00q4dlj02.jpg" alt="image-20210928112756388"></p><ul><li><a href="https://arxiv.org/abs/2005.00436">Bipartite Flat-Graph Network for Nested Named Entity Recognition</a></li></ul><p>将引入的词汇作为额外的链路，与原始序列一起构建成输入图，字作为节点，链接是关系，然后通过对图进进行建模获得图节点的嵌入式表征，最后使用CRF进行解码。</p><ul><li>FLAT：<a href="https://zhuanlan.zhihu.com/p/391560782?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=30098367447040&amp;utm_campaign=shareopn">Chinese NER Using Flat-Lattice Transformer</a>（ACL2020）</li></ul><p>FLAT的基本思想来源于Lattice-LSTM，Lattice-LSTM采取的RNN结构无法捕捉长距离依赖，同时引入词汇信息是有损的，同时动态的Lattice结构也不能充分进行GPU并行。为解决<strong>计算效率低下、引入词汇信息有损</strong>的这两个问题，FLAT基于Transformer结构进行了两大改进：</p><p><strong>改进1：Flat-Lattice Transformer，无损引入词汇信息</strong>。FLAT不去设计或改变原生编码结构，设计巧妙的位置向量就融合了词汇信息。具体来说，对于每一个字符和词汇都构建两个head position encoding 和tail position encoding，词汇信息直接拼接到原始输入序列的末尾（避免了引入额外的链路，增加模型复杂度），并用位置编码与原始输入序列的对应位置相关联，间接指明了添加词汇所在的位置信息。</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1guw6m1fl6zj61e40q4aep02.jpg" alt="image-20210928112926373"></p><p><strong>改进2：相对位置编码，让Transformer适用NER任务</strong></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1guw6maoktcj61dy0pwte802.jpg" alt="image-20210928112942573"></p><ul><li>Lex-BERT: Enhancing BERT based NER with lexicons（2021）</li></ul><p>Lex-BERT相比于FLAT有三点：1. 不需要利用word embedding；2. 可以引入实体类型type信息，作者认为在领域内，可以收集包含类型信息的词汇；3. 相比FLAT，Lex-BERT推断速度更快、内存占用更小；</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1guw6nfex6lj61ea0q2wjv02.jpg" alt="image-20210928113047712"></p><ul><li>Simple-Lexicon</li></ul><p><a href="https://ningshixian.github.io/2020/09/04/论文笔记-Simplify-the-Usage-of-Lexicon-in-Chinese-NER/">博客：Simplify the Usage of Lexicon in Chinese NER</a></p><p>词汇信息是有用的，但是如何使用，学术界还未形成统一。可以看得出来，上述文章在引入词汇的方式上五花八门，计算复杂度都比较高。Simple-Lexicon该篇论文直击痛点，对于词汇信息的引入更加简单有效，采取静态加权的方法可以提前离线计算。作者首先分析列举了几种引入词汇信息方法；最终论文发现，将词汇的信息融入到特殊<code>token&#123;B,M,E,S&#125;</code>中，并和原始词向量进行concat，能够带来明显的提升。通过特殊<code>token</code>表征额外信息的方式，在NER与NRE联合学习任务中也逐渐成为一种趋势。具体细节可参考<a href="https://www.bilibili.com/video/av543580471/">视频讲解</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后，我们来看一下，上述各种「词汇增强」方法在中文NER任务上的性能：</p><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/640-20230424170825693" class="" title="图片"><p>上图可以发现：总的来看，ACL2020中的FLAT和Simple-Lexicon效果最佳。具体地说：</p><ul><li>引入词汇信息的方法，都相较于baseline模型biLSTM+CRF有较大提升，可见引入词汇信息可以有效提升中文NER性能。</li><li>采用相同词表对比时，FLAT和Simple-Lexicon好于其他方法。</li><li>结合BERT效果会更佳。</li></ul><h2 id="四、评估标准"><a href="#四、评估标准" class="headerlink" title="四、评估标准"></a>四、评估标准</h2><p>NER任务的目标，通常是“尽量发现所有的命名实体，发现的命名实体要尽量纯净”，也就是要求查全率和查准率比较高。当然，场景也有可能要求其中一项要非常高。</p><p>通常通过与人类标注水平进行比较判断NER系统的优劣。评估分两种：精确匹配评估和宽松匹配评估。</p><h3 id="4-1-精确匹配评估"><a href="#4-1-精确匹配评估" class="headerlink" title="4.1 精确匹配评估"></a>4.1 精确匹配评估</h3><p>NER任务需要同时确定<strong>实体边界</strong>以及<strong>实体类别。</strong>在精确匹配评估中，只有当实体边界以及实体类别同时被精确标出时，实体识别任务才能被认定为成功。</p><p>基于数据的 true positives（TP），false positives（FP），以及false negatives（FN），可以计算NER任务的精确率，召回率以及 F-score 用于评估任务优劣。</p><p>对NER中的 true positives（TP），false positives（FP）与false negatives（FN）有如下解释：</p><ul><li>true positives（TP）：NER能正确识别实体</li><li>false positives（FP）：NER能识别出实体但类别或边界判定出现错误</li><li>false negatives（FN）：应该但没有被NER所识别的实体</li></ul><p><strong>P\R\F的计算公式如下：</strong></p><p><strong>精确率</strong>： <img src="https://www.zhihu.com/equation?tex=Precision%3D\frac{TP}{TP%2BFP}" alt="[公式]"></p><p><strong>召回率</strong>： <img src="https://www.zhihu.com/equation?tex=Recall+%3D+\frac{TP}{TP%2BFN}" alt="[公式]"></p><p><strong>F-score：</strong><img src="https://www.zhihu.com/equation?tex=2\times\frac{Precision\times+Recall}{Precision%2BRecall}" alt="[公式]"></p><p>其中 F1 值又可以分为 macro-averaged 和 micro-averaged，前者是按照不同实体类别计算 F1，然后取平均；后者是把所有识别结果合在一起，再计算 F1。这两者的区别在于实体类别数目不均衡，因为通常语料集中类别数量分布不均衡，模型往往对于大类别的实体学习较好。</p><h3 id="4-2-宽松匹配评估"><a href="#4-2-宽松匹配评估" class="headerlink" title="4.2 宽松匹配评估"></a>4.2 宽松匹配评估</h3><p>简言之，可视为实体位置区间部分重叠，或位置正确类别错误的，都记为正确或按照匹配的位置区间大小评测。</p><h2 id="五、经验总结"><a href="#五、经验总结" class="headerlink" title="五、经验总结"></a>五、经验总结</h2><p>1、提升NER性能（performance）的⽅式往往不是直接堆砌⼀个BERT+CRF，这样做不仅效果不一定会好，推断速度也非常堪忧。就算BERT效果还不错，付出的代价也是惨重的。</p><blockquote><p>就算直接使用BERT+CRF进行finetune，BERT和CRF层的学习率也不要设成一样，让CRF层学习率要更大一些（一般是BERT的5～10倍），要让CRF层快速收敛。</p></blockquote><p>2、在NER任务上，也不要试图对BERT进⾏蒸馏压缩，很可能吃⼒不讨好。</p><blockquote><p>哈哈，也许废了半天劲去蒸馏，效果下降到还不如1层lstm+crf，推断速度还是慢～</p></blockquote><p>3、NER任务是⼀个重底层的任务，上层模型再深、性能提升往往也是有限的（甚至是下降的）。</p><blockquote><p>不要盲目搭建很深的网络，也不要痴迷于各种attention了。</p></blockquote><p>4、NER任务不同的解码方式（CRF/指针网络/Biaffine<a href="https://zhuanlan.zhihu.com/p/152463745#ref_1">[1]</a>）之间的差异其实也是有限的，不要过分拘泥于解码⽅式。</p><p>5、通过QA阅读理解的方式进行NER任务，效果也许会提升，但计算复杂度上来了，你需要对同⼀⽂本进行多次编码(对同⼀文本会构造多个question)。</p><p>6、设计NER任务时，尽量不要引入嵌套实体，不好做，这往往是一个长尾问题。</p><p>7、不要直接拿Transformer做NER，这是不合适的，详细可参考TENER<a href="https://zhuanlan.zhihu.com/p/152463745#ref_2">[2]</a>。</p><p>8、1层lstm+CRF的基础上，引入更丰富的embedding特征（比如char、bigram、词典特征、词性特征、elmo等），并进行多策略组合，这大概率可以解决垂直领域的NER问题。除此之外，不要忘记一个快速提升的重要手段：<strong>规则+领域词典</strong>。</p><p>9、我们要更加稳妥地解决复杂NER问题（词汇增强、冷启动、低资源、噪声、不平衡、领域迁移、可解释、低耗时），这是一个需要权衡的过程，切记不要盲目追前沿，很多脏活累活还是要干一干的。</p><p>10、在垂直领域，如果可以预训练一个领域相关的字向量&amp;语言模型，那是最好不过的了。</p><blockquote><p>补充：<a href="https://arxiv.org/abs/1911.04474">TENER: Adapting Transformer Encoder for Named Entity Recognition</a><br>论文详细分析了为什么原始BERT模型在NER上表现不佳的原因：位置编码只具有距离感受能力，不具有方向感受能力；并在借鉴<code>XL-Net</code>的基础上，提出了相对位置编码的方法；使用相对位置编码后，明显提升了BERT在NER上的效果。</p></blockquote><h1 id="对话系统之槽位提取"><a href="#对话系统之槽位提取" class="headerlink" title="对话系统之槽位提取"></a>对话系统之槽位提取</h1><blockquote><p> <a href="https://www.6aiq.com/article/1605448505199">58 同城 | 槽位识别与纠错在智能语音机器人中的实践</a></p></blockquote><p>在对话机器人中，经常需要槽位识别来提取用户回答中的关键信息，以便用于控制对话逻辑跳转和标识对话关键词等，我们通过命名实体识别（Named Entity Recognition，NER）技术来提取对话中的实体词（关键词），即对话机器人中的槽位词。</p><p>槽位识别是一种序列标注问题，序列标注就是对给定文本的每一个字符打上标签，然后提取出我们关心的关键词，通常采用实体识别[1,2]方式实现。标签的格式通常使用两种标准：IOB2 和 IOBES。</p><h2 id="槽位提取的应用"><a href="#槽位提取的应用" class="headerlink" title="槽位提取的应用"></a>槽位提取的应用</h2><p>在公司业务场景下，我们将识别以下类别的槽位：</p><ul><li><p>客服对话中的槽位提取(Slot Filling)</p><p><strong>航道、城市、城市类型、单据号、职级序列、婚姻状况、订票类型、请假类型、分公司、*姓名</strong></p><ul><li>序列标注功能：http→image→单据号→时间→姓氏→二次姓氏→职级→手机号码→姓名→城市（不改变分词的结果）</li><li>追问判断功能：slot_set=[‘ask]</li></ul></li><li><p>冠寓录音文本的成分分析</p><p>all：联系方式（主要是<strong>电话号码</strong>）<strong>、*看房时间、预算、姓氏</strong></p></li><li><p>辅助Askdata Query中的特定实体（标签）识别/抽取</p><p><strong>ALM产品名和角色名抽取</strong></p></li><li><p>坐席知识库库名提槽</p><p>即<strong>产品名</strong>（完全匹配+模糊匹配）</p></li><li><p>未使用</p><p>C1客户描摹分析、人事员工评价、<strong>龙湖地产项目名</strong></p><p><a href="http://kuaibao.qq.com/s/20200228A09V2X00">基于词汇增强的中文命名实体识别</a></p></li></ul><h2 id="槽位提取的方法"><a href="#槽位提取的方法" class="headerlink" title="槽位提取的方法"></a>槽位提取的方法</h2><p><strong>为什么需要实体词典匹配？</strong></p><p><strong>答：</strong>主要有以下四个原因：</p><p>一是搜索中用户查询的头部流量通常较短、表达形式简单，且集中在商户、品类、地址等三类实体搜索，实体词典匹配虽简单但处理这类查询准确率也可达到 90%以上。</p><p>二是NER领域相关，通过挖掘业务数据资源获取业务实体词典，经过在线词典匹配后可保证识别结果是领域适配的。</p><p>三是新业务接入更加灵活，只需提供业务相关的实体词表就可完成新业务场景下的实体识别。</p><p>四是NER下游使用方中有些对响应时间要求极高，词典匹配速度快，基本不存在性能问题。</p><p><strong>有了实体词典匹配为什么还要模型预测？</strong></p><p><strong>答：</strong>有以下两方面的原因：</p><p>一是随着搜索体量的不断增大，中长尾搜索流量表述复杂，越来越多OOV（Out Of Vocabulary）问题开始出现，实体词典已经无法满足日益多样化的用户需求，模型预测具备泛化能力，可作为词典匹配的有效补充。</p><p>二是实体词典匹配无法解决歧义问题，比如“黄鹤楼美食”，“黄鹤楼”在实体词典中同时是武汉的景点、北京的商家、香烟产品，词典匹配不具备消歧能力，这三种类型都会输出，而模型预测则可结合上下文，不会输出“黄鹤楼”是香烟产品。</p><p><strong>实体词典匹配、模型预测两路结果是怎么合并输出的？</strong></p><p><strong>答：</strong>目前我们采用训练好的CRF权重网络作为打分器，来对实体词典匹配、模型预测两路输出的NER路径进行打分。在词典匹配无结果或是其路径打分值明显低于模型预测时，采用模型识别的结果，其他情况仍然采用词典匹配结果。</p><h2 id="槽位词纠错"><a href="#槽位词纠错" class="headerlink" title="槽位词纠错"></a>槽位词纠错</h2><blockquote><p><a href="https://www.6aiq.com/article/1605448505199">58 同城 | 槽位识别与纠错在智能语音机器人中的实践</a></p></blockquote><p>由于语音识别（Automatic Speech Recognition，ASR）技术采用的是第三方通用的语音识别，而在不同的业务中往往涉及到很多专有的名词，因此经过 ASR 后的文本很容易发生拼音正确但词识别错误的情况，或因用户口音、噪音、通话质量等问题导致 ASR 后的文本容易存在拼音相似但词不正确的问题（谐音词错误）。</p><p>为了提升对话流畅度，需要对识别出的槽位词进行纠错。例如在冠寓录音分析场景中，用户回复文本 “我星辰，尔东陈”，NER 未识别出姓氏，但其实用户想表达的是“姓陈”，通过槽位纠错可以将姓氏“星辰”纠正成“姓陈”，可以更好识别用户意图。</p><p>基于我们已有的领域知识（车品牌库、车系库及车品牌与车系的映射关系），首先对领域知识中的词进行预处理（去空格、去特殊符号、车系需要去掉车品牌前缀、大小写统一等），并将预处理后的词与原始领域词进行映射（chr 海外—&gt; 丰田 C-HR(海外)），后续所有与领域知识词进行比较，都采用预处理后的领域知识词，最终再映射到原始的领域知识词输出。</p><p>例如，如果识别出的槽位词与相应领域知识中预处理过的词完全匹配，则返回映射的原始领域词，如果不匹配，那么认为槽位词错误，需要纠正，如何纠正呢？</p><p>上述提到错误主要是由谐音或混淆音导致的，基于我们已有的车品牌、车系库，可以通过拼音相似度算法进行匹配，得到相似度最大的实体词，即认为是正确的实体词（会通过阈值来控制）。</p><p>拼音相似度算法采用编辑距离，将待纠错词与车品牌、车系库中的所有词都转成拼音，待纠错词与库中的词计算编辑距离（基于拼音的每个字母计算编辑距离），即待纠错的拼音字符串需要改动多少次才能变成目标拼音字符串，同时考虑到字符串长度的影响，再除以两个字符串拼音的长度，编辑距离越小，匹配程度越高，最终通过阈值来控制是否采纳纠错结果。</p><p>具体地，在语音机器人新车相关话术中，对于用户回复买车的文本，首先会提取出本句话所有识别出的车品牌，对错误的车品牌进行纠正，这里车品牌纠错设置的阈值比较大，尽可能使纠正后的车品牌正确，这样才能基于正确的车品牌进行车系的纠错。</p><p>车品牌纠错后，再对所有的车系进行纠错，本句话有车品牌的会基于这些车品牌下的车系进行纠错，本句话没有车品牌的会根据待纠错词的拼音去车系库中找到带有该拼音的相关车系，再进行纠错。</p><p>以车系为例，图中的车系映射表包括文字的车系映射表（h6—&gt; 哈弗 H6）和拼音的车系映射表（ei,qi,liu—&gt; 哈弗 H6），如果 ASR 后的车系文字是正确的，或车系拼音是正确的，可以直接通过车系映射表得到完整的车系名称。</p><p>车系纠错时，如果本句话有车品牌，则可以直接获取该车品牌下的所有车系进行纠错；如果本句话没有车品牌，车系需要与车系库中 3000 多的车系词进行相似度比较，耗时较大，因此这里建立车系的倒排索表，key 是车系库中每个字的拼音，value 是包含这个拼音的所有车系词，对某个词纠错时，去索引表中取每个拼音的所有相关车系词进行比较。</p><img src="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A7%BD%E4%BD%8D%E6%8F%90%E5%8F%96/image-84d8e197.png" class="" title="image.png"><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NzA5OTAwMA==&amp;mid=2650005853&amp;idx=1&amp;sn=2c6bb9e9c3751fdc3fd95e89b8b6377d&amp;chksm=bed865ca89afecdcdf0ecde9ed2385fb613cb2a40ad0c491582c7faf91841d17efdfe59718e1&amp;mpshare=1&amp;scene=1&amp;srcid=0304keVTiRXgpPHVGxGFL6mI#rd">填槽与多轮对话 | AI产品经理需要了解的AI技术概念</a></p></li><li><p><a href="[https://github.com/yuanxiaosc/Slot-Filling-and-Intention-Prediction-in-Paper-Translation/blob/master/槽填充和意图识别任务相关论文发展脉络.md](https://github.com/yuanxiaosc/Slot-Filling-and-Intention-Prediction-in-Paper-Translation/blob/master/槽填充和意图识别任务相关论文发展脉络.md"><strong>槽填充和意图识别任务相关论文发展脉络</strong></a>)</p></li><li><p><a href="https://github.com/snipsco/snips-nlu">snips-nlu</a></p></li><li><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2101.11420.pdf">2021-Recent Trends in Named Entity Recognition (NER)</a></p></li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1812.09449.pdf">2020- Survey on Deep Learning for Named Entity Recognition</a></li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1910.11470.pdf">2019-A survey on recent advances in named entity recognition from deep learning models</a></li><li><a href="https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/P18-3006.pdf">2018-Recognizing complex entity mentions: A review and future directions</a></li><li><a href="https://link.zhihu.com/?target=https%3A//www.sciencedirect.com/science/article/abs/pii/S1574013717302782">2018-Recent named entity recognition and classification techniques: A systematic review</a></li><li><a href="https://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.714.342%26rep%3Drep1%26type%3Dpdf">2013-Named entity recognition: fallacies, challenges and opportunities</a></li><li><p><a href="https://link.zhihu.com/?target=https%3A//time.mk/trajkovski/thesis/li07.pdf">2007-A survey of named entity recognition and classification</a></p></li><li><p>NER相关数据集可以参考：<a href="https://link.zhihu.com/?target=https%3A//github.com/SimmerChan/corpus">SimmerChan/corpus</a></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>槽位提取</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>对话系统之智能客服</title>
    <link href="/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E9%BE%99%E5%B0%8F%E6%B9%96%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"/>
    <url>/2021/10/31/2021-10-31-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B9%8B%E9%BE%99%E5%B0%8F%E6%B9%96%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="对话系统之智能客服"><a href="#对话系统之智能客服" class="headerlink" title="对话系统之智能客服"></a>对话系统之智能客服</h1><blockquote><p>简述对话系统的组成和逻辑，以及如何应用于智能客服</p></blockquote><p>一、对话系统的定义</p><p>二、对话系统的概述</p><p>三、对话系统的组件架构</p><p>四、智能客服的实现</p><p>五、龙小湖</p><p>对话系统开源框架</p><span id="more"></span><h2 id="一、对话系统的定义"><a href="#一、对话系统的定义" class="headerlink" title="一、对话系统的定义"></a>一、对话系统的定义</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvys9jvey1j31c20imwi0.jpg" alt="image-20211031205004867"></p><p>智能对话系统是可通过语音识别、自然语言理解、机器学习等人工智能技术，使机器理解人类语言并与人类进行有效沟通， 进而根据对人类语言中的意图进行理解并执行相应任务或做出回答的系统。智能对话系统可加载于智能硬件，基于对话交 互满足智能硬件的操作控制需求，使人机交互更加自然；智能对话系统也可赋能于服务场景，以文本机器人、语音机器人、 多模态数字人、智能质检和坐席辅助等对话机器人产品形式服务于客服、营销、企业信息服务等场景。</p><h2 id="二、对话系统的概述"><a href="#二、对话系统的概述" class="headerlink" title="二、对话系统的概述"></a>二、对话系统的概述</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvw767ph3kj31bk0k6n3b.jpg" alt="image-20211029150909153"></p><p>常见的对话系统可分为三类： 聊天型，任务导向型和问答型：</p><ul><li><strong>闲聊型</strong>：通常是不关注某项特定任务，它的主要的目标是和人进行开放领域的对话，关注点是生成流畅、合理且自然的回复。</li><li><strong>任务型</strong>：通常是帮助用户完成某项任务指令，如查找酒店、查询订单状态、解决用户的退款申请等等。用户的需求通常比较复杂，需要通过多轮交互来不断收集任务所需的必要信息，进而根据信息进行决策，执行不同的动作，最终完成用户的指令。</li><li><strong>问答型</strong>：侧重于一问一答，即直接根据用户的问题给出精准答案。问答型和任务型最本质的区别在于，系统是否需要维护一个用户目标状态的表示和是否需要一个决策过程来完成任务。</li></ul><p>在技术实现上，通常又可以划分为检索式、生成式和任务式：</p><ul><li><strong>检索式</strong>：主要思路是从对话语料库中找出与输入语句最匹配的回复，这些回复通常是预先存储的数据。</li><li><strong>生成式</strong>：主要思路是基于深度学习的Encoder-Decoder架构，从大量语料中习得语言能力，根据问题内容及相关实时状态信息直接生成回答话术。</li><li><strong>任务式</strong>：就是任务型对话，通常要维护一个对话状态，根据不同的对话状态决策下一步动作，是查询数据库还是回复用户等等。</li></ul><p><strong>检索式 vs. 生成式 的优劣：</strong></p><ul><li><strong>生成模型</strong>的对话更灵活，更像人，但是依赖的语料数据的训练、生成的语言结构等不是特别完善，容易出现语言结构混乱语无伦次看不懂的问题；</li><li>而<strong>检索匹配式</strong>，需要更多的精力去维护问答知识库，同时也会存在意图识别不准，问东答西的情况，但由于实现逻辑相较而言容易实现，且问答是可控的，不会出现语言结构错乱的问题。</li></ul><p>一图总结如下：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvzuru34hqj30kb0gzack.jpg" alt="2bc2fd8b4804aed78b220effac34ad32"></p><p>闲聊、问答、任务型对话本质都是在被动地响应用户需求。在具体业务中还会有问题推荐、商品推荐等来主动引导用户交互。在<a href="https://tech.meituan.com/2021/09/30/artificial-intelligence-customer-service.html">美团的业务</a>场景里主要是任务型和问答型，中间也会穿插一些闲聊，闲聊主要是打招呼或者简单情绪安抚，起到润滑人机对话的作用。</p><h2 id="三、对话系统的组件架构"><a href="#三、对话系统的组件架构" class="headerlink" title="三、对话系统的组件架构"></a>三、对话系统的组件架构</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvw7ax8z3kj31cu0lmwl5.jpg" alt="image-20211029151342858"></p><p>用户说话，对话系统的语音识别器（ASR）将输入转为文本，文本由自然语言理解组件（NLU）进行语义理解，接着对话管理器（DM）分析语义信息，保持对话的历史与状态，并管理对话的一般流程；自然语言生成器（NLG）根据对话管理器的对话策略生成对话的文本，最后文本通过语音合成器（TTS）渲染输出；</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvw7jir943j31cg0mgahf.jpg" alt="image-20211029152159074"></p><p>对话系统的关键是要让计算机“理解”并“生成”自然语言，主要涉及到NLP的三大核心任务：</p><ol><li><strong>自然语言理解：</strong>NLU（Natural Language Understanding）产生适合对话任务的语义表示（语义表示常见有一阶逻辑、语义网络、概念依存、基于框架的表示），主要通过分词、词性标注、命名实体识别、句法分析、指代消解等进行语义解析产生句子意义（即理解文本是什么意思），进行意图识别（一般通过动宾短语，事件提及，比如查询天气），从中抽取槽的填充值，进而完成语义表示；</li><li>对话管理器：DM（Dialog Management）为对话系统的主体，控制着对话的架构和结构，从ASR/NLU组件接受输入，维护一些状态，与任务管理器（知识库）交互，并将输出传递给NLG/TTS模块；</li><li>自然语言生成与语音合成：NLG（Natural Language Generation）组件选择需要向用户表达的概念，计划如何用词句表达这些概念，并赋予这些词必要的韵律，TTS（Text To Speech）组件接受这些词句及其韵律注解，并合成波形图，生成语音；NLG模块仍广泛采用传统的基于规则的方法，根据规则可以将各个系统动作映射成自然语言表达。 </li></ol><p>自然语言处理技术的处理流程需经过获取语料、语料预处理、特征工程、特征选择与模型训练，实现机器与人的对话交互。近年来，深度学习技术的大量使用和不断突破极大推动自然语言处理技术的落地发展。</p><h2 id="四、智能客服的实现"><a href="#四、智能客服的实现" class="headerlink" title="四、智能客服的实现"></a>四、智能客服的实现</h2><h3 id="智能客服是什么"><a href="#智能客服是什么" class="headerlink" title="智能客服是什么"></a>智能客服是什么</h3><p>智能客服背后的技术主要是以对话交互技术为核心，它能让大部分简单的问题得以快速自助解决，让复杂问题有机会被人工高效解决。</p><p><img src="https://p0.meituan.net/travelcube/64f2e7e5ed14850917373612adb8b24f267239.png" alt="img"></p><p>用户的沟通对象可能有两个，除了跟机器人沟通外，还可能跟人工沟通。机器人的能力主要包括问题推荐、问题理解、对话管理以及答案供给。</p><p>目前，衡量机器人能力好坏的评价一般有外在和内在的评价指标，外在指标指的是我们业务可见的一些指标，比如智能客服的问题解决率（满意率）和转人工率，分别衡量问题解决的好坏，以及能帮人工处理多少问题；内在指标指的是模型算法的一些指标，信息检索常见的评价指标：准确率（precision）、召回率（recall）、F1值等。可根据具体业务场景选取适合的评价指标。而在人工辅助方面，我们提供了话术推荐和会话摘要等能力，核心指标是ATT和ACW的降低，ATT是人工和用户的平均沟通时长，ACW是人工沟通后的其它处理时长。</p><h3 id="智能客服——多轮对话"><a href="#智能客服——多轮对话" class="headerlink" title="智能客服——多轮对话"></a>智能客服——多轮对话</h3><p>理解了用户意图后，有些问题是可以直接给出答案解决的，而有些问题则需要进一步厘清。机器人对话示例：</p><p><img src="https://p0.meituan.net/travelcube/2474ae90ac83e7955f72509c5326ed18419810.png" alt="img"></p><p>这个例子背后的机器人是怎么工作的呢？首先当用户输入“如何联系骑手”的时候，问题理解模块将它与知识库中的拓展问进行匹配，进而得到对应的标准问即意图“如何联系骑手”。然后对话管理模块根据意图“如何联系骑手”触发相应的任务流程，先查询订单接口，获取骑手电话号码，进而输出对话状态给到答案生成模块，根据模板生成最终结果，如右边的红框内容所示。在这个过程中涉及到要先有意图体系、定义好Task流程，以及订单的查询接口，这些都是业务强相关的，主要由各业务的运营团队来维护。那么，对话系统要做的是什么呢？一是将用户的输入与意图体系中的标准问进行匹配，二是完成多轮交互里面的调度。</p><p><img src="https://p0.meituan.net/travelcube/0c9c76ca8c23d8b0289ce984b9b96da7398937.png" alt="img"></p><p>问题理解是将用户问题与意图体系进行匹配，匹配到的拓展问所对应的标准问即用户意图。机器人的工作过程实际是要做<strong>召回和精排</strong>两件事情。召回更多地是用现有检索引擎实现，技术上更多地关注精排。</p><p>此外，一些场景需要先行确定用户要咨询的是哪个业务，这里的一个任务是“判断用户Query是属于哪个业务”，该任务我们叫做<strong>领域识别</strong>。若能明确判断领域时，则直接用该领域知识来解答；当不能明确判断时，则还需要多轮对话交互与用户进行澄清。</p><h3 id="智能客服——问题推荐"><a href="#智能客服——问题推荐" class="headerlink" title="智能客服——问题推荐"></a>智能客服——问题推荐</h3><p>当用户进入服务门户后，机器人首先是要如何引导用户精准地表达需求，这样即可降低用户迷失或者直接转人工服务，也降低了若机器人不能正确理解时带来的多轮澄清等无效交互。</p><p>该问题是一个标准的曝光点击问题，它的本质是推荐问题。</p><h3 id="智能客服——评价体系"><a href="#智能客服——评价体系" class="headerlink" title="智能客服——评价体系"></a>智能客服——评价体系</h3><p><a href="https://zhuanlan.zhihu.com/p/27044501">你问我答之「智能客服评价体系」</a></p><h2 id="五、龙小湖⭐️"><a href="#五、龙小湖⭐️" class="headerlink" title="五、龙小湖⭐️"></a>五、龙小湖⭐️</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvysmzna8hj31d20he78j.jpg" alt="image-20211031210259194"></p><p>龙小湖智能客服偏向于<strong>【检索式的FAQ问答型对话系统】</strong>：FAQ（Frequently Asked Questions）指常见问题的解答，具体形式是问题和与问题相关的答案组成的问答对（QA pair），通常这类 QA pair 数量较多。</p><p>当用户输入一个问题，龙小湖智能客服会去FAQ知识库里面找一个和客户的问题最相似的问题（Q），然后将该Q所对应的回答（A）返回给客户，完成知识问答工作。</p><p>整个流程主要是QQ 匹配，也即使用用户的问题和问答库里的问题去匹配。涉及到传统文本匹配方法（信息检索中的BM25， 向量空间模型VSM）、深度语义匹配方法（孪生网络、交互网络 ）、度量学习方法（margin-based softmax、对比学习）等。</p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><blockquote><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ%3D%3D&amp;chksm=e882535fdff5da49cec5450cd692a29562eb929e49217d482fc9e51eb993e2c7387549c87c33&amp;idx=1&amp;mid=2247486401&amp;scene=21&amp;sn=1c4b9be4b359ed4979e3f892d6e55975#wechat_redirect">心法利器[13] | 任务方案思考：句子相似度和匹配</a></p></blockquote><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwfze2t8hhj31n00syq6k.jpg" alt="image-20211115175111233"></p><ol><li>预处理模块：分词、实体识别、领域识别、句法分析、关键词抽取、同义词扩展、纠错等基本文本处理流程；</li><li><p>检索召回模块：通过同义词召回、传统BM25召回和语义召回，从 FAQ 库召回与 Query 相关的问题；</p></li><li><p>排序模块：通过 LTR 模型或者文本相似度模型对召回的问题进行排序，选出 Top k 返回；</p></li><li>最后是业务规则、流量干预等进行顺序调整；</li></ol><p>在智能客服的框架中，最重要的模块是 FAQ 问答库的构建、语义召回、相似度模型和模型更新，它们性能的好坏对用户的使用体验有很大影响。</p><h3 id="问答知识库构建"><a href="#问答知识库构建" class="headerlink" title="问答知识库构建"></a>问答知识库构建</h3><blockquote><p> <a href="https://mp.weixin.qq.com/s/1NHuDn62G1SnATNjNWWuEA">智能客服新三网场景问答模型优化实践</a></p></blockquote><p>QABot是基于问答知识库来实现的，问答知识库是由一系列的问题-答案对组成的数据集。我们将问题划分为标准问题和扩展问题，标准问题是对同一个语义的所有扩展问的抽象。如：”如何发布帖子”，”怎么样发布帖子”，”个人中心里是怎么发布帖子的呢？”等这些问题都表达的是同一个意思，这样我们就可以将”如何发布帖子”抽象为一个标准问，其他相似问题作为该问题的扩展问。</p><p>由于我们的问答引擎是基于知识库来实现的，所以知识库的构建就显得非常重要，那么我们是怎么构建知识库的呢？</p><ol><li><p>首先我们会对场景历史积累的问题进行整理，经过抽象、标注形成标准问和扩展问，同时对于扩展问较少的问题使用相关技术做问题扩展，形成初始问答知识库。</p></li><li><p>然后当系统上线以后，我们会不断的对场景进行问题挖掘，再经过标注整理来不断的扩充知识库。</p></li><li>半自动化挖掘新问题扩展知识库。从线上问答日志中挖掘用户提问，TextCNN新问题分类模型筛选出新标准问题，经过提前训练好的Word2Vec获取句向量，并用Kmeans聚类的方法对新标准问题进行聚类，将语义相同新标准问题聚为一类。最后将聚类结果交给编辑团队去审核，最终由编辑团队归纳总结出新的标准问题和对应的扩展问题并入知识库。</li></ol><h3 id="候选问题的语义召回"><a href="#候选问题的语义召回" class="headerlink" title="候选问题的语义召回"></a>候选问题的语义召回</h3><p>由于 FAQ 库里面的 QA pair 可能成千上万，如果每接收一个 query 就将其和库里所有的Q进行匹配计算相似度的话，那给用户的响应速度是不可忍受的；因此需要先从 FAQ 库里召回部分与 query 相关的候选 Q，然后再使用文本匹配模型计算 query 和候选 Q 的相似度。</p><p>通常的做法是将 FAQ 里面的 Q 存入 Elasticsearch 这样的搜索引擎中来做粗略的召回，但是 es 检索召回只能召回和 query 字面意思相关的候选 Q。另外可以使用表示型文本匹配模型提取 FAQ 库中的所有 Q 的语义向量，使用Faiss 或者 annoy 工具做向量索引，这样就能做语义层面的召回了。其实更进一步，我们可以将所有 Q 先按大类归纳分类，然后再将这些分好类的 Q进行向量索引，在召回时先使用分类模型对 query 进行分类，然后在相对应大类的向量索引里进行语义层面的召回。</p><h3 id="相似度模型"><a href="#相似度模型" class="headerlink" title="相似度模型"></a>相似度模型</h3><p>在上文我们已经提到了使用表示型文本匹配模型进行语义召回，表示型文本匹配模型其实就是一个相似度模型了，在得到候选召回 Q 之后我们还需要做一个问题排序的工作，然后将排序最高的问题所对应的回答做用户 query 的回答，这里需要使用一个比表示型文本匹配模型性能更高的相似度模型来计算query 和候选 Q 之间的相似度来作为排序模型的特征。我们可以使用交互式的文本匹配模型来完成这个工作。</p><h1 id="对话系统开源框架"><a href="#对话系统开源框架" class="headerlink" title="对话系统开源框架"></a>对话系统开源框架</h1><p><strong>ChatterBot</strong> ❤❤❤</p><blockquote><p>8K+ star： <a href="https://github.com/gunthercox/ChatterBot">https://github.com/gunthercox/ChatterBot</a></p></blockquote><p>ChatterBot是一个Python库，使用一系列机器学习算法来生成不同类型的回答。 （主要是 <strong>匹配式</strong>）</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> chatterbot<br></code></pre></td></tr></table></figure><p>程序通过搜索与输入匹配的最接近的匹配已知语句来选择最接近的匹配响应，然后从选择对该语句的已知响应中选择响应。</p><p><strong>rasa</strong> ❤❤❤</p><blockquote><p>对话管理：<a href="https://www.jianshu.com/p/515385a7c7f0">https://www.jianshu.com/p/515385a7c7f0</a></p><p>rasa中文：<a href="https://jverson.com/2018/09/11/rasa-nlu/">https://jverson.com/2018/09/11/rasa-nlu/</a></p></blockquote><p>使用rasa_nlu 理解用户的意图（预定义好） ，纯任务型对话。</p><ul><li>意图识别</li><li>实体识别</li></ul><p><strong>DeepQA</strong> ❤</p><p>DeepQA项目试图重现神经会话模型的结果（又名谷歌聊天机器人），它采用了RNN（seq2seq模型）判刑预测，使用Python和TensorFlow实现。</p><p>如果需要自研 闲聊对话系统，可以参考这个。 但是需要大量中文对话数据，在数据来源方面，没有调研。</p><p><strong>DeepPavlov❤❤❤❤</strong></p><blockquote><p>3.2K star <a href="https://github.com/deepmipt/DeepPavlov">https://github.com/deepmipt/DeepPavlov</a></p><p>参考：<a href="https://zhuanlan.zhihu.com/p/58133705">https://zhuanlan.zhihu.com/p/58133705</a></p></blockquote><p>开源的对话 AI 库，建立在 TensorFlow 和 Keras 上。<br>DeepPavlov框架层次：</p><p><a href="http://ww1.sinaimg.cn/large/006qDjsOly1g4form1ztgj30k0044wen.jpg"><img src="http://ww1.sinaimg.cn/large/006qDjsOly1g4form1ztgj30k0044wen.jpg" alt="img"></a></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://tech.meituan.com/2021/09/30/artificial-intelligence-customer-service.html">美团智能客服核心技术与实践</a></p><p><a href="http://www.woshipm.com/ai/1764361.html">对话系统的简单综述及应用智能客服</a></p><p><a href=""> <strong>《自然语言处理实践：聊天机器人技术原理与应用》</strong>  王昊奋，邵浩 等 著</a></p><p><a href="http://www.sohu.com/a/302618857_99979179">NLP实践：对话系统技术原理和应用 </a></p><p><a href="https://cn.100offer.com/blog/posts/296">NLP 算法工程师的学习、成长和实战经验 | 知乎 Live 笔录 </a></p><p><a href="https://mp.weixin.qq.com/s/5ewD2xD8J08W89-Rwixw4Q">回顾·五八同城智能客服系统“帮帮”技术揭秘</a></p><p><a href="https://mp.weixin.qq.com/s/o7x1BigfIMJIJEGxlFlLow">58智能客服QABot问答机器人算法实践</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzAwOTgwMjQ4OQ==&amp;mid=2649990182&amp;idx=1&amp;sn=11a98d9cecd0deecb6400d7288b793e6&amp;chksm=835d03b0b42a8aa62d084f11f7382923267fc81ea2efcc20dce82a0e89e9e7540027b922900e&amp;scene=21#wechat_redirect">大话知识图谱—聊聊智能客服</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>对话系统</tag>
      
      <tag>智能客服</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>58智能客服QABot</title>
    <link href="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/"/>
    <url>/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/</url>
    
    <content type="html"><![CDATA[<p>本文根据58同城AI Lab负责人詹坤林在DataFunTalk人工智能技术沙龙所分享的《五八同城智能客服系统“帮帮”技术揭秘》编辑整理而成，在未改变原意的基础上稍做整理。</p><span id="more"></span><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/0ceaeb9e8ff4455d99a46638c181aa1b.webp" class="" title="img"><p>“帮帮”整体技术架构如图所示，包括基础服务层、应用服务层、编辑运营层、接入层以及在线客服系统。基础服务层提供对话系统的基础技术能力，系统需要对用户输入的一段语句进行理解，这里需要自然语言理解模块，对语句进行分词、词性标注、实体识别、关键词抽取和句法分析等；同时需要识别用户的意图，包括通用意图和业务意图，通用意图是指用户是来做业务咨询还是闲聊，业务意图是指若用户是做业务咨询，具体咨询什么业务，这里会使用文本分类的技术去识别用户意图。基础服务之上是应用服务层，这一层具体实现了KB-Bot基于问答知识库的机器人、Task-Bot任务对话型机器和Chat-Bot闲聊类型机器人，这是“帮帮”系统的三种核心能力。编辑运营层是指有一个编辑团队支撑着“帮帮”的算法策略迭代，主要完成数据标注、问答运营、数据分析和效果评估的工作，这些工作输出会作用到基础服务层和应用服务层。基于应用服务层，对外提供通用的接口服务以便于业务方接入，我们支持Android、iOS和web端的接入。此外，机器不是万能的，用户有很多复杂的问题仍需要人工解决，这里有一套在线客服系统提供了人工在线客服的能力，应用服务层会和这套在线客服系统做无缝对接。</p><h2 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h2><p>“帮帮”系统的核心是提供KB-Bot、Task-Bot和Chat-Bot三种能力，下面分别介绍下这里使用到的技术。</p><h3 id="KB-Bot"><a href="#KB-Bot" class="headerlink" title="KB-Bot"></a>KB-Bot</h3><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/da279ad185da4f5f9185266ef2daa7bc.webp" class="" title="img"><p>KB-Bot是指基于问答知识库的对话机器人，它主要实现了“帮帮”最重要的能力——提供业务咨询类服务。58的用户使用帮帮主要是来进行业务咨询，例如询问账号为何被锁、帖子为何被删、如何购买帖子置顶服务等等。业务咨询类的回答需要基于问答知识库来实现，这里的问答知识库是一个包含众多问答对的数据集。我们将问题划分为标准问题和扩展问题，例如“为什么删除我的帖子”这个是一个标准问题，语句表达很标准，它会有一个标准答案，其近似的问法我们称之为扩展问题，例如“为什么删我贴”、“告诉我为啥删帖”等，这些都表达的是一个意思，这些问题同样对应的是相同的标准答案。有了问答知识库，用户来询问时就是一个问题匹配的过程了，只需要将用户输入的问题和知识库中的问题做匹配，得到意思最相近的那条问题，然后将对应的答案返回给用户，这就完成了一次问答操作。问答知识库的构建非常关键，这里会首先对客服团队历史积累的问题数据进行抽象，形成标准问题，然后结合算法和标注对标准问题做扩展，形成初始问答知识库，在系统上线后，对新产生的数据又会进行挖掘，不断扩充知识库。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/c24373f56b114a1aaf5d7cbb98c2f4b2.webp" class="" title="img"><p>基于知识库的问答可以使用<strong>检索或者分类模型</strong>来实现。</p><p>检索式回答的流程是：首先对用户的输入问题做处理，如分词、抽取关键词、同义词扩展、计算句子向量等；然后基于处理结果在知识库中做检索匹配，例如利用BM25、TF-IDF或者向量相似度等匹配出一个问题集合，这类似推荐系统中的召回过程；由于我们是一个问答系统，最终是直接返回给用户一个答案，因此需要从问题集合中挑出最相似的那个问题，这里会对问题集合做重排序，例如利用规则、机器学习或者深度学习模型做排序，每个问题会被打上一个分值，最终挑选出top1，将这个问题对应的答案返回给用户，这就完成了一次对话流程。在实际应用中，我们还会设置阈值来保证回答的准确性，若最终每个问题的得分低于阈值，会将头部的几个问题以列表的形式返回给用户，最终用户可以选择他想问的问题，进而得到具体的答案。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/22289626302e48808feff6b8ca7b65a3.webp" class="" title="img"><p>这里还可以使用分类模型来实现问答，一个标准问题有多种扩展问法，每个标准问题可以看做是一个分类，将用户的输入映射到标准问题上即可完成回答，因此可以将问答看做是一个大规模短文本分类的问题。我们采用了多特征、多模型、多分类结果融合的方式来完成短文本分类，在特征层尝试使用了单字、词、词性、词语属性等多种特征，在模型层应用了FastText、TextCNN和Bi-LSTM等模型，各模型的结果输出最终会做融合得到最终分类结果。</p><h3 id="TaskBot"><a href="#TaskBot" class="headerlink" title="TaskBot"></a>TaskBot</h3><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/3a0d8b00b38cfb04b3bd3d2a36b8cfb1.png" class="" title="img"><p>TaskBot 是一类帮助用户完成特定任务的聊天机器人。借鉴通用的 Frame 数据结构，我们采用意图+词槽的组合对用户 query 进行知识表示。在 58 场景下，我们将用户提供的特定词槽值称作商机，TaskBot 的目标即是引导用户提供商机</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/a00yy08d2e49dc5d4c1c3e6f2c084746.png" class="" title="img"><p>通过学习人工客服的会话，我们发现人工客服回复存在固定的模式，一般人工客服会先回答用户的提问，再根据需要问的信息对用户进行反问。在这个基础上，我们选用了结合一问一答的 QABot 和商机引导多轮 Taskbot 的方案，实现类似人工客服回答模式的商机引导过程，目前已在房产等多个业务线落地。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/a540fcb5dfcf0735e8a48c4b453859d7.png" class="" title="img"><p>举个例子，在实际会话中，用户的第一个问题命中某标准问题，此时我们先查询答案库，获取到一问一答（QABot）的答案，同时，我们根据 TaskBot 的触发条件判断，如果触发 TaskBot，则继续回复 TaskBot 的反问。</p><p>在触发 TaskBot 后，用户在 TaskBot 的依次引导下，逐步透露商机，此时 QABot 不会命中标准问题，TaskBot 引导用户直到对话结束。</p><h2 id="核心模型迭代"><a href="#核心模型迭代" class="headerlink" title="核心模型迭代"></a>核心模型迭代</h2><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/2b2cc25d2c5a9b82d5d786f119486403.png" class="" title="img"><p>大家可以看到，问答模型的效果对用户体验非常重要。为提升问答模型效果，达到 F1 值大于 0.8 的目标，我们结合线上数据进行了多次模型迭代。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/1d477b696ae76c28f56bdd40868ec717.png" class="" title="img"><p>在项目初期，我们使用 FastText 作为基准，快速得到问答模型。选择 FastText 这个浅层神经网络，一方面是考虑网络的训练和预测速度快，可以快速验证数据是否存在异常；另一方面是 NGram 能够快速捕捉关键词特征，在一些关键词比较明显的类别上能取得不错的效果。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/db28840f4f26d4e022f45623de8b5b32.png" class="" title="img"><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/461d462b2df174b31b0afffb541c4d8b.png" class="" title="img"><p>DSSM 作为一个文本匹配模型，其优势在于对样本少的类别也有较好的匹配效果。句子的表示方法对 DSSM 模型的效果有显著影响。这里我们先试用了 LSTM 模型对句子进行表示。相对于 NGram 而言，LSTM 更能够提取出句中的长程特征，提升了样本中长句子的识别效果。其次我们也对 LSTM+NGram 特征的结合进行了尝试，使得模型在有效处理长句的同时，也能不遗漏关键词特征。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/58858e06e4314b2fd2905f1575018ec9.png" class="" title="img"><p>58 微聊场景下积累了大量的无监督数据，在预训练模型出现后，我们得以进一步挖掘这些数据的潜力。实践中，BERT 模型作为此类半监督模型的代表，在分类任务中取得了良好的效果。BERT 的主要特点是预训练掩码语言模型+微调两个步骤的结合，同时为了配合 Transformer encoder 的使用，使用了位置编码机制。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-58%E5%90%8C%E5%9F%8E/fc68e3f3b6f85b10d77d53a828ab8b46.png" class="" title="img"><p>SPTMs 这个轻量级的预训练模型实现比较早，BERT 在 18 年 10 月底发布，这个模型是 19 年 4 月实现的。这里当时看主要创新点是去掉 BERT 的 NSP 任务，另外替换成 Bi-LSTM 也是做一种尝试（实际也可以用一层的 Transformer），在机器资源有限情况下让预训练变得容易。如果直接用 BERT 去预训练，耗时很久，而 SPTM 的推理耗时较低，可以在 CPU 上直接跑，十几到几十毫秒。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.6aiq.com/article/1536149308075">五八同城智能客服系统“帮帮”技术揭秘</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>58</tag>
      
      <tag>智能问答系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OPPO小布深度语义问答FQA技术实践</title>
    <link href="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-OPPO%E5%B0%8F%E5%B8%83/"/>
    <url>/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-OPPO%E5%B0%8F%E5%B8%83/</url>
    
    <content type="html"><![CDATA[<p>OPPO 旗下的人工智能助手“小布助手”月度活跃用户数突破一亿，成为国内首个月活用户数破亿的手机语音助手。</p><p>经过 2 年多的成长，小布助手在能力上实现大幅升级，也融入了我们身边便捷的服务功能。小布团队亦克服了诸多技术难点，为用户带来了更智能的服务。为此，小布团队撰写了一系列文章，详细介绍小布助手背后的技术支撑，本文是揭秘小布背后技术的第三篇。</p><p>第一篇：<a href="https://segmentfault.com/a/1190000040348269">对话系统简介与OPPO小布助手的工程实践</a></p><p>第二篇：<a href="https://segmentfault.com/a/1190000040382789">小布助手算法系统的探索、实践与思考</a></p><span id="more"></span><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在智能客服、语义问答的业务中，更注重的是query与query之间的匹配，即短文本之间的相似度计算。长文本匹配更多注重文本的关键词或者主题的匹配，业界使用的较多的算法如：TF-IDF、LSA、LDA；而短文本匹配更多的是句子整体的语义一致性，业界较为主流的算法有：word2vec、esim、abcnn、bert等深度模型。相比于长文本的相似度计算，短文本的相似度计算存在更大的挑战。其一，短文本可以利用的上下文信息有限，语义刻画不够全面；其二，短文本通常情况下，口语化程度更高，存在缺省的可能性更大；第三，短文本更注重文本整体语义的匹配，对文本的语序、句式等更为敏感。</p><div class="table-container"><table><thead><tr><th><strong>query1 </strong></th><th><strong>query2 </strong></th></tr></thead><tbody><tr><td>我要打给你</td><td>我要打你</td></tr><tr><td>你叫什么</td><td>你叫我什么</td></tr><tr><td>我叫小布</td><td>我不叫小布</td></tr><tr><td>你有男票吗</td><td>你是单身狗吗</td></tr><tr><td>你真搞笑</td><td>你是个逗比啊</td></tr><tr><td>我喜欢看动漫</td><td>你不知道我喜欢看动漫吗</td></tr></tbody></table></div><p>业界常用的短文本相似度计算方案大致可以分为两类：监督学习与无监督学习，通常情况下，监督学习效果相对较好。在没有足够的训练数据需要冷启动的情况下，可优先考虑使用无监督学习来进行上线。</p><p><strong>无监督学习</strong></p><p>最简单有效的无监督学习方案就是预训练的方式，使用word2vec或者bert等预训练模型，对任务领域内的无标签数据进行预训练。使用得到的预训练模型，获取每个词以及句子的语义表示，用于相似度的计算。</p><p>Word2vec是nlp领域一个划时代的产物，将word的表征从离散的one-hot的方式转化成连续的embedding的形式，不仅降低了计算维度，各个任务上的效果也取得了质的飞跃。Word2vec通过对大规模语料来进行语言模型（language model）的建模，使得语义相近的word，在embedding的表示上，也具有很强的相关性。</p><p>通过cbow或者max-pooling的方式，使用句子中每个词的word embedding计算得到sentence embedding，可以使得语义相似的句子在sentence embedding的表示上也具备较高的相关性，相比于传统的TF-IDF等相似度计算具有更好的泛化性。但是cbow的方式来计算sentence embedding，句子中所有word使用相同的权重，无法准确获取句子中的keyword，导致语义计算的准确率有限，难以达到上线标准。</p><p>虽然Word2vec提供了一定的泛化性，但其最大的弱点是在不同的语境下，同一个word的表征完全相同，无法满足丰富的语言变化。gpt、bert等大规模预训练模型的出现，彻底解决了这个问题，做到了word的表征与上下文相关，同时也不断刷新了各个领域任务的榜单。</p><p>但实验证明直接使用bert输出的token embedding来计算句子的sentence embedding，无论使用cbow的方式对所有token embedding求平均或者直接使用[CLS] token的embedding来表示，语义计算的效果都不佳，甚至不如GloVe。究其原因，在bert的预训练过程中，高频词之间共现概率更大，MLM任务训练使得它们之间语义表征更加接近，而低频词之间的分布更为稀疏。语义空间分布的不均匀，导致低频词周围中存在很多语义的“hole”，由于这些“hole”的存在，导致语义计算的相似度存在偏差。</p><p><img src="https://oscimg.oschina.net/oscnet/9ca67f87-1a66-41a4-a2b2-fd7834adb43e.png" alt="img"></p><p>为了解决bert语义空间不均匀的问题，CMU与字节跳动合作的bert-flow提出将bert的语义空间映射到一个标准的高斯隐空间，由于标准高斯分布满足各向同性，区域内不存在“hole”，不会破坏语义空间的连续性。</p><p><strong>监督学习</strong> </p><p>Bert-flow的出现使得无监督学习在文本相似度计算方面取得了较大进步，但是在特定任务上相比于监督学习，效果还存在一定的差距。监督学习常用的相似度计算模型大致可以分为两类：语义表征模型，语义交互式模型。语义表征模型常用于海量query召回，交互式模型更多使用于语义排序阶段。</p><h2 id="深度语义问答FQA技术实践"><a href="#深度语义问答FQA技术实践" class="headerlink" title="深度语义问答FQA技术实践"></a>深度语义问答FQA技术实践</h2><p>语义匹配问答架构主要分为拒绝策略层、Query预处理、智能改写包括代词指代消解、同义词改写以及后续规划的语义递进或转折场景下的完整性改写、语义检索、语义排序、语义消歧等可配置模块，可配置的策略模块用于快速解决线上高优BADCASE问题，同时小布助手在百度语义匹配赛道冲击过第一名。</p><p><img src="https://segmentfault.com/img/bVcTN2H" alt="img"></p><p><strong>语义召回模型</strong>——采用<strong>SiameseNetwork</strong>框架，使用分类任务建模思路进行训练，且训练样本数据范式采用listwise范式，模型目标是最大化正样本之间的相关性，同时抑制负样本之间的相关性。</p><p><img src="https://segmentfault.com/img/bVcTN2M" alt="img"></p><p>相似度上，核心就在于离线需要训练一个足够靠谱的语义相似度模型，这块非常机智的参考了人脸识别的一些思路。我们希望语义相似的句子能分布在一个比较集中的空间内，而语义不同的则能差的足够远，按照人脸识别领域的经验，通过损失函数引导模型往这方面学，则有很大的收益了。如果只是平行样本（q1,q2,label），那训练起来信息不够，参考DSSM的思路（说实话，DSSM给这块真的提供了和很多思路，不能仅要关注模型本身），<strong>可以1个anchor，若干正负样本（对比anchor）来构成一串输入样本</strong>，来进行计算，更有好处，即有了对比，A和B是相似，和C是不相似，那我们可以很有把握地定位好A的位置，从而训练好模型</p><p>语义匹配模型实践方面，对于负样本采样策略优先使用标注的负样本，标注负样本不足情况下，通过卡字面相关性阈值策略进行负采样，以保证负样本有一定相关性，尽量构建semi-hard数据集用来加速模型收敛，同时提升模型语义表征精度；损失函数方面我们主要尝试hinge loss、Triplet Loss、AMSoftMax等。</p><p><img src="https://segmentfault.com/img/bVcTN2S" alt="img"><br><img src="https://segmentfault.com/img/bVcTN2V" alt="img"></p><p>AM-softmax在余弦相似度上添加一个margin，可以提高类间可分及类别的可辨别性， 使用cnn网络，结合AM-softmax的loss函数，能够取得最好的语义表征能力，通用评测集下的准确率提升1%，同时召回提升明显。</p><p><img src="https://segmentfault.com/img/bVcTN20" alt="img"></p><p>高纬数据索引算法主要有树索引和图索引两种，通过基于同等召回率的召回性能试验对比，我们选择NGT图索引算法来进行向量索引，相对于树索引算法，具有更好的索引效果及索引性能。</p><p><img src="https://segmentfault.com/img/bVcTN27" alt="img"></p><p>语义排序算法模型主要落地实践了交互表征模型<strong>ABCNN、ESIM</strong>以及Transformer模型等，在召回阶段我们得到的候选query集合，然后基于用户当前query与候选query进行交互式表征建模能提高语义表征精度从而提高语义匹配准确率；另外ESIM模型优点在于精细设计的序列式推断结构，考虑局部推断和全局推断的融合，准召效果相对也会高一些 。</p><p><img src="https://segmentfault.com/img/bVcTN28" alt="img"><br><img src="https://segmentfault.com/img/bVcTN3a" alt="img"></p><p>做个总结，OPPO小布的检索式FAQ方案就是：<strong>使用siamese cnn语义表征模型来进行语义召回，用蒸馏后的transformer语义交互模型来做排序（自研的XBert）。</strong></p><p>在语义表征模型的loss构建上，使用了<strong>1个标准query，1个正样本，5个负样本</strong>（尝试过其他负样本数量，在我们的数据上效果不如5个负样本），训练过程其实是在这6个样本中，识别出对应正样本的位置，因此可将其<strong>转化为二分类任务</strong>来进行训练，每个正负样本分别对应一个类别。传统的softmax归一化构建的分类边界使得类别之间可分，为了更好的语义表征效果，需要使得类内更加汇聚，类间更加分散。这里参考了人脸识别领域的损失函数AM-Softmax。</p><p>在排序模型方面，尝试了ABCNN、ESIM、transformer等交互式语义模型，但效果相比于bert等预训练模型，还存在一定的差距。自研的预训练模型Xbert，在与Roberta large同规模的情况下，融入了自研知识图谱数据，添加了WWM（whole word MLM）、DAE、Entity MLM等任务，使用LAMB优化器进行优化。我们使用XBert在业务数据上进行了测试，相比于同规模的Roberta large准确率有接近0.9%的提升。为了满足上线需求，我们参考tiny bert的方式，用Xbert蒸馏了一个4层的transformer model用于线上推断。</p><h3 id="如何对不同排序方案做效果对比？"><a href="#如何对不同排序方案做效果对比？" class="headerlink" title="如何对不同排序方案做效果对比？"></a>如何对不同排序方案做效果对比？</h3><p><strong>用语义召回top1的准确率来评估语义表征模型的效果</strong>，并且通过消歧模块进一步提升应答准确率；测试排序模型效果时，我们使用了多路召回，共召回30个候选，<strong>使用排序模型对候选排序，选择排序后的top1作为最终答案</strong>。</p><p><img src="https://oscimg.oschina.net/oscnet/7ae47f60-0e1a-4cb9-a799-ee6c407002b2.png" alt="img" style="zoom:50%;" /></p><h2 id="算法服务工程"><a href="#算法服务工程" class="headerlink" title="算法服务工程"></a>算法服务工程</h2><p>算法服务架构主要基于领域分治、NLU服务多进程并发高性能的原则进行设计。关键点：</p><ol><li>业务微服务化，业务域按垂域拆分，降低耦合性、提升算法服务性能，提高敏捷迭代效率；</li><li>模型tfservering服务化，TF Serving集群管理TF模型易部署，天然支持模型热更新机制，复杂模型可利用GPU进行inference，同时通过本地模型服务实现服务降级策略。</li></ol><p><img src="https://segmentfault.com/img/bVcTN4k" alt="img"></p><p>NLU算法服务内部分层架构，多层次拒绝/召回，域外粗召回到域外高召回再到域内高准确， 通过可配置模块高效解决Case类问题，避免引入破坏模型平衡的问题，模型分布收敛到线上数据分布后保持稳定。</p><p><img src="https://segmentfault.com/img/bVcTN4m" alt="img"></p><p>服务内部分层架构快速拒绝域外请求，极大提升算法服务性能，保障小布助手端到端流畅度的性能体验。</p><p><img src="https://segmentfault.com/img/bVcTN4o" alt="img"></p><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>封闭域任务型对话技术关键挑战有多个，比如，语义理解模型的鲁棒性，特别是高频技能线上query分布变化快，如何保证线上模型泛化性是关键；对话能力更自然流畅的对话管理技术；上下文意图理解（当前已经具备人称/实体代词、部分零指代的指代消解能力，但泛化能力有限）；融合设备渠道、LBS位置信息及客户端状态等场景特征+用户基本属性+用户兴趣画像的综合意图理解。</p><p>开放域对话技术方面，检索式聊天技术能解决高频头部query问题，但解决不了长尾问题以及开放域多轮对话问题。融合知识问答、闲聊、对话推荐为一体的端到端多轮生成式，是未来ChatBot的关键技术方向。</p><p>这里也存在不少技术挑战，包括包罗万象的知识问答、情绪感知的共情能力、融合系统画像与用户画像的回复生成、逻辑/观点/人设一致性、回合制被动问答向主动式对话演进、全双工的持续对话能力。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://segmentfault.com/a/1190000040430851">对话交互：封闭域任务型与开放域闲聊算法技术</a></p><p><a href="https://my.oschina.net/u/4273516/blog/4950320">基于深度学习的短文本相似度学习与行业测评</a></p><p><a href="http://live.baidu.com/m/media/pclive/pchome/live.html?room_id=4262477703&amp;source=h5pre">OPPO小布助手-百度千言直播分享</a></p><p><a href="https://mp.weixin.qq.com/s/uYfmkebEAPqMrgAx_2qLRQ">前沿重器[7] | 小布助手登顶百度千言短文本相似度的秘诀</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>智能问答系统</tag>
      
      <tag>OPPO小布</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>来也对话机器人平台智能问答技术拆解</title>
    <link href="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E6%9D%A5%E4%B9%9F/"/>
    <url>/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E6%9D%A5%E4%B9%9F/</url>
    
    <content type="html"><![CDATA[<p>在我们的生活中，智能对话被广泛应用在客服、营销等重复性对话频繁发生的场景，或者作为GUI的补充，为用户提供高效、个性化的体验，甚至是直接集成到智能音箱、智能家居、智能导航等硬件设备中，独立承载人机交互的重担。</p><p>按照对话的智能程度，我们可以把智能问答分为5个阶段：单轮问答、多轮会话、意图推理、个性化以及情感互动。</p><p>而从问答的种类来讲，我们又可以将其分为Community QA、KBQA、TableQA、PassageQA、VQA这5大类。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/b8HeYwrn5j5BJQUWYNYz.png" alt="img"></p><span id="more"></span><h2 id="智能问答工程实践"><a href="#智能问答工程实践" class="headerlink" title="智能问答工程实践"></a>智能问答工程实践</h2><p>面对智能问答的广泛应用，本篇文章以如何搭建一套智能问答系统为切入点，深入浅出介绍一下在Community QA上所做的尝试。</p><h3 id="1-业务简介"><a href="#1-业务简介" class="headerlink" title="1. 业务简介"></a>1. 业务简介</h3><p>我们再基于几个实际的例子看一下问答的使用场景：</p><ul><li>第一类是常见的客服场景，询问产品、询问业务、询问经验，这类场景比较泛，他会衍生在生活中的各个方面，客服机器人相比传统的客服，他永不打烊，回答永远标准，而且面对各种刁钻甚至不友好的问题，都会永远积极正面地给出响应。</li><li>同时，该类问答机器人进一步深化拓宽到领域，会孵化出某些领域通用的机器人，比如HR领域，相信不同公司的HR，很大程度上面临的问题集合都是固定的，也就是说，垂直领域的问答机器人是可以有效渗透并横向复制给不同企业使用的。</li><li>在英语学习相关产品上，我们想给小孩报名英文课，但是不清楚课程内容和价格，那我们会问相关的问题。对于商家来讲，我们不仅是提问的人，还是潜在的商机，可能我们仅仅只想问一个问题，但是商家却希望能获得我们更多的信息，电话、小孩年纪、上课意向等，进而让我们留存下来最后能成功消费，类似的场景还有很多，我们把这一类应用场景叫营销机器人，它是商务团队的好帮手，能在多个平台自由切换，回答问题标准且及时，最终往往能通过更小的人力投入，获取更多的有效线索。</li></ul><h3 id="2-问答建模"><a href="#2-问答建模" class="headerlink" title="2. 问答建模"></a>2. 问答建模</h3><p>基于前面的例子，我们对问答场景有了更具体的画像，需要做什么已经很清晰了，那么怎么做呢，我们开始尝试对该问题进行建模。</p><p>首先是对知识的结构化表示：</p><ul><li>用户问题我们用q表示；</li><li>一个问答对表示一条领域知识，我们把它叫做一个知识点ki；</li><li>答案我们定义为ai；</li><li>由于一个语义的问题可能有多种不同的表述方式，因此一个知识点的问题由多个不同的表述组成，这些表述都叫相似问pij；</li><li>一个领域的知识库，由多个知识点构成。</li></ul><p><img src="http://image.woshipm.com/wp-files/2019/11/4OCoNFCjvjq31PeBEJKp.png" alt="img"></p><p>图2 知识库表示</p><p>我们为什么要用这种方式来管理知识，不用图表、也不用表格？</p><p>实际上，知识的管理方式是来源于实际业务场景的，这种方式非常易于维护，多个同义问题用一个知识点管理，也能减轻维护答案工作量，同一知识点下的问题也将会是很好的训练数据。</p><p>现在有了领域内知识库，用户提问后，我们还需要一个问答模型，这个模型能找到和用户query最匹配的问题，进而给出对应的答案，这里我们采用检索+匹配+排序的架构。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/iSdG4qnDaPHJSqFwSZHn.png" alt="img"></p><p>图3 问答建模流程</p><p>下图是基于知识库和问答模型在母婴场景的应用举例：</p><p><img src="http://image.woshipm.com/wp-files/2019/11/MpEGyRmL5PvGm7p2LAIS.png" alt="img"></p><p>图4 QA应用举例</p><h3 id="3-数据储备"><a href="#3-数据储备" class="headerlink" title="3.数据储备"></a>3.数据储备</h3><p><img src="http://image.woshipm.com/wp-files/2019/11/zu6m9aR6muomaV4mpdm6.png" alt="img"></p><p>图5 数据类型分布</p><p>在正式开始问答模型构建之前，我们需要思考目前有哪些数据可被我们使用，以及我们需要什么数据来支撑后续的工作。</p><p>通用领域的贴吧、豆瓣、微博、知道等问答数据，可被用来训练词向量，或是统计共现、互信息词典；人工标注的q/p对，可被用来训练有监督的分类模型；垂直领域的知识库可被用来训练领域相关的分类模型，也可用作词向量的fine-tune，当然也是有效的评估数据。</p><h3 id="4-模型迭代"><a href="#4-模型迭代" class="headerlink" title="4. 模型迭代"></a>4. 模型迭代</h3><p><img src="http://image.woshipm.com/wp-files/2019/11/sfHP6yaLK5m60SgTiLsM.png" alt="img"></p><p>图6 QA架构图</p><p>整体的QA架构图如图6所示，下面我们简单介绍一下历次迭代的思路。</p><p><strong>1. BoW+LR</strong></p><p>第一次迭代我们只引入了词袋（Bag of Words）模型，5维代表特征。</p><ul><li>Jaccard：q和p词交集个数与词并集个数的比值；</li><li>Coverity：最长公共子串特征，q和p最长公共子串在p中的占比；</li><li>Edit-Distance：最小编辑距离，q和p的最小编辑距离除以q、p长度的平均值；</li><li>TM：共现特征，基于bigram/trigram词典，计算q、p共现词平均score、共现词最高score的平均值、共现词去除相同词后最高score的平均值共3维特征；</li><li>MI：互信息特征，基于互信息词典，计算q、p两两词的互信息平均值。</li></ul><p>上述特征集合，均由大数据文本的统计特征衍生而来，对句子的语义表示能力较弱。</p><p><strong>2. BoW+WE+LR</strong></p><p>第二次迭代我们给模型引入了一定的语义表示能力，对于了解深度学习、自然语言处理的同学来讲，word2vec在很多任务上都有着非常杰出的贡献，这一模型从不同角度刻画了周围词和当前词的关系。经过训练后，我们能得到一份词汇的向量表示。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/anANQr2GXqzvbbM5gKWo.png" alt="img"></p><p>图7 word2vec的2个模型</p><p>基于训练得到的词向量，我们采用IDF对词向量进行加权平均，以此得到q、p词粒度的句向量表示，并最终通过余弦相似度来度量两者的语义相关性。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/htXlbjQLbLlIK6MKc7D2.png" alt="img"></p><p>图8 基于句子表示的w2v特征</p><p>通过余弦相似度给出的相似，本质上描述的还是两个词的相对一致，而且word2vec不考虑词序，余弦相似度大表示两个词搭配出现、或者和同一批词搭配出现的可能性较大，这一特征所显示出来的弊端就是往往相似度高的2个词具有可替换性但却语义不完全相同。</p><p>比如q=“宝宝感冒怎么办”，p=”宝宝发烧怎么办”，”感冒”和”发烧”互相替换，句子依然具有合理性，而且由于他们经常在同一批词汇中搭配出现，具有比较相似的分布特征，相关性比较高，然而他们语义并不同。</p><p>接下来我们引入另一种语义度量方法：WMD (the Word Mover’s Distance)，它刻画了如何用最小的代价将q中的每个词以不同权重匹配到p中每个词上，这是一种基于句子间的交互来表示语义的方法。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/Wgah2ye0OyLiVFhqEVCx.png" alt="img"></p><p>图9 基于句子交互的WMD特征</p><p>在利用WMD计算q/p的相关性时，我们会对句子进行切词、去停，对于q中的每个词，找到p中的另一个词，进行语义转移，转移的代价我们用两个词汇间的word2vec余弦相似度来度量。</p><p>当2个词语义较相近时，我们可以多转移一点，如果语义相差很大，我们可以选择少转移或者不转移，最后使得所有转移的加权和最小，这里加权采用词频作为特征，具体示例如图10所示。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/CFMpTuxwZxMFHridaeZB.png" alt="img"></p><p>图10 the Word Mover’s Distance (WMD)</p><p>由于WMD也高度依赖word2vec词向量，因此上文提到的word2vec cosine特征所有的不足，WMD特征依旧存在，既没有考虑语序信息，同时对OOV (Out of Vocabulary)情况也很不友好，语义泛化能力弱，相似意图区分能力差。</p><p><strong>3. BoW+WE+SE+fine-tune</strong></p><p>前两次迭代都没有考虑知识库内的数据，比较适用于无语料或者语料较少的知识库，当知识库具有一定的规模后，正如前面提到的，同一个知识点下的相似问，将是很好的训练数据。</p><p>我们采用了fastText模型，充分利用知识点中的问题语义相同/相近这个事实作为监督信号，训练一个分类模型，直接用一个问题的词去预测问题所属的知识点。</p><p>FastText是Tomas Makolov为了弥补word2vec的某些不足而提出的改进方案，和word2vec中CBOW不同的是，它的输入不再是上下文，而是一整个句子，同时它接收子词和ngram特征，能捕捉到更多的信息。</p><p>比如单词的前后缀，以及词的语序特征等。相比其它深度学习模型，fastText结构简单，本质上是一个线性模型，不太适合处理较长或者线性不可分的样本，而对于偏线性的短文本分类任务却比较合适，能在较少训练集合的基础上，非常快速地得到不错的效果。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/M7QVX65TUBAMpsu7Ko9y.png" alt="img"></p><p>图11 fastText模型结构</p><p>同时fastText在训练过程中也会产生一份词向量，经实验验证，基于知识库训练fastText的词向量对基于大数据训练的word2vec词向量进行fine-tune，能一定程度上提升该领域的问答效果。</p><p><strong>4. BoW+WE+SE+DM+fine-tune</strong></p><p>前面我们利用一个知识库的相似问语义相近作为监督信号，从中抽取出了一个知识点的语义信息，但我们最终的目标是判断用户输入和相似问之间的相关性，这里我们使用一个深度学习的模型ESIM (Enhanced LSTM for Natural Language Inference)，利用要比较的两句话的监督信号来训练模型，观察句对的交互对模型的影响。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/fu4UeMbeZC6ug4wJDzqZ.png" alt="img"></p><p>图12 ESIM模型架构（左侧）</p><p>上图是ESIM的网络结构，它在众多短文本分类任务中都有不错的表现，主要在于输入2句话分别接embedding+BiLSTM， 利用BiLSTM学习一句话中的word和它上下文的关系后，在进行inference之前，会利用attention操作计算2个句子word之间的相似度来更新embedding，也就是说比较的两句话在模型训练中产生了交互，相比其它的类似网络只在最后一层求距离来讲，这种交互的影响能学到更全局的信息。</p><p><strong>5. BERT+MTL+fine-tune</strong></p><p>当然，学术界是在不断变化的，对于效果较好的模型，我们也需要进行尝试，寻找适合在工业界落地的条件和场景。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/iseODLSNNO3Y68FRaM5W.png" alt="img"></p><p>图13 BERT+多任务学习MTL框架图</p><p>在BERT横扫了11项NLP测评任务的效果后，我们把它应用在知识点分类任务上，期望利用BERT本身的优势，来提升分类任务的效果。同时我们还基于知识库数据，在BERT的基础上，通过MTL进行fine-tune，再以BERT-MTL为基础，通过单个任务分别进行fine-tune。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/Gz90QDP2dHZoWDcRui7r.png" alt="img"></p><p>图14 BERT+MTL的fine-tune过程</p><h3 id="5-评估"><a href="#5-评估" class="headerlink" title="5. 评估"></a>5. 评估</h3><p><img src="http://image.woshipm.com/wp-files/2019/11/BopOLTwVACuB2goOQjqD.png" alt="img"></p><p>图15 评估数据举例</p><p>效果变好了，有多好？肉眼可见显然不能作为一个反馈的指标，所以我们需要去科学地评估这些改进，评估这一环节我们选了6个不同领域，每个领域50个知识点，每个知识点12个相似问作训练，3个作评估测试，示例数据见图15，在此数据集基础上去评估准召和F1。具体结果见图16，大家可以看到在不卡阈值的情况下，准确率从0.8提升到了0.968。</p><p><img src="http://image.woshipm.com/wp-files/2019/11/GxmOwDgS3KkIzkGQD0V5.png" alt="img"></p><p>图16 历次迭代评估数据表</p><p>迭代是一个循序渐进的过程，可能有人会有疑惑，我怎么知道要用什么特征、选哪个模型，从多次迭代的经验来讲，Badcase分析很重要，特征的设计一方面来源于你对问题的直观感知。</p><p>比如我们需要从多方面（统计层面、词汇表示、句子表示、句子间交互等）来设计方法对句子进行语义表示，另一方面来源于对模型现有Badcase的弥补，通过分析case表现出来的规律或者倾向来设计有针对性的特征。</p><p>同时学术界也在不断更新新的模型，3年前时兴的技术，到现在被完全替代的可能性是非常大的，因此我们需要与时俱进。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>整个智能问答系统升级的过程，主要围绕四个步骤进行，首先面对任务要理解问题的本质，对问题进行合理的建模，然后评估选择合适的语言工具去实现它，再由浅入深稳步迭代，形成数据、模型、反馈的闭环，最后就是要持续性学习，拥抱变化，拥抱技术。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.woshipm.com/ai/3074814.html">http://www.woshipm.com/ai/3074814.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>智能问答系统</tag>
      
      <tag>来也</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>平安人寿智能问答系统</title>
    <link href="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E5%B9%B3%E5%AE%89%E4%BA%BA%E5%AF%BF/"/>
    <url>/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E5%B9%B3%E5%AE%89%E4%BA%BA%E5%AF%BF/</url>
    
    <content type="html"><![CDATA[<p>平安在ROCLING 2019上发表了一篇有关智能问答系统的论文，比较全面的讨论了一套智能问答的方案，这篇文章的特点在于详细系统地讲解了FAQ任务的常规方法和体系，对我们构造单论检索式对话、智能问答等任务的建设有很大意义，加之平安本身有非常强的问答需求，文章对智能问答的理解深度也体现的非常明确。</p><p>知乎上平安官博给了文章的中文版解释：<a href="https://zhuanlan.zhihu.com/p/111380177">https://zhuanlan.zhihu.com/p/111380177</a></p><p>懒人目录：</p><ul><li>总体框架</li><li>预处理模块</li><li>检索模块</li><li>排序模块</li><li>知识库</li><li>小结</li></ul><span id="more"></span><h2 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h2><p>首先介绍平安人寿智能问答引擎算法架构，如下图：</p><p><img src="https://pic4.zhimg.com/80/v2-fe0ea3d1c78f94457253dac585af0167_1440w.jpg" alt="img"></p><p>从问题输入开始，这里包括用户的问题以及语境中心提供上下文，其中包含用户的历史对话信息以及一些关于用户意图的结构化数据。</p><p>用户的问题输入后，首先进入<strong>预处理模块</strong>。在预处理模块里，分词、词性标注、实体识别都是比较成熟的技术，配合业务专用名词词典，我们采用Hanlp工具来做；<strong>多意图识别</strong>则用分类来做，主要处理用户一句话里有多个问题的意图，并给予不同的回答；<strong>问句改写</strong>主要是对保险名词的缩写和全称做改写；<strong>情感分析</strong>主要是通过句法分析去判断用户的话语是肯定意图或是否定意图。</p><p>预处理结束后，会进入<strong>检索模块</strong>。如果预处理经过纠错和问句改写，就会是多个query并行进入检索，触发ES字面检索和深度语义匹配。经过这两个检索模块得到的答案后，我们会从知识库以及Redis本地存储，把答案拿到后做多路结果归并。然后简单计算字面得分、语义得分、关键词得分，编辑距离作为LR的feature。</p><p>还有<strong>保险实体对齐</strong>，主要是重要名词、疾病、地区等的对齐。在排序模块里，比如用户问的问题是关于A保险，匹配的答案是B保险，处理的方式是在实体对齐的时候把答案去除，剩余的答案会做深度语义精排。</p><p>排序后，就进入<strong>输出模块</strong>。在输出模块里，有直接问输出、推荐问输出等，如果阈值比较低，还会做问句澄清。在输出模块，关联问可能会用到用户画像。</p><p>剩下的就慢慢开始展开讲吧。</p><h2 id="一、问句预处理核心技术"><a href="#一、问句预处理核心技术" class="headerlink" title="一、问句预处理核心技术"></a>一、问句预处理核心技术</h2><p>但凡做过这个预处理模块，就知道Query预处理工作的繁杂，这里面列举的是非常常见的子模块了：</p><ul><li>分词。NLP基操。</li><li>POS。词性标注，一般使用的是序列标注的方法，也可能和实体抽取一起做。</li><li>NER。实体识别使用的是加入了保险专用名词词典的NLP现成工具。</li><li>纠错。<strong>纠错模块</strong>主要是为了<strong>处理用户输入出现错别字的情况</strong>，因为错别字可能会对后面的模型识别造成影响，所以需要先进行纠错动作。比较直观的做法是<strong>基于字典和规则的纠错</strong>。</li><li>句子压缩。NLP对于<strong>长难句</strong>处理可能不能精准识别关键信息，有的时候还需要拆句，比较直观的一种方法是<strong>语法树分析+关键词典</strong>。<ul><li>第一步，通过标点或空格分割长句成若干个短句，然后对短句分类，去掉口水语句。</li><li>第二步，基于概率和句法分析的句子压缩方案，只保留主谓宾等核心句子成分。配合保险关键词典，确保关键词被保留。</li></ul></li><li>指代消解。NLP经典难题，分析“他她它”、“这个那个”之类所指代的内容。其实现思路是：<strong>分词→词性标注→依存句法分析→主谓宾提取→实体替换/指代消解</strong></li><li>问句改写。有的时候用户的说法会比较多，会有自己的习惯说法，尤其是一些专名，例如英雄联盟里面用户不喜欢说“法外狂徒”、“格雷福斯”，而喜欢说“男枪”，那就需要我们进行改写。问句改写主要针对保险名。</li><li>情感分析。这块在小冰那里体现的非常丰富，在平安客服中也有体现，对生气的用户进行一定的安抚。</li></ul><h2 id="二、检索和深度语义匹配技术"><a href="#二、检索和深度语义匹配技术" class="headerlink" title="二、检索和深度语义匹配技术"></a>二、检索和深度语义匹配技术</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/https://zhuanlan.zhihu.com/p/393130531">NLP.TM[38] |对话系统经典：检索式对话</a></p></blockquote><p>平安的做法是将智能客服当做是经典的检索式对话来处理，真可谓是简单快捷。</p><h3 id="1-ElasticSearch字面检索"><a href="#1-ElasticSearch字面检索" class="headerlink" title="1. ElasticSearch字面检索"></a>1. ElasticSearch字面检索</h3><p>目前字面检索用的是<strong>ElasticSearch，这</strong>是一个基于lucene的高可用分布式开源搜索引擎。</p><p>除了ElasticSearch外，其实还有Solr搜索引擎。选择前者的原因是，在处理实时的搜索应用时，<strong>ES的效率明显比Solr要高</strong>。线上的产品其实对运行时间有较高的要求，整个系统跑下来要求控制在100毫秒以内。</p><p>我们会根据知识库去建立所有数据的索引，同时支持一些分类和机构的查询。</p><p>其中ES的分词进行了统一配置的动作，里面配置了保险专用名词和同义词。ES默认为TFIDF的算法，但也支持BM25的算法。ES搜索结果的得分则会被零到一的归一化以及分片优化。</p><h3 id="2-孪生网络"><a href="#2-孪生网络" class="headerlink" title="2. 孪生网络"></a>2. 孪生网络</h3><p>接下来介绍深度语义匹配模块，其中主要使用的是<strong>孪生网络Siamese CBOW</strong>。</p><p>词向量会经过预训练，然后用求和取平均的方式来表征句向量，对于每个query，使用标注的相似说法作为正样本，负样本使用随机采样的方式产生，且<strong>每次迭代都进行随机采样</strong>，大大增加训练数据的随机性，提升模型的泛化能力，损失函数为<strong>Contrastive Loss</strong>，让正样本之间的句向量表征尽量相似。预先算出语料的所有句向量表征，将用户问题通过模型转化成句向量，搜索语料里最相似的若干个句向量作为候选答案列表。</p><p><img src="https://pic4.zhimg.com/80/v2-70791e4ff1dad269187d5d83a6b3cc13_1440w.jpg" alt="img"></p><h3 id="3-BERT-for-QA"><a href="#3-BERT-for-QA" class="headerlink" title="3. BERT for QA"></a>3. BERT for QA</h3><p>BERT这一块的主要工作是，<strong>在BERT之后做一层微调</strong>。我们会自定义Fine-tune这块的Processor，然后把BERT表征之后的句向量再接一个孪生网络进行训练。</p><p>实验结果显示，<strong>加入BERT表征会比之前存的词向量准确率提升3个点左右。</strong></p><h3 id="4-交互矩阵"><a href="#4-交互矩阵" class="headerlink" title="4. 交互矩阵"></a>4. 交互矩阵</h3><p>前文提到的孪生网络Siamese CBOW其实是一个表示模型。深度语义匹配除了表示模型以外，还有一类是<strong>交互模型</strong>。从论文看，前面一种叫做<strong>representation model</strong>，后面这种交互模型主要是叫<strong>interaction model</strong>。</p><p>具体可查阅博客《<a href="./2021-05-26-语义相似度&amp;对比学习.md">2021-05-26-语义相似度&amp;对比学习</a>》</p><h3 id="5-检索-amp-语义匹配小结"><a href="#5-检索-amp-语义匹配小结" class="headerlink" title="5. 检索&amp;语义匹配小结"></a>5. 检索&amp;语义匹配小结</h3><p>这里要详细说的是语义召回，在对话系统里面主要是两个方案：</p><ul><li>第一种是将用户query向量化（例如word2vector，当然含有更复杂的方法），然后召回历史出现过的相似问题，相似问题对应的回答就可以是答案。也就是Q-&gt;Q-&gt;A的模式。</li><li>第二种就是Q-&gt;A的方式。直接召回结果。</li></ul><p>无论是第一种还是第二种，实质上就是要对句子进行语义表征，然后通过向量召回的方式召回结果，于是解决问题的关键就落在了两个关键点上：</p><ul><li>平行数据的构建。</li><li>语义相似度建模。</li></ul><p>前者是数据集构建的问题，常规方式就是直接从历史对话里面获取，然后根据效果整理、拓展、增强，这就是算法工程师的内功了。后者则是一个学术界已经被很经常谈到的问题了，以DSSM为基础，辅以多种特征抽取层进行处理，平安对LSTM、CNN、Attention、Bert都进行了尝试，虽然BERT不出意外的得到了最好的效果，但是由于耗时的原因最终选择的是cbow（呃呃呃），而在切词粒度上，使用的是词组粒度效果最好，优于简单的字和词语，语义更为丰富，大家可以参考。</p><p>这里给大家推荐一个优秀的开源工具：<strong>MatchZoo</strong>，这是一个开源的Python环境下基于TensorFlow开发的文本匹配工具，实现了主流的20多种深度语义匹配算法。其主要用Keras实现，代码结构非常好。</p><p><img src="https://pic2.zhimg.com/80/v2-f29aaec4c671920e47a79be947f35d25_1440w.jpg" alt="img"></p><h2 id="三、基于深度学习的问答排序算法"><a href="#三、基于深度学习的问答排序算法" class="headerlink" title="三、基于深度学习的问答排序算法"></a>三、基于深度学习的问答排序算法</h2><p>通过多路召回，检索返回与用户问最匹配的问句列表，去重归并，通过<strong>实体对齐</strong>去掉一些不合理匹配问，再经过深度学习排序打分，输出最终的相关问列表，最后以业务格式根据匹配问查找答案返回给用户。</p><h3 id="1-Deeprank"><a href="#1-Deeprank" class="headerlink" title="1. Deeprank"></a>1. Deeprank</h3><p>通过索引可以得到若干个答案，将这若干个答案合并去除重复的答案之后，就进入排序模块。接下来将详细介绍我们采用的<strong>深度学习排序算法</strong>：</p><ul><li>构造格式为&lt;用户query，候选答案&gt;的pair对输入样本；</li><li>语义向量获取，这个可以来自于各种预训练语言模型；</li><li>构造打分器<code>CNN+Pooling+相似度矩阵&amp;&amp;拼接tfidf/词共现特征+MLP+Softmax</code>进行 learning to rank，使得正确匹配的样本打分尽量高（归一化后趋近于1），错误匹配的样本打分尽量低（归一化后趋近于0）</li><li>可以在 learng2rank 模型的输入向量中方便地融入外部特征</li></ul><p><img src="https://pic3.zhimg.com/80/v2-7b19bea25e5d7ccea40e6c39209e545a_1440w.jpg" alt="img"></p><p>首先做分词，每一个词向量的维度矩阵就是<strong>sentence matrix这个矩阵高度</strong>。sentence matrix出来之后，会经过好几个过滤器去提取特征，例如下图是用tri-gram来提取。</p><p>把这些特征提取之后拼接在一起，有多个Filter的话就会生成多个Feature maps。多个卷积Feature maps提取好之后，会经过pooling层，把每个Feature maps做Pooling后，再把Pooling拼接到一起。</p><p>中间看到的Similarity Matching有个矩阵M。<strong>矩阵M</strong>是在模型里是通过参数训练得到的；<strong>Xd</strong>是用户匹配的问题；<strong>Xq</strong>是用户问题经过Pooing后提取的特征。</p><p>通过Xd*M*Xq得到Xsim，得到Join Layer。除了Xsim向量以外，Join Layer还会有额外Feature，包括<strong>问答对的共现词情况（交集个数、重要度、占比等），如TF-IDF、BM25、编辑距离</strong>，属于文本层面的匹配；抽取关键词和实体后的关键词匹配打分、实体匹配打分；意图和标签的匹配等。</p><p>实验发现，<strong>共现词特征其实影响比较大，加上这个特征之后大概会有2-3个点的提升。</strong></p><p>过了join Layer之后，最后是一个MLP多层感知机接softmax，主要分5级打分S、A、B、C、D：S=完全匹配，A=非常相关，B=相关，C、D基本上不怎么相关。</p><p>该排序打分模型支持<strong>对pointwise和pairwise方法进行训练</strong>。假设只是pointwise，就看用户的问题跟匹配的问句是否相关；pairwise的话，就看用户匹配的两个问题，针对用户的query到底哪个更加相关。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>平安虽然并非所谓的大厂，但是其结构化的一些思路，尤其是本篇文章，依旧给我们提供了很多具体方案上的建议，让我们在方案设计时有更多扎实可靠的参考，通读本文下来，我们能够获得的应该是一套完整的智能问答系统方案，非常稳妥可靠。划几个重点吧：</p><ul><li>智能团队对话系统的具体框架和流程。尤其是以检索和语义召回为中心的两条线的上下游处理。</li><li>预处理模块可能需要涉及的工作，有必要的，也有升级的，可以根据项目进展迭代进程逐步增加新功能。</li><li>排序上列举了不少可以使用的特征，让我们在优化效果的时候有更多明确的方向。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/426505179">前沿重器[3] | 平安智能问答系统</a></p><p><a href="https://zhuanlan.zhihu.com/p/70203821">智能问答系统：问句预处理、检索和深度语义匹配技术</a></p><p><a href="https://zhuanlan.zhihu.com/p/111380177">ROCLING 2019 | 基于深度语义匹配，上下文相关的问答系统</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>智能问答系统</tag>
      
      <tag>平安人寿</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>贝壳找房智能客服</title>
    <link href="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/"/>
    <url>/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文地址：<a href="https://www.6aiq.com/article/1590190626464">【深度语义匹配模型】实践篇：语义匹配在贝壳找房智能客服中的应用</a></p></blockquote><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>贝壳智能客服主要解决经纪人在业务中遇到的常见的问题，主要场景有闲聊、卡片触发、精准问答、sug、QA问答。对于非寒暄的场景下，QA问答的占比是最大的，有70%以上，而QA问答效果强烈依赖于数据建设。</p><p>QA问答的场景所需的模块大致分为数据层、NLU层、召回层和排序层，数据建设为整个流程中的语义召回、匹配、排序提供足够的query和title的匹配对。</p><span id="more"></span><h2 id="二、整体架构"><a href="#二、整体架构" class="headerlink" title="二、整体架构"></a>二、整体架构</h2><p>整个智能客服的架构如下图所示：</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/59c5ab7e56e150f915ba7a5f0c37e3c3.png" class="" title="img"><p>其中，意图识别NLU的结构如下：</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171445070" class="" title="图片"><ul><li><strong>NLU 层</strong>：NLU 层是对用户输入的 query 进行解析，包含意图识别、分词与词法分析、短语改写等功能。意图识别是为了识别出 query 的意图，我们通过日志分析总结出了 116 个意图，其中包含 7 个 Task 意图，1 个寒暄意图，和 108 个具体业务意图，会根据不同的意图识别结果走不同的 bot。这里的意图识别模型用的是 fastText，该模型有高效的训练速度和较高的识别准确率，做出来的结果也可以达到上线使用的标准。词法分析维护了一些词典，通过词典匹配能获得 query 中的关键词和关键短语。短语改写的目的是为了纠错，比如“搏学考试”手误输入成了“博学考试”，短语改写便能将其纠正，query 改写后能更容易召回正确答案。</li><li><strong>召回层</strong>：召回层是将候选答案从 FAQ 库中拿回，获得待排序的候选集。此处用了两种召回方式：<strong>检索召回和语义召回</strong>。检索召回会根据 NLU 层的意图识别与词法分析结果进行关键词和意图的加权，同时，n-gram 的使用也一定程度上减弱了分词错误带来的负面影响。语义召回是将相似问构建成一个 <strong>faiss 索引</strong>，这些相似问已经由知识库运营人员总结成标准问，并和知识产生了挂接。query 和相似问会被映射到同一个语义向量空间，<strong>通过 KNN 能快速拿到和 query 最相近的相似问</strong>，从而拿到最可能回答 query 的答案。下图直观展示了语义召回的过程：</li></ul><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/image-62fbaf12fba54c68bb6f647fc13af7f2.png" class=""><ul><li><strong>排序层</strong>：排序层是将召回层拿到的候选知识进行排序，将和 query 最相关的知识尽可能往前排。排序模型采用了 <strong>GBDT</strong>，GBDT 作为一种常用的树模型，可天然地对原始特征进行特征划分、特征组合和特征选择，并得到高阶特征属性和非线性映射。我们考虑用 GBDT 可以组合多种特征，可扩展性强，并且后期验证 GBDT 的效果好于单独使用匹配算法效果，因此，当前<strong>匹配算法在排序层中作为一种特征来使用</strong>。</li><li><strong>返回层</strong>：query 经过上述处理之后会对处理结果进行返回。四种返回层场景展示，依次是 Chat-bot,Task-Bot,KBQA 和 QA-Bot：</li></ul><h2 id="三、FAQ数据挖掘流程"><a href="#三、FAQ数据挖掘流程" class="headerlink" title="三、FAQ数据挖掘流程"></a>三、FAQ数据挖掘流程</h2><h3 id="3-1-标准问生产流程"><a href="#3-1-标准问生产流程" class="headerlink" title="3.1 标准问生产流程"></a>3.1 标准问生产流程</h3><p>标准问的挖掘流程可以分为三大部分：挖掘可用query并推送、标注人员进行标准问生产、知识库的知识挂接。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171541819" class="" title="图片"><p>标准问的整体挖掘流程从拿到埋点日志开始，选择点踩query、无召回query和有返回知识但是用户没有点击的query（这些query是适合标准问产出，为整个召回排序提效的数据），然后经过分类去除寒暄等意图的query，faiss召回和query相似的现有标准问，然后把query聚类后推送给标注人员。</p><p>数据会根据意图推送给不同的标注人员，标注人员会首先判断query是否有效，若是无效query则直接结束标注，有效query会先和现有的标准问进行判断是否匹配，若匹配，则选出正确的，若不匹配，则总结成新标准问。最终获得推送query和对应的标准问。</p><p>由于一条标准问，对应的不同城市的知识会有不同，比如同样是问“业绩分配比例”，可能北京的业绩分配结果和天津的不同，所以标准问需要查找和挂接所有城市的知识，挂接完成后标准问就和一条知识一样被存储在知识库中。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171552216" class="" title="图片"><h3 id="3-2-相似问生产流程"><a href="#3-2-相似问生产流程" class="headerlink" title="3.2 相似问生产流程"></a>3.2 相似问生产流程</h3><p>相似问的整体挖掘流程和标准问类似，也是从拿到埋点日志开始，选择点击标准问的query，同样经过分类去除寒暄等意图的query，判断query和标准问的相似度是否较高 ，若足够高，达到设定的阈值则进行推送。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171603167" class="" title="图片"><p>标注人员只需要判断，query和标准问是否是在问同一问题，若是，query是否需要改写为更为完整通用的句子，然后把改写后或者直接可用的相似问推入线上知识库即可。</p><p>由于日志中的标准问已经挂接了知识，产出的相似问不需要在进行知识挂接，且标注人员只需要对现有query进行修改，标注简单。相似问和标准问相比产出快、产量高。</p><h2 id="四、相似问挖掘实践"><a href="#四、相似问挖掘实践" class="headerlink" title="四、相似问挖掘实践"></a>四、相似问挖掘实践</h2><p>下面来具体介绍一下，相似问挖掘中我们所尝试的各种算法以及效果。</p><p>相似问挖掘中所尝试的模块主要有在召回层使用数据增强的同义词替换，聚类、knn和生成式方法，以及排序层的相似度排序。</p><h3 id="4-1-数据增强—同义词替换"><a href="#4-1-数据增强—同义词替换" class="headerlink" title="4.1 数据增强—同义词替换"></a>4.1 数据增强—同义词替换</h3><p>同义词替换所需的数据有标准问和同义词组，其中同义词又可分为行业同义词（知识生产人员积累）和日常同义词（算法挖掘+人工筛选）。</p><p>替换方式分为简单同义词和同义词组两种，简单同义词是标准问进行分词后替换，若为同义词组的话，则直接替换不需分词。</p><p>行业同义替换后效果如下图所示（std表示标准问，替换的同义词用”[ ]”标出），其中“核销、下架、无效”为同义词，“查询、查看、查找”为同义词，行业同义词是人工积累获得，替换后可直接使用，不需要人工过滤。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171611280" class="" title="图片"><p>日常同义词质量没有行业同义词高，替换后query需人工过滤。如下图中“哪里、在哪儿”是同义词，标准问“签后报单的入口在哪里？”替换为“签后报单的入口在在哪儿? ”，是不通顺的。但日常同义词数据量大，当其他相似问挖掘方法产出有限时，同义词替换可作为兜底数据。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171615076" class="" title="图片"><h3 id="4-2-相似度–cos相似度"><a href="#4-2-相似度–cos相似度" class="headerlink" title="4.2 相似度–cos相似度"></a>4.2 相似度–cos相似度</h3><p>计算用户query和点击的标准问之间的相似度，若query和标准问的相似度足够高，则把该query作为标准问的相似问。这里我们使用的是cos相似度，阈值设置为0.6，如下图可知，标准问为“VR带看没有声音怎么办？”点击这条标准问的query有“vr来电不响 ”、“vr带看没声音 ”、“为什么我的小被vr讲房没有声音呢”，和标准问的相似度都达到了0.6以上，经过人工判断，可知，只有“vr带看没声音”和原标准问的意思相同，是原标准问的相似问。而第二个例子，由于query“vr讲房稿在那里 ”应该是“vr讲房稿在哪里 ”，所以是标准问“VR讲房的模板是什么？”的相似问，但需修改错别字。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171618699" class="" title="图片"><h3 id="4-3-聚类–KMeans-DBSCAN"><a href="#4-3-聚类–KMeans-DBSCAN" class="headerlink" title="4.3 聚类–KMeans/DBSCAN"></a>4.3 聚类–KMeans/DBSCAN</h3><p>聚类算法用到用户query和标准问，做法是将query和标准问映射到同一空间一起进行聚类，根据聚类结果，将每个类中的query作为该聚类中的标准问的相似问。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171622019" class="" title="图片"><p>尝试的具体算法有KMeans聚类（k=80，数据量1k+），聚类效果为每个聚类的query个数较均衡，但聚类后query意图分散，图中可以看出聚类label=2时，query包含有离职、转店、审核等意图。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171624983" class="" title="图片"><p>处理KMeans聚类还尝试了DBSCAN聚类，DBSCAN聚类是基于密度的距离，不需要设置聚类个数，但是需要设置类内最大距离，这里希望聚在一起的query意图足够相近，设置类内最大距离较小，导致每个聚类的query个数相差较大，聚类个数有1k+，大多为单点，但同一类内的query意图分散，图中可以看出聚类label=1时，query包含有手机号、房源录入、400电话、贝壳币等意图。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171627916" class="" title="图片"><h3 id="4-4-k近邻"><a href="#4-4-k近邻" class="headerlink" title="4.4 k近邻"></a>4.4 k近邻</h3><p>用到的数据是用户query（圆圈表示）和标注标准问的query（菱形表示），将标注标准问的query和用户query映射到同一空间，查看离用户query最近的k个query所对应的标准问类别，若用户query周围大于的k/2的query都对应同一标准问，则用户query也作为该标准问的相似问。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171630644" class="" title="图片"><p>knn所用数据为用户query和标注标准问的query，根据数据量的大小、k值大小分别进行对比实验。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171633627" class="" title="图片"><p>测评1和测评3说明数据从2500增加到6w时，准确率从0.567增加到0.786（绿框），测评3和4说明了k值从5减到3时准确率从0.732增加到0.786（黄框）。</p><p>实验还尝试了在数据量一定时，分别使用k=3标注query和k=5标注query，且只有两者标注一致时才作为标签使用，发现标注的query的标签为对应标准问的准确率为0.838，可知叠加不同k值下的效果进行标注时效果较好（蓝框）。</p><p>但是测评1-5都是对训练集进行测试，测评6为对测试集进行测试，发现数据量6w时，训练集的准确率为0.732，测试集的准确率为0.599，可知模型对新数据的判断效果不佳（红圈），但是随着数据量的增长，knn效果会有所提升，后期数据积累到一定水平时，该方法还存在尝试的价值。</p><h3 id="4-5-生成式—seq2seq-attention-SLCVAE"><a href="#4-5-生成式—seq2seq-attention-SLCVAE" class="headerlink" title="4.5 生成式—seq2seq+attention/SLCVAE"></a>4.5 生成式—seq2seq+attention/SLCVAE</h3><p>生成式算法需要的训练数据为标准问和标注标准问的query，以标准问为输入，对应的query为输出，训练GEN模型。预测则是输入标准问，以生成的新query作为相似问。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171636820" class="" title="图片"><p>这里采用了两种生成式模型，seq2seq+attention和SLCVAE模型。</p><p>seq2seq+attention是在seq2seq的基础上，添加attention（注意力），当前的输入与目标状态越相似，那么在当前的输入的权重就会越大，说明当前的输出越依赖于当前的输入。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171639958" class="" title="图片"><p>我们在统计了模型训练30轮、40轮、50轮之后的结果发现，生成的query会有字重复现象，停止较慢，且生成的query质量不高的现象。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171642719" class="" title="图片"><p>然后尝试了阿里在19年提出的SLCVAE模型，说起SLCVAE模型要先从AE（AutoEncoder）模型开始，它就是简单的encoder、decoder，encoder输出的中间向量是确定的，为了增加输出的多样性，出现了VAE(Variational AutoEncoder)，即学习输入的均值和方差采样得到中间向量，来保证解码的多样性。</p><p>而SLCVAE（Self Labeling Conditional Variational Auto Encoder）模型则是添加了输出作为Conditional来限制生成的结果，并且添加Labeling phase，解码共用相同的参数网络。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171648083" class="" title="图片"><p>SLCVAE的最终的效果如下：pred0、pred1、pred3、pred4都是原训练集里存在的数据，只有pred2是新生成的query。可以看出生成的query会有“照搬”训练集的情况出现，生成的query质量尚可但数量少。尝试增加数据量（37w+），”照搬”情况依然存在，调整参数增加输出的多样性，query质量下降。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/640-20230424171651192" class="" title="图片"><h3 id="4-6-算法对比"><a href="#4-6-算法对比" class="headerlink" title="4.6 算法对比"></a>4.6 算法对比</h3><p>最后关于相似问的各种算法实践，进行了3个方面的对比：</p><ul><li><strong>挖掘难度</strong>：同义词替换&gt;聚类、knn、生成式&gt; 相似度</li></ul><p>从挖掘难度上看，同义词替换难度最大，相似度的方式难度最小，原因是同义词替换需要依赖同义词，同义词的积累和总结是一个长期的过程。而聚类、knn、生成式方法，则需要标准问和标注标准问的数据，而相似度挖掘只需要标准问即可。</p><ul><li><strong>数据产量</strong>：日常同义词&gt;相似度&gt;行业同义词</li></ul><p>数据产量上来说，由于聚类、knn、生成式方法产出效果不好，没有进行测评。剩下的日常同义词占比最大，相似度挖掘是以日志为基础，能够持续挖掘数量也很多，行业同义词由于数量较少，所以产出有限。</p><ul><li><strong>数据质量</strong>：行业同义词&gt;日常同义词&gt;相似度&gt;聚类、knn、生成式</li></ul><p>虽然行业同义词产量少，但是数据质量是最高的，因为它是纯人工积累的，然后是日常同义词，之后是相似度，最后是聚类、knn和生成式方法。</p><h2 id="五、匹配算法实践"><a href="#五、匹配算法实践" class="headerlink" title="五、匹配算法实践"></a>五、匹配算法实践</h2><h3 id="5-1-数据准备"><a href="#5-1-数据准备" class="headerlink" title="5.1 数据准备"></a>5.1 数据准备</h3><p>随着标准问数量的增加，知识库中存在大量query和标准问的匹配对，将这些匹配对当作正样本，label为1。将query随机抽取一条与之不匹配的标准问作为负样本，label为0。共产生12w+的训练样本和5000条测试样本，正负样本比例为1:1。标准问是由人工标注总结，因此数据集质量较高，正样本匹配关系很强，数据比较规整。训练样本展示：</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/image-df7609226a064c3b90b1626b03e87dee.png" class="" title="img"><h3 id="5-2-模型训练"><a href="#5-2-模型训练" class="headerlink" title="5.2 模型训练"></a>5.2 模型训练</h3><p>我们对4种交互型的深度语义匹配模型进行尝试，分别是ABCNN（ABCNN-1，ABCNN-2，ABCNN-3），PairCNN，ESIM和Bimpm。使用交互型匹配模型的原因如下：</p><ol><li>交互型的深匹配模型能很好地把握语义焦点，对上下文重要性进行合理建模。</li><li>从训练集数据来看，正样本是人工标注的有监督数据，交互型模型能保证较高的准确率。</li></ol><p>四种模型都有优秀的表现效果，因线上使用的模型是ABCNN-2，这里为大家展示该模型的部分实现代码。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">#attention矩阵<br>def make<span class="hljs-constructor">_attention_mat(<span class="hljs-params">x1</span>, <span class="hljs-params">x2</span>)</span>:<br>            # 作者论文中提出计算attention的方法 在实际过程中反向传播计算梯度时 容易出现NaN的情况 这里面加以修改<br>            # euclidean = tf.sqrt(tf.reduce<span class="hljs-constructor">_sum(<span class="hljs-params">tf</span>.<span class="hljs-params">square</span>(<span class="hljs-params">x1</span> - <span class="hljs-params">tf</span>.<span class="hljs-params">matrix_transpose</span>(<span class="hljs-params">x2</span>)</span>), axis=<span class="hljs-number">1</span>))<br>            # return <span class="hljs-number">1</span><span class="hljs-operator"> / </span>(<span class="hljs-number">1</span> + euclidean)<br> <br>            x1 = tf.transpose(tf.squeeze(x1, <span class="hljs-literal">[-<span class="hljs-number">1</span>]</span>), <span class="hljs-literal">[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]</span>)<br>            # 矩阵乘法einsum(&#x27;ij,jk-&gt;ik&#x27;, m0, m1)  # output<span class="hljs-literal">[<span class="hljs-identifier">i</span>,<span class="hljs-identifier">k</span>]</span> = sum_j m0<span class="hljs-literal">[<span class="hljs-identifier">i</span>,<span class="hljs-identifier">j</span>]</span><span class="hljs-operator"> * </span>m1<span class="hljs-literal">[<span class="hljs-identifier">j</span>, <span class="hljs-identifier">k</span>]</span><br>            attention = tf.einsum(<span class="hljs-string">&quot;ijk,ikl-&gt;ijl&quot;</span>, x1, tf.squeeze(x2, <span class="hljs-literal">[-<span class="hljs-number">1</span>]</span>))<br>            return attention<br> <br>#全部cnn层相关操作<br> def <span class="hljs-constructor">CNN_layer(<span class="hljs-params">variable_scope</span>, <span class="hljs-params">x1</span>, <span class="hljs-params">x2</span>, <span class="hljs-params">d</span>)</span>:<br>            <span class="hljs-keyword">with</span> tf.variable<span class="hljs-constructor">_scope(<span class="hljs-params">variable_scope</span>)</span>:<br>  <br>                #卷积<br>                left_conv = convolution(name_scope=<span class="hljs-string">&quot;left&quot;</span>, x=pad<span class="hljs-constructor">_for_wide_conv(<span class="hljs-params">x1</span>)</span>, d=d, reuse=False)<br>                right_conv = convolution(name_scope=<span class="hljs-string">&quot;right&quot;</span>, x=pad<span class="hljs-constructor">_for_wide_conv(<span class="hljs-params">x2</span>)</span>, d=d, reuse=True)<br>          <br>                #获得注意力矩阵<br>                left_attention, right_attention = None, None<br>                att_mat = make<span class="hljs-constructor">_attention_mat(<span class="hljs-params">left_conv</span>, <span class="hljs-params">right_conv</span>)</span><br>                left_attention, right_attention = tf.reduce<span class="hljs-constructor">_sum(<span class="hljs-params">att_mat</span>, <span class="hljs-params">axis</span>=2)</span>, tf.reduce<span class="hljs-constructor">_sum(<span class="hljs-params">att_mat</span>, <span class="hljs-params">axis</span>=1)</span><br>                 <br>                #pooling（average pooling（w-ap）和qverage pooling（all-ap））<br>                left_wp = w<span class="hljs-constructor">_pool(<span class="hljs-params">variable_scope</span>=<span class="hljs-string">&quot;left&quot;</span>, <span class="hljs-params">x</span>=<span class="hljs-params">left_conv</span>, <span class="hljs-params">attention</span>=<span class="hljs-params">left_attention</span>)</span><br>                left_ap = all<span class="hljs-constructor">_pool(<span class="hljs-params">variable_scope</span>=<span class="hljs-string">&quot;left&quot;</span>, <span class="hljs-params">x</span>=<span class="hljs-params">left_conv</span>)</span><br>                right_wp = w<span class="hljs-constructor">_pool(<span class="hljs-params">variable_scope</span>=<span class="hljs-string">&quot;right&quot;</span>, <span class="hljs-params">x</span>=<span class="hljs-params">right_conv</span>, <span class="hljs-params">attention</span>=<span class="hljs-params">right_attention</span>)</span><br>                right_ap = all<span class="hljs-constructor">_pool(<span class="hljs-params">variable_scope</span>=<span class="hljs-string">&quot;right&quot;</span>, <span class="hljs-params">x</span>=<span class="hljs-params">right_conv</span>)</span><br> <br>                return  left_ap, right_ap<br> <br> <br>#x1,x2是需要匹配的文本embedding后的结果<br>x1_expanded = tf.expand<span class="hljs-constructor">_dims(<span class="hljs-params">x1</span>, -1)</span><br>x2_expanded = tf.expand<span class="hljs-constructor">_dims(<span class="hljs-params">x2</span>, -1)</span><br> <br>#average pooling(all-ap)最终获得为列向量<br>LO_0 = all<span class="hljs-constructor">_pool(<span class="hljs-params">variable_scope</span>=<span class="hljs-string">&quot;input-left&quot;</span>, <span class="hljs-params">x</span>=<span class="hljs-params">x1_expanded</span>)</span><br>RO_0 = all<span class="hljs-constructor">_pool(<span class="hljs-params">variable_scope</span>=<span class="hljs-string">&quot;input-right&quot;</span>, <span class="hljs-params">x</span>=<span class="hljs-params">x2_expanded</span>)</span><br> <br>#x1,x2全部cnn层相关操作后所得特征<br> LO_1,  RO_1 = <span class="hljs-constructor">CNN_layer(<span class="hljs-params">variable_scope</span>=<span class="hljs-string">&quot;CNN-1&quot;</span>, <span class="hljs-params">x1</span>=<span class="hljs-params">x1_expanded</span>, <span class="hljs-params">x2</span>=<span class="hljs-params">x2_expanded</span>, <span class="hljs-params">d</span>=<span class="hljs-params">d0</span>)</span><br> <br>#计算相似度<br>#作者论文中提出计算相似度的方法是将L01_1,R0_1拼接后进行逻辑回归，这里为了简化计算直接将all_pooling层和L01_1,R0_1分别计算相似度后拼接，然后进行逻辑回归<br>self.sims = <span class="hljs-literal">[<span class="hljs-identifier">cos_sim</span>(LO<span class="hljs-identifier">_0</span>, RO<span class="hljs-identifier">_0</span>), <span class="hljs-identifier">cos_sim</span>(LO<span class="hljs-identifier">_1</span>, RO<span class="hljs-identifier">_1</span>)]</span><br></code></pre></td></tr></table></figure><h3 id="5-3-效果测评"><a href="#5-3-效果测评" class="headerlink" title="5.3 效果测评"></a>5.3 效果测评</h3><p>5000条测评数据的情况下，上述模型都取得了较好的匹配效果。</p><img src="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/image-6df328d4084146baa9311135265ddf21.png" class="" title="img"><p>从<strong>耗时</strong>来看Bimpm训练耗时较长，相比于其他模型收敛速度较慢，整体的耗时排序为t(ABCNN) &lt; t(PairCNN) &lt; t(ESIM) &lt; t(Bimpm)，ABCNN的三种算法训练时间没有明显差别。需要特别说明的是，由于Bimpm模型复杂，在cpu上训练时间过长，最终的训练是在GPU上完成。</p><p>从<strong>准确率</strong>来看ABCNN-3和PairCNN的准确率低于95%，剩余都高于95%。ABCNN的三种算法中，ABCNN-1和ABCNN-2的效果相差较小，ABCNN-3的网络结构综合了ABCNN-1和ABCNN-2的结构，但是实验效果并没有更好，反而更差，从侧面反映网络结构更复杂，准确率不一定就更好。</p><h3 id="5-4-经验与踩坑"><a href="#5-4-经验与踩坑" class="headerlink" title="5.4 经验与踩坑"></a>5.4 经验与踩坑</h3><p><strong>Q：数据对模型有什么影响？</strong></p><p><strong>A:</strong> 数据质量决定了模型效果的好坏，也尝试采用点击title与query计算相似度，大于某个阈值作为正样本，未点击title与query计算相似度，小于某个阈值作为负样本，因存在错误数据，模型训练效果大打折扣。</p><p><strong>Q: 除了数据外，ABCNN模型训练效果受什么影响？</strong></p><p><strong>A:</strong> ABCNN模型最初几轮的训练效果与模型的随机初始值有很大关系，初始值较差和较好的时候训练达到相同效果的情况下，训练epoch number相差10多轮不足为奇。</p><p><strong>Q：为何Bimpm训练时间长？</strong></p><p><strong>A:</strong> Bimpm模型结构相对复杂，从训练时间上和内存使用率上来说，bimpm应该是放在有GPU情况下考虑选择的模型，并且用原文的参数进行训练时会遇到显存用尽的报错，上表中的结果是删掉两层全连接层，并且减少了神经元个数后训练得出的。</p><p><strong>Q：复杂模型的构建与训练有什么建议？</strong></p><p><strong>A：</strong> 检查写的神经网络是否本身有问题，训练时先用小样本，再逐步增大样本量，通过是否过拟合来判断网络是否正确。</p><h2 id="五、总结与展望"><a href="#五、总结与展望" class="headerlink" title="五、总结与展望"></a>五、总结与展望</h2><p>当前的深度语义匹配模型已经在贝壳智能客服的在线咨询主场景中使用，模型上线后，QA-bot的列表点击率有了2%的绝对提升，对于一些简单的问题已经能够将较匹配的答案排到top3。</p><p>但是对于需要深度语义及具有知识背景的问题，如：“A3到A4需要多少分”的问题，还没有办法将答案“经纪人积分与级别是如何对应的？”排在靠前的位置。当前正在进行知识图谱方向的开发，对知识库内的知识进行结构化的梳理，希望在匹配的同时能够具有简单的推理，来更好的理解用户语言背后的需求。</p><p><strong>展望：</strong></p><ol><li>数据是效果的基础，智能客服效果所依赖的FAQ库也需要不断的知识扩充，如何通过自动或半自动的方法挖出更多高质量的相似问或者标准问，为知识运营人员提效，也是我们现阶段正在探索的方向。</li><li>当前的匹配算法仅作为一种特征使用在gbdt排序模型中，后期随着匹配算法的不断积累，会将所有的匹配模型进行整合，以一种更通用的模块化的方式，为有文本匹配需求的各个业务场景提供匹配算法的支持。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.zhuanzhi.ai/document/77fcf0cca73b38e383b041f289a7cf69">【深度语义匹配模型】实践篇：语义匹配在贝壳找房智能客服中的应用</a></p><p><a href="https://www.infoq.cn/article/9zuez8y*kvzyloz5lkpg">智能客服系统的构建与算法迭代</a></p><p><a href="https://mp.weixin.qq.com/s/C70HVVxG4VK1oCyz2dKGJg">贝壳智能客服中的数据建设</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>智能问答系统</tag>
      
      <tag>贝壳找房</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>美团智能客服核心技术与实践</title>
    <link href="/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E7%BE%8E%E5%9B%A2/"/>
    <url>/2021/10/31/2021-10-31-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D-%E7%BE%8E%E5%9B%A2/</url>
    
    <content type="html"><![CDATA[<p>客服是在用户服务体验不完美的情况下，尽可能帮助体验顺畅进行下去的一种解决办法，是问题发生后的一种兜底方案。而智能客服能让大部分简单的问题得以快速自助解决，让复杂问题有机会被人工高效解决。在用户服务的全旅程中，美团平台/搜索与NLP部提供了问题推荐、问题理解、对话管理、答案供给、话术推荐和会话摘要等六大智能客服核心能力，以期达到低成本、高效率、高质量地与用户进行沟通的目的。本文主要介绍了美团智能客服核心技术以及在美团的实践。</p><p>转载自《<a href="https://tech.meituan.com/2021/09/30/artificial-intelligence-customer-service.html">美团智能客服核心技术与实践</a>》</p><span id="more"></span><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>目前，美团的年交易用户量为6.3亿，服务了770万生活服务类商家。此外，在美团优选业务中还有一个很大的团长群体。美团平台涵盖吃、住、行、游、购、娱等200多个生活服务品类，在平台服务的售前、售中、售后各个环节，都有大量信息咨询、订单状态获取以及申诉投诉等沟通诉求。另外，作为一家拥有几万名员工的上市企业，员工之间亦有大量的沟通诉求。面对以上这些需求，如果都是通过人力进行实现，显然不符合公司长远发展的目标，这就需要引入智能客服。</p><p><img src="https://p0.meituan.net/travelcube/b38ebfaf9fad3ba51277457aa36e8087210091.png" alt="img"></p><h3 id="1-1-面对不同场景的智能客服落地"><a href="#1-1-面对不同场景的智能客服落地" class="headerlink" title="1.1 面对不同场景的智能客服落地"></a>1.1 面对不同场景的智能客服落地</h3><p>首先，我们看看日常生活中几种最为常见的客服场景。</p><p><img src="https://p1.meituan.net/travelcube/f0543662b1247fe27ed5c49832c24c7d336818.png" alt="img"></p><ul><li><strong>售前场景</strong>：比如消费者在平台选择入住酒店，对房型价格、酒店设施、入退房政策等，下单前都有很强的信息咨询诉求。</li><li><strong>售中场景</strong>：比如外卖催单还没到，添加备注不要辣、加开发票等咨询等等，售前和售中场景主要发生在消费者和商家或平台之间。</li><li><strong>售后场景</strong>：比如外卖场景投诉菜品少送、骑手送餐超时、要求退款等，酒店场景投诉酒店到店无法入住等，售后往往涉及到客服座席、消费者、骑手和商家，需要多方协同解决。</li><li><strong>办公场景</strong>：比如IT、人力资源、财务、法务等咨询，产运研对提供的接口产品的咨询答疑，产品对销售顾问的答疑，以及销售顾问对商家的答疑等等。</li></ul><h3 id="1-2-面对不同人群的智能客服落地"><a href="#1-2-面对不同人群的智能客服落地" class="headerlink" title="1.2 面对不同人群的智能客服落地"></a>1.2 面对不同人群的智能客服落地</h3><p>沟通是人类的一项基本需求，在绝大多数场景下，我们对沟通的追求都是以低成本、高效率和高质量为目标，而对话机器人也需要同时满足这三点要求。目前我们按照服务的群体进行划分，智能客服落地场景大体可以分为以下四类：</p><p><img src="https://p0.meituan.net/travelcube/b12576f745bc8b7a8923ba1da639e613283002.png" alt="img"></p><ul><li><strong>面向用户</strong>：提供智能客服机器人，来帮助他们自助解决大部分的问题。</li><li><strong>面向座席</strong>：用话术推荐或者会话摘要等能力来提升人工座席的工作效率，改善人工座席的工作体验。</li><li><strong>面向商家</strong>：打造商家助手来降低商家回复的费力度，改善消费者和商家的沟通体验。</li><li><strong>面向员工</strong>：通过对话机器人，可以自助给员工进行答疑，从而提升办公效率。</li></ul><h3 id="1-3-智能客服是什么"><a href="#1-3-智能客服是什么" class="headerlink" title="1.3 智能客服是什么"></a>1.3 智能客服是什么</h3><p>要回答智能客服是什么，可以先看看客服是什么。我们的理解是，客服是在用户服务体验不完美的时候，来帮助体验顺畅进行下去的一种解决办法，是问题发生后的一种兜底方案。而智能客服能让大部分简单的问题得以快速自助解决，让复杂问题有机会被人工高效解决。</p><p><img src="https://p0.meituan.net/travelcube/b46da68cc6550ba1ea85265e2d0b7f8c115531.png" alt="img"></p><p>上图展示的是用户服务旅程。首先，用户会通过在线打字或者拨打热线电话的方式进线寻求服务，其中在线咨询流量占比在85%以上。当用户进入到服务门户后，先是用户表达需求，然后是智能机器人响应需求，过程中机器人先要理解问题，比如是追加备注或是修改地址，还是申请退款等等，继而机器人尝试自助解决。如果解决不了，再及时地流转到人工进行兜底服务。最后，当用户离开服务时，系统会发送调查问卷，期待用户对本次服务进行评价。</p><h2 id="2-智能客服核心技术"><a href="#2-智能客服核心技术" class="headerlink" title="2 智能客服核心技术"></a>2 智能客服核心技术</h2><h3 id="2-1-对话交互技术概述"><a href="#2-1-对话交互技术概述" class="headerlink" title="2.1 对话交互技术概述"></a>2.1 对话交互技术概述</h3><p><img src="https://p1.meituan.net/travelcube/37f4459686d11e5197b879b6dc978071230811.png" alt="img"></p><p>智能客服背后的技术主要是以对话交互技术为核心。常见的对话任务可分为闲聊型、任务型和问答型：</p><ul><li><strong>闲聊型</strong>：通常是不关注某项特定任务，它的主要的目标是和人进行开放领域的对话，关注点是生成流畅、合理且自然的回复。</li><li><strong>任务型</strong>：通常是帮助用户完成某项任务指令，如查找酒店、查询订单状态、解决用户的退款申请等等。用户的需求通常比较复杂，需要通过多轮交互来不断收集任务所需的必要信息，进而根据信息进行决策，执行不同的动作，最终完成用户的指令。</li><li><strong>问答型</strong>：侧重于一问一答，即直接根据用户的问题给出精准答案。问答型和任务型最本质的区别在于，系统是否需要维护一个用户目标状态的表示和是否需要一个决策过程来完成任务。</li></ul><p>在技术实现上，通常又可以划分为检索式、生成式和任务式：</p><ul><li><strong>检索式</strong>：主要思路是从对话语料库中找出与输入语句最匹配的回复，这些回复通常是预先存储的数据。</li><li><strong>生成式</strong>：主要思路是基于深度学习的Encoder-Decoder架构，从大量语料中习得语言能力，根据问题内容及相关实时状态信息直接生成回答话术。</li><li><strong>任务式</strong>：就是任务型对话，通常要维护一个对话状态，根据不同的对话状态决策下一步动作，是查询数据库还是回复用户等等。</li></ul><p>闲聊、问答、任务型对话本质都是在被动地响应用户需求。在具体业务中还会有问题推荐、商品推荐等来主动引导用户交互。在美团的业务场景里主要是任务型和问答型，中间也会穿插一些闲聊，闲聊主要是打招呼或者简单情绪安抚，起到润滑人机对话的作用。</p><p><img src="https://p0.meituan.net/travelcube/64f2e7e5ed14850917373612adb8b24f267239.png" alt="img"></p><p>如前面用户服务流程所介绍的那样，用户的沟通对象可能有两个，除了跟机器人沟通外，还可能跟人工沟通。如果是找客服场景人工就是客服座席，如果是找商家场景人工就是商家。机器人的能力主要包括问题推荐、问题理解、对话管理以及答案供给。</p><p>目前，衡量机器人能力好坏的核心输出指标是不满意度和转人工率，分别衡量问题解决的好坏，以及能帮人工处理多少问题。而在人工辅助方面，我们提供了话术推荐和会话摘要等能力，核心指标是ATT和ACW的降低，ATT是人工和用户的平均沟通时长，ACW是人工沟通后的其它处理时长。</p><h3 id="2-2-智能机器人——多轮对话"><a href="#2-2-智能机器人——多轮对话" class="headerlink" title="2.2 智能机器人——多轮对话"></a>2.2 智能机器人——多轮对话</h3><p><img src="https://p1.meituan.net/travelcube/9c8e118c2ad215bc84961a2ef9f0442a484180.png" alt="img"></p><p>这是一个真实的多轮对话的例子。当用户进入到服务门户后，先选择了一个推荐的问题“如何联系骑手”，机器人给出了联系方式致电骑手。同时为了进一步厘清场景，询问用户是否收到了餐品，当用户选择“还没有收到”的时候，结合预计送达时间和当前时间，发现还未超时，给出的方案是“好的，帮用户催一下”，或者是“我再等等吧”，这时候用户选择了“我再等等吧”。</p><p><img src="https://p0.meituan.net/travelcube/2474ae90ac83e7955f72509c5326ed18419810.png" alt="img"></p><p>这个例子背后的机器人是怎么工作的呢？首先当用户输入“如何联系骑手”的时候，问题理解模块将它与知识库中的拓展问进行匹配，进而得到对应的标准问即意图“如何联系骑手”。然后对话管理模块根据意图“如何联系骑手”触发相应的任务流程，先查询订单接口，获取骑手电话号码，进而输出对话状态给到答案生成模块，根据模板生成最终结果，如右边的红框内容所示。在这个过程中涉及到要先有意图体系、定义好Task流程，以及订单的查询接口，这些都是业务强相关的，主要由各业务的运营团队来维护。那么，对话系统要做的是什么呢？一是将用户的输入与意图体系中的标准问进行匹配，二是完成多轮交互里面的调度。</p><p><img src="https://p0.meituan.net/travelcube/0c9c76ca8c23d8b0289ce984b9b96da7398937.png" alt="img"></p><p>问题理解是将用户问题与意图体系进行匹配，匹配到的拓展问所对应的标准问即用户意图。机器人的工作过程实际是要做召回和精排两件事情。召回更多地是用现有检索引擎实现，技术上更多地关注精排。</p><p>美团自研的智能客服系统是从2018年开始搭建的，在建设的过程中，我们不断地将业界最先进的技术引入到我们的系统中来，同时根据美团业务的特点，以及问题理解这个任务的特点，对这些技术进行适配。</p><p>比如说，当2018年底BERT（参见《<a href="https://tech.meituan.com/2019/11/14/nlp-bert-practice.html">美团BERT的探索和实践</a>》一文）出现的时候，我们很快全量使用BERT替换原来的DSSM模型。后面，又根据美团客服对话的特点，我们将BERT进行了二次训练及在线学习改造，同时为了避免业务之间的干扰，以及通过增加知识区分性降低噪音的干扰，我们还做了多任务学习（各业务在上层为独立任务）以及多域学习（Query与拓展问匹配，改为与拓展问、标准问和答案的整体匹配），最终我们的模型为Online Learning based Multi-task Multi-Field RoBERTa。经过这样一系列技术迭代，我们的识别准确率也从最初不到80%到现在接近90%的水平。</p><p><img src="https://p0.meituan.net/travelcube/70c33298481b98519a94f089ae25a4fe584787.png" alt="img"></p><p>理解了用户意图后，有些问题是可以直接给出答案解决的，而有些问题则需要进一步厘清。比如说“如何申请餐损”这个例子，不是直接告诉申请的方法，而是先厘清是哪一个订单，是否影响食用，进而厘清一些用户的诉求是部分退款还是想安排补送，从而给出不同的解决方案。这样的一个流程是跟业务强相关的，需要由业务的运营团队来进行定义。如右边任务流程树所示，我们首先提供了可视化的TaskFlow编辑工具，并且把外呼、地图以及API等都组件化，然后业务运营人员可以通过拖拽的方式来完成Task流程设计。</p><p>对话引擎在与用户的真实交互中，要完成Task内各步骤的匹配调度。比如这个例子里用户如果不是点选”可以但影响就餐了…”这条，而是自己输入说“还行，我要部分退款”，怎么办？这个意图也没有提前定义，这就需要对话引擎支持Task内各步骤的模糊匹配。我们基于Bayes Network搭建的TaskFlow Engine恰好能支持规则和概率的结合，这里的模糊匹配算法复用了问题理解模型的语义匹配能力。</p><p><img src="https://p0.meituan.net/travelcube/67238a4a77181d11b35c236eca53c0fa550682.png" alt="img"></p><p>这是另外一个例子，在用户问完“会员能否退订”后，机器人回复的是“无法退回”，虽然回答了这个问题，但这个时候用户很容易不满意，转而去寻找人工服务。如果这个时候我们除了给出答案外，还去厘清问题背后的真实原因，引导询问用户是“外卖红包无法使用”或者是“因换绑手机导致的问题”，基于顺承关系建模，用户大概率是这些情况，用户很有可能会选择，从而会话可以进一步进行，并给出更加精细的解决方案，也减少了用户直接转人工服务的行为。</p><p>这个引导任务称为多轮话题引导，具体做法是对会话日志中的事件共现关系以及顺承关系进行建模。如右边图所示，这里原本是要建模句子级之间的引导，考虑到句子稀疏性，我们是将其抽象到事件之间的引导，共现关系我们用的是经典的协同过滤方式建模。另外，考虑到事件之间的方向性，我们对事件之间的顺承关系进行建模，公式如下：</p><p><img src="https://p1.meituan.net/travelcube/f9c2b134a5e067a4dc5efbc75a75940217279.png" alt="img"></p><p>并通过多目标学习，同时考虑点击指标和任务指标，如在非转人工客服数据和非不满意数据上分别建模顺承关系，公式如下：</p><p><img src="https://p0.meituan.net/travelcube/7b649f9ec1239193b558997f65d9abf814642.png" alt="img"></p><p>最终，我们在点击率、不满意度、转人工率层面，都取得了非常正向的收益。</p><p><img src="https://p0.meituan.net/travelcube/39c15fd13d6f6cdb736c8695d400113f387670.png" alt="img"></p><p>美团平台涵盖吃、住、行、游、购、娱等200多个生活服务品类，当用户是从美团App或点评App等综合服务门户入口进入服务时，需要先行确定用户要咨询的是哪个业务，这里的一个任务是“判断用户Query是属于哪个业务”，该任务我们叫做领域识别。若能明确判断领域时，则直接用该领域知识来解答；当不能明确判断时，则还需要多轮对话交互与用户进行澄清。比如用户输入“我要退款”，在多个业务里都存在退款意图，这个时候就需要我们先判断是哪个业务的退款意图，如果判断置信度不高，则给出业务列表让用户自行选择来进行澄清。</p><p>领域识别模型主要是对三类数据建模：各领域知识库的有标数据、各领域大量弱监督无标数据和个性化数据。</p><ol><li>依据从各领域知识库的有标数据中学习得到的问题理解模型信号，可以判断用户输入属于各业务各意图的可能性。</li><li>我们注意到除了美团App、点评App等综合服务入口涉及多个业务外，还有大量能够明确业务的入口，比如说订单入口，从商品详情页进来的入口，这些入口进来的对话数据是有明确业务标签信息的。因此，我们可以得到大量的弱监督的各业务领域的数据，基于这些数据我们可以训练一个一级分类模型。</li><li>同时，有些问题是需要结合用户订单状态等个性化数据才能进一步明确的。比如“我要退款”，多个业务里都会有。因此，又要结合用户状态特征一起来训练一个二级模型，最终来判断用户的输入属于哪个业务。</li></ol><p>最终，该二级领域识别模型在满意度、转人工率以及成功转接率指标上都取得了非常不错的收益。</p><h3 id="2-3-智能机器人——问题推荐"><a href="#2-3-智能机器人——问题推荐" class="headerlink" title="2.3 智能机器人——问题推荐"></a>2.3 智能机器人——问题推荐</h3><p><img src="https://p0.meituan.net/travelcube/3348f02d6832c46ed710ff2a53399bbe488438.png" alt="img"></p><p>在介绍完多轮对话基础模块问题理解和对话管理后，接下来我们介绍一下智能机器人的另外两个模块：问题推荐和答案供给。如前面多轮对话的例子所示，当用户进入服务门户后，机器人首先是要如何引导用户精准地表达需求，这样即可降低用户迷失或者直接转人工服务，也降低了若机器人不能正确理解时带来的多轮澄清等无效交互。</p><p>该问题是一个标准的曝光点击问题，它的本质是推荐问题。我们采用了CTR预估任务经典的FM模型来作为基础模型，同时结合业务目标，期望用户点击的问题的解决方案能够解决用户问题，该问题最终定义为“曝光、点击、解决”问题，最终的模型是结合多目标学习的ESSM-FM，对有效交互的转化率、转人工率和不满意度等指标上都带来了提升。</p><h3 id="2-4-智能机器人——答案供给"><a href="#2-4-智能机器人——答案供给" class="headerlink" title="2.4 智能机器人——答案供给"></a>2.4 智能机器人——答案供给</h3><p><img src="https://p0.meituan.net/travelcube/edbed186d8ce5d25602a09ee326a31e7327803.png" alt="img"></p><p>售后客服场景通常问题较集中，且问题的解决多依赖业务内部系统数据及规则，通常是业务部门维护知识库，包括意图体系、Task流程和答案等。但在售前场景，知识多来自于商户或商品本身、用户体验及评价信息等，具有用户问题开放、知识密度高、人工难以整理答案等特点。比如去哪个城市哪个景点游玩，附近有哪些酒店，酒店是否有浴缸，酒店地址在哪里等，都需要咨询”决策”，针对这些诉求，我们通过智能问答来解决咨询以及答案供给问题。</p><p><img src="https://p0.meituan.net/travelcube/ed6f6fbdfea29e32095c1c1d1217c631440754.png" alt="img"></p><p>智能问答就是从美团数据中习得答案供给，来快速回答用户的问题，基于不同的数据源，我们建设了不同的问答技术。</p><ul><li>针对商家基础信息，比如问营业时间、地址、价格等，我们通过图谱问答（KBQA）来解决。利用商家基础信息构建图谱，通过问题理解模型来理解问题，进而查询图谱获取准确的答案。</li><li>针对社区数据，即商户详情页中“问大家”模块的用户问用户答的社区数据，构建社区问答（Community QA）能力，通过对用户问题与问大家中的”问答对”的相似度建模，选择相似度最高的作为答案，来回答用户的一些开放性问题。</li><li>针对UGC评论数据以及商户政策等无结构化数据，构建文档问答（Document QA）能力，针对用户问题利用机器阅读理解技术从文档中抽取答案，类似我们小时候语文考试中的阅读理解题，进一步回答用户的一些开放性问题。</li></ul><p>最后，针对多个问答模块给出的答案，进行多答案来源的答案融合排序，来挑选最终的答案，此外这里还考察了答案真实性，即对“相信多数认为正确的则正确”建模。这部分的详细介绍大家可以参考《<a href="http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247517833&amp;idx=1&amp;sn=0cb67429fa434d3dcd5afd6167754313&amp;chksm=fbd734e5cca0bdf3f0cf43b588153d8117dec25d130240dcb9c42d5219cd94b972e463b55063&amp;scene=21#wechat_redirect">美团智能问答技术探索与实践</a>》一文。</p><h2 id="3-人工辅助核心技术"><a href="#3-人工辅助核心技术" class="headerlink" title="3 人工辅助核心技术"></a>3 人工辅助核心技术</h2><h3 id="3-1-人工辅助——话术推荐"><a href="#3-1-人工辅助——话术推荐" class="headerlink" title="3.1 人工辅助——话术推荐"></a>3.1 人工辅助——话术推荐</h3><p><img src="https://p1.meituan.net/travelcube/b2fa946d9aef38a5dcdcfbeec5e21596267701.png" alt="img"></p><p>前文介绍的都是智能机器人技术，用户除了跟机器人沟通外，还可能是跟人工沟通。我们在客服座席职场调研过程中发现，座席在与用户的对话聊天中经常回复相似甚至相同的话术，他们一致期望提供话术推荐的能力来提高效率。此外，除了请求客服座席帮助外，很多情况下用户与商家直接沟通会使得解决问题更高效，而沟通效率不仅影响到消费者的体验，也影响到了商家的经营。比如在外卖业务中，消费者的下单率和商家的回复时长有较为明显的反比关系，无论是客服座席还是商家，都有很强的话术推荐诉求。</p><p>那么，话术推荐具体要怎么做呢？常见的做法是先准备好常用通用话术库，部分座席或商家也会准备个人常见话术库，然后系统根据用户的Query及上下文来检索最合适的话术来推荐。我们根据调查发现，这部分知识库维护得很不好，既有业务知识变更频繁导致已维护的知识很快不可用因素，也有座席或商家本身意愿不强的因素等。另外，针对新客服座席或者新商家，可用的经验更少。因此我们采用了自动记忆每个座席及其同技能组的历史聊天话术，商家及其同品类商家的历史聊天话术，根据当前输入及上下文，预测接下来可能的回复话术，无需人工进行整理，大大提升了效率。</p><p>我们将历史聊天记录构建成“N+1”QA问答对的形式建模，前N句看作问题Q，后1句作为回复话术A，整个框架就可以转化成检索式的问答模型。在召回阶段，除了文本信息召回外，我们还加入了上文多轮槽位标签，Topic标签等召回优化，排序为基于BERT的模型，加入角色信息建模，角色为用户、商家或者座席。</p><p><img src="https://p0.meituan.net/travelcube/65013909dff115fcb8ea82546414f62b220797.png" alt="img"></p><p>整个架构如上图所示，分为离线和在线两部分。另外上线后我们也加入了一层CTR预估模型来提升采纳率。当前多个业务的话术推荐平均采纳率在24%左右，覆盖率在85%左右。话术推荐特别是对新座席员工价值更大，新员工通常难以组织话术，通过采纳推荐的话术可以来缩减熟练周期，观测发现，3个月内座席员工的平均采纳率是3个月以上座席员工的3倍。</p><h3 id="3-2-人工辅助——会话摘要"><a href="#3-2-人工辅助——会话摘要" class="headerlink" title="3.2 人工辅助——会话摘要"></a>3.2 人工辅助——会话摘要</h3><p><img src="https://p0.meituan.net/travelcube/229f4bcb8bbdfe95db0a37cef1babe67114904.png" alt="img"></p><p>在客服场景座席跟用户沟通完后，还需要对一些必要信息进行工单纪要，包括是什么事件，事件发生的背景是什么，用户的诉求是什么，最后的处理结果是什么等等。而填写这些内容对座席来说其实是很不友好，通常需进行总结归纳，特别是有些沟通进行的时间还比较长，需要来回翻看对话历史才能正确总结。另外，为了持续对于服务产品进行改善，也需要对会话日志进行相应事件抽取及打上标签，从而方便经营分析。</p><p><img src="https://p0.meituan.net/travelcube/f07c14d555730bdeed7fe84b46e61df5420614.png" alt="img"></p><p>这里有些问题是选择题，有些问题是填空题，比如这通会话具体聊的是哪个事件，我们提前整理有比较完整的事件体系，可以看成是个选择题，可以用分类或者语义相似度计算模型来解决。又比如说事件发生的背景，如外卖退款的背景是因餐撒了、酒店退款的背景是到店没有房间等，是个开放性问题，分析发现可以很好地从对话内容中抽取，可以用摘要抽取模型来解决。而对于处理结果，不仅仅依赖对话内容，还包括是否外呼，外呼了是否商家接通了，后续是否需要回访等等，我们实验发现生成模型更有效。具体使用的模型如上图所示，这里事件选择考虑到经常有新事件的添加，我们转成了双塔的相似度计算任务，背景抽取采用的是BERT-Sum模型，处理结果采用的是谷歌的PEGASUS模型。</p><h2 id="04-小结与下一步计划"><a href="#04-小结与下一步计划" class="headerlink" title="04 小结与下一步计划"></a>04 小结与下一步计划</h2><h3 id="4-1-小结——交互立方"><a href="#4-1-小结——交互立方" class="headerlink" title="4.1 小结——交互立方"></a>4.1 小结——交互立方</h3><p><img src="https://p0.meituan.net/travelcube/73c0e4f8c8049dee4a3b67802d01854c252670.png" alt="img"></p><p>前面介绍了美团智能客服实践中的一些核心技术，过程中也穿插着介绍了客服座席与消费者/商家/骑手/团长等之间的沟通提效，以及消费者与商家之间的沟通提效。除了这两部分之外，在企业办公场景，其实还有员工之间、销售顾问与商家之间的大量沟通。如果一个个去做，成本高且效率低，解决方案是把智能客服中沉淀的能力进行平台化，最好“一揽子”进行解决，以固定成本来支持更多的业务需求。于是我们搭建了美团的对话平台-摩西对话平台，用“一揽子”方案以固定成本来解决各业务的智能客服需求。</p><h3 id="4-2-小结——对话平台“摩西”"><a href="#4-2-小结——对话平台“摩西”" class="headerlink" title="4.2 小结——对话平台“摩西”"></a>4.2 小结——对话平台“摩西”</h3><p><img src="https://p0.meituan.net/travelcube/c8bc099a2e09d3fd128e88eb217f7344346944.png" alt="img"></p><p>构建一个怎么样的对话平台，才能提供期望的没有NLP能力的团队也能拥有很好的对话机器人呢？首先是把对话能力工具化和流程化。如上图所示，系统可分为四层：应用场景层、解决方案层、对话能力层、平台功能层。</p><ul><li><strong>应用场景层</strong>：在售前应用场景，一类需求是商家助手，如图中所列的美团闪购IM助手和到综IM助手，需要辅助商家输入和机器人部分接管高频问题能力；还有一类需求是在没有商家IM的场景需要智能问答来填补咨询空缺，比如图中所列的酒店问一问和景点问答搜索；另外售中、售后以及企业办公场景，各自需求也不尽相同。</li><li><strong>解决方案层</strong>：这就要求我们有几套解决方案，大概可以分为智能机器人、智能问答、商家辅助、座席辅助等。每个解决方案的对话能力要求也有所不同，这些解决方案是需要很方便地对基础对话能力进行组装，对使用方是透明的，可以拿来即用。</li><li><strong>对话能力层</strong>：前面也进行了相应的介绍，六大核心能力包括问题推荐、问题理解、对话管理、答案供给、话术推荐和会话摘要。</li><li><strong>平台功能层</strong>：此外，我们需要提供配套的运营能力，提供给业务方的运营人员来日常维护知识库、数据分析等等。</li></ul><p>其次，提供“一揽子”的解决方案，还需要针对处在不同阶段的业务提供不同阶段的解决方案。</p><ul><li>有些业务只希望维护好常用的问答，能回答高频的问题就好，那么他们只需要维护一个入门级的机器人，只需要在意图管理模块来维护它的意图，意图的常见说法以及答案就可以了。</li><li>而对于有运营资源的团队，他们希望不断地去丰富知识库来提升问答能力，这个时候可以使用知识发现模块，可以自动地从每天的日志里面发现新意图及意图的新说法，运营人员只需要每天花一点时间来确认添加及维护答案即可，这是一个进阶的业务方。</li><li>还有一些高级的业务方希望调用他们业务中的API来完成复杂问题的求解。这个时候他们可以使用TaskFlow编辑引擎，在平台上直接注册业务的API，通过可视化拖拽的方式来完成Task编辑。</li></ul><p>此外， 为了进一步方便更多的业务介入，我们也提供了一些闲聊、通用指令、地区查询等官方技能包，业务方可以直接勾选使用。另外，随着我们不断在业务中沉淀，也会有越来越多的官方行业技能包。整体方向上是逐步让业务方使用的门槛变得越来越低。</p><h3 id="4-3-下一步计划"><a href="#4-3-下一步计划" class="headerlink" title="4.3 下一步计划"></a>4.3 下一步计划</h3><p><img src="https://p0.meituan.net/travelcube/543211ed2821d704f313310a48b1af14141739.png" alt="img"></p><p>前文所介绍的对话系统是一种Pipeline式对话系统，按照功能划分为不同的模块，各个模块单独建模，依次串联。这种方式的好处是可以做到不同团队职责的有效分工，比如研发同学专注于建设好问题推荐模型、问题理解模型和Task引擎等；业务运营同学专注于意图体系维护、Task流程设计以及答案设计等等。它的劣势也很明显，模块耦合，误差累积，很难联合优化，进而各模块负责的同学可能会去修修补补，容易导致动作变形。</p><p>另一类建模方式是End-to-End，将Pipeline式对话系统的各个模块联合建模成一个模型，直接实现语言到语言的转变，此类方法最初应用在闲聊式对话系统里面，近期随着大规模预训练模型的快速发展，学术上也逐渐开始研究基于预训练模型的端到端任务型对话系统。它的优点是模型可以充分利用无监督人人会话，用数据驱动可以快速迭代；缺点是模型的可控性差，不易解释且缺乏干预能力。目前主要以学术研究为主，未见成熟的应用案例。</p><p>除了使用这种大量无监督的人人会话日志外，还有一种思路是基于Rule-Based TaskFlow构建规则的用户模拟器，进行交互以生成大量的对话数据，进而训练对话模型。为了保证对话系统的鲁棒性，也可使用类似对抗攻击的方法优化，可以模拟Hard User的行为，不按顺序执行TaskFlow，随机打断、跳转某个对话节点等等。</p><p>此外，通过对比分析人机对话日志和人人对话日志，人机对话比较僵硬死板，无法有效捕捉用户的情绪，而人就很擅长这方面。这在客服场景非常重要，用户往往进来就是带着负面情绪的，机器人需要有共情能力。而端到端数据驱动的对话和对话共情能力建设，也将是接下来一段时间我们尝试的重点方向。</p>]]></content>
    
    
    
    <tags>
      
      <tag>智能问答系统</tag>
      
      <tag>美团</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NER嵌套实体如何处理</title>
    <link href="/2021/10/30/2021-10-30-NER%E5%B5%8C%E5%A5%97%E5%AE%9E%E4%BD%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/"/>
    <url>/2021/10/30/2021-10-30-NER%E5%B5%8C%E5%A5%97%E5%AE%9E%E4%BD%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>先给出方法<strong>总结</strong>（主要参考自<a href="https://zhuanlan.zhihu.com/p/77868938">nlp中的实体关系抽取方法总结Q2-JayJay</a>）：</p><ol><li><p>修改BIO 标签：比如B-city和 B-organization 组合为 B-city|organization</p><p>简单，改动小；但是导致标签稀疏，难以学习；</p></li><li><p>修改模型的Decoder</p><p>①token-level 的多标签分类：比如B-city（1）、B-organization（1）、其他（0）<br>②采用<a href="https://www.yuque.com/ningshixian/pz10h0/ckvfg8">指针网络</a>，转化为n个2元Sigmoid分类预测头指针和尾指针：比如针对 city，预测头（1 0 0 0），预测尾（0 1 0 0）<br>③转换成阅读理解问题：比如“[CLS]北京大学[SEP]提及的城市是啥”，类似 prompt的模板拼接；<br>④采用<a href="https://www.yuque.com/ningshixian/pz10h0/ckvfg8">GlobalPointer</a>，解决指针网络的Exposure Bias问题；</p></li><li><p>对句子所有n-gram进行分类；</p></li></ol><h2 id="嵌套实体问题"><a href="#嵌套实体问题" class="headerlink" title="嵌套实体问题"></a>嵌套实体问题</h2><p>NER是一个比较常见的NLP任务，通常采用LSTM+CRF处理一些简单NER任务。NER还存在嵌套实体问题（实体重叠问题），实体嵌套是指在一句文本中出现的实体，存在某个较短实体完全包含在另外一个较长实体内部的情况，如「《叶圣陶散文选集》」中会出现两个实体「叶圣陶」和「叶圣陶散文选集」分别代表「作者」和「作品」两个实体。而传统做法由于每一个token只能属于一种Tag，无法解决这类问题。下面归纳了几种常见并易于理解的解决办法：</p><h2 id="1、序列标注：SoftMax和CRF"><a href="#1、序列标注：SoftMax和CRF" class="headerlink" title="1、序列标注：SoftMax和CRF"></a>1、序列标注：SoftMax和CRF</h2><p>命名实体识别本来属于基于字（token-level）的多分类问题，通常采用CNNs/RNNs/BERT+CRF处理这类问题，与SoftMax相比，CRF进了标签约束。但由于这种序列标注采取BILOU标注框架，每一个token只能属于一种，不能解决重叠实体问题，如图所示。</p><img src="/2021/10/30/2021-10-30-NER%E5%B5%8C%E5%A5%97%E5%AE%9E%E4%BD%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/1656322193757-1d1270cb-d69c-46f3-8bc2-02a104c21c1c.png" class="" title="image.png"><p>基于BILOU标注框架，下面列出2种改进方法解决实体重叠问题：</p><ul><li>改进方法1：采取token-level 的多label分类，将SoftMax替换为Sigmoid，如图所示。当然这种方式可能会导致：1)label之间依赖关系的缺失，可采取后处理规则进行约束。2)学习难度较大</li></ul><img src="/2021/10/30/2021-10-30-NER%E5%B5%8C%E5%A5%97%E5%AE%9E%E4%BD%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/1656322249719-c147129b-f6ee-4b26-9342-af18a7273f00.png" class="" title="image.png"><ul><li>改进方法2：依然采用CRF，但设置多个标签层，对于每一个token给出其所有的label，然后将所有标签层合并。显然这可能会导致：1）增加label数量，导致label不平衡问题。2）指数级增加了标签3）对于多层嵌套，稀疏问题较为棘手</li></ul><img src="/2021/10/30/2021-10-30-NER%E5%B5%8C%E5%A5%97%E5%AE%9E%E4%BD%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/1656322263903-fd6639f5-25d6-4977-9f03-a6d4d35da167.png" class="" title="image.png"><h2 id="2、Span抽取：指针网络"><a href="#2、Span抽取：指针网络" class="headerlink" title="2、Span抽取：指针网络"></a>2、Span抽取：指针网络</h2><p>指针网络（PointerNet）最早应用于MRC中，而MRC中通常根据1个question从passage中抽取1个答案片段，转化为2个n元SoftMax分类预测头指针和尾指针。对于NER可能会存在多个实体Span，因此需要转化为n个2元Sigmoid分类预测头指针和尾指针。</p><p>将指针网络应用于NER中，可以采取以下两种方式：</p><ul><li>第一种：MRC-QA+单层指针网络。构建query问题指代所要抽取的实体类型，同时也引入了先验语义知识。对不同实体类型构建query，并采取指针标注，此外也构建了矩阵来判断span是否构成一个实体mention。如图所示，由于构建query问题已经指代了实体类型，所以使用单层指针网络即可；除了使用指针网络预测实体开始位置、结束位置外，还基于开始和结束位置对构成的所有实体Span预测实体概率。此外，这种方法也适合于给定事件类型下的事件主体抽取，可以将事件类型当作query，也可以将单层指针网络替换为CRF。</li></ul><img src="/2021/10/30/2021-10-30-NER%E5%B5%8C%E5%A5%97%E5%AE%9E%E4%BD%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/1656322361148-72188ac0-f25e-4060-a4b1-2ec9e6669edb.png" class="" title="image.png"><ul><li>第二种：多层label指针网络。由于只使用单层指针网络时，无法抽取多类型的实体，我们可以构建多层指针网络，每一层都对应一个实体类型。</li></ul><img src="/2021/10/30/2021-10-30-NER%E5%B5%8C%E5%A5%97%E5%AE%9E%E4%BD%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/1656322362845-3eea1dce-e326-442c-8a79-44e0aed50b55.png" class="" title="image.png"><p>需要注意的是：</p><ol><li>MRC-QA会引入query进行实体类型编码，这会导致需要对愿文本重复编码输入，以构造不同的实体类型query，这会提升计算量。</li><li>n个2元Sigmoid分类的指针网络，会导致样本Tag空间稀疏，同时收敛速度会较慢，特别是对于实体span长度较长的情况。</li></ol><h2 id="3、片段排列-分类"><a href="#3、片段排列-分类" class="headerlink" title="3、片段排列+分类"></a>3、片段排列+分类</h2><p>上述序列标注和Span抽取的方法都是停留在token-level进行NER，间接去提取span-level的特征。而基于片段排列的方式，显示的提取所有可能的片段排列，由于选择的每一个片段都是独立的，因此可以直接提取span-level的特征去解决重叠实体问题。</p><p>对于含T个token的文本，理论上共有 （T+1）/2种片段排列。如果文本过长，会产生大量的负样本，在实际中需要限制span长度并合理削减负样本。</p><img src="/2021/10/30/2021-10-30-NER%E5%B5%8C%E5%A5%97%E5%AE%9E%E4%BD%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/1656322366104-29183679-53a4-423c-b35b-7d3b968b220b.png" class="" title="image.png"><p>需要注意的是：</p><ul><li>1）实体span的编码表示：在span范围内采取注意力机制与基于原始输入的LSTM编码进行交互。然后所有的实体span表示并行的喂入SoftMax进行实体分类。</li><li>2）这种片段排列的方式对于长文本复杂度是较高的。</li></ul><h2 id="4、Seq2Seq"><a href="#4、Seq2Seq" class="headerlink" title="4、Seq2Seq"></a>4、Seq2Seq</h2><p>ACL2019的一篇paper中采取Seq2Seq方法<a href="https://zhuanlan.zhihu.com/p/77868938#ref_3">[3]</a>，encoder部分输入的原文tokens，而decoder部分采取hard attention方式one-by-one预测当前token所有可能的tag label，直至输出<eow> (end of word) label，然后转入下一个token再进行解码。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/77868938">nlp中的实体关系抽取方法总结-JayJay</a></li><li><a href="https://zhuanlan.zhihu.com/p/126347862">浅谈嵌套命名实体识别（Nested NER）</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>NER</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NER综述</title>
    <link href="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/"/>
    <url>/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="命名实体识别简述"><a href="#命名实体识别简述" class="headerlink" title="命名实体识别简述"></a>命名实体识别简述</h1><h2 id="一、什么是NER"><a href="#一、什么是NER" class="headerlink" title="一、什么是NER"></a>一、什么是NER</h2><p>命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。NER是信息提取、问答系统、句法分析、机器翻译、面向Semantic Web的元数据标注等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要的地位。NER是深度查询理解（Deep Query Understanding，简称 DQU）的底层基础信号，主要应用于搜索召回、用户意图识别、实体链接等环节。</p><h2 id="二、NER主要方法"><a href="#二、NER主要方法" class="headerlink" title="二、NER主要方法"></a>二、NER主要方法</h2><h3 id="2-1-基于规则和词典的方法"><a href="#2-1-基于规则和词典的方法" class="headerlink" title="2.1 基于规则和词典的方法"></a>2.1 基于规则和词典的方法</h3><p>一般来说，我们在做命名实体的时候，可以首先考虑一下可否使用正则。假如命名实体的名称规律比较简单，我们可以找出模式，然后设计相应的正则表达式或者规则，然后把符合模式的字符串匹配出来，作为命名实体识别的结果。</p><p>优点：这种NER系统的特点是高精确率与低召回率；<br>缺点：难以迁移应用到别的领域中去，基于领域的规则往往不通用，对新的领域而言，需要重新制定规则且不同领域字典不同；此外，需要保证用户输入的关键词和预存词表完全一致，且当词表数量较大时，正则表达式将面临匹配速度、内存占用等挑战。</p><h3 id="2-2-无监督学习方法"><a href="#2-2-无监督学习方法" class="headerlink" title="2.2 无监督学习方法"></a>2.2 无监督学习方法</h3><p>主要是基于聚类的方法，根据文本相似度得到不同的簇，表示不同的实体类别组。常用到的特征或者辅助信息有词汇资源、语料统计信息（TF-IDF）、浅层语义信息（分块NP-chunking）等。</p><h3 id="2-3-基于特征的监督学习方法"><a href="#2-3-基于特征的监督学习方法" class="headerlink" title="2.3 基于特征的监督学习方法"></a>2.3 基于特征的监督学习方法</h3><p>NER可以被转换为一个分类问题或序列标记问题。分类问题就是判断一个词语是不是命名实体、是哪一种命名实体。常见的做法就是，基于一个词语或者字的上下文构造特征，来判断这个词语或者字是否为命名实体；序列标注方法就是给句子中的每个词按照需求的方式打上一个标签，标签的格式通常有IOB2和IOBES两种标准。缺陷是无法处理嵌套实体的情况。</p><p>上述问题涉及到特征工程和模型选择，需要训练模型使其能够对句子给出标记序列作为预测。</p><ul><li>特征工程：word级别特征（词法特征、词性标注等），词汇特征（维基百科、DBpdia知识），文档及语料级别特征。</li><li>模型选择：隐马尔可夫模型、决策树、最大熵模型、最大熵马尔科夫模型、支持向量机、条件随机场。</li></ul><h3 id="2-4-深度学习方法"><a href="#2-4-深度学习方法" class="headerlink" title="2.4 深度学习方法"></a>2.4 深度学习方法</h3><p>近年来，基于DL的NER模型占据了主导地位并取得了最先进的成果。与基于特征的方法相比，深度学习有利于自动发现隐藏的特征。NN把语言看做是序列数据，然后用自身极强的拟合能力，把这种序列转换为标签序列。BiLSTM+CRF方案结合了神经网络的拟合能力和CRF的全局视野，是非常经典、有效的一种NER模型结构。</p><h4 id="1）BiLSTM-CRF"><a href="#1）BiLSTM-CRF" class="headerlink" title="1）BiLSTM+CRF"></a>1）BiLSTM+CRF</h4><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/640-20230424185911111.png" class=""><p>BiLSTM的输出作为CRF的发射概率矩阵，而CRF层可以加入一些约束来保证最终预测结果是有效的。这些约束可以在训练数据时被CRF层自动学习得到。</p><h4 id="2）IDCNN-CRF"><a href="#2）IDCNN-CRF" class="headerlink" title="2）IDCNN+CRF"></a>2）IDCNN+CRF</h4><p>尽管BILSTM在NER任务中有很好的表现，但是却不能充分利用GPU的并行性，导致该模型的想能较差，因此出现了一种新的NER模型方案IDCNN+CRF。</p><p>在IDCNN+CRF模型结构中，待识别query先经过Embedding层获取向量表示；然后经过空洞卷积层（IDCNN），IDCNN通过空洞卷积增大模型的感受野， 相较于传统的CNN，IDCNN能够捕捉更长的上下文信息，更适合序列标注这类需要全局信息的任务；在IDCNN之后经过一层全连接神经网络（FF层）后引入CRF，同样CRF的目的在于防止非法槽位标记（BIO）的出现。</p><blockquote><p>补充：尽管传统的CNN有明显的计算优势，但是传统的CNN在经过卷积之后，末梢神经元只能得到输入文本的一小部分信息，为了获取上下文信息，需要加入更多的卷积层，导致网络越来越深，参数越来越多，容易发生过拟合。</p></blockquote><p>文本空洞卷积的示意图如下：</p><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/v2-63f77c2c69c5055f1bacc11429afa7a5_1440w-20230424185922749.jpg" class=""><h4 id="3）Bert-BiLSTM-CRF"><a href="#3）Bert-BiLSTM-CRF" class="headerlink" title="3）Bert+BiLSTM+CRF"></a>3）Bert+BiLSTM+CRF</h4><p>Bert由谷歌大佬与2018年提出来，刚出来的时候横扫了11项NLP任务。BERT通过微调的方法可以灵活的应用到下游业务，所以这里我们也可以考虑使用Bert作为embedding层，将特征输入到Bilstm+CRF中，以谋求更好的效果。</p><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/640-20230424185934632.png" class=""><h4 id="PS：NER模型之CRF的作用"><a href="#PS：NER模型之CRF的作用" class="headerlink" title="PS：NER模型之CRF的作用"></a>PS：NER模型之CRF的作用</h4><blockquote><p>关于 CRF 的介绍可以参考《<a href="https://www.yuque.com/ningshixian/pz10h0/ah4kxz">2021-02-22-条件随机场 (CRF) 概述</a>》</p></blockquote><p>在上述模型中，在NER任务上，我们看到很多深度学习之后都会接上一层CRF，那么CRF在整个过程中到底发挥着什么样的作用呢？通常我们直接使用逐帧softmax时，是将序列标注过程作为n个k分类问题，相当于每个token相互独立的进行分类（假设深度模型内部交互不明显的话），而采用CRF实质上是在进行一个$k^n$分类，相当于直接从所有的序列空间里找出转移概率最大的那条序列。其实质上是局部最优（token最优）与全局最优（序列最优）的区别，因而采用CRF能够有效避免出现非法的序列标记，从而确保序列有效。</p><h2 id="三、NER模型效果优化"><a href="#三、NER模型效果优化" class="headerlink" title="三、NER模型效果优化"></a>三、NER模型效果优化</h2><h3 id="3-1-模型优化之数据增强"><a href="#3-1-模型优化之数据增强" class="headerlink" title="3.1 模型优化之数据增强"></a>3.1 模型优化之数据增强</h3><p>针对启动阶段存在的数据不足问题，可以采用数据增强的方式来补充训练数据，NER做数据增强，和别的任务有啥不一样呢？很明显，NER是一个token-level的分类任务，在进行全局结构化预测时，一些增强方式产生的数据噪音可能会让NER模型变得敏感脆弱，导致指标下降、最终奔溃。</p><h4 id="An-Analysis-of-Simple-Data-Augmentation-for-Named-Entity-Recognition"><a href="#An-Analysis-of-Simple-Data-Augmentation-for-Named-Entity-Recognition" class="headerlink" title="- An Analysis of Simple Data Augmentation for Named Entity Recognition"></a>- An Analysis of Simple Data Augmentation for Named Entity Recognition</h4><p>参考论文《<strong>An Analysis of Simple Data Augmentation for Named Entity Recognition</strong>》主要是将传统的数据增强方法应用于NER中、并进行全面分析与对比。效果如何？</p><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/640-20230424185947453.png" class=""><p>作者借鉴sentence-level的传统数据增强方法，将其应用于NER中，共有4种方式（如上图所示）：</p><ul><li><strong>Label-wise token replacement (LwTR)</strong> ：即同标签token替换，采用二项分布概率对句子进行采样，概率替换某位置的token为同标签其它token，如果token长度不一致，则进行延展，句子长度发生变化。</li><li><strong>Synonym replacement (SR)</strong> ：即同义词替换，利用WordNet查询同义词，然后根据二项分布随机替换。如果替换的同义词大于1个token，那就依次延展BIO标签。</li><li>**_Mention replacement (MR)_ ：即实体提及替换，与同义词方法类似，利用训练集中的相同实体类型进行替换，如果替换的mention大于1个token，那就依次延展BIO标签，如上图：「headache」替换为「neuropathic pain syndrome」，依次延展BIO标签。</li><li><strong>Shuffle within segments (SiS)</strong> ：按照mention来切分句子，然后再对每个切分后的片段进行shuffle。如上图，共分为5个片段： [She did not complain of], [headache], [or], [any other neurological symptoms], [.]. 。也是通过二项分布判断是否被shuffle（mention片段不会被shuffle），如果shuffle，则打乱片段中的token顺序。</li><li><strong>总结规则模板，直接生成数据</strong>。（收益不小）</li></ul><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/640-20230424185956836.png" class=""><p>由上图得出以下结论：</p><ul><li>各种数据增强方法都超过不使用任何增强时的baseline效果。</li><li>对于RNN网络，<strong>实体提及替换优于其他方法</strong>；对于Transformer网络，<strong>同义词替换最优。</strong></li><li>总体上看，所有增强方法一起使用（<strong>ALL</strong>）会由于单独的增强方法。</li><li>低资源条件下，数据增强效果增益更加明显；</li><li>充分数据条件下，数据增强可能会带来噪声，甚至导致指标下降；</li></ul><h3 id="3-2-模型优化之词汇增强"><a href="#3-2-模型优化之词汇增强" class="headerlink" title="3.2 模型优化之词汇增强"></a>3.2 <a href="https://zhuanlan.zhihu.com/p/142615620">模型优化之词汇增强</a></h3><blockquote><p><a href="http://kuaibao.qq.com/s/20200228A09V2X00">基于词汇增强的中文命名实体识别</a> </p></blockquote><p><strong>有的学者开始另辟蹊径，利用外部词汇信息力求与BERT一战；</strong><br><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/1644373309297-b1992c67-d2f6-4fe7-b7c6-bca669531529.png" class="" title="image.png"></p><h4 id="Lattice-LSTM：Chinese-NER-Using-Lattice-LSTM"><a href="#Lattice-LSTM：Chinese-NER-Using-Lattice-LSTM" class="headerlink" title="- Lattice LSTM：Chinese NER Using Lattice LSTM"></a>- Lattice LSTM：<a href="https://arxiv.org/abs/1805.02023">Chinese NER Using Lattice LSTM</a></h4><p>引入词汇信息，在原有的输入序列的基础上添加匹配到的词汇作为额外的链路，整体看起来有点像<code>ResNet</code>的短路链接，两端分别连接原始输入序列的词首尾，称之为<code>Latttice-LSTM</code>。事实也证明词典带来的提升是明显的，一举超越<code>BERT</code>，重回武林宝座。缺点： 计算性能低下，不能batch并行化；信息损失：每个字符只能 获取以它为结尾的词汇信息；可迁移性差；</p><h4 id="LR-CNN：CNN-Based-Chinese-NER-with-Lexicon-Rethinking"><a href="#LR-CNN：CNN-Based-Chinese-NER-with-Lexicon-Rethinking" class="headerlink" title="- LR-CNN：CNN-Based Chinese NER with Lexicon Rethinking"></a>- LR-CNN：<a href="https://pdfs.semanticscholar.org/1698/d96c6fffee9ec969e07a58bab62cb4836614.pdf&#39;">CNN-Based Chinese NER with Lexicon Rethinking</a></h4><p>该篇指出<code>Latttice-LSTM</code>第一：速度太慢，第二：无法进行词汇匹配的选择。为了解决这两个问题，将原始输入序列按照词典匹配的词汇信息进行<code>Bigram,Trigram</code>合并然后<code>CNN</code>特征提取，然后将匹配到词汇信息，进行时间维度上attention计算后，利用<code>Rethinking</code>机制，反馈到原始<code>Bigram,Trigram</code>层，进行词汇匹配的选择，以解决词汇冲突的问题。</p><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/LR-CNN.png" class="" title="image.png"><h4 id="Bipartite-Flat-Graph-Network-for-Nested-Named-Entity-Recognition"><a href="#Bipartite-Flat-Graph-Network-for-Nested-Named-Entity-Recognition" class="headerlink" title="- Bipartite Flat-Graph Network for Nested Named Entity Recognition"></a>- <a href="https://arxiv.org/abs/2005.00436">Bipartite Flat-Graph Network for Nested Named Entity Recognition</a></h4><p>将引入的词汇作为额外的链路，与原始序列一起构建成输入图，字作为节点，链接是关系，然后通过对图进进行建模获得图节点的嵌入式表征，最后使用CRF进行解码。</p><h4 id="FLAT：Chinese-NER-Using-Flat-Lattice-Transformer（ACL2020）"><a href="#FLAT：Chinese-NER-Using-Flat-Lattice-Transformer（ACL2020）" class="headerlink" title="- FLAT：Chinese NER Using Flat-Lattice Transformer（ACL2020）"></a>- FLAT：<a href="https://zhuanlan.zhihu.com/p/391560782?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=30098367447040&amp;utm_campaign=shareopn">Chinese NER Using Flat-Lattice Transformer</a>（ACL2020）</h4><p>FLAT的基本思想来源于Lattice-LSTM，Lattice-LSTM采取的RNN结构无法捕捉长距离依赖，同时引入词汇信息是有损的，同时动态的Lattice结构也不能充分进行GPU并行。为解决<strong>计算效率低下、引入词汇信息有损</strong>的这两个问题，FLAT基于Transformer结构进行了两大改进：</p><p><strong>改进1：Flat-Lattice Transformer，无损引入词汇信息</strong>。FLAT不去设计或改变原生编码结构，设计巧妙的位置向量就融合了词汇信息。具体来说，对于每一个字符和词汇都构建两个head position encoding 和tail position encoding，词汇信息直接拼接到原始输入序列的末尾（避免了引入额外的链路，增加模型复杂度），并用位置编码与原始输入序列的对应位置相关联，间接指明了添加词汇所在的位置信息。</p><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/FLAT.png" class="" title="image.png"><p><strong>改进2：相对位置编码，让Transformer适用NER任务</strong></p><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/FLAT2.png" class="" title="image.png"><h4 id="Lex-BERT-Enhancing-BERT-based-NER-with-lexicons（2021）"><a href="#Lex-BERT-Enhancing-BERT-based-NER-with-lexicons（2021）" class="headerlink" title="- Lex-BERT: Enhancing BERT based NER with lexicons（2021）"></a>- Lex-BERT: Enhancing BERT based NER with lexicons（2021）</h4><p>Lex-BERT相比于FLAT有三点：1. 不需要利用word embedding；2. 可以引入实体类型type信息，作者认为在领域内，可以收集包含类型信息的词汇；3. 相比FLAT，Lex-BERT推断速度更快、内存占用更小；</p><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/Lex-BERT.png" class="" title="image.png"><h4 id="Simple-Lexicon"><a href="#Simple-Lexicon" class="headerlink" title="- Simple-Lexicon"></a>- Simple-Lexicon</h4><p><a href="https://www.yuque.com/ningshixian/pz10h0/veey7r">博客：Simplify the Usage of Lexicon in Chinese NER</a></p><p>词汇信息是有用的，但是如何使用，学术界还未形成统一。可以看得出来，上述文章在引入词汇的方式上五花八门，计算复杂度都比较高。Simple-Lexicon该篇论文直击痛点，对于词汇信息的引入更加简单有效，采取静态加权的方法可以提前离线计算。作者首先分析列举了几种引入词汇信息方法；最终论文发现，将词汇的信息融入到特殊<code>token&#123;B,M,E,S&#125;</code>中，并和原始词向量进行concat，能够带来明显的提升。通过特殊<code>token</code>表征额外信息的方式，在NER与NRE联合学习任务中也逐渐成为一种趋势。具体细节可参考<a href="https://www.bilibili.com/video/av543580471/">视频讲解</a></p><h3 id="3-3-总结"><a href="#3-3-总结" class="headerlink" title="3.3 总结"></a>3.3 总结</h3><p>最后，我们来看一下，上述各种「词汇增强」方法在中文NER任务上的性能：</p><img src="/2021/10/30/2021-10-30-NER%E7%BB%BC%E8%BF%B0/640-20230424190539138.png" class=""><p>上图可以发现：总的来看，ACL2020中的FLAT和Simple-Lexicon效果最佳。具体地说：</p><ul><li>引入词汇信息的方法，都相较于baseline模型biLSTM+CRF有较大提升，可见引入词汇信息可以有效提升中文NER性能。</li><li>采用相同词表对比时，FLAT和Simple-Lexicon好于其他方法。</li><li>结合BERT效果会更佳。</li></ul><h2 id="四、评估标准"><a href="#四、评估标准" class="headerlink" title="四、评估标准"></a>四、评估标准</h2><p>NER任务的目标，通常是“尽量发现所有的命名实体，发现的命名实体要尽量纯净”，也就是要求查全率和查准率比较高。当然，场景也有可能要求其中一项要非常高。</p><p>通常通过与人类标注水平进行比较判断NER系统的优劣。评估分两种：精确匹配评估和宽松匹配评估。</p><h3 id="4-1-精确匹配评估"><a href="#4-1-精确匹配评估" class="headerlink" title="4.1 精确匹配评估"></a>4.1 精确匹配评估</h3><p>NER任务需要同时确定<strong>实体边界</strong>以及<strong>实体类别。</strong>在精确匹配评估中，只有当实体边界以及实体类别同时被精确标出时，实体识别任务才能被认定为成功。</p><p>基于数据的 true positives（TP），false positives（FP），以及false negatives（FN），可以计算NER任务的精确率，召回率以及 F-score 用于评估任务优劣。</p><p>对NER中的 true positives（TP），false positives（FP）与false negatives（FN）有如下解释：</p><ul><li>true positives（TP）：NER能正确识别实体</li><li>false positives（FP）：NER能识别出实体但类别或边界判定出现错误</li><li>false negatives（FN）：应该但没有被NER所识别的实体</li></ul><p><strong>P\R\F的计算公式如下：</strong></p><p><strong>精确率</strong>： <img src="https://www.zhihu.com/equation?tex=Precision%3D%5Cfrac%7BTP%7D%7BTP%2BFP%7D#id=bXC1T&amp;originHeight=43&amp;originWidth=190&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p><strong>召回率</strong>： <img src="https://www.zhihu.com/equation?tex=Recall+%3D+%5Cfrac%7BTP%7D%7BTP%2BFN%7D#id=ZHxpY&amp;originHeight=43&amp;originWidth=163&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p><strong>F-score：</strong><img src="https://www.zhihu.com/equation?tex=2%5Ctimes%5Cfrac%7BPrecision%5Ctimes+Recall%7D%7BPrecision%2BRecall%7D#id=XI7Wb&amp;originHeight=45&amp;originWidth=194&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>其中 F1 值又可以分为 macro-averaged 和 micro-averaged，前者是按照不同实体类别计算 F1，然后取平均；后者是把所有识别结果合在一起，再计算 F1。这两者的区别在于实体类别数目不均衡，因为通常语料集中类别数量分布不均衡，模型往往对于大类别的实体学习较好。</p><h3 id="4-2-宽松匹配评估"><a href="#4-2-宽松匹配评估" class="headerlink" title="4.2 宽松匹配评估"></a>4.2 宽松匹配评估</h3><p>简言之，可视为实体位置区间部分重叠，或位置正确类别错误的，都记为正确或按照匹配的位置区间大小评测。</p><h2 id="五、工业界如何解决NER问题？12个trick，与你分享～"><a href="#五、工业界如何解决NER问题？12个trick，与你分享～" class="headerlink" title="五、工业界如何解决NER问题？12个trick，与你分享～"></a>五、<a href="https://zhuanlan.zhihu.com/p/152463745">工业界如何解决NER问题？12个trick，与你分享～</a></h2><h3 id="5-1-工业界中的NER问题为什么不易解决？"><a href="#5-1-工业界中的NER问题为什么不易解决？" class="headerlink" title="5.1 工业界中的NER问题为什么不易解决？"></a>5.1 工业界中的NER问题为什么不易解决？</h3><p>在真实的工业界场景中，通常面临<strong>标注成本昂贵</strong>、<strong>泛化迁移能力不足</strong>、<strong>可解释性不强</strong>、<strong>计算资源受限</strong>等问题，想要将NER完美落（bian）地（xian）可不简单，那些在经典benchmark上自称做到SOTA的方法放在现实场景中往往“也就那样”。以医疗领域为例：</p><ol><li>不同医院、不同疾病、不同科室的文本描述形式不一致，而标注成本又很昂贵，一个通用的NER系统往往不具备“想象中”的泛化迁移能力。<strong>当前的NER技术在医疗领域并不适合做成泛化的工具</strong>。</li><li>由于医疗领域的严肃性，我们既要知其然、更要知其所以然：<strong>NER系统往往不能采用“一竿子插到底”的黑箱算法</strong>，处理过程应该随着处理对象的层次和深度而逐步叠加模块，下级模块使用上级结果，方便进行迭代优化、并具备可解释性，这样做可解耦医学事件、也便于进行医学实体消歧。</li><li>仅仅使用统计模型的NER系统往往不是万能的，医疗领域相关的实体词典和特征挖掘对NER性能也起着关键作用。此外，NER结果往往不能直接使用，还需进行医学术语标准化。</li><li>由于医院数据不可出院，需要在院内部署NER系统。而通常医院内部的GPU计算资源又不是很充足（成本问题），我们需要让机器学习模型又轻又快（BERT上不动哇），同时要更充分的利用显存。</li></ol><p>以上种种困难，导致了工业界场景求解NER问题不再那么容易，不是一个想当然的事情。</p><h3 id="5-2-做NER的几条教训（趟过的坑）"><a href="#5-2-做NER的几条教训（趟过的坑）" class="headerlink" title="5.2 做NER的几条教训（趟过的坑）"></a>5.2 做NER的几条教训（趟过的坑）</h3><p>下面给出笔者在医疗领域做NER的经验教训（趟过的坑）：</p><p>1、提升NER性能（performance）的⽅式往往不是直接堆砌⼀个BERT+CRF，这样做不仅效果不一定会好，推断速度也非常堪忧。就算BERT效果还不错，付出的代价也是惨重的。</p><blockquote><p>就算直接使用BERT+CRF进行finetune，BERT和CRF层的学习率也不要设成一样，让CRF层学习率要更大一些（一般是BERT的5～10倍），要让CRF层快速收敛。</p></blockquote><p>2、在NER任务上，也不要试图对BERT进⾏蒸馏压缩，很可能吃⼒不讨好。</p><blockquote><p>哈哈，也许废了半天劲去蒸馏，效果下降到还不如1层lstm+crf，推断速度还是慢～</p></blockquote><p>3、NER任务是⼀个重底层的任务，上层模型再深、性能提升往往也是有限的（甚至是下降的）。</p><blockquote><p>不要盲目搭建很深的网络，也不要痴迷于各种attention了。</p></blockquote><p>4、NER任务不同的解码方式（CRF/指针网络/Biaffine<a href="https://zhuanlan.zhihu.com/p/152463745#ref_1">[1]</a>）之间的差异其实也是有限的，不要过分拘泥于解码⽅式。</p><p>5、通过QA阅读理解的方式进行NER任务，效果也许会提升，但计算复杂度上来了，你需要对同⼀⽂本进行多次编码(对同⼀文本会构造多个question)。</p><p>6、设计NER任务时，尽量不要引入嵌套实体，不好做，这往往是一个长尾问题。</p><p>7、不要直接拿Transformer做NER，这是不合适的，详细可参考TENER<a href="https://zhuanlan.zhihu.com/p/152463745#ref_2">[2]</a>。</p><blockquote><p>补充：<a href="https://arxiv.org/abs/1911.04474">TENER: Adapting Transformer Encoder for Named Entity Recognition</a><br>论文详细分析了为什么原始BERT模型在NER上表现不佳的原因：位置编码只具有距离感受能力，不具有方向感受能力；并在借鉴<code>XL-Net</code>的基础上，提出了相对位置编码的方法；使用相对位置编码后，明显提升了BERT在NER上的效果。</p></blockquote><h3 id="5-3-工业界中NER问题的12个trick"><a href="#5-3-工业界中NER问题的12个trick" class="headerlink" title="5.3 工业界中NER问题的12个trick"></a>5.3 工业界中NER问题的12个trick</h3><p>笔者首先给出一个非常直接的打开方式：<strong>1层lstm+crf！</strong><br>从模型层面看，你也许会问：为什么非是1层lstm+crf？1层lstm+crf不能解决业务问题怎么办？遇到更为复杂的场景该怎么办？不着急，且听我慢慢道来。<br>让我们回到一开始列出的那12个问题，并逐一解答：</p><h4 id="Q1、如何快速有效地提升NER性能？"><a href="#Q1、如何快速有效地提升NER性能？" class="headerlink" title="Q1、如何快速有效地提升NER性能？"></a>Q1、如何快速有效地提升NER性能？</h4><p>如果1层lstm+crf，这么直接的打开方式导致NER性能达不到业务目标，这一点也不意外（这是万里长征的第一步～）。这时候除了badcase分析，不要忘记一个快速提升的重要手段：<strong>规则+领域词典</strong>。</p><ul><li>在垂直领域，一个不断积累、不断完善的实体词典对NER性能的提升是稳健的，基于规则+词典也可以快速应急处理一些badcase；</li><li>对于通⽤领域，可以多种分词工具和多种句法短语⼯具进行融合来提取候选实体，并结合词典进行NER。</li></ul><h4 id="Q2、如何在模型层面提升NER性能？"><a href="#Q2、如何在模型层面提升NER性能？" class="headerlink" title="Q2、如何在模型层面提升NER性能？"></a>Q2、如何在模型层面提升NER性能？</h4><p>如果想在模型层面（仍然是1层lstm+crf）搞点事情，上文讲过NER是一个重底层的任务，1层lstm足以很好捕捉NER任务中的方向信息和局部特征了。我们应该集中精力在embedding层下功夫，那就是<strong>引入丰富的特征</strong>：比如char、bigram、词典特征、词性特征、elmo等等，还有更多业务相关的特征；在垂直领域，如果可以预训练一个领域相关的字向量&amp;语言模型，那是最好不过的了。</p><p>总之，<strong>底层的特征越丰富、差异化越大越好。</strong>我们需要构造不同视角下的特征。</p><h4 id="Q3、如何构建引入词汇信息（词向量）的NER？"><a href="#Q3、如何构建引入词汇信息（词向量）的NER？" class="headerlink" title="Q3、如何构建引入词汇信息（词向量）的NER？"></a>Q3、如何构建引入词汇信息（词向量）的NER？</h4><p>具体可参考专栏文章《<a href="https://zhuanlan.zhihu.com/p/142615620">中文NER的正确打开方式：词汇增强方法总结</a>》。ACL2020的Simple-Lexicon<a href="https://zhuanlan.zhihu.com/p/152463745#ref_4">[4]</a>和FLAT<a href="https://zhuanlan.zhihu.com/p/152463745#ref_5">[5]</a>两篇论文，不仅词汇增强模型十分轻量、而且可以比肩BERT的效果。</p><h4 id="Q4、如何解决NER实体span过长的问题？"><a href="#Q4、如何解决NER实体span过长的问题？" class="headerlink" title="Q4、如何解决NER实体span过长的问题？"></a>Q4、如何解决NER实体span过长的问题？</h4><p>如果NER任务中某一类实体span比较长（⽐如医疗NER中的⼿术名称是很长的），直接采取CRF解码可能会导致很多连续的实体span断裂。除了加入规则进行修正外，这时候也可尝试引入<strong>指针网络+CRF</strong>构建<strong>多任务学习</strong>解决。</p><blockquote><p>指针网络会更容易捕捉较长的span，不过指针网络的收敛是较慢的，可以对CRF和指针网络设置不同学习率，或者设置不同的loss权重。</p></blockquote><h4 id="Q5、如何客观看待BERT在NER中的作用？"><a href="#Q5、如何客观看待BERT在NER中的作用？" class="headerlink" title="Q5、如何客观看待BERT在NER中的作用？"></a>Q5、如何客观看待BERT在NER中的作用？</h4><p>对于工业场景中的绝大部分NLP问题（特别是垂直领域），都没有必要堆资源。但这绝不代表BERT是“一无是处”的，在不受计算资源限制、通用领域、小样本的场景下，BERT表现会更好。我们要更好地去利用BERT的优势：</p><ul><li>在低耗时场景中，BERT可以作为一个“对标竞品”，我们可以采取<strong>轻量化</strong>的多种策略组合去逼近甚至超越BERT的性能；</li><li>在垂直领域应用BERT时，我们首先确认领域内的语料与BERT原始的预训练语料之间是否存在gap，如果这个gap越大，那么我们就<strong>不要停止预训练</strong>：继续在领域内进行预训练，继续在具体任务上进行预训练。</li><li>在小样本条件下，利用BERT可以更好帮助我们解决低资源问题：比如基于BERT等预训练模型的文本增强技术<a href="https://zhuanlan.zhihu.com/p/152463745#ref_6">[6]</a>，又比如与主动学习、半监督学习、领域自适应结合（后续详细介绍）。</li><li>在竞赛任务中，BERT很有用！我们可以选取不同的预训练语⾔模型在底层进行特征拼接。具体地，可以将char、bigram和BERT、XLNet等一起拼接喂入1层lstm+crf中。语⾔模型的差异越⼤，效果越好。如果需要对语言模型finetune，需要设置不同的学习率。</li></ul><h4 id="Q6、如何冷启动NER任务？"><a href="#Q6、如何冷启动NER任务？" class="headerlink" title="Q6、如何冷启动NER任务？"></a>Q6、如何冷启动NER任务？</h4><p>如果⾯临的是⼀个冷启动的NER任务，业务问题定义好后，首先要做的就是维护好一个领域词典，而不是急忙去标数据、跑模型；当基于规则+词典的NER系统不能够满足业务需求时，才需要启动人工标注数据、构造机器学习模型。</p><p>当然，我们可以采取一些省成本的标注方式，如结合<strong>领域化的预训练语言模型+主动学习</strong>，挖掘那些“不确定性高”、并且“具备代表性”的高价值样本。</p><blockquote><p>需要注意的是，由于NER通常转化为一个<strong>序列标注任务</strong>，不同于传统的分类任务，我们需要设计一个专门针对序列标注的主动学习框架。</p></blockquote><h4 id="Q7、如何有效解决低资源NER问题？"><a href="#Q7、如何有效解决低资源NER问题？" class="headerlink" title="Q7、如何有效解决低资源NER问题？"></a>Q7、如何有效解决低资源NER问题？</h4><p>如果拿到的NER标注数据还是不够，又不想标注人员介入，这确实是一个比较困难的问题。<br>低资源NLP问题的解决方法通常都针对分类任务，这相对容易一些，如可以采取文本增强、半监督学习等方式，可参考专栏文章《<a href="https://zhuanlan.zhihu.com/p/146777068">标注样本少怎么办？「文本增强+半监督学习」总结</a> 》。</p><p>上述解决低资源NLP问题的方法，往往在NER中提升并不明显。NER本质是基于token的分类任务，其对噪声极其敏感的。如果盲目应用弱监督方法去解决低资源NER问题，可能会导致全局性的性能下降，甚至还不如直接基于词典的NER。<br>这里给出一些可以尝试的解决思路（笔者个人建议，也许还会翻车啊）：</p><ul><li>上文已介绍BERT在低资源条件下能更好地发挥作用：我们可以使用BERT（领域预训练的BERT）进行<strong>数据蒸馏</strong>（半监督学习+置信度选择），同时利用实体词典辅助标注。</li><li>还可以利用<strong>实体词典+BERT相结合</strong>，进行<strong>半监督自训练</strong>，具体可参考文献<a href="https://zhuanlan.zhihu.com/p/152463745#ref_7">[7]</a>。</li><li>工业界毕竟不是搞学术，要想更好地解决低资源NER问题，RD在必要时还是要干预、并进行核查的。</li></ul><h4 id="Q8、如何缓解NER标注数据的噪声问题？"><a href="#Q8、如何缓解NER标注数据的噪声问题？" class="headerlink" title="Q8、如何缓解NER标注数据的噪声问题？"></a>Q8、如何缓解NER标注数据的噪声问题？</h4><p>实际工作中，我们常常会遇到NER数据可能存在标注质量问题，也许是标注规范就不合理（一定要提前评估风险，不然就白干了），当然，正常的情况下只是存在一些小规模的噪声。<br>一种简单地有效的方式就是对训练集进行交叉验证，然后人工去清洗这些“脏数据”。当然也可以将noisy label learning应用于NER任务，惩罚那些噪音大的样本loss权重，具体可参考文献<a href="https://zhuanlan.zhihu.com/p/152463745#ref_8">[8]</a>。</p><h4 id="Q9、如何克服NER中的类别不平衡问题？"><a href="#Q9、如何克服NER中的类别不平衡问题？" class="headerlink" title="Q9、如何克服NER中的类别不平衡问题？"></a>Q9、如何克服NER中的类别不平衡问题？</h4><p>NER任务中，常常会出现某个类别下的实体个数稀少的问题，而常规的解决方法无外乎是重采样、loss惩罚、Dice loss<a href="https://zhuanlan.zhihu.com/p/152463745#ref_9">[9]</a>等等。而在医疗NER中，我们常常会发现这类实体本身就是一个长尾实体（填充率低），如果能挖掘相关规则模板、构建词典库也许会比模型更加鲁棒。</p><h4 id="Q10、如何对NER任务进行领域迁移？"><a href="#Q10、如何对NER任务进行领域迁移？" class="headerlink" title="Q10、如何对NER任务进行领域迁移？"></a>Q10、如何对NER任务进行领域迁移？</h4><p>在医疗领域，我们希望NER模型能够在不同医院、不同疾病间进行更好地泛化迁移（这是一个<strong>领域自适应</strong>问题：源域标注数据多，目标域标注数据较少），领域自适应针对NER的相关研究不多，通常是对抗迁移<a href="https://zhuanlan.zhihu.com/p/152463745#ref_10">[10]</a>或特征迁移<a href="https://zhuanlan.zhihu.com/p/152463745#ref_11">[11]</a>。</p><p>在具体实践中，对抗&amp;特征迁移通常还不如直接采取finetune方式（对源域进行预训练，在目标域finetune），特别是在后BERT时代。此外，在医疗领域，泛化迁移问题并不是一个容易解决的问题，试图去将NER做成一个泛化工具往往是困难的。或许我们更应该从业务角度出发去将NER任务定制化，而不是拘泥于那些无法落地的前沿技术。</p><h4 id="Q11、如何让NER系统变得“透明”且健壮？"><a href="#Q11、如何让NER系统变得“透明”且健壮？" class="headerlink" title="Q11、如何让NER系统变得“透明”且健壮？"></a>Q11、如何让NER系统变得“透明”且健壮？</h4><p>一个好的NER系统并不是“一竿子插到底”的黑箱算法。在医疗领域，实体类型众多，我们往往需要构建一套<strong>多层级、多粒度、多策略</strong>的NER系统。 例如：</p><ul><li>多层级的NER系统更加“透明”，可以回溯实体的来源（利于医学实体消歧），方便“可插拔”地迭代优化；同时也不需要构建数目众多的实体类型，让模型“吃不消”。</li><li>多粒度的NER系统可以提高准召。如，第⼀步抽取⽐较粗粒度的实体，通过模型+规则+词典等多策略保证⾼召回；第⼆步进⾏细粒度的实体分类，通过模型+规则保证⾼准确。</li></ul><h4 id="Q12、如何解决低耗时场景下的NER任务？"><a href="#Q12、如何解决低耗时场景下的NER任务？" class="headerlink" title="Q12、如何解决低耗时场景下的NER任务？"></a>Q12、如何解决低耗时场景下的NER任务？</h4><p>笔者经验，重点应放在工程层面，而不是模型层面的压缩：<br>因为，从模型层面来看，1层lstm+CRF已经够快了</p><ul><li>如果觉得lstm会慢，换成cnn或transformer也许更快一些，不过效果好不好要具体分析；通常来说，lstm对于NER任务的⽅向性和局部特征捕捉会好于别的编码器。</li><li>如果觉得crf的解码速度慢，引入label attention机制把crf拿掉，比如LAN这篇论文<a href="https://zhuanlan.zhihu.com/p/152463745#ref_12">[12]</a>；当然可以⽤指针网络替换crf，不过指针网络收敛慢⼀些。</li><li>如果想进行模型压缩，比如对lstm+crf做量化剪枝也是⼀个需要权衡的⼯作，有可能费力不讨好~<blockquote><p>lstm+crf已经够小了，对小模型进行压缩往往不如对大模型压缩更加健壮<a href="https://zhuanlan.zhihu.com/p/152463745#ref_13">[13]</a>。</p></blockquote></li></ul><p>从模型+工程层面来看，重点应放在如何在多层级的NER系统中进行显存调度、或者使当前层级的显存占用最大化等。</p><h3 id="5-4-总结"><a href="#5-4-总结" class="headerlink" title="5.4 总结"></a>5.4 总结</h3><p>我们要更加稳妥地解决复杂NER问题（词汇增强、冷启动、低资源、噪声、不平衡、领域迁移、可解释、低耗时），这是一个需要权衡的过程，切记不要盲目追前沿，很多脏活累活还是要干一干的。综上：</p><ul><li>我们要在1层lstm+CRF的基础上，引入更丰富的embedding特征，并进行多策略组合，这大概率可以解决垂直领域的NER问题。</li><li>我们要更好地利用BERT、使其价值最大化。<strong>BERT虽好，可不要过度信任啊～</strong></li><li>我们要更加稳妥地解决复杂NER问题（词汇增强、冷启动、低资源、噪声、不平衡、领域迁移、可解释、低耗时），这是一个需要权衡的过程，切记不要盲目追前沿，很多脏活累活还是要干一干的。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://github.com/snipsco/snips-nlu">snips-nlu</a> </li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2101.11420.pdf">2021-Recent Trends in Named Entity Recognition (NER)</a> </li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1812.09449.pdf">2020- Survey on Deep Learning for Named Entity Recognition</a> </li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1910.11470.pdf">2019-A survey on recent advances in named entity recognition from deep learning models</a> </li><li><a href="https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/P18-3006.pdf">2018-Recognizing complex entity mentions: A review and future directions</a> </li><li><a href="https://link.zhihu.com/?target=https%3A//www.sciencedirect.com/science/article/abs/pii/S1574013717302782">2018-Recent named entity recognition and classification techniques: A systematic review</a> </li><li><a href="https://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.714.342%26rep%3Drep1%26type%3Dpdf">2013-Named entity recognition: fallacies, challenges and opportunities</a> </li><li><a href="https://link.zhihu.com/?target=https%3A//time.mk/trajkovski/thesis/li07.pdf">2007-A survey of named entity recognition and classification</a> </li><li>NER相关数据集可以参考：<a href="https://link.zhihu.com/?target=https%3A//github.com/SimmerChan/corpus">SimmerChan/corpus</a> </li><li><a href="https://tech.meituan.com/2020/07/23/ner-in-meituan-nlp.html">美团搜索中NER技术的探索与实践</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>NER</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pointer Network &amp; GlobalPointer</title>
    <link href="/2021/10/30/2021-10-30-Pointer%20Network%20&amp;%20GlobalPointer/"/>
    <url>/2021/10/30/2021-10-30-Pointer%20Network%20&amp;%20GlobalPointer/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Pointer-Network（Ptr-Nets）"><a href="#1-Pointer-Network（Ptr-Nets）" class="headerlink" title="1. Pointer Network（Ptr-Nets）"></a>1. Pointer Network（Ptr-Nets）</h1><blockquote><p>论文：<a href="https://arxiv.org/pdf/1506.03134.pdf">Pointer Networks</a><br>博客：<a href="https://blog.csdn.net/u012744245/article/details/112368408">Pointer Network【文本生成】发展与应用</a>、<a href="https://zhuanlan.zhihu.com/p/48959800">Pointer Networks简介及其应用</a><br>⭐️<a href="https://www.youtube.com/watch?v=VdOyqNQ9aww">李宏毅Pointer Network视频</a></p></blockquote><p>传统的 Seq2Seq 模型中 Decoder 输出的目标数量是固定的，例如翻译时 Decoder 预测的目标数量等于字典的大小。这导致 Seq2Seq 不能用于一些组合优化的问题，例如凸包问题（Convex Hull ），三角剖分（Delaunay Triangulation ），旅行商问题 (TSP) 等。Pointer Network 可以解决输出字典大小可变的问题，Pointer Network 的输出字典大小等于 Encoder 输入序列的长度并修改了 Attention 的方法，根据 Attention 的值从 Encoder 的输入中选择一个作为 Decoder 的输出。</p><h2 id="1-1-Why-Pointer-Network"><a href="#1-1-Why-Pointer-Network" class="headerlink" title="1.1 Why Pointer Network?"></a>1.1 Why Pointer Network?</h2><p>提出Pointer Network的动机是什么？<br>需要先回顾下Seq2Seq，顾名思义，它实现了把一个序列转换成另外一个序列的功能，并且不要求输入序列和输出序列等长。</p><img src="/2021/10/30/2021-10-30-Pointer%20Network%20&%20GlobalPointer/1655191995453-5f9cf351-82d2-4825-ac03-630003dc353e.png" class="" title="image.png"><p>后来，Attention Mechanism[6]的加入使得seq2seq模型的性能大幅提升，从而大放异彩。那么Attention Mechanism做了些什么事呢？一言以蔽之，Attention Mechanism的作用就是将encoder的隐状态按照一定权重加和之后拼接（或者直接加和）到decoder的隐状态上，以此作为额外信息。如下面的公式所示，其中$e_j$是Encoder的隐状态，$d_i$是Decoder的隐状态，$v,W_1,W_2$都是可学习的参数。</p><img src="/2021/10/30/2021-10-30-Pointer%20Network%20&%20GlobalPointer/1655190688730-5a992d9d-19a2-41d5-a61f-c032d1908033.png" class="" title="image.png"><p>Attention 起到所谓“软对齐”的作用，并且提高了整个模型的预测准确度。简单举个例子，在机器翻译中一直存在对齐的问题，也就是说源语言的某个单词应该和目标语言的哪个单词对应，如“Who are you”对应“你是谁”，如果我们简单地按照顺序进行匹配的话会发现单词的语义并不对应，显然“who”不能被翻译为“你”。而Attention Mechanism非常好地解决了这个问题。如前所述，Attention Mechanism会给输入序列的每一个元素分配一个权重，如在预测“你”这个字的时候输入序列中的“you”这个词的权重最大，这样模型就知道“你”是和“you”对应的，从而实现了软对齐。</p><p><strong>Attention机制三步骤：</strong></p><ol><li>编码器和解码器对每个单词的embedding，做权重和之后输入到tanh激活函数，来求编码器和解码器的单词embedding的相似性；</li><li>归一化；</li><li>求权重和 . 求得的值就是attention权重。每当Decoder生成一个单词的时候，都会考虑不同权重的Input。</li></ol><p>通过Attention Mechanism将encoder的隐状态和decoder的隐状态结合成一个中间向量C，然后使用decoder解码并预测，最后经由softmax层得到了针对词汇表的概率分布，从中选取概率最高的作为当前预测结果。<br>传统的seq2seq模型是无法解决输出序列的词汇表随着输入序列序列长度的改变而改变的问题。<br>如寻找凸包（Convex Hull ）等。因为对于这类问题，输出往往是输入集合的子集。基于这种特点，作者考虑能不能找到一种结构类似编程语言中的指针，每个指针对应输入序列的一个元素，从而我们可以直接操作输入序列而不需要特意设定输出词汇表。</p><img src="/2021/10/30/2021-10-30-Pointer%20Network%20&%20GlobalPointer/1655192094548-09ae4e49-1266-4982-97b4-5c580b34d0ad.png" class="" title="image.png"><h2 id="1-2-Structure-of-Pointer-Network"><a href="#1-2-Structure-of-Pointer-Network" class="headerlink" title="1.2 Structure of Pointer Network"></a>1.2 Structure of Pointer Network</h2><p>为啥叫pointer network呢？对于凸包的求解，就是从输入序列$P_1,…., P_{1000}$中选点的过程。选点的方法就叫pointer，它不像attention mechanism将输入信息通过encoder整合成context vector，而是将attention转化为一个pointer，来选择原来输入序列中的元素。</p><img src="/2021/10/30/2021-10-30-Pointer%20Network%20&%20GlobalPointer/1655191681893-c95a2ac9-dbcc-41ae-a40b-586a593193cd.png" class="" title="image.png"><p>Pointer Network 对注意力模型的简单修改，用于解决输出序列的大小取决于输入序列中元素的数量这个问题。</p><p>Pointer Network 计算 Attention 值之后不会把 Encoder 的输出融合，而是将 Attention 作为输入序列$P$中每一个位置输出的概率。</p><img src="/2021/10/30/2021-10-30-Pointer%20Network%20&%20GlobalPointer/1655190937429-dece9abc-050d-4c6a-9536-aedcc0d9464c.png" class="" title="image.png"><p><strong>Pointer Network三步骤：</strong></p><ol><li>编码器和解码器对每个单词做embedding，求其权重和之后输入到tanh激活函数，来求编码器和解码器的单词的embedding相似性 ；</li><li>归一化；</li><li>将前$i-1$个输出的单词和Attention权重作为条件概率，来生成第$i$个单词。</li></ol><p>这种方法专门针对输出离散且与输入位置相对应的问题。这种方法适用于可变大小的输入（产生可变大小的输出序列）。 本质上，input Sequence里对生成第i个单词的影响（相似性，相关性）越大，权重就越大。</p><p><strong>Pointer Network 和 Seq2Seq 的区别：</strong></p><ul><li>Seq2Seq 的 Decoder 会预测每一个位置的输出 (但是<strong>输出目标的数量是固定的</strong>)；而 Pointer Network 的 Decoder 直接根据 Attention 得到输入序列中每一个位置的概率，取概率最大的输入位置作为当前输出。</li><li>seq2seq需要遍历全局词表，可能出现OOV问题；而Pointer Network 无需遍历全局词表，只需遍历source text（input sequence），输出的也是input sequence中出现的单词，避免OOV。</li></ul><h2 id="1-3-Optimization"><a href="#1-3-Optimization" class="headerlink" title="1.3 Optimization"></a>1.3 Optimization</h2><blockquote><p><a href="https://nifengfeixiang.github.io/article/Pointer-Networks/">Pointer Networks</a></p></blockquote><p><strong>那么接下来如何训练pointer network？</strong>在这个地方使用（model-free和model-based） RL方法来优化pointer network的参数$θ$；我们的训练目标是，给定序列$s$，最小化期望路径长度：</p><p>$J(θ∣s)=E_{π∼pθ(.∣s)}L(π∣s)$</p><p>在训练的过程中，为了能够具有泛化性，考虑序列点的图是服从分布$S$，因此总的优化目标包含图的分布信息：</p><p>$J(θ)=E_{s∼S}J(θ∣s)$</p><p>我们借助策略梯度policy gradient方法和随机梯度stochastic gradient下降来优化参数；（3）式子的梯度使用REINFORCE algorithm (Williams, 1992) （这个不是很理解，后面补一下）表示为：</p><p>$∇_θJ(θ∣s)=E_{π∼pθ(.∣s)}[(L(π∣s)−b(s))∇_θlogp_θ(π∣s)]$</p><p>其中<code>b(s)</code>表示不依赖<code>π</code>的基线函数，并估计预期的行程长度以减小梯度的方差。通过对图进行sample，假设采样了<code>B</code>个图，$s_1,s_2,…,s_B$，则梯度（4）可以近似为：</p><p>$∇_θJ(θ)≈\frac{1}{B}∑_i^B=1(L(π_i∣s_i)−b(s_i))∇_θlogp_θ(π_i∣s_i)$</p><h6 id="3-2-Baseline-critic"><a href="#3-2-Baseline-critic" class="headerlink" title="3.2 Baseline-critic"></a>3.2 Baseline-critic</h6><p>上面提到，找到一个合适baseline $b(s)$可以提高训练学习的效果，这里采用critic 网络，参数为$θ_v$，来学习我们当前策略$p_θ$下的期望路径长度。评论家接受了随机梯度下降训练，其均方误差目标介于其预测值$b_{θ_v}(s)$与最新策略采样出来的一个实际长度之间，（这个地方为什么是减$L()$，而不是$J(\theta)$，这个地方是从当前策略中sample出一个确定的路径）</p><p>$L(θ_v)=\frac{1}{B}∑_{i=1}^B∥b_{θ_v}(s_i)−L(π_i∣s_i)∥_2^2$</p><p>Critic architecture：现在来解释critic的构造，主要包含有三个模块：一个LSTM encoder，一个LSTM process block，和一个2-layer ReLU网络decoder。LSTM encoder与pointer network类似结构，输入sequence ss并将其转为latent memory states以及一个<code>隐藏状态hidden h</code>。然后是处理模块process block，处理块对隐藏状态h执行P个计算步骤。 每个处理步骤都通过glimpsing memory states状态来更新此隐藏状态h，并将瞥见功能的输出作为输入提供给下一个处理步骤。process block结束后，获得的隐藏state被解码为一个baseline的预测结果。</p><blockquote><p>_LSTM: _<a href="https://zhuanlan.zhihu.com/p/32085405">https://zhuanlan.zhihu.com/p/32085405</a></p></blockquote><h6 id="3-2-Search-strategy"><a href="#3-2-Search-strategy" class="headerlink" title="3.2 Search strategy"></a>3.2 Search strategy</h6><p>由于评估游程长度的成本很低，我们的TSP代理可以很容易地在推理时通过考虑每个图形的多个候选方案并选择最佳方案来模拟搜索过程。推理过程类似于求解器如何在大量可行解的集合上进行搜索。在本文中，我们考虑两种搜索策略，详述如下，我们将其称为采样sample和主动搜索active search。</p><p>（1）sampling：从训练的随机策略$p_θ(⋅|s)$中随机采样几个，找到其中的最短路径。当从我们的非参数softmax中取样时，用温度超参数控制取样游的多样性。</p><p>$S_i=\frac{e_i}{∑_jej}$</p><p>_<strong>Softmax:</strong> 用于多分类中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类，假设我们有一个数组，V，Vi表示V中的第i个元素，那么这个元素的softmax值就是_</p><p>（2）Active search：。。。</p><h1 id="2-GlobalPointer"><a href="#2-GlobalPointer" class="headerlink" title="2. GlobalPointer"></a>2. GlobalPointer</h1><blockquote><p><a href="https://spaces.ac.cn/archives/8373">《GlobalPointer：用统一的方式处理嵌套和非嵌套NER》</a><br>《<a href="https://spaces.ac.cn/archives/8877">Efficient GlobalPointer：少点参数，多点效果</a>》</p></blockquote><p><a href="https://spaces.ac.cn/usr/uploads/2021/05/2377306125.png"><img src="/2021/10/30/2021-10-30-Pointer%20Network%20&%20GlobalPointer/1655197516009-49bcd46d-1c28-4940-996f-09c786f039c5.png" class=""></a></p><p>GlobalPoniter多头识别嵌套实体示意图</p><h2 id="GlobalPointer"><a href="#GlobalPointer" class="headerlink" title="GlobalPointer#"></a>GlobalPointer<a href="https://spaces.ac.cn/archives/8373#GlobalPointer">#</a></h2><p>常规的Pointer Network的设计在做实体识别或者阅读理解时，一般是用两个模块分别识别实体的首和尾，这会带来训练和预测时的不一致。而GlobalPointer就是针对这个不一致而设计的，它将首尾视为一个整体去进行判别，所以它更有“全局观”（更Global）。</p><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路#"></a>基本思路<a href="https://spaces.ac.cn/archives/8373#%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF">#</a></h3><p>具体来说，假设要识别文本序列长度为$n$，简单起见先假定只有一种实体要识别，并且假定每个待识别实体是该序列的一个连续片段，长度不限，并且可以相互嵌套（两个实体之间有交集），那么该序列有多少个“候选实体”呢？</p><p>不难得出，答案是$n(n+1)/2$个，即长度为n的序列有$n(n+1)/2$个不同的连续子序列，这些子序列包含了所有可能的实体，而我们要做的就是从这$n(n+1)/2$个“候选实体”里边挑出真正的实体，其实就是一个“$n(n+1)/2$选 k”的多标签分类问题。如果有m种实体类型需要识别，那么就做成m个“$n(n+1)/2$选k”的多标签分类问题。这就是GlobalPointer的基本思想，以实体为基本单位进行判别，如本文开头的图片所示。</p><p>可能有读者会问：这种设计的复杂度明明就是$𝒪(n^2)$呀，不会特别慢吗？如果现在还是RNN/CNN的时代，那么它可能就显得很慢了，但如今是Transformer遍布NLP的时代，Transformer的每一层都是$𝒪(n^2)$的复杂度，多GlobalPointer一层不多，少GlobalPointer一层也不少，关键是$𝒪(n^2)$的复杂度仅仅是空间复杂度，如果并行性能好的话，时间复杂度甚至可以降到$𝒪(1)$，所以不会有明显感知。</p><h3 id="数学形式"><a href="#数学形式" class="headerlink" title="数学形式#"></a>数学形式<a href="https://spaces.ac.cn/archives/8373#%E6%95%B0%E5%AD%A6%E5%BD%A2%E5%BC%8F">#</a></h3><p>设长度为n的输入t经过编码后得到向量序列$[h_1,h_2,⋯,h_n]$，通过变换$q_i,α=W_q,αh_i+b_q,α$和$k_i,α=W_k,αh_i+b_k,α$我们可以得到序列向量序列$[q_1,α,q_2,α,⋯,q_n,α]$和$[k_1,α,k_2,α,⋯,k_n,α]$，它们是识别第α种类型实体所用的向量序列。此时我们可以定义</p><p>$s_α(i,j)=q^⊤_{i,α}k_{j,α}$</p><p>作为从i到j的连续片段是一个类型为α的实体的打分。也就是说，用$q_{i,α}$与$k_{j,α}$的内积，作为片段$t[i:j]$是类型为α的实体的打分（logits），这里的$t[i:j]$指的是序列t的第i个到第j个元素组成的连续子串。在这样的设计下，GlobalPointer事实上就是Multi-Head Attention的一个简化版而已，有多少种实体就对应多少个head，相比Multi-Head Attention去掉了<strong>_V_</strong>V相关的运算。</p><h2 id="优化细节"><a href="#优化细节" class="headerlink" title="优化细节#"></a>优化细节<a href="https://spaces.ac.cn/archives/8373#%E4%BC%98%E5%8C%96%E7%BB%86%E8%8A%82">#</a></h2><p>在这部分内容中，我们会讨论关于GlobalPointer在训练过程中的一些细节问题，包括损失函数的选择以及评价指标的计算和优化等，从中我们可以看到，GlobalPointer以实体为单位的设计有着诸多优雅和便利之处。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数#"></a>损失函数<a href="https://spaces.ac.cn/archives/8373#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">#</a></h3><p>到目前为止，我们已经设计好了打分$s_α(i,j)$，识别特定的类α的实体，则变成了共有$n(n+1)/2$类的多标签分类问题。接下来的关键是损失函数的设计。最朴素的思路是变成$n(n+1)/2$个二分类，然而实际使用时n往往并不小，那么$n(n+1)/2$更大，而每个句子的实体数不会很多（每一类的实体数目往往只是个位数），所以如果是$n(n+1)/2$个二分类的话，会带来极其严重的类别不均衡问题。</p><p>这时候我们之前研究的<a href="https://spaces.ac.cn/archives/7359">《将“softmax+交叉熵”推广到多标签分类问题》</a>就可以派上用场了。简单来说，这是一个用于多标签分类的损失函数，它是单目标多分类交叉熵的推广，特别适合总类别数很大、目标类别数较小的多标签分类问题。其形式也不复杂，在GlobalPointer的场景，它为</p><img src="/2021/10/30/2021-10-30-Pointer%20Network%20&%20GlobalPointer/1655198168623-3bb09c76-b013-4e3e-a93f-cd9f0cffe2c7.png" class="" title="image.png"><p>其中Pα是该样本的所有类型为α的实体的首尾集合，Qα是该样本的所有非实体或者类型非α的实体的首尾集合，注意我们只需要考虑i≤j的组合，即</p><img src="/2021/10/30/2021-10-30-Pointer%20Network%20&%20GlobalPointer/1655198180025-8aa0b4dd-0071-4887-bca0-b20e7dfac372.png" class="" title="image.png"><p>而在解码阶段，所有满足$s_α(i,j)&gt;0$的片段$t_{[i:j]}$都被视为类型为α的实体输出。可见，解码过程是及其简单的，并且在充分并行下解码效率就是$𝒪(1)$！</p><h2 id="思考拓展"><a href="#思考拓展" class="headerlink" title="思考拓展#"></a>思考拓展<a href="https://spaces.ac.cn/archives/8373#%E6%80%9D%E8%80%83%E6%8B%93%E5%B1%95">#</a></h2><p>在本节中，我们将进一步对CRF和GlobalPointer做一个理论上的对比，并且介绍一些与GlobalPointer相关的工作，以方便读者更好地理解和定位GlobalPointer。</p><h3 id="相比CRF"><a href="#相比CRF" class="headerlink" title="相比CRF#"></a>相比CRF<a href="https://spaces.ac.cn/archives/8373#%E7%9B%B8%E6%AF%94CRF">#</a></h3><p>CRF（条件随机场，Conditional Random Field）是序列标注的经典设计，由于大多数NER也能转化为序列标注问题，所以CRF也算是NER的经典方法，笔者也曾撰写过<a href="https://spaces.ac.cn/archives/5542">《简明条件随机场CRF介绍（附带纯Keras实现）》</a>和<a href="https://spaces.ac.cn/archives/7196">《你的CRF层的学习率可能不够大》</a>等文章来介绍CRF。在之前的介绍中，我们介绍过，如果序列标注的标签数为kk，那么逐帧softmax和CRF的区别在于：</p><blockquote><p>前者将序列标注看成是n个k分类问题，后者将序列标注看成是1个$k^n$分类问题。</p></blockquote><p>这句话事实上也说明了逐帧softmax和CRF用于NER时的理论上的缺点。怎么理解呢？逐帧softmax将序列标注看成是n个k分类问题，那是过于宽松了，因为某个位置上的标注标签预测对了，不代表实体就能正确抽取出来了，起码有一个片段的标签都对了才算对；相反，CRF将序列标注看成是1个$k^n$分类问题，则又过于严格了，因为这意味着它要求所有实体都预测正确才算对，只对部分实体也不给分。虽然实际使用中我们用CRF也能出现部分正确的预测结果，但那只能说明模型本身的泛化能力好，CRF本身的设计确实包含了“全对才给分”的意思。</p><p>所以，CRF在理论上确实都存在不大合理的地方，而相比之下，GlobalPointer则更加贴近使用和评测场景：它本身就是以实体为单位的，并且它设计为一个“多标签分类”问题，这样它的损失函数和评价指标都是实体颗粒度的，哪怕只对一部分也得到了合理的打分。因此，哪怕在非嵌套NER场景，GlobalPointer能取得比CRF好也是“情理之中”的。</p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作#"></a>相关工作<a href="https://spaces.ac.cn/archives/8373#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">#</a></h3><p>如果读者比较关注实体识别、信息抽取的进展，那么应该可以发现，GlobalPointer与前段时间的关系抽取新设计<a href="https://arxiv.org/abs/2010.13415">TPLinker</a>很相似。但事实上，这种全局归一化的思想，还可以追溯到更远。</p><p>对于笔者来说，第一次了解到这种思想，是在百度2017年发表的一篇<a href="https://arxiv.org/abs/1709.02828">《Globally Normalized Reader</a>》，里边提出了一种用于阅读理解的全局归一化设计（GNR），里边不单单将(首, 尾)视为一个整体了，而是(句子, 首, 尾)视为一个整体（它是按照先选句子，然后在句子中选首尾的流程，所以多了一个句子维度），这样一来组合数就非常多了，因此它还用了<a href="https://arxiv.org/abs/1606.02960">《Sequence-to-Sequence Learning as Beam-Search Optimization》</a>里边的思路来降低计算量。</p><p>有了GNR作铺垫，其实GlobalPointer就很容易可以想到的，事实上早在前年笔者在做LIC2019的关系抽取赛道的时候，类似的想法就已经有了，但是当时还有几个问题没有解决。</p><p>第一，当时Transformer还没有流行起来，总觉得$𝒪(n^2)$的复杂度很可怕；第二，当时<a href="https://spaces.ac.cn/archives/7359">《将“softmax+交叉熵”推广到多标签分类问题》</a>也还没想出来，所以多标签分类的不均衡问题没有很好的解决方案；第三，当时笔者对NLP各方面的理解也还浅，bert4keras也没开发，一旦实验起来束手束脚的，出现问题也不知道往哪里调（比如开始没加上RoPE，降低了30个点以上，如果是两年前，我肯定没啥调优方案了）。</p><p>所以，GlobalPointer算是这两年来笔者经过各方面积累后的一个有点“巧合”但又有点“水到渠成”的工作。至于TPLinker，它还真跟GlobalPointer起源没什么直接联系。当然，在形式上GlobalPointer确实跟TPLinker很相似，事实上TPLinker还可以追溯到更早的<a href="https://www.sciencedirect.com/science/article/abs/pii/S095741741830455X">《Joint entity recognition and relation extraction as a multi-head selection problem》</a>，只不过这系列文章都主要是把这种Global的思想用于关系抽取了，没有专门针对NER优化。</p>]]></content>
    
    
    
    <tags>
      
      <tag>NER</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>黑客马拉松</title>
    <link href="/2021/09/29/2021-09-29-%E9%BB%91%E5%AE%A2%E9%A9%AC%E6%8B%89%E6%9D%BE/"/>
    <url>/2021/09/29/2021-09-29-%E9%BB%91%E5%AE%A2%E9%A9%AC%E6%8B%89%E6%9D%BE/</url>
    
    <content type="html"><![CDATA[<p>黑客马拉松通常是一天（可以是36小时，48小时等）编程竞赛，软件程序员、开发人员、设计师等聚集在一起构建和设计一些很酷的东西。 黑客马拉松是关于创造力、创新和解决问题的。仅仅拥有一个令人兴奋的想法并不足以让你获得黑客马拉松，你还需要建立一个全明星团队，战略性地规划方法、确定优先顺序，并成功完成demo。接下来要讲的6个步骤将帮助你为黑客马拉松做准备，为你的成功助力！</p><span id="more"></span><h2 id="第一步：准备"><a href="#第一步：准备" class="headerlink" title="第一步：准备"></a>第一步：准备</h2><p>在准备时要问自己两个重要问题：</p><ul><li>你将针对什么问题并将如何解决它？</li><li>你想要向评委展示什么？</li></ul><p>此外一旦开始开发解决方案和原型，可以做两件事来节省时间：</p><ul><li>提前学习技术</li><li>预先制作模型和线框</li></ul><h2 id="第二步：专注"><a href="#第二步：专注" class="headerlink" title="第二步：专注"></a>第二步：专注</h2><p>技术实际上是赢得黑客马拉松的简单部分。你要做的事，90％是尽可能将挑战降低到一个基本问题并找到解决方案。请牢记以下这句话：</p><p><strong>一个想法=一个明确定义的问题+你将如何解决它。</strong></p><p>不同于商业产品，在黑客马拉松取胜的产品往往具有创新性和独创性。虽然你需要专注于一个问题，但请不要限制你的想象力或创造力，用新的想法让评委们眼前一亮，将技术和解决方案用到全新的地方！</p><h2 id="第三步：定义你要解决的核心问题"><a href="#第三步：定义你要解决的核心问题" class="headerlink" title="第三步：定义你要解决的核心问题"></a>第三步：定义你要解决的核心问题</h2><p>你想解决的问题的核心是什么？你的解决方案的核心是什么？这决定了你将如何向评委展示。</p><p>你不会有太多时间进行编程和展示，因此不要浪费时间来创建或演示登录屏幕或其他无关部分，抓住那些关键元素。你不是在推销一个创业公司，而是在介绍一个能解决问题的杀手级功能或解决方案。</p><h2 id="第四步：充分利用你的时间"><a href="#第四步：充分利用你的时间" class="headerlink" title="第四步：充分利用你的时间"></a>第四步：充分利用你的时间</h2><p>“首先，请务必了解黑客马拉松的评判标准。第二，你需要给他们看一些东西与他们进行互动。光演示是不够的，你需要实际编写一些东西，因为这是黑客马拉松不是商业创业大赛。第三，大多数评委都不会知道你的后端是否有效，但他们会看你的设计，所以设计在黑客马拉松中非常重要。”</p><p>因为只有24小时你就必须充分利用时间。留意以下一些注意事项：</p><p><strong>宜：</strong></p><ul><li>关注MVP：最小可行产品</li><li>将工作细分为可能的版本。首先关注第一个版本并记下你希望在版本二或三中包含的任何其他组件，说不定会有多余时间。</li><li>使用框架和库。提前研究可用的库和API。查找并选择正在开发并具有良好文档的受信任库，安装并运行测试，以确保有效。</li></ul><p><strong>忌：</strong></p><ul><li>不应该重复。不要浪费时间创建登录屏幕、确认页面，感谢页面、页脚、社交按钮或其他任何不必要的东西。</li><li>不用写太漂亮的代码：你没有可挥霍的时间来修复缩进。</li><li>不要浪费时间寻找完美的数据集。</li></ul><h2 id="第五步：了解你的评委，练习你的演讲"><a href="#第五步：了解你的评委，练习你的演讲" class="headerlink" title="第五步：了解你的评委，练习你的演讲"></a>第五步：了解你的评委，练习你的演讲</h2><p>充分了解评委、供应商和赞助商。知道他们是谁、他们的评估标准，以及过去的黑客马拉松获胜者来相应地自定义你提交的作品和最终展示。你在黑客马拉松中只有几分钟的时间来展示你的作品。不要因为糟糕的表现而浪费所有的努力。花足够的时间练习你的演讲，并为评委的潜在问题做好准备。团队中负责展示的人应在评判前的最后几小时不间断练习。在展示的时候，这个人应该具有回答评委所有问题的能力。</p><h2 id="第六步：好好享受"><a href="#第六步：好好享受" class="headerlink" title="第六步：好好享受"></a>第六步：好好享受</h2><p>黑客马拉松是一次非常棒的学习经历，也是一个做实验性尝试和结识有趣的人很好的机会。如果你没有实现目标或获胜，不要给自己施加太多压力或过分担心结果。尽你所能，尽最大努力，从整个体验中汲取所有积极的东西就好啦。</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1guxdclmostj60kw1tidr902.jpg" alt="3caf5467f3eec6f1037eb8fc0db9af6d" style="zoom:67%;" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>黑客马拉松</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>elastic使用指南</title>
    <link href="/2021/09/28/2021-09-28-elastic%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2021/09/28/2021-09-28-elastic%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<p>整理了一些Python Elastic库的使用指南</p><ol><li><p><a href="https://cloud.tencent.com/developer/article/1799392">Python更新Elasticsearch数据方法大全</a></p><p>这篇总结了 Python 更新 Elasticsearch 数据的几个方法，包括全局更新index、局部更新update、搜索更新update_by_query、批量更新bulk（数据量较大时，更为高效的更新）</p></li><li><p><a href="https://www.cnblogs.com/Neeo/articles/10615739.html">Elasticsearch - For Python之操作篇</a></p><p>可以关注这篇博客的第1、2部分，操作elasticsearch对象，处理一些简单的索引信息，包括：向指定索引添加或更新文档index、查询索引中指定文档get/get_source、执行搜索查询并获取与查询匹配的搜索匹配search、执行查询并获取该查询的匹配数count、删除与查询匹配的所有文档delete_by_query等</p></li><li><p><a href="https://www.cnblogs.com/ExMan/p/11323984.html">python 查询 elasticsearch 常用方法（Query DSL）</a></p><p>这篇博客主要讲了query dsl语法，如何通过es.search中的body来找到你需要的信息，包括：查询所有数据match_all、等于查询term与terms、包含查询match与multi_match、复合查询bool、范围查询range、前缀查询prefix、通配符查询wildcard、排序sort等</p></li><li><p><a href="https://blog.csdn.net/fwj_ntu/article/details/87863788">使用python查询Elasticsearch并导出所有数据</a></p><p>Elasticsearch默认情况下只会返回10或20条结果，如果你想要得到所有结果，需要借助游标scroll查询出的所有结果</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>elasticsearch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>elastic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>日志管理ELK</title>
    <link href="/2021/09/28/2021-09-28-%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86ELK/"/>
    <url>/2021/09/28/2021-09-28-%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86ELK/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>对于分布式系统，特别是基于容器的微服务系统，详细的系统日志和日志数据的实时收集和传输到集中平台是极其有必要的。主要基于两个原因，一是日志数据不能随容器的切换而丢失（当然也可以将日志数据存于持久存储层，但这种架构可能违背微服务自主和自助的设计原则），二是分布式的系统架构分散复杂，更加有必要对任何一个环节进行及时的监控。</p><h2 id="什么是日志管理？"><a href="#什么是日志管理？" class="headerlink" title="什么是日志管理？"></a>什么是日志管理？</h2><p>日志管理就是指对系统和应用程序产生的日志进行处理的方法，包括对日志进行统一收集，对日志数据进行筛选和解析，统一存储，还要让它们可以方便被检索。</p><p>目前常用的日志平台架构如下，服务器端用logstash与各个端的日志进行对接，存入elatic数据库中，然后通过kibana来编制报表展示监控结果，见下图所示。当然，在beat与logstash 之间，为了增加数据传输的可靠性和及时性，还可采用kafaka消息传输软件。</p><p><img src="2021-09-28-日志管理ELK\t_70.png" alt="img"></p><span id="more"></span><h2 id="ELK介绍"><a href="#ELK介绍" class="headerlink" title="ELK介绍"></a>ELK介绍</h2><p>ELK 是 Elasticsearch+Logstash+Kibana 的缩写：</p><ul><li><p>ElasticSearch是一套搜索框架，提供了方便的接口，可以方便的做全文检索，可以用来对日志进行检索. </p></li><li><p>Logstash 是一个数据收集工具，可以用来收集日志数据，并将其输送给Elasticsearch存储供以后使用（如，搜索）。</p></li><li><p>Kibana 是一套可以和 ElasticSearch 交互的界面，通过Kibana可以方便的检索 ElasticSearch 内的所有数据，还可以用图形化的方式汇总、分析和搜索重要数据日志</p></li></ul><h2 id="大厂的日志管理系统的架构是什么样子？"><a href="#大厂的日志管理系统的架构是什么样子？" class="headerlink" title="大厂的日志管理系统的架构是什么样子？"></a>大厂的日志管理系统的架构是什么样子？</h2><p>参考</p><ul><li>阿里云：《<a href="https://yq.aliyun.com/articles/590431">基于 ELK 实时日志分析的最佳实践</a>》</li><li>新浪：《<a href="https://www.modb.pro/db/57059">ELK Stack 在新浪微博的最佳实践</a>》</li><li>新浪：《<a href="https://www.open-open.com/lib/view/open1437018728240.html">新浪是如何分析处理 32 亿条实时日志的？</a>》</li><li>七牛：《<a href="https://juejin.im/post/5b63e859e51d4517c564d1a9">如何快速搭建智能化的统一日志管理系统</a>》</li></ul>]]></content>
    
    
    <categories>
      
      <category>elasticsearch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>elk</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQLModel</title>
    <link href="/2021/09/15/2021-09-15-SQLModel%E8%B0%83%E7%A0%94%E6%80%BB%E7%BB%93/"/>
    <url>/2021/09/15/2021-09-15-SQLModel%E8%B0%83%E7%A0%94%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="SQLModel-–-SQL-Databases-in-FastAPI"><a href="#SQLModel-–-SQL-Databases-in-FastAPI" class="headerlink" title="SQLModel – SQL Databases in FastAPI"></a>SQLModel – SQL Databases in FastAPI</h2><p>SQLModel 是一个用于与 SQL DB 交互的库，基于 Python 类型提示。</p><p>由 Pydantic（数据校验库）和 SQLAlchemy（SQL 对象映射器）提供技术支持，并且都针对 FastAPI 进行了优化。</p><p>GitHub 在这里： <a href="https://github.com/tiangolo/sqlmodel">https://github.com/tiangolo/sqlmodel</a></p><p>此 Twitter 线程中的更多信息： <a href="https://twitter.com/tiangolo/status/1430252646968004612">https://twitter.com/tiangolo/status/1430252646968004612</a></p><p>文档在这里： <a href="https://sqlmodel.tiangolo.com/">https://sqlmodel.tiangolo.com/</a></p><span id="more"></span><h2 id="基于-SQLAlchemy¶"><a href="#基于-SQLAlchemy¶" class="headerlink" title="基于 SQLAlchemy¶"></a>基于 SQLAlchemy<a href="https://sqlmodel.tiangolo.com/features/#based-on-sqlalchemy">¶</a></h2><p><strong>SQLModel</strong>也基于 SQLAlchemy 并将其用于一切。</p><p>在下面，✨一个<strong>SQLModel</strong>模型也是一个<strong>SQLAlchemy</strong>模型。✨</p><p>有<strong>很多</strong>研究和努力致力于使其成为这种方式。特别是，要使单个模型<strong>同时成为 SQLAlchemy 模型和 Pydantic</strong>模型，需要付出很多努力和实验。</p><p>这意味着您可以获得 SQLAlchemy（<a href="https://www.jetbrains.com/lp/python-developers-survey-2020/">Python 中使用最广泛的数据库库）的</a>所有功能、稳健性和确定性。</p><p><strong>SQLModel</strong>提供了自己的实用程序来改善开发人员体验，但在<strong>底层</strong>，它使用了所有 SQLAlchemy。</p><p>您甚至可以<strong>将</strong>SQLModel 模型与 SQLAlchemy 模型<strong>结合起来</strong>。</p><p>SQLModel 旨在满足<strong>最常见的用例</strong>，并为这些用例尽可能简单方便，提供最佳的开发人员体验。</p><p>但是，当您有更多需要更复杂功能的奇特用例时，您仍然可以将 SQLAlchemy 直接插入 SQLModel 并在您的代码中使用其所有功能。</p><h2 id="ORM介绍"><a href="#ORM介绍" class="headerlink" title="ORM介绍"></a>ORM介绍</h2><p>面向对象编程把所有实体看成对象（object），关系型数据库则是采用实体之间的关系（relation）连接数据。很早就有人提出，关系也可以用对象表达，这样的话，就能使用面向对象编程，来操作关系型数据库。</p><p><img src="https://www.wangbase.com/blogimg/asset/201902/bg2019021802.png" alt="img"></p><p><strong>简单说，ORM 就是通过实例对象的语法，完成关系型数据库的操作的技术，是”对象-关系映射”（Object/Relational Mapping） 的缩写。</strong></p><p>ORM 把数据库映射成对象。</p><blockquote><ul><li>数据库的表（table） —&gt; 类（class）</li><li>记录（record，行数据）—&gt; 对象（object）</li><li>字段（field）—&gt; 对象的属性（attribute）</li></ul></blockquote><p><img src="https://www.wangbase.com/blogimg/asset/201902/bg2019021803.png" alt="img"></p><p>总结起来，ORM 有下面这些优点。</p><blockquote><ul><li>数据模型都在一个地方定义，更容易更新和维护，也利于重用代码。</li><li>ORM 有现成的工具，很多功能都可以自动完成，比如数据消毒、预处理、事务等等。</li><li>它迫使你使用 MVC 架构，ORM 就是天然的 Model，最终使代码更清晰。</li><li>基于 ORM 的业务代码比较简单，代码量少，语义性好，容易理解。</li><li>你不必编写性能不佳的 SQL。</li></ul></blockquote><p>但是，ORM 也有很突出的缺点。</p><blockquote><ul><li>ORM 库不是轻量级工具，需要花很多精力学习和设置。</li><li>对于复杂的查询，ORM 要么是无法表达，要么是性能不如原生的 SQL。</li><li>ORM 抽象掉了数据库层，开发者无法了解底层的数据库操作，也无法定制一些特殊的 SQL。</li></ul></blockquote><h2 id="Installation¶"><a href="#Installation¶" class="headerlink" title="Installation¶"></a>Installation<a href="https://sqlmodel.tiangolo.com/#installation">¶</a></h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> sqlmodel<br></code></pre></td></tr></table></figure><h2 id="CRUD-operations-with-SQLModel-using-a-single-table"><a href="#CRUD-operations-with-SQLModel-using-a-single-table" class="headerlink" title="CRUD operations with SQLModel using a single table"></a>CRUD operations with <strong>SQLModel</strong> using a single table</h2><p>Create an SQLModel: <code>models.py</code></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-keyword">from</span> sqlmodel <span class="hljs-keyword">import</span> SQLModel,Field<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> Optional<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-symbol">Book</span>(<span class="hljs-symbol">SQLModel,<span class="hljs-symbol">table</span></span>=<span class="hljs-symbol">True</span>):<br>    <span class="hljs-symbol">id:<span class="hljs-symbol">Optional</span></span>[<span class="hljs-symbol">int</span>]=<span class="hljs-symbol">Field</span>(<span class="hljs-symbol">default</span>=<span class="hljs-symbol">None,<span class="hljs-symbol">primary_key</span></span>=<span class="hljs-symbol">True</span>)<br>    <span class="hljs-symbol">title:<span class="hljs-symbol">str</span></span><br>    <span class="hljs-symbol">description:<span class="hljs-symbol">str</span></span><br></code></pre></td></tr></table></figure><p>该<code>id</code>字段<strong>不能<code>NULL</code></strong>在数据库中，因为它是<strong>主键</strong>，我们使用<code>Field(primary_key=True)</code>. 但是在 Python 代码中<code>id</code>实际上<strong>可以有<code>None</code></strong>相同的字段，所以我们用 声明类型<code>Optional[int]</code>，并将默认值设置为<code>Field(default=None)</code>：</p><p>Database setup: <code>database.py</code></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">from sqlmodel import SQLModel,create_engine<br>import os<br><br><br>BASE_DIR=os<span class="hljs-selector-class">.path</span><span class="hljs-selector-class">.dirname</span>(os<span class="hljs-selector-class">.path</span><span class="hljs-selector-class">.realpath</span>(__file__))<br><br>conn_str=<span class="hljs-string">&#x27;sqlite:///&#x27;</span>+os<span class="hljs-selector-class">.path</span><span class="hljs-selector-class">.join</span>(BASE_DIR,<span class="hljs-string">&#x27;books.db&#x27;</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(conn_str)</span></span><br><br>engine=<span class="hljs-built_in">create_engine</span>(conn_str,echo=True)<br></code></pre></td></tr></table></figure><p>在这个例子中，我们还使用了参数 echo=True。它将使引擎打印它执行的所有 SQL 语句，这可以帮助您了解正在发生的事情。</p><p>Creating the database <code>create_db.py</code></p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-keyword">from</span> sqlmodel <span class="hljs-keyword">import</span> SQLModel<br><span class="hljs-keyword">from</span> models <span class="hljs-keyword">import</span> Book<br><span class="hljs-keyword">from</span> database <span class="hljs-keyword">import</span> engine<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;CREATING DATABASE.....&quot;</span>)<br><br>SQLModel.metadata.create_all(engine)<br></code></pre></td></tr></table></figure><p>导入models的Book类，Python 执行所有代码，创建从 SQLModel 继承的类并将它们注册到 SQLModel.metadata 中。create_all通过engine来创建数据库和在此 MetaData 对象中注册的所有表。</p><p>Get all items <code>main.py</code></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> fastapi <span class="hljs-keyword">import</span> FastAPI<br><span class="hljs-keyword">from</span> fastapi <span class="hljs-keyword">import</span> status<br><span class="hljs-keyword">from</span> fastapi.exceptions <span class="hljs-keyword">import</span> HTTPException<br><span class="hljs-keyword">from</span> models <span class="hljs-keyword">import</span> Book<br><span class="hljs-keyword">from</span> <span class="hljs-keyword">database</span> <span class="hljs-keyword">import</span> engine<br><span class="hljs-keyword">from</span> sqlmodel <span class="hljs-keyword">import</span> <span class="hljs-keyword">Session</span>,<span class="hljs-keyword">select</span><br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> Optional,List<br><br><br>app=FastAPI()<br><span class="hljs-keyword">session</span>=<span class="hljs-keyword">Session</span>(bind=engine)<br><br><br>@app.<span class="hljs-keyword">get</span>(<span class="hljs-string">&#x27;/books&#x27;</span>,response_model=List[Book],<br>status_code=status.HTTP_200_OK)<br>async def get_all_books():<br>    <span class="hljs-keyword">statement</span>=<span class="hljs-keyword">select</span>(Book)<br>    results=<span class="hljs-keyword">session</span>.exec(<span class="hljs-keyword">statement</span>).<span class="hljs-keyword">all</span>()<br><br>    <span class="hljs-keyword">return</span> results<br></code></pre></td></tr></table></figure><p>Get one item</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-variable">@app</span>.<span class="hljs-built_in">get</span>(<span class="hljs-string">&quot;/book/&#123;book_id&#125;&quot;</span>, response_model=Book)<br>async def <span class="hljs-built_in">get_a_book</span>(<span class="hljs-attribute">book_id</span>: int):<br>    statement = <span class="hljs-built_in">select</span>(Book).<span class="hljs-built_in">where</span>(Book.id == book_id)<br>    # statement = <span class="hljs-built_in">select</span>(Hero).<span class="hljs-built_in">where</span>(Hero.age &gt;= <span class="hljs-number">35</span>, Hero.age &lt; <span class="hljs-number">40</span>)<br>    # statement = <span class="hljs-built_in">select</span>(Hero).<span class="hljs-built_in">where</span>(<span class="hljs-built_in">or_</span>(Hero.age &lt;= <span class="hljs-number">35</span>, Hero.age &gt; <span class="hljs-number">90</span>))<br><br>    result = session.<span class="hljs-built_in">exec</span>(statement).<span class="hljs-built_in">first</span>()<br><br>    if result == <span class="hljs-attribute">None</span>:<br>        raise <span class="hljs-built_in">HTTPException</span>(status_code=status.HTTP_404_<span class="hljs-keyword">NOT</span>_FOUND)<br><br>    return result<br></code></pre></td></tr></table></figure><p>Create an item</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs routeros">@app.post(<span class="hljs-string">&quot;/books&quot;</span>, <span class="hljs-attribute">response_model</span>=Book, <span class="hljs-attribute">status_code</span>=status.HTTP_201_CREATED)<br>async def create_a_book(book: Book):<br>    new_book = Book(<span class="hljs-attribute">title</span>=book.title, <span class="hljs-attribute">description</span>=book.description)<br><br>    session.<span class="hljs-built_in">add</span>(new_book)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After adding to the session&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;new_book:&quot;</span>, new_book)<br><br>    session.commit()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After committing the session&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;new_book:&quot;</span>, new_book.id)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;new_book:&quot;</span>, new_book.title)<br><br>    session.refresh(new_book)  # 显式刷新对象,保证获取的是最新数据<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After refreshing the heroes&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;new_book:&quot;</span>, new_book)<br><br>    return new_book<br></code></pre></td></tr></table></figure><p>id默认设置为None，<strong>在与数据库交互之前</strong>，值实际上可能一直是<code>None</code>.</p><p>Update a book</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-variable">@app</span>.<span class="hljs-built_in">put</span>(<span class="hljs-string">&quot;/book/&#123;book_id&#125;&quot;</span>, response_model=Book)<br>async def <span class="hljs-built_in">update_a_book</span>(<span class="hljs-attribute">book_id</span>: int, <span class="hljs-attribute">book</span>: Book):<br>    statement = <span class="hljs-built_in">select</span>(Book).<span class="hljs-built_in">where</span>(Book.id == book_id)<br><br>    result = session.<span class="hljs-built_in">exec</span>(statement).<span class="hljs-built_in">first</span>()<br><br>    result.title = book.title<br>    result.description = book.description<br><br>    session.<span class="hljs-built_in">commit</span>()<br>    session.<span class="hljs-built_in">refresh</span>(result)<br><br>    return result<br></code></pre></td></tr></table></figure><p>Delete a book </p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-variable">@app</span>.<span class="hljs-built_in">delete</span>(<span class="hljs-string">&quot;/book/&#123;book_id&#125;&quot;</span>, status_code=status.HTTP_204_NO_CONTENT)<br>async def <span class="hljs-built_in">delete_a_book</span>(<span class="hljs-attribute">book_id</span>: int):<br>    statement = <span class="hljs-built_in">select</span>(Book).<span class="hljs-built_in">where</span>(Book.id == book_id)<br><br>    result = session.<span class="hljs-built_in">exec</span>(statement).<span class="hljs-built_in">one_or_none</span>()<br><br>    if result == <span class="hljs-attribute">None</span>:<br>        raise <span class="hljs-built_in">HTTPException</span>(<br>            status_code=status.HTTP_404_<span class="hljs-keyword">NOT</span>_FOUND, detail=<span class="hljs-string">&quot;Resource Not Found&quot;</span><br>        )<br><br>    session.<span class="hljs-built_in">delete</span>(result)<br><br>    return result<br></code></pre></td></tr></table></figure><h2 id="Connect-Tables-JOIN"><a href="#Connect-Tables-JOIN" class="headerlink" title="Connect Tables - JOIN"></a>Connect Tables - JOIN</h2><p>联合查询的另一种方式，使用关键字<code>JOIN</code>而不是<code>WHERE</code>.</p><p>使用<code>WHERE</code>：</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">SELECT</span> hero.id, hero.<span class="hljs-keyword">name</span>, <span class="hljs-built_in">team</span>.<span class="hljs-keyword">name</span><br>FROM hero, <span class="hljs-built_in">team</span><br><span class="hljs-keyword">WHERE</span> hero.team_id = <span class="hljs-built_in">team</span>.id<br></code></pre></td></tr></table></figure><p>这是使用的替代版本<code>JOIN</code>：</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">SELECT</span> hero.id, hero.<span class="hljs-keyword">name</span>, <span class="hljs-built_in">team</span>.<span class="hljs-keyword">name</span><br>FROM hero<br>JOIN <span class="hljs-built_in">team</span><br>ON hero.team_id = <span class="hljs-built_in">team</span>.id<br></code></pre></td></tr></table></figure><p>两者是等价的。</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-variable">@app</span>.<span class="hljs-built_in">get</span>(<span class="hljs-string">&quot;/books_join&quot;</span>, response_model=List[Book], status_code=status.HTTP_200_OK)<br>async def <span class="hljs-built_in">select_heroes</span>():<br>    statement = <span class="hljs-built_in">select</span>(Book, Team).<span class="hljs-built_in">where</span>(Book.id==Team.id)<br>    results = session.<span class="hljs-built_in">exec</span>(statement)<br>    <span class="hljs-built_in">print</span>(results.<span class="hljs-built_in">all</span>())<br><br>    statement = <span class="hljs-built_in">select</span>(Book, Team).<span class="hljs-built_in">join</span>(Team)<br>    results2 = session.<span class="hljs-built_in">exec</span>(statement)<br>    for hero, team in <span class="hljs-attribute">results2</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hero:&quot;</span>, hero, <span class="hljs-string">&quot;Team:&quot;</span>, team)<br></code></pre></td></tr></table></figure><p>当使用 时<code>.join()</code>，因为我们<code>foreign_key</code>在创建模型时已经声明了什么，所以我们不必传递<code>ON</code>部分，它会自动推断</p><p><code>.join()</code>有一个参数，我们可以<code>isouter=True</code>用来使<code>JOIN</code>a <code>LEFT OUTER JOIN</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Code above omitted 👆</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_heroes</span>():<br>    <span class="hljs-keyword">with</span> Session(engine) <span class="hljs-keyword">as</span> session:<br>        statement = select(Hero, Team).join(Team, isouter=<span class="hljs-literal">True</span>)<br>        results = session.<span class="hljs-built_in">exec</span>(statement)<br>        <span class="hljs-keyword">for</span> hero, team <span class="hljs-keyword">in</span> results:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hero:&quot;</span>, hero, <span class="hljs-string">&quot;Team:&quot;</span>, team)<br><br><span class="hljs-comment"># Code below omitted 👇</span><br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>SQLModel 官方文档 <a href="https://sqlmodel.tiangolo.com/">https://sqlmodel.tiangolo.com/</a></p><p>SQLAlchemy ORM 官方文档 <a href="https://www.tutorialspoint.com/sqlalchemy/sqlalchemy_orm_filter_operators.htm">https://www.tutorialspoint.com/sqlalchemy/sqlalchemy_orm_filter_operators.htm</a></p><p><a href="https://docs.sqlalchemy.org/en/14/orm/query.html">https://docs.sqlalchemy.org/en/14/orm/query.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>SQLModel</tag>
      
      <tag>ORM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>faiss实现的高效 K-means 聚类</title>
    <link href="/2021/09/06/2021-09-06-faiss%E5%AE%9E%E7%8E%B0%E7%9A%84%E9%AB%98%E6%95%88%20K-means%20%E8%81%9A%E7%B1%BB/"/>
    <url>/2021/09/06/2021-09-06-faiss%E5%AE%9E%E7%8E%B0%E7%9A%84%E9%AB%98%E6%95%88%20K-means%20%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://www.aiuai.cn/aifarm1662.html">https://www.aiuai.cn/aifarm1662.html</a><br>faiss安装报错参考：<a href="https://github.com/facebookresearch/faiss/issues/821">https://github.com/facebookresearch/faiss/issues/821</a></p></blockquote><h2 id="1-K-means-聚类"><a href="#1-K-means-聚类" class="headerlink" title="1. K-means 聚类"></a>1. K-means 聚类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> faiss<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> time<br><br>x = np.random.random((<span class="hljs-number">100000</span>, <span class="hljs-number">2048</span>)).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>ncentroids = <span class="hljs-number">10</span><br>niter = <span class="hljs-number">500</span><br>verbose = <span class="hljs-literal">True</span><br>d = x.shape[<span class="hljs-number">1</span>]<br><br>start_time = time.time()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">d：向量维度</span><br><span class="hljs-string">ncentroids：聚类中心</span><br><span class="hljs-string">niter：迭代次数</span><br><span class="hljs-string">verbose：是否打印迭代情况</span><br><span class="hljs-string">gpu：是否使用GPU</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-comment">#cpu</span><br>kmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose)<br><span class="hljs-comment">#gpu，使用所有的gpu</span><br>kmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose, gpu=<span class="hljs-literal">True</span>)<br><span class="hljs-comment">#gpu，使用 3 个gpu</span><br>kmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose, gpu=<span class="hljs-number">3</span>)<br><br>kmeans.train(x)<br>train_time = time.time()<br><span class="hljs-built_in">print</span>(train_time - start_time)<br><br>cluster_centers = kmeans.centroids <span class="hljs-comment">#聚类后的聚类中心</span><br>obj = kmeans.obj <span class="hljs-comment">#目标函数，kmeans 中为总的平方差</span><br>iteration_stats = kmeans.iteration_stats <span class="hljs-comment">#聚类中的统计信息</span><br><br><span class="hljs-comment"># 预测</span><br>D, I = kmeans.index.search(x, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># 其中， I 中为 x 中每一行的向量所对应的最接近的聚类(centroid)，D 包含了对应的平方 L2 距离.</span><br><br>search_time = time.time()<br><span class="hljs-built_in">print</span>(search_time - train_time)<br><br><span class="hljs-comment"># 倒排索引</span><br>index = faiss.IndexFlatL2 (d)<br>index.add (x)<br>D, I = index.search (kmeans.centroids, <span class="hljs-number">15</span>)<br><span class="hljs-built_in">print</span>(D)<br><br></code></pre></td></tr></table></figure><h2 id="2-PCA-计算"><a href="#2-PCA-计算" class="headerlink" title="2. PCA 计算"></a>2. PCA 计算</h2><p>例如，将 40D 向量降维到 10D,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#随机生成训练数据</span><br>mt = np.random.rand(<span class="hljs-number">1000</span>, <span class="hljs-number">40</span>).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>mat = faiss.PCAMatrix (<span class="hljs-number">40</span>, <span class="hljs-number">10</span>)<br>mat.train(mt)<br><span class="hljs-keyword">assert</span> mat.is_trained<br>tr = mat.apply_py(mt)<br><span class="hljs-comment">#print this to show that the magnitude of tr&#x27;s columns is decreasing</span><br><span class="hljs-built_in">print</span>((tr ** <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure></p><h2 id="3-PQ-量化"><a href="#3-PQ-量化" class="headerlink" title="3. PQ 量化"></a>3. PQ 量化</h2><p>如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">d = <span class="hljs-number">32</span>  <span class="hljs-comment"># data dimension</span><br>cs = <span class="hljs-number">4</span>  <span class="hljs-comment"># code size (bytes)</span><br><br><span class="hljs-comment">#随机生成数据集</span><br>nt = <span class="hljs-number">10000</span><br>xt = np.random.rand(nt, d).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br><span class="hljs-comment"># dataset to encode (could be same as train)</span><br>n = <span class="hljs-number">20000</span><br>x = np.random.rand(n, d).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br><span class="hljs-comment">#</span><br>pq = faiss.ProductQuantizer(d, cs, <span class="hljs-number">8</span>)<br>pq.train(xt)<br><br><span class="hljs-comment"># encode </span><br>codes = pq.compute_codes(x)<br><br><span class="hljs-comment"># decode</span><br>x2 = pq.decode(codes)<br><br><span class="hljs-comment"># compute reconstruction error</span><br>avg_relative_error = ((x - x2)**<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>() / (x ** <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br></code></pre></td></tr></table></figure><br>标量量化(scalar quantizer):<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">d = <span class="hljs-number">32</span>  <span class="hljs-comment"># data dimension</span><br><br><span class="hljs-comment"># train set </span><br>nt = <span class="hljs-number">10000</span><br>xt = np.random.rand(nt, d).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br><span class="hljs-comment"># dataset to encode (could be same as train)</span><br>n = <span class="hljs-number">20000</span><br>x = np.random.rand(n, d).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br><span class="hljs-comment"># QT_8bit allocates 8 bits per dimension (QT_4bit also works)</span><br>sq = faiss.ScalarQuantizer(d, faiss.ScalarQuantizer.QT_8bit)<br>sq.train(xt)<br><br><span class="hljs-comment"># encode </span><br>codes = sq.compute_codes(x)<br><br><span class="hljs-comment"># decode</span><br>x2 = sq.decode(codes)<br><br><span class="hljs-comment"># compute reconstruction error</span><br>avg_relative_error = ((x - x2)**<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>() / (x ** <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>文本聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K-means聚类详解</title>
    <link href="/2021/09/06/2021-09-06-K-means%E8%81%9A%E7%B1%BB%E8%AF%A6%E8%A7%A3/"/>
    <url>/2021/09/06/2021-09-06-K-means%E8%81%9A%E7%B1%BB%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>K-means 算法是一种基于相似属性将一组数据点划分为不同集群或组的方法。它是一种无监督学习算法，这意味着它不需要标记数据来查找数据集中的模式。</p><ul><li>K-Means使用步骤</li><li>初始中心点怎么确定</li><li>K值怎么确定</li><li>「K均值聚类」的Tips</li></ul><h2 id="什么是聚类？"><a href="#什么是聚类？" class="headerlink" title="什么是聚类？"></a>什么是聚类？</h2><p>聚类的目标是将项目分成组，使得组中的对象比组外的对象更相似，我们可以在 k-means 中使用我们想要的任何相似度函数来比较两个点。</p><h3 id="如何定义集群中的相似性？"><a href="#如何定义集群中的相似性？" class="headerlink" title="如何定义集群中的相似性？"></a>如何定义集群中的相似性？</h3><p>比较两个数据点相似性的最常见和最直接的方法是使用距离。与使用勾股定理求对角线长度的方式相同，我们可以计算两个数据点之间的距离。</p><p><img src="https://i1.wp.com/analyticsarora.com/wp-content/uploads/2021/07/euclidean-distance-between-two-points-in-x-y-coordinates.png?resize=800%2C600&amp;ssl=1#id=QEtTm&amp;originHeight=600&amp;originWidth=800&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>这种比较相似度的方法称为<strong>欧几里得距离</strong>。K-means 聚类中另一个常用的相似度函数称为<strong>曼哈顿距离</strong>。</p><p><img src="https://i2.wp.com/analyticsarora.com/wp-content/uploads/2021/07/manhattan-distance-between-two-points-in-x-y-coordinates.png?resize=800%2C600&amp;ssl=1#id=iLrws&amp;originHeight=600&amp;originWidth=800&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>这两个距离公式对用于聚类任务的各种数据集都很有效。然而，给定的相似度函数可能并非对数据点的每个分布都有效。这就提出了一个问题，“一个好的相似度函数的特性是什么？”。</p><h3 id="良好相似函数的特征"><a href="#良好相似函数的特征" class="headerlink" title="良好相似函数的特征"></a>良好相似函数的特征</h3><p>可以使用三个基本属性来评估相似度函数是否良好。当然，还有更多的标准可以考虑，但这些是最重要的。在符号下方d(x,y)_d_ ( _x_ ,_和_) 读作“x 和 y 之间的距离”。</p><p><strong>对称性</strong>： $d(x,y) = d(y,x)$</p><p>对称性很重要，否则我们可以说 A 看起来像 B，但 B 看起来一点也不像 A。</p><p><strong>正可分离性</strong>：$d(x,y) = 0 \quad if \quad and \quad only \quad if \quad x=y$</p><p>正可分性的属性很重要，因为否则可能会有两个不同的数据点 A 和 B，我们无法区分。</p><p><strong>三角不等式</strong>：$d(x,y) \leq d(x,z) + d(z,y)$</p><p>三角不等式很重要，否则我们可以说 A 看起来像 B，B 看起来像 C，但 A 看起来不像 C。</p><h3 id="常用聚类方法概述"><a href="#常用聚类方法概述" class="headerlink" title="常用聚类方法概述"></a>常用聚类方法概述</h3><p>本文重点介绍 K 均值聚类，但这只是现有的众多聚类算法之一。</p><h4 id="1-分区聚类"><a href="#1-分区聚类" class="headerlink" title="1.分区聚类"></a>1.分区聚类</h4><p>K-means 是<strong>分区聚类算法</strong>（也称为<strong>基于质心的聚类</strong>）的一个例子。这意味着它接受用户提供的集群数量_k_，并将数据划分为许多分区。在分区聚类中，每个数据点只能属于一个集群，任何集群都不能为空。</p><p>许多分区聚类算法<strong>是非确定性的</strong>。这意味着，即使您保持输入固定，每次运行算法时也可能最终得到不同的集群。</p><p><img src="https://i0.wp.com/analyticsarora.com/wp-content/uploads/2021/07/partitional-clustering-example-of-a-set-of-data-points.png?resize=800%2C600&amp;ssl=1#id=ne7yX&amp;originHeight=600&amp;originWidth=800&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h4 id="2-层次聚类"><a href="#2-层次聚类" class="headerlink" title="2.层次聚类"></a>2.<strong>层次聚类</strong></h4><p><strong>层次聚类</strong>下的算法通过<strong>自上而下</strong>或<strong>自下而上</strong>构建<strong>层次结构来</strong>将对象分配给集群。</p><ul><li>自顶向下的方法称为<strong>分裂聚类</strong>。它的工作原理是从一个集群中的所有点开始，然后在每一步拆分最不相似的集群，直到每个数据点都在一个单独的集群中。 </li><li>自底向上的方法称为<strong>凝聚聚类</strong>。这种方法迭代地合并集群中两个最相似的点，直到只有一个大集群。 </li></ul><p>与分区聚类方法不同，层次聚类是<strong>确定性的</strong>。这意味着集群分配在同一数据集上的运行之间不会有所不同。</p><p>自上而下和自下而上的方法都会产生一个基于树的点层次结构，称为 <a href="https://en.wikipedia.org/wiki/Dendrogram"><strong>dendrogram</strong></a>。事实证明，此树状图可用于选择要使用的集群数量。您可以在任何深度切割树以生成不相交树的子集，每个树代表一个集群。</p><p><img src="https://i1.wp.com/analyticsarora.com/wp-content/uploads/2021/07/Hierarchical-Clustering-example-with-resulting-dendrogram.png?resize=800%2C600&amp;ssl=1#id=W7uHq&amp;originHeight=600&amp;originWidth=800&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h4 id="比较分区聚类和层次聚类"><a href="#比较分区聚类和层次聚类" class="headerlink" title="比较分区聚类和层次聚类"></a><strong>比较分区聚类和层次聚类</strong></h4><p>两种聚类方法各有优缺点。首先我们来看看使用K-means（partitional clustering）的优缺点。</p><div class="table-container"><table><thead><tr><th>K-means 的优点</th><th>K-means 的缺点</th></tr></thead><tbody><tr><td>易于实施</td><td>难以预测聚类的数量（K-Value）</td></tr><tr><td>k-Means 可能比具有大量变量的层次聚类更快（如果 K 很小）</td><td>初始种子对最终结果有很大影响</td></tr><tr><td>k-Means 可能产生比层次聚类更紧密的聚类</td><td>数据的顺序对最终结果有影响</td></tr><tr><td>重新计算质心时，实例可以更改集群（移动到另一个集群）</td><td>对缩放敏感：重新缩放数据集（标准化或标准化）将 完全改变结果。</td></tr></tbody></table></div><p>现在，让我们来看看层次聚类的优点和缺点。</p><div class="table-container"><table><thead><tr><th>层次聚类的优点</th><th>层次聚类的缺点</th></tr></thead><tbody><tr><td>输出比 k-means 返回的非结构化集群集信息更多的层次结构。</td><td>一旦一个点被分配给一个 集群，它就不能再四处移动了。</td></tr><tr><td>易于实施</td><td>时间复杂度：不适合大数据集</td></tr><tr><td></td><td>初始种子对最终结果有很大影响</td></tr><tr><td></td><td>数据的顺序对最终结果有影响</td></tr></tbody></table></div><p>我已经概述了两个聚类算法系列。还有其他类型的聚类算法我没有介绍，例如<strong>基于密度的聚类</strong>。有关聚类算法的更完整概述，请访问<a href="https://developers.google.com/machine-learning/clustering/clustering-algorithms">此资源</a>。</p><h2 id="K-means-Clustering-如何在视觉上工作？"><a href="#K-means-Clustering-如何在视觉上工作？" class="headerlink" title="K-means Clustering 如何在视觉上工作？"></a><strong>K-means Clustering 如何在视觉上工作？</strong></h2><p>此可视化显示了使用 3 个集群运行的 k-means 算法。首先，初始化三个<strong>质心</strong>。这些是每个集群的初始中心，由蓝色、红色和绿色大点表示。</p><p>接下来，计算给定数据点与三个质心中的每一个之间的距离。这是针对所有数据点完成的。每个点都分配给它最接近的质心的集群。当我在可视化中单击<strong>重新分配点</strong>时会发生这种情况。</p><p>然后，通过对相应集群中每个数据点的坐标求平均值来重新计算质心。当我单击<strong>Update Centroids</strong>时会发生这种情况。</p><p>这个重新分配点和更新质心的过程一直持续到质心不再移动。</p><p>您可以使用此可视化自己直观地了解 k-means！<a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">在这里</a>试试。</p><h2 id="K-Means伪代码"><a href="#K-Means伪代码" class="headerlink" title="K-Means伪代码"></a>K-Means伪代码</h2><p>K-Means聚类步骤是一个循环迭代的算法，非常简单易懂：</p><ol><li>假定我们要对N个样本观测做聚类，要求聚为K类，首先选择K个点作为<code>初始中心点</code>；</li><li>接下来，按照<code>距离初始中心点最小</code>的原则，把所有观测分到各中心点所在的类中；</li><li>每类中有若干个观测，计算K个类中<code>所有样本点的均值</code>，作为第二次迭代的K个中心点；</li><li>然后根据这个中心重复第2、3步，直到<code>收敛（中心点不再改变或达到指定的迭代次数）</code>，聚类过程结束。</li></ol><p><img src="https://i2.wp.com/analyticsarora.com/wp-content/uploads/2021/07/K-Means-Pseudocode.png?resize=800%2C300&amp;ssl=1#id=jFfSp&amp;originHeight=300&amp;originWidth=800&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h2 id="⭐️使用-sklearn-中的-K-Means"><a href="#⭐️使用-sklearn-中的-K-Means" class="headerlink" title="⭐️使用 sklearn 中的 K-Means"></a>⭐️使用 sklearn 中的 K-Means</h2><blockquote><p><a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/kmeans_cluster.ipynb">https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/kmeans_cluster.ipynb</a></p></blockquote><p>KMeans类的主要参数有：</p><ol><li>n_clusters: 即我们的k值，一般需要多试一些值以获得较好的聚类效果。k值好坏的评估标准在下面会讲。</li><li>max_iter： 最大的迭代次数，一般如果是凸数据集的话可以不管这个值，如果数据集不是凸的，可能很难收敛，此时可以指定最大的迭代次数让算法可以及时退出循环。</li><li>n_init：用不同的初始化质心运行算法的次数。由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次以选择一个较好的聚类效果，默认是10，一般不需要改。如果你的k值较大，则可以适当增大这个值。</li><li>init： 即初始值选择的方式，可以为完全随机选择’random’,优化过的’k-means++’或者自己指定初始化的k个质心。一般建议使用默认的’k-means++’。</li><li>algorithm：有“auto”, “full” or “elkan”三种选择。”full”就是我们传统的K-Means算法， “elkan”是我们原理篇讲的elkan K-Means算法。默认的”auto”则会根据数据值是否是稀疏的，来决定如何选择”full”和“elkan”。一般数据是稠密的，那么就是 “elkan”，否则就是”full”。一般来说建议直接用默认的”auto”</li><li>n_jobs：表示任务使用CPU数量</li><li>random_state：表示随机数生成器的种子。</li><li>verbose：0表示不输出日志信息；1表示每隔一段时间打印一次日志信息。如果大于1，打印次数频繁。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.datasets.samples_generator <span class="hljs-keyword">import</span> make_blobs<br><br><span class="hljs-comment"># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，簇中心在[-1,-1], [0,0],[1,1], [2,2]， 簇方差分别为[0.4, 0.2, 0.2]</span><br>X, y = make_blobs(n_samples=<span class="hljs-number">1000</span>, n_features=<span class="hljs-number">2</span>, centers=[[-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]], cluster_std=[<span class="hljs-number">0.4</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>], <br>                  random_state =<span class="hljs-number">9</span>)<br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.show()<br><br><span class="hljs-comment"># 训练 &amp; 预测</span><br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br>km = KMeans(n_clusters=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">9</span>, n_init=<span class="hljs-number">10</span>, max_iter=<span class="hljs-number">300</span>, init=<span class="hljs-string">&#x27;k-means++&#x27;</span>, verbose=<span class="hljs-number">1</span>)<br>y_pred = km.fit_predict(X)<br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y_pred)<br>plt.show()<br><br><span class="hljs-comment"># 观察在不同的k值下Calinski-Harabasz分数，分数值𝑠越大则聚类效果越好</span><br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br>s = metrics.calinski_harabaz_score(X, y_pred)  <br></code></pre></td></tr></table></figure><h2 id="如何在-Python-中从头开始编写-K-means？"><a href="#如何在-Python-中从头开始编写-K-means？" class="headerlink" title="如何在 Python 中从头开始编写 K-means？"></a>如何在 Python 中从头开始编写 K-means？</h2><p>我们的 k-means 实现将分为五个辅助方法和一个运行算法的主循环。让我们一一介绍这些功能。</p><ul><li>成对距离</li><li>初始化中心</li><li>更新分配</li><li>更新中心</li><li>计算损失</li><li>主循环</li><li>完整的实现</li></ul><h3 id="计算成对距离"><a href="#计算成对距离" class="headerlink" title="计算成对距离"></a><strong>计算成对距离</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pairwise_dist</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            x: N x D numpy array</span><br><span class="hljs-string">            y: M x D numpy array</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            dist: N x M array, where dist2[i, j] is the euclidean distance between </span><br><span class="hljs-string">            x[i, :] and y[j, :]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        xSumSquare = np.<span class="hljs-built_in">sum</span>(np.square(x),axis=<span class="hljs-number">1</span>);<br>        ySumSquare = np.<span class="hljs-built_in">sum</span>(np.square(y),axis=<span class="hljs-number">1</span>);<br>        mul = np.dot(x, y.T);<br>        dists = np.sqrt(<span class="hljs-built_in">abs</span>(xSumSquare[:, np.newaxis] + ySumSquare-<span class="hljs-number">2</span>*mul))<br>        <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure><p>pairwise_dist 函数与前面描述的相似性函数等效。这是我们比较两点相似性的指标。在这里，我使用的是<strong>欧几里得距离</strong>。我使用的公式可能看起来与欧几里德距离函数的常规公式不同。这是因为我们正在执行矩阵操作，而不是使用两个单个向量。在<a href="https://www.dabblingbadger.com/blog/2020/2/27/implementing-euclidean-distance-matrix-calculations-from-scratch-in-python">这里</a>深入阅读。</p><h3 id="初始化中心"><a href="#初始化中心" class="headerlink" title="初始化中心"></a><strong>初始化中心</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_centers</span>(<span class="hljs-params">self, points, K, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            points: NxD numpy array, where N is # points and D is the dimensionality</span><br><span class="hljs-string">            K: number of clusters</span><br><span class="hljs-string">            kwargs: any additional arguments you want</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            centers: K x D numpy array, the centers. </span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        row, col = points.shape<br>        retArr = np.empty([K, col])<br>        <span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>            randIndex = np.random.randint(row)<br>            retArr[number] = points[randIndex]<br>        <br>        <span class="hljs-keyword">return</span> retArr<br></code></pre></td></tr></table></figure><p>该函数接收点数组并随机选择其中的 K 个作为初始质心。该函数仅返回 K 个选定点。</p><h3 id="更新分配"><a href="#更新分配" class="headerlink" title="更新分配"></a><strong>更新分配</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_assignment</span>(<span class="hljs-params">self, centers, points</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            centers: KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Hint: You could call pairwise_dist() function.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        row, col = points.shape<br>        cluster_idx = np.empty([row])<br>        distances = self.pairwise_dist(points, centers)<br>        cluster_idx = np.argmin(distances, axis=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> cluster_idx<br></code></pre></td></tr></table></figure><p>更新分配函数负责选择每个点应该属于哪个集群。首先，我使用 pairwise_dist 函数计算每个点和每个质心之间的距离。然后，我得到每一行的最小距离的索引。最小距离的索引也是给定数据点的聚类分配索引，因为我们希望将每个点分配给最近的质心。</p><h3 id="更新中心"><a href="#更新中心" class="headerlink" title="更新中心"></a><strong>更新中心</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_centers</span>(<span class="hljs-params">self, old_centers, cluster_idx, points</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            old_centers: old centers KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            centers: new centers, K x D numpy array, where K is the number of clusters, and D is the dimension.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        K, D = old_centers.shape<br>        new_centers = np.empty(old_centers.shape)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>            new_centers[i] = np.mean(points[cluster_idx == i], axis = <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> new_centers<br></code></pre></td></tr></table></figure><p>更新中心功能负责对属于给定集群的所有点进行平均。该平均值是相应聚类的新质心。该函数返回新中心的数组。</p><h3 id="计算损失"><a href="#计算损失" class="headerlink" title="计算损失"></a><strong>计算损失</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_loss</span>(<span class="hljs-params">self, centers, cluster_idx, points</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            centers: KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            loss: a single float number, which is the objective function of KMeans. </span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        dists = self.pairwise_dist(points, centers)<br>        loss = <span class="hljs-number">0.0</span><br>        N, D = points.shape<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>            loss = loss + np.square(dists[i][cluster_idx[i]])<br>        <br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><p>损失函数是我们评估聚类算法性能的指标。我们的损失只是每个点与其聚类质心之间的平方距离之和。在我们的实现中，我们首先调用成对距离来获得每个点和每个中心之间的距离矩阵。我们使用 cluster_idx 为每个点选择与集群对应的适当距离2。</p><h3 id="主循环"><a href="#主循环" class="headerlink" title="主循环"></a><strong>主循环</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, points, K, max_iters=<span class="hljs-number">100</span>, abs_tol=<span class="hljs-number">1e-16</span>, rel_tol=<span class="hljs-number">1e-16</span>, verbose=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            points: NxD numpy array, where N is # points and D is the dimensionality</span><br><span class="hljs-string">            K: number of clusters</span><br><span class="hljs-string">            max_iters: maximum number of iterations (Hint: You could change it when debugging)</span><br><span class="hljs-string">            abs_tol: convergence criteria w.r.t absolute change of loss</span><br><span class="hljs-string">            rel_tol: convergence criteria w.r.t relative change of loss</span><br><span class="hljs-string">            verbose: boolean to set whether method should print loss (Hint: helpful for debugging)</span><br><span class="hljs-string">            kwargs: any additional arguments you want</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            cluster assignments: Nx1 int numpy array</span><br><span class="hljs-string">            cluster centers: K x D numpy array, the centers</span><br><span class="hljs-string">            loss: final loss value of the objective function of KMeans</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        centers = self._init_centers(points, K, **kwargs)<br>        <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_iters):<br>            cluster_idx = self._update_assignment(centers, points)<br>            centers = self._update_centers(centers, cluster_idx, points)<br>            loss = self._get_loss(centers, cluster_idx, points)<br>            K = centers.shape[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">if</span> it:<br>                diff = np.<span class="hljs-built_in">abs</span>(prev_loss - loss)<br>                <span class="hljs-keyword">if</span> diff &lt; abs_tol <span class="hljs-keyword">and</span> diff / prev_loss &lt; rel_tol:<br>                    <span class="hljs-keyword">break</span><br>            prev_loss = loss<br>            <span class="hljs-keyword">if</span> verbose:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;iter %d, loss: %.4f&#x27;</span> % (it, loss))<br>        <span class="hljs-keyword">return</span> cluster_idx, centers, loss<br></code></pre></td></tr></table></figure><p>现在，在主循环中，我们可以组合所有实用函数并实现伪代码。首先，中心用<strong>_init_centers</strong>随机初始化。然后，对于指定的迭代次数，我们重复<strong>update_assignment</strong>和<strong>update_centers</strong>步骤。每次迭代后，我们计算总损失并将其与之前的损失进行比较。如果差异小于我们的阈值，则算法执行完成。</p><h3 id="完整的-K-Means-类实现"><a href="#完整的-K-Means-类实现" class="headerlink" title="完整的 K-Means 类实现"></a><strong>完整的 K-Means 类实现</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> absolute_import<br><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function<br><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> division<br><br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> matplotlib<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> axes3d<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-comment"># Load image</span><br><span class="hljs-keyword">import</span> imageio<br><br><br><span class="hljs-comment"># Set random seed so output is all same</span><br>np.random.seed(<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">KMeans</span>(<span class="hljs-title class_ inherited__">object</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># No need to implement</span><br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">pairwise_dist</span>(<span class="hljs-params">self, x, y</span>):  <span class="hljs-comment"># [5 pts]</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            x: N x D numpy array</span><br><span class="hljs-string">            y: M x D numpy array</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">                dist: N x M array, where dist2[i, j] is the euclidean distance between </span><br><span class="hljs-string">                x[i, :] and y[j, :]</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br>        xSumSquare = np.<span class="hljs-built_in">sum</span>(np.square(x),axis=<span class="hljs-number">1</span>);<br>        ySumSquare = np.<span class="hljs-built_in">sum</span>(np.square(y),axis=<span class="hljs-number">1</span>);<br>        mul = np.dot(x, y.T);<br>        dists = np.sqrt(<span class="hljs-built_in">abs</span>(xSumSquare[:, np.newaxis] + ySumSquare-<span class="hljs-number">2</span>*mul))<br>        <span class="hljs-keyword">return</span> dists<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_centers</span>(<span class="hljs-params">self, points, K, **kwargs</span>):  <span class="hljs-comment"># [5 pts]</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            points: NxD numpy array, where N is # points and D is the dimensionality</span><br><span class="hljs-string">            K: number of clusters</span><br><span class="hljs-string">            kwargs: any additional arguments you want</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            centers: K x D numpy array, the centers. </span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        row, col = points.shape<br>        retArr = np.empty([K, col])<br>        <span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>            randIndex = np.random.randint(row)<br>            retArr[number] = points[randIndex]<br>        <br>        <span class="hljs-keyword">return</span> retArr<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_assignment</span>(<span class="hljs-params">self, centers, points</span>):  <span class="hljs-comment"># [10 pts]</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            centers: KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Hint: You could call pairwise_dist() function.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        row, col = points.shape<br>        cluster_idx = np.empty([row])<br>        distances = self.pairwise_dist(points, centers)<br>        cluster_idx = np.argmin(distances, axis=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> cluster_idx<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_centers</span>(<span class="hljs-params">self, old_centers, cluster_idx, points</span>):  <span class="hljs-comment"># [10 pts]</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            old_centers: old centers KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            centers: new centers, K x D numpy array, where K is the number of clusters, and D is the dimension.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        K, D = old_centers.shape<br>        new_centers = np.empty(old_centers.shape)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>            new_centers[i] = np.mean(points[cluster_idx == i], axis = <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> new_centers<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_loss</span>(<span class="hljs-params">self, centers, cluster_idx, points</span>):  <span class="hljs-comment"># [5 pts]</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            centers: KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            loss: a single float number, which is the objective function of KMeans. </span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        dists = self.pairwise_dist(points, centers)<br>        loss = <span class="hljs-number">0.0</span><br>        N, D = points.shape<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>            loss = loss + np.square(dists[i][cluster_idx[i]])<br>        <br>        <span class="hljs-keyword">return</span> loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, points, K, max_iters=<span class="hljs-number">100</span>, abs_tol=<span class="hljs-number">1e-16</span>, rel_tol=<span class="hljs-number">1e-16</span>, verbose=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            points: NxD numpy array, where N is # points and D is the dimensionality</span><br><span class="hljs-string">            K: number of clusters</span><br><span class="hljs-string">            max_iters: maximum number of iterations (Hint: You could change it when debugging)</span><br><span class="hljs-string">            abs_tol: convergence criteria w.r.t absolute change of loss</span><br><span class="hljs-string">            rel_tol: convergence criteria w.r.t relative change of loss</span><br><span class="hljs-string">            verbose: boolean to set whether method should print loss (Hint: helpful for debugging)</span><br><span class="hljs-string">            kwargs: any additional arguments you want</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            cluster assignments: Nx1 int numpy array</span><br><span class="hljs-string">            cluster centers: K x D numpy array, the centers</span><br><span class="hljs-string">            loss: final loss value of the objective function of KMeans</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        centers = self._init_centers(points, K, **kwargs)<br>        <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_iters):<br>            cluster_idx = self._update_assignment(centers, points)<br>            centers = self._update_centers(centers, cluster_idx, points)<br>            loss = self._get_loss(centers, cluster_idx, points)<br>            K = centers.shape[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">if</span> it:<br>                diff = np.<span class="hljs-built_in">abs</span>(prev_loss - loss)<br>                <span class="hljs-keyword">if</span> diff &lt; abs_tol <span class="hljs-keyword">and</span> diff / prev_loss &lt; rel_tol:<br>                    <span class="hljs-keyword">break</span><br>            prev_loss = loss<br>            <span class="hljs-keyword">if</span> verbose:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;iter %d, loss: %.4f&#x27;</span> % (it, loss))<br>        <span class="hljs-keyword">return</span> cluster_idx, centers, loss<br></code></pre></td></tr></table></figure><p>这是我们在 Python 中完整的 K-means 类实现。我鼓励您复制此代码（或自己实现！）并在您自己的机器上运行它。这是巩固您对 K-means 算法理解的最佳方式。<a href="https://cloud.tencent.com/developer/article/1010876">scikit-learn中的KMeans聚类实现</a></p><h2 id="初始中心点怎么确定"><a href="#初始中心点怎么确定" class="headerlink" title="初始中心点怎么确定"></a>初始中心点怎么确定</h2><p>在k-means算法步骤中，有两个地方降低了SSE：</p><ol><li>把样本点分到最近邻的簇中，这样会降低SSE的值；</li><li>重新优化聚类中心点，进一步的减小了SSE。</li></ol><p>这样的重复迭代、不断优化，会找到<code>局部最优解（局部最小的SSE）</code>，如果想要找到全局最优解需要找到合理的初始聚类中心。</p><p><strong>那合理的初始中心怎么选？</strong></p><p>方法有很多，譬如先随便选个点作为第1个初始中心C1，接下来计算所有样本点与C1的距离，距离最大的被选为下一个中心C2，直到选完K个中心。这个算法叫做<code>K-Means++</code>，可以理解为 K-Means的改进版，它可以能有效地解决初始中心的选取问题，但<code>无法解决离群点问题</code>。</p><h2 id="K值怎么确定"><a href="#K值怎么确定" class="headerlink" title="K值怎么确定"></a>K值怎么确定</h2><p>使用 K 均值算法，性能可能会因您使用的集群数量而有很大差异。要知道，<strong>K设置得越大，样本划分得就越细，每个簇的聚合程度就越高，误差平方和SSE自然就越小</strong>。所以不能单纯像选择初始点那样，用不同的K来做尝试，选择SSE最小的聚类结果对应的K值，因为这样选出来的肯定是你尝试的那些K值中最大的那个。</p><h3 id="“手肘法”（Elbow-Method）"><a href="#“手肘法”（Elbow-Method）" class="headerlink" title="“手肘法”（Elbow Method）"></a>“手肘法”（Elbow Method）</h3><p>为了使用肘部方法，您只需多次运行 K-means 算法，每次迭代将聚类数增加一个。记录每次迭代的损失，然后<strong>制作 num cluster vs loss 的折线图</strong>。</p><p>下面是肘部方法的简单实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br>metrics.calinski_harabaz_score(X, y_pred) <br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><br>loss = []<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>):<br>    kmeans = KMeans(n_clusters=i, max_iter=<span class="hljs-number">100</span>).fit(p_list)<br>    loss.append(kmeans.inertia_ / point_number / K)<br>    <br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), loss)<br>plt.show()<br></code></pre></td></tr></table></figure><p>运行此方法将输出一个类似于您在下面看到的图：</p><p><img src="https://i1.wp.com/analyticsarora.com/wp-content/uploads/2021/07/elbow-method-example-choosing-number-of-clusters-k-means.png?resize=464%2C299&amp;ssl=1#id=JyGLl&amp;originHeight=299&amp;originWidth=464&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>现在，为了选择正确数量的簇，我们进行目视检查。损耗曲线开始弯曲的<strong>点</strong>称为 <strong>肘点</strong>。肘点代表误差和聚类数量之间的合理权衡。在此示例中，肘点位于<strong>x = 3 处</strong>。这意味着最佳聚类数为 3。</p><h3 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h3><p>数据的平均轮廓是评估集群自然数的另一个有用标准。数据实例的轮廓是衡量它与集群内数据匹配程度以及与相邻集群数据匹配程度的度量。</p><p>轮廓值是衡量一个对象与其自己的集群（内聚）相比其他集群（分离）的相似程度。轮廓范围从 -1 到 +1，其中高值表示对象与其自己的集群匹配良好，而与相邻集群匹配不佳。如果大多数对象具有较高的值，则集群配置是合适的。如果许多点具有低值或负值，则聚类配置可能具有过多或过少的聚类。</p><p>如果您想实现用于聚类分析的轮廓系数，我建议使用 scikit-learn。访问<a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html">此</a>资源以获取有关实施的完整指南。</p><h2 id="正确使用「K均值聚类」的Tips"><a href="#正确使用「K均值聚类」的Tips" class="headerlink" title="正确使用「K均值聚类」的Tips"></a>正确使用「K均值聚类」的Tips</h2><ol><li><strong>输入数据一般需要做缩放</strong>，如标准化。原因很简单，K均值是建立在距离度量上的，因此不同变量间如果维度差别过大，可能会造成少数变量“施加了过高的影响而造成垄断”。</li><li><strong>输出结果非固定，多次运行结果可能不同。</strong>首先要意识到K-means中是有随机性的，从初始化到收敛结果往往不同。一种看法是强行固定随机性，比如设定sklearn中的random state为固定值。另一种看法是，如果你的K均值结果总在大幅度变化，比如不同簇中的数据量在多次运行中变化很大，那么K均值不适合你的数据，不要试图稳定结果 [2]。</li><li><strong>运行时间往往可以得到优化，选择最优的工具库。</strong>基本上现在的K均值实现都是K-means++，速度都不错。但当数据量过大时，依然可以使用其他方法，如MiniBatchKMeans [3]。上百万个数据点往往可以在数秒钟内完成聚类，推荐Sklearn的实现。</li><li><strong>高维数据上的有效性有限。</strong>建立在距离度量上的算法一般都有类似的问题，那就是在高维空间中距离的意义有了变化，且并非所有维度都有意义。这种情况下，K均值的结果往往不好，而通过划分子空间的算法（sub-spacing method）效果可能更好。</li><li><strong>运行效率与性能之间的取舍。</strong></li></ol><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>因此不难看出，K-Means优点在于原理简单，运行速度快且容易实现，能够处理的数据量大。</p><p>当然，也有一些缺点：</p><ul><li>在高维上可能不是最佳选项</li><li>K值、初始点的选取不好确定；</li><li>得到的结果只是局部最优；</li><li>受离群值影响大。</li></ul><p>一个比较粗浅的结论是，在数据量不大时，可以优先尝试其他算法。当数据量过大时，可以试试HDBSCAN。仅当数据量巨大，且无法降维或者降低数量时，再尝试使用K均值。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/75477709">用人话讲明白快速聚类kmeans</a> </li><li><a href="https://zhuanlan.zhihu.com/p/35959301">从零开始教你 KMeans 算法</a> </li><li><a href="https://zhuanlan.zhihu.com/p/34330242">如何正确使用「K均值聚类」？</a> </li><li><a href="https://www.biaodianfu.com/k-means-choose-k.html">K-Means算法之K值的选择</a> </li><li><a href="http://zhouchen.tech/2017/12/22/sklearn%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E5%88%86%E7%B1%BB%E3%80%81%E8%81%9A%E7%B1%BB%E3%80%81%E5%9B%9E%E5%BD%92%E5%92%8C%E9%99%8D%E7%BB%B4/">sklearn入门教程：分类、聚类、回归和降维</a> </li></ul><p><a href="https://analyticsarora.com/k-means-for-beginners-how-to-build-from-scratch-in-python">K-means for Beginners: How to Build from Scratch in Python</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>文本聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文本聚类算法总结</title>
    <link href="/2021/09/06/2021-09-06-%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <url>/2021/09/06/2021-09-06-%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="1-聚类介绍"><a href="#1-聚类介绍" class="headerlink" title="1. 聚类介绍"></a>1. 聚类介绍</h2><ul><li>聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster).</li><li>通过这样的划分，使得<strong>同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大</strong>。</li><li>聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者自己来把握。</li><li>聚类既能作为一个单独的过程用于寻找数据内在的分布结构，也可以作为分类等其他学习任务的前驱过程。</li></ul><h3 id="1-1-聚类算法："><a href="#1-1-聚类算法：" class="headerlink" title="1.1 聚类算法："></a>1.1 聚类算法：</h3><ul><li>基于原型的聚类(Prototype-based Clustering) <ul><li><strong>K均值聚类(K-means)</strong></li><li><strong>学习向量量化聚类(Learning Vector Quantization)</strong></li><li><strong>高斯混合模型聚类 (Gaussian Mixture Model)</strong></li></ul></li><li>基于密度的聚类 (Density-based Clustering) <ul><li><strong>DBSCAN (Density-Based Spatial Clustering of Application with Noise)</strong></li><li><strong>OPTICS (Ordering Points To Identify the Clustering Structure)</strong></li></ul></li><li><strong>层次聚类 (Hierarchical Clustering)</strong></li><li>基于模型的聚类 (Model-based Clustering) <ul><li><strong>混合回归模型 (Mixture Regression Model)</strong><h3 id="1-2-聚类的一般过程"><a href="#1-2-聚类的一般过程" class="headerlink" title="1.2 聚类的一般过程"></a>1.2 聚类的一般过程</h3></li></ul></li></ul><ol><li>数据准备：特征标准化和降维</li><li>特征选择：从最初的特征中选择最有效的特征，并将其存储在向量中</li><li>特征提取：通过对选择的特征进行转换形成新的突出特征</li><li>聚类：基于某种距离函数进行相似度度量，获取簇</li><li>聚类结果评估：分析聚类结果，如<code>距离误差和(SSE)</code>等</li></ol><h3 id="1-3-聚类距离计算："><a href="#1-3-聚类距离计算：" class="headerlink" title="1.3 聚类距离计算："></a>1.3 聚类距离计算：</h3><p><strong>距离度量(distance measure)函数 </strong>$d()$<strong> 需满足的基本性质：</strong></p><ul><li><strong>非负性</strong>： $d(x,y)&gt;=0$</li><li><strong>同一性</strong>：$d(x,y) = 0 \quad if \quad and \quad only \quad if \quad x=y$</li><li><strong>对称性</strong>： $d(x,y) = d(y,x)$</li><li><strong>直递性</strong>：$d(x,y) \leq d(x,z) + d(z,y)$ (可不满足)</li></ul><p>常用基于距离的相似度度量方法：</p><p><img src="https://pic3.zhimg.com/80/v2-921d5ff0b665e67d5df54047b9d6531a_1440w.jpg#id=PaP92&amp;originHeight=362&amp;originWidth=1440&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p><strong>欧几里得距离：</strong></p><p><img src="https://i1.wp.com/analyticsarora.com/wp-content/uploads/2021/07/euclidean-distance-between-two-points-in-x-y-coordinates.png?resize=800%2C600&amp;ssl=1#id=AHTlA&amp;originHeight=600&amp;originWidth=800&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p><strong>曼哈顿距离：</strong></p><p><img src="https://i2.wp.com/analyticsarora.com/wp-content/uploads/2021/07/manhattan-distance-between-two-points-in-x-y-coordinates.png?resize=800%2C600&amp;ssl=1#id=uqjXz&amp;originHeight=600&amp;originWidth=800&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h3 id="1-4-聚类性能度量："><a href="#1-4-聚类性能度量：" class="headerlink" title="1.4 聚类性能度量："></a>1.4 聚类性能度量：</h3><p><strong>聚类性能度量亦称聚类“有效性指标”(validity index).</strong></p><p><strong>设置聚类性能度量的目的:</strong></p><ul><li>对聚类结果，通过某种性能度量来评估其好坏；</li><li>若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果。</li></ul><p><strong>什么样的聚类结果比较好？</strong></p><ul><li>“簇内相似度”(intra-cluster similarity)高</li><li>“蔟间相似度”(inter-cluster similarity)低</li></ul><p><strong>聚类性能度量分类：</strong></p><ul><li>“外部指标”(external index) ：将聚类结果与某个“参考模型”(reference model)进行比较</li><li>“内部指标”(internal index): 直接考察聚类结果而不利用任何参考模型</li></ul><p><a href="http://rstudio-pubs-static.s3.amazonaws.com/202367_03e1ddfcbca74cba97c4a4be7bbabd48.html"><strong>性能度量指标：</strong></a></p><p><strong>(1) 外部指标</strong></p><p><strong>(2) 内部指标</strong></p><h2 id="2-聚类算法介绍及实现"><a href="#2-聚类算法介绍及实现" class="headerlink" title="2 聚类算法介绍及实现"></a><strong>2 聚类算法介绍及实现</strong></h2><p><strong>聚类算法类型：</strong></p><ul><li>基于原型的聚类(Prototype-based Clustering) <ul><li><strong>K均值聚类(K-means )</strong></li><li><strong>学习向量量化聚类(Learning vector Quantization)</strong></li><li><strong>高斯混合聚类(Mixture-of-Gaussian)</strong></li></ul></li><li><strong>基于密度的聚类(Density-based Clustering)</strong></li><li><strong>层次聚类(Hierarchical Clustering)</strong></li></ul><p><img src="https://pic2.zhimg.com/80/v2-04a2252ba991e07045613463e0414dc9_1440w.jpg#id=hv6EN&amp;originHeight=494&amp;originWidth=1440&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h3 id="2-1-基于原型的聚类"><a href="#2-1-基于原型的聚类" class="headerlink" title="2.1 基于原型的聚类"></a><a href="https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.2.c4339a9da7c9ec88.md">2.1 基于原型的聚类</a></h3><p>原型聚类<code>prototype-based clustering</code>，假设聚类结构能通过一组原型刻画（原型，即簇类的数目或者聚类中心）. <strong>通常情况下, 算法先对原型进行初始化, 然后对原型进行迭代更新求解, 采用不同的原型表示, 不同的求解方式,将产生不同的算法.</strong></p><h4 id="2-1-1-K-means"><a href="#2-1-1-K-means" class="headerlink" title="2.1.1 K-means"></a>2.1.1 K-means</h4><h5 id="1-算法介绍"><a href="#1-算法介绍" class="headerlink" title="(1) 算法介绍"></a><strong>(1) 算法介绍</strong></h5><p>给定样本集 $D={x_1,x_2,…,x_n}$, K-means 算法针对聚类所得簇划分 $C={C_1,C_2,…,C_k}$, 最小化平方误差:</p><p><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/843d8aa63286c6920455f9c3605c3e87.svg#id=x1vBU&amp;originHeight=68&amp;originWidth=236&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>其中 $u_k$ 是簇 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/3a840486dde29f61a0ec8bccfa2c09ff.svg#id=gu5qD&amp;originHeight=21&amp;originWidth=25&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的均值向量：</p><p><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/932d6a577db749a6bdd2fbe822009ead.svg#id=Urgvp&amp;originHeight=35&amp;originWidth=176&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>直观上看, 平方误差在一定程度上刻画了簇内样本围绕均值向量的紧密程度, $err$ 值越小簇内样本相似度越高。但最小化 $err$  不容易，是一个NP难问题, K-means 算法采用了贪心策略，通过迭代优化来近似求解 EE 的最小值。具体算法如下：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1guivdot39xj60nl0hkgnt02.jpg#id=WkUiY&amp;originHeight=632&amp;originWidth=849&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h5 id="2-算法实现"><a href="#2-算法实现" class="headerlink" title="(2) 算法实现"></a><strong>(2) 算法实现</strong></h5><p>我们的 k-means 实现将分为五个辅助方法和一个运行算法的主循环：</p><p><strong>1.计算成对距离</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pairwise_dist</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            x: N x D numpy array</span><br><span class="hljs-string">            y: M x D numpy array</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            dist: N x M array, where dist2[i, j] is the euclidean distance between </span><br><span class="hljs-string">            x[i, :] and y[j, :]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        xSumSquare = np.<span class="hljs-built_in">sum</span>(np.square(x),axis=<span class="hljs-number">1</span>);<br>        ySumSquare = np.<span class="hljs-built_in">sum</span>(np.square(y),axis=<span class="hljs-number">1</span>);<br>        mul = np.dot(x, y.T);<br>        dists = np.sqrt(<span class="hljs-built_in">abs</span>(xSumSquare[:, np.newaxis] + ySumSquare-<span class="hljs-number">2</span>*mul))<br>        <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure><p>pairwise_dist 函数与前面描述的相似性函数等效。这是我们比较两点相似性的指标。在这里，我使用的是<strong>欧几里得距离</strong>。我使用的公式可能看起来与欧几里德距离函数的常规公式不同。这是因为我们正在执行矩阵操作，而不是使用两个单个向量。在<a href="https://www.dabblingbadger.com/blog/2020/2/27/implementing-euclidean-distance-matrix-calculations-from-scratch-in-python">这里</a>深入阅读。</p><p><strong>2.初始化中心</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_centers</span>(<span class="hljs-params">self, points, K, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            points: NxD numpy array, where N is # points and D is the dimensionality</span><br><span class="hljs-string">            K: number of clusters</span><br><span class="hljs-string">            kwargs: any additional arguments you want</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            centers: K x D numpy array, the centers. </span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        row, col = points.shape<br>        retArr = np.empty([K, col])<br>        <span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>            randIndex = np.random.randint(row)<br>            retArr[number] = points[randIndex]<br>        <br>        <span class="hljs-keyword">return</span> retArr<br></code></pre></td></tr></table></figure><p>该函数接收点数组并随机选择其中的 K 个作为初始质心。该函数仅返回 K 个选定点。</p><p><strong>3.更新分配</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_assignment</span>(<span class="hljs-params">self, centers, points</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            centers: KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Hint: You could call pairwise_dist() function.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        row, col = points.shape<br>        cluster_idx = np.empty([row])<br>        distances = self.pairwise_dist(points, centers)<br>        cluster_idx = np.argmin(distances, axis=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> cluster_idx<br></code></pre></td></tr></table></figure><p>更新分配函数负责选择每个点应该属于哪个集群。首先，我使用 pairwise_dist 函数计算每个点和每个质心之间的距离。然后，我得到每一行的最小距离的索引。最小距离的索引也是给定数据点的聚类分配索引，因为我们希望将每个点分配给最近的质心。</p><p><strong>4.更新中心</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_centers</span>(<span class="hljs-params">self, old_centers, cluster_idx, points</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            old_centers: old centers KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            centers: new centers, K x D numpy array, where K is the number of clusters, and D is the dimension.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        K, D = old_centers.shape<br>        new_centers = np.empty(old_centers.shape)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>            new_centers[i] = np.mean(points[cluster_idx == i], axis = <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> new_centers<br></code></pre></td></tr></table></figure><p>更新中心功能负责对属于给定集群的所有点进行平均。该平均值是相应聚类的新质心。该函数返回新中心的数组。</p><p><strong>5.计算损失</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_loss</span>(<span class="hljs-params">self, centers, cluster_idx, points</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            centers: KxD numpy array, where K is the number of clusters, and D is the dimension</span><br><span class="hljs-string">            cluster_idx: numpy array of length N, the cluster assignment for each point</span><br><span class="hljs-string">            points: NxD numpy array, the observations</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            loss: a single float number, which is the objective function of KMeans. </span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        dists = self.pairwise_dist(points, centers)<br>        loss = <span class="hljs-number">0.0</span><br>        N, D = points.shape<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>            loss = loss + np.square(dists[i][cluster_idx[i]])<br>        <br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><p>损失函数是我们评估聚类算法性能的指标。我们的损失只是每个点与其聚类质心之间的平方距离之和。在我们的实现中，我们首先调用成对距离来获得每个点和每个中心之间的距离矩阵。我们使用 cluster_idx 为每个点选择与集群对应的适当距离2。</p><p><strong>主循环</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, points, K, max_iters=<span class="hljs-number">100</span>, abs_tol=<span class="hljs-number">1e-16</span>, rel_tol=<span class="hljs-number">1e-16</span>, verbose=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            points: NxD numpy array, where N is # points and D is the dimensionality</span><br><span class="hljs-string">            K: number of clusters</span><br><span class="hljs-string">            max_iters: maximum number of iterations (Hint: You could change it when debugging)</span><br><span class="hljs-string">            abs_tol: convergence criteria w.r.t absolute change of loss</span><br><span class="hljs-string">            rel_tol: convergence criteria w.r.t relative change of loss</span><br><span class="hljs-string">            verbose: boolean to set whether method should print loss (Hint: helpful for debugging)</span><br><span class="hljs-string">            kwargs: any additional arguments you want</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            cluster assignments: Nx1 int numpy array</span><br><span class="hljs-string">            cluster centers: K x D numpy array, the centers</span><br><span class="hljs-string">            loss: final loss value of the objective function of KMeans</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        centers = self._init_centers(points, K, **kwargs)<br>        <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_iters):<br>            cluster_idx = self._update_assignment(centers, points)<br>            centers = self._update_centers(centers, cluster_idx, points)<br>            loss = self._get_loss(centers, cluster_idx, points)<br>            K = centers.shape[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">if</span> it:<br>                diff = np.<span class="hljs-built_in">abs</span>(prev_loss - loss)<br>                <span class="hljs-keyword">if</span> diff &lt; abs_tol <span class="hljs-keyword">and</span> diff / prev_loss &lt; rel_tol:<br>                    <span class="hljs-keyword">break</span><br>            prev_loss = loss<br>            <span class="hljs-keyword">if</span> verbose:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;iter %d, loss: %.4f&#x27;</span> % (it, loss))<br>        <span class="hljs-keyword">return</span> cluster_idx, centers, loss<br></code></pre></td></tr></table></figure><p>现在，在主循环中，我们可以组合所有实用函数并实现伪代码。首先，中心用<strong>_init_centers</strong>随机初始化。然后，对于指定的迭代次数，我们重复<strong>update_assignment</strong>和<strong>update_centers</strong>步骤。每次迭代后，我们计算总损失并将其与之前的损失进行比较。如果差异小于我们的阈值，则算法执行完成。</p><blockquote><p>补充：</p><p><code>k-means</code> 优点：</p><ul><li>计算复杂度低，为 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/9fafdc95f33bed9348c04625220c11e5.svg#id=Z24x3&amp;originHeight=23&amp;originWidth=130&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> ，其中 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/116c79e23707e25a1b0bbb5791245858.svg#id=JI1RR&amp;originHeight=16&amp;originWidth=10&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 为迭代次数。<br>通常 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/b0f80a77b6404455ddb6f55b039e4105.svg#id=mITU7&amp;originHeight=17&amp;originWidth=19&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 和 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/116c79e23707e25a1b0bbb5791245858.svg#id=G9NMS&amp;originHeight=16&amp;originWidth=10&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 要远远小于 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f80122dd44760c38ab0f530c50496433.svg#id=YDfdv&amp;originHeight=17&amp;originWidth=19&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，此时复杂度相当于 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/ce755bd2c810562b16b377d6d586e552.svg#id=obhgp&amp;originHeight=23&amp;originWidth=51&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">。 </li><li>思想简单，容易实现。 </li></ul><p><code>k-means</code> 缺点：</p><ul><li>需要首先确定聚类的数量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/b0f80a77b6404455ddb6f55b039e4105.svg#id=dMN7L&amp;originHeight=17&amp;originWidth=19&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </li><li>分类结果严重依赖于分类中心的初始化。<br>通常进行多次<code>k-means</code>，然后选择最优的那次作为最终聚类结果。 </li><li>结果不一定是全局最优的，只能保证局部最优。 </li><li>对噪声敏感。因为簇的中心是取平均，因此聚类簇很远地方的噪音会导致簇的中心点偏移。 </li><li>无法解决不规则形状的聚类。 </li><li>无法处理离散特征，如：<code>国籍、性别</code> 等。 </li></ul><p><strong>正确使用「K均值聚类」的Tips</strong></p><ol><li><strong>输入数据一般需要做缩放</strong>，如标准化。原因很简单，K均值是建立在距离度量上的，因此不同变量间如果维度差别过大，可能会造成少数变量“施加了过高的影响而造成垄断”。</li><li><strong>输出结果非固定，多次运行结果可能不同。</strong>首先要意识到K-means中是有随机性的，从初始化到收敛结果往往不同。一种看法是强行固定随机性，比如设定sklearn中的random state为固定值。另一种看法是，如果你的K均值结果总在大幅度变化，比如不同簇中的数据量在多次运行中变化很大，那么K均值不适合你的数据，不要试图稳定结果 [2]。</li><li><strong>运行时间往往可以得到优化，选择最优的工具库。</strong>基本上现在的K均值实现都是K-means++，速度都不错。但当数据量过大时，依然可以使用其他方法，如MiniBatchKMeans [3]。上百万个数据点往往可以在数秒钟内完成聚类，推荐Sklearn的实现。</li><li><strong>高维数据上的有效性有限。</strong>建立在距离度量上的算法一般都有类似的问题，那就是在高维空间中距离的意义有了变化，且并非所有维度都有意义。这种情况下，K均值的结果往往不好，而通过划分子空间的算法（sub-spacing method）效果可能更好。</li><li><strong>运行效率与性能之间的取舍。</strong></li></ol><p><strong>K值怎么确定</strong></p><ul><li>使用 K 均值算法，性能可能会因您使用的集群数量而有很大差异。要知道，<strong>K设置得越大，样本划分得就越细，每个簇的聚合程度就越高，误差平方和SSE自然就越小</strong>。所以不能单纯像选择初始点那样，用不同的K来做尝试，选择SSE最小的聚类结果对应的K值，因为这样选出来的肯定是你尝试的那些K值中最大的那个。 </li><li>“手肘法”（Elbow Method）<br>为了使用肘部方法，您只需多次运行 K-means 算法，每次迭代将聚类数增加一个。记录每次迭代的损失，然后<strong>制作 num cluster vs loss 的折线图</strong>。 </li></ul><p>下面是肘部方法的简单实现：</p><p>运行此方法将输出一个类似于您在下面看到的图：</p><p><img src="https://i1.wp.com/analyticsarora.com/wp-content/uploads/2021/07/elbow-method-example-choosing-number-of-clusters-k-means.png?resize=464%2C299&amp;ssl=1#id=wbgsR&amp;originHeight=299&amp;originWidth=464&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>现在，为了选择正确数量的簇，我们进行目视检查。损耗曲线开始弯曲的<strong>点</strong>称为 <strong>肘点</strong>。肘点代表误差和聚类数量之间的合理权衡。在此示例中，肘点位于<strong>x = 3 处</strong>。这意味着最佳聚类数为 3。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_optimal_num_clusters</span>(<span class="hljs-params">self, data, max_K=<span class="hljs-number">15</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Plots loss values for different number of clusters in K-Means</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            image: input image of shape(H, W, 3)</span><br><span class="hljs-string">            max_K: number of clusters</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            None (plot loss values against number of clusters)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        y_val = np.empty(max_K)<br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_K):<br>            cluster_idx, centers, y_val[i] = KMeans()(data, i + <span class="hljs-number">1</span>)<br>            <br>        plt.plot(np.arange(max_K) + <span class="hljs-number">1</span>, y_val)<br>        plt.show()<br>        <span class="hljs-keyword">return</span> y_val<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <br><span class="hljs-keyword">import</span> KMeans<br><br>loss = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>):    <br>  kmeans = KMeans(n_clusters=i, max_iter=<span class="hljs-number">100</span>).fit(p_list)    <br>  loss.append(kmeans.inertia_ / point_number / K)    <br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), loss)plt.show()<br></code></pre></td></tr></table></figure><h4 id="2-1-2-k-means"><a href="#2-1-2-k-means" class="headerlink" title="2.1.2 k-means++"></a>2.1.2 k-means++</h4><p><code>k-means++</code>是针对<code>k-means</code>中初始质心点选取的优化算法。该算法的流程和<code>k-means</code>类似，改变的地方只有初始质心的选取，该部分的算法流程如下</p><p>算法步骤：</p><ul><li>从 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f81a08fd463c530d8b3e5b5790d35eb9.svg#id=lOfqo&amp;originHeight=18&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 中随机选择1个样本作为初始均值向量组 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/3070ea35fe1159b870f6aa1cc01738bf.svg#id=XdzXW&amp;originHeight=24&amp;originWidth=52&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。 </li><li>迭代，直到初始均值向量组有 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/b0f80a77b6404455ddb6f55b039e4105.svg#id=bMKqw&amp;originHeight=17&amp;originWidth=19&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 个向量。<br>假设初始均值向量组为 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/00ee1bc819cd700380a69cc21d1b9566.svg#id=b6XAW&amp;originHeight=24&amp;originWidth=117&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">。迭代过程如下： <ul><li>对每个样本 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=qRVeN&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> ，分别计算其距 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/d64c74ddfe138ec1221781b174741959.svg#id=KlA3E&amp;originHeight=24&amp;originWidth=96&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的距离。这些距离的最小值记做 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/2082e1e10ad0ab302859cf3ad88440fe.svg#id=GCJck&amp;originHeight=28&amp;originWidth=191&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。</li><li>对样本 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=Kdvcu&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，其设置为初始均值向量的概率正比于 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/488b641047a05746274968f01d03e311.svg#id=JWfWQ&amp;originHeight=21&amp;originWidth=18&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。即：离所有的初始均值向量越远，则越可能被选中为下一个初始均值向量。</li><li>以概率分布 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/0ce1d8fcf48d73cefc70b2e4ee02f9e0.svg#id=Ew7s7&amp;originHeight=23&amp;originWidth=187&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> （未归一化的）随机挑选一个样本作为下一个初始均值向量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f1d3aeb4edffdd3ab8d6117b3e73e8e9.svg#id=sEUWM&amp;originHeight=25&amp;originWidth=47&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。</li></ul></li><li>一旦挑选出初始均值向量组 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/630550619a4d10bc1e247e086c763726.svg#id=SQDZc&amp;originHeight=24&amp;originWidth=117&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，剩下的迭代步骤与<code>k-means</code> 相同。 </li></ul><h4 id="2-1-3-LVQ"><a href="#2-1-3-LVQ" class="headerlink" title="2.1.3 LVQ"></a>2.1.3 LVQ</h4><h5 id="1-算法介绍-1"><a href="#1-算法介绍-1" class="headerlink" title="(1) 算法介绍"></a>(1) 算法介绍</h5><p>Learning vector Quantization (LVQ) 假设数据样本带有<strong>类别标记</strong>，学习过程利用样本的这些监督信息来辅助<strong>聚类</strong>。使用通过LVQ得到的原型向量来代表整个簇的过程，称为“向量量化”（Vector Quantization），这种数据压缩方法属于“有损压缩”（Lossy Compression）。</p><p>LVQ的目标是学得一组原型向量 ${p_1,p_2,…,p_q}$，每个原型向量代表一个聚类簇。LVQ在训练过程中通过对神经元权向量（原型向量）的不断更新，对其学习率的不断调整，能够使不同类别权向量之间的边界逐步收敛至贝叶斯分类边界。算法中，对获胜神经元（最近邻权向量）的选取是通过计算输入样本和权向量之间的距离的大小来判断的。</p><p>具体算法流程图如下所示：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gubps12a4sj60m80f476g02.jpg#id=uDg4i&amp;originHeight=544&amp;originWidth=800&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p><strong>算法解释</strong></p><ul><li>算法第1行：对原型向量进行初始化。例如：对第$i, i=(1,2,…,q)$ 个簇,从类别标记为 $t_i$ 的样本中随机选取一个作为原型向量。如果类别数小于聚类数，即 k&gt;m ，则重新从第一个类别中继续选取； </li><li>算法第2-12行：对原型向量进行迭代优化，直到算法收敛。在每一轮迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行相应的更新。 </li><li>算法第2-5行：从样本集中随机选取一个样本 $x_j$，计算该样本与每个原型向量 $p_i$ 之间的欧式距离，并找到与该样本距离最近的原型向量 $p_{i^<em>}$ 的类别标记  $t_{i^</em>}$ </li><li>第6-10行：如何更新原型向量。迭代过程如下： <ul><li>从样本集 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f81a08fd463c530d8b3e5b5790d35eb9.svg#id=LdPh4&amp;originHeight=18&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 中随机选取样本 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/54ef450213ab84e5905678533d3cf49b.svg#id=fao4P&amp;originHeight=23&amp;originWidth=63&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> ，挑选出距离 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/54ef450213ab84e5905678533d3cf49b.svg#id=y6L8W&amp;originHeight=23&amp;originWidth=63&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 最近的原型向量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=PJ0mY&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">：<img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/3060a084edb504f78286ccd0f43f02b8.svg#id=cYaxY&amp;originHeight=26&amp;originWidth=215&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。</li><li>如果 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=kOmtN&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的类别等于 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/8192faf97d009a1cd2dcc35d384beabf.svg#id=qCLiS&amp;originHeight=17&amp;originWidth=17&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，则：<img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/3f347a03a31666489c2323ab8ad40e48.svg#id=xOeei&amp;originHeight=26&amp;originWidth=218&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> ，将该原型向量更靠近 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=cvjcQ&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></li><li>如果 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=vmDHw&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的类别不等于 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/8192faf97d009a1cd2dcc35d384beabf.svg#id=vOAZ5&amp;originHeight=17&amp;originWidth=17&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，则：<img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/e442a73182d43c1387f88da9af08352b.svg#id=B2jKQ&amp;originHeight=26&amp;originWidth=218&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> ，将该原型向量更远离 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=EoyRy&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></li></ul></li></ul><p>在原型向量的更新过程中： </p><ul><li><p>如果 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=G2QOi&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的类别等于 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/8192faf97d009a1cd2dcc35d384beabf.svg#id=R8xHI&amp;originHeight=17&amp;originWidth=17&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，则更新后， <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=gbstd&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 与 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=N7nbD&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 距离为：<br><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/0e16fe56396a2f7dce687a85c073618a.svg#id=chvTC&amp;originHeight=26&amp;originWidth=573&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </p><p>即，更新后的原型向量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=dqwqV&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 距离 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=hELxB&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 更近。 </p></li><li><p>如果 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=qU7pU&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的类别不等于 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/8192faf97d009a1cd2dcc35d384beabf.svg#id=r2iqH&amp;originHeight=17&amp;originWidth=17&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，则更新后， <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=NbpFb&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 与 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=iY0Y9&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 距离为：<br><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/d55aaebfebeeca907473c51e3f925c23.svg#id=gYViR&amp;originHeight=26&amp;originWidth=573&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><br>即，更新后的原型向量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f04fcfee63d019e5f5307fb91f61fc50.svg#id=RLaV3&amp;originHeight=26&amp;originWidth=29&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 距离 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=XQuxD&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 更远。 </p><ul><li>算法第12行：若算法的<strong>停止条件</strong>已满足(例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新)，则将当前原型向量作为最终结果返回。 </li><li>在学得一组原型向量 ${p_1,p_2,…,p_q}$ 后即可实现对样本空间 $X$ 的簇划分. 对任意样本 $x$, 他将被划入与其距离最近的原型向量所代表的簇中 </li></ul></li></ul><p><strong>可能涉及到的问题：</strong></p><blockquote><p>首先，既然已经有标签数据，那么为什么还要进行聚类呢？</p></blockquote><p>我们输入的先验标签数据不一定是绝对真实的取值，可能是其他分类器输出的结果，可能是片面的、残缺的，需要靠聚类进行矫正和修补。（可以将先验知识当做一个约束值，该算法相当于在已知的约束条件下求解最最优值的过程）</p><p>对于矫正作用，由于输入的只有标签值，需要靠聚类进行类别边界的拟合；对于修补作用，由于LVQ聚类算法允许聚类的簇比输入的标签的类别多，这意味着一个类别可以被重新分割成多个类别。 </p><blockquote><p>其次，标签值是必须的吗？</p></blockquote><p>一种说法是标签值是非必须的，如果没有先验的标签值，可以输入随机的标签值。我不太认同这种做法。因为标签值是一种约束，随机的约束即相当于没有约束，那么LVQ算法其实就退化为一般的聚类的算法，甚至更加严重，随机会使得系统更加混沌，算法会因为错误的指导给出效果更差的答案。所以我更偏向于将其归为<strong>半监督</strong>的聚类算法。 </p><blockquote><p>其他注意事项</p></blockquote><ul><li>样本异常点对聚类有影响，一般我们需要提前剔除掉 </li><li>难以应用在高维特征空间 </li><li>可以多选择几个簇心，来降低压缩率，提高保持较高的精度？ </li><li>为了得到更精确的代表点需要调整迭代次数和学习率，时间复杂度比较 </li></ul><h5 id="2-LVQ算法实现"><a href="#2-LVQ算法实现" class="headerlink" title="(2) LVQ算法实现"></a>(2) LVQ算法实现</h5><blockquote><p><strong>《</strong><a href="https://www.cnblogs.com/lunge-blog/p/11666563.html"><strong>手写LVQ（学习向量量化）聚类算法</strong></a><strong>》</strong><br>《<a href="https://github.com/zhoupengfeigjqh/ml/blob/master/lvq/lvq_test/lvq_test.py">ml/lvq/lvq_test/lvq_test.py</a>》√<br>《<a href="https://github.com/itdxer/neupy/blob/master/neupy/algorithms/competitive/lvq.py">neupy/neupy/algorithms/competitive/lvq.py</a> 》√<br>《<a href="https://blog.csdn.net/qq_40793975/article/details/82177189">Learning Vector Quantization详解</a>》</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lvq</span>(<span class="hljs-params">data: np, k_num: <span class="hljs-built_in">int</span>, labels: <span class="hljs-built_in">list</span>, lr=<span class="hljs-number">0.01</span>, max_iter=<span class="hljs-number">15000</span>, delta=<span class="hljs-number">1e-3</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    :param data: 样本集, 最后一列feature表示原始数据的label</span><br><span class="hljs-string">    :param k_num: 簇数，原型向量个数</span><br><span class="hljs-string">    :param labels: 1-dimension list or array,label of the data（去重）</span><br><span class="hljs-string">    :param max_iter: 最大迭代数</span><br><span class="hljs-string">    :param lr: 学习效率</span><br><span class="hljs-string">    :param delta: max distance for two vectors to be &#x27;equal&#x27;.</span><br><span class="hljs-string">    :return: 返回向量中心点、簇标记</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 随机初始化K个原型向量</span><br>    v = rand_initial_center(data, k_num, labels)<br><br>    <span class="hljs-comment"># 确认是否所有中心向量均已更新</span><br>    all_vectors_updated = np.zeros(shape=(k_num,), dtype=np.<span class="hljs-built_in">bool</span>)<br>    <span class="hljs-comment"># 记录各个中心向量的更新次数</span><br>    v_update_cnt = np.zeros(k_num, dtype=np.float32)<br><br>    j = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        j = j + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> j%<span class="hljs-number">100</span>==<span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;iter:&quot;</span>, j)<br><br>        <span class="hljs-comment"># 迭代停止条件：已到达最大迭代次数，或者原型向量全都更新过</span><br>        <span class="hljs-keyword">if</span> j &gt;= max_iter <span class="hljs-keyword">or</span> all_vectors_updated.<span class="hljs-built_in">all</span>():<br>            <span class="hljs-keyword">break</span><br>        <span class="hljs-comment"># # 迭代停止条件：超过阈值且每个中心向量都更新超过5次则退出</span><br>        <span class="hljs-comment"># if j &gt;= max_iter and sum(v_update_cnt &gt; 5) == k_num:</span><br>        <span class="hljs-comment">#     break</span><br><br>        <span class="hljs-comment"># 随机选择一个样本, 并计算与当前各个簇中心点的距离, 取距离最小的</span><br>        sel_sample = random.choice(data)<br>        min_dist = distance(sel_sample, v[<span class="hljs-number">0</span>])<br>        sel_k = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> ii <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, k_num):<br>            dist = distance(sel_sample, v[ii])<br>            <span class="hljs-keyword">if</span> min_dist &gt; dist:<br>                min_dist = dist<br>                sel_k = ii<br><br>        <span class="hljs-comment"># 保存更新前向量</span><br>        temp_v = v[sel_k].copy()<br><br>        <span class="hljs-comment"># 更新v：如果标签相同，则q更新后接近样本x，否则远离</span><br>        <span class="hljs-keyword">if</span> sel_sample[-<span class="hljs-number">1</span>] == v[sel_k][-<span class="hljs-number">1</span>]:<br>            v[sel_k][<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>] = v[sel_k][<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>] + lr * (sel_sample[<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>] - v[sel_k][<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">else</span>:<br>            v[sel_k][<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>] = v[sel_k][<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>] - lr * (sel_sample[<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>] - v[sel_k][<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>])<br><br>        <span class="hljs-comment"># 更新记录数组（更新后簇心基本不变，就认为更新好了）</span><br>        <span class="hljs-keyword">if</span> distance(temp_v, v[sel_k]) &lt; delta:<br>            all_vectors_updated[sel_k] = <span class="hljs-literal">True</span><br>        <span class="hljs-comment"># v的更新次数+1</span><br>        v_update_cnt[sel_k] = v_update_cnt[sel_k] + <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># 更新完毕后, 把各个样本点进行标记, 记录放在categories变量里</span><br>    m, n = np.shape(data)<br>    cluster_assment = np.mat(np.zeros((m, <span class="hljs-number">2</span>)), dtype=np.float32)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        min_distji = np.inf<br>        min_distji_index = -<span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k_num):<br>            distji = distance(data[i, :], v[j, :])<br>            <span class="hljs-comment"># print(distji)</span><br>            <span class="hljs-keyword">if</span> min_distji &gt; distji:<br>                min_distji = distji<br>                min_distji_index = j<br>        cluster_assment[i, <span class="hljs-number">0</span>] = min_distji_index<br>        cluster_assment[i, <span class="hljs-number">1</span>] = min_distji<br><br>    <span class="hljs-keyword">return</span> v, cluster_assment<br></code></pre></td></tr></table></figure><h5 id="3-LVQ2算法改进"><a href="#3-LVQ2算法改进" class="headerlink" title="(3) LVQ2算法改进"></a>(3) LVQ2算法改进</h5><p>LVQ算法中，对于某个输入向量X，算法只对与其具有最小Euclidean距离的权向量$w_k$ 进行调整。</p><p>在LVQ2[2]中，算法还要考虑与X具有次近Euclidean距离的权向量$w_r$，X与$w_k$及$w_r$间的距离分别记为 $d_k$ 与 $d_r$。如果下述3个条件都满足的话，算法就对$w_k$及$w_r$同时进行调整，否则就按照原先的LVQ算法调整权向量$w_k$：</p><ol><li>$w_k$及$w_r$代表不同的分类。</li><li>次近权向量$w_r$与X代表同一个分类。</li><li>$d_k$ 与 $d_r$大致相等。一般来说．如果 $d_k$ 与 $d_r$能够满足$d_k/d_r&gt;(1一￡)$且 $d_r/d_k&lt;(1+￡)$，我们就认为 $d_k$ 与 $d_r$是大致相等的，其中￡的取值依赖于训练例的多少。</li></ol><p>在上述3个条件都满足的情况下，按下述公式调整$w_k$及$w_r$，使得$w_k$远离输入向量X，而$w_r$向输入向量的方向靠近：</p><ol><li>$W_k(new)=W_k(old)一\alpha(X—W_k(old))$</li><li>$W_r(new)=W_r(old)+\alpha(X—W_r(old))$</li></ol><p>LVQ2算法通过同时考察两个权值向量$w_k$及$w_r$，可以加快算法的收敛速度，使得各个权向量快速的向目标位置移动。</p><h5 id="4-LVQ2-1算法改进"><a href="#4-LVQ2-1算法改进" class="headerlink" title="(4) LVQ2.1算法改进"></a>(4) LVQ2.1算法改进</h5><p>LVQ2.1在LVQ2的基础上做了一些改进．LVQ2.1也是同时考察与某个输入向量X最近的两个权向量 $w_{c1},w_{c_2}$，但并不关心$w_{c1},w_{c_2}$哪一个离X更近(对应的距离分别为$d_{c1},d_{c_2}$)。当下面两个条件同时满足时将对$w_{c1},w_{c_2}$进行调整：</p><ol><li>$w_{c1},w_{c_2}$中，有一个权向量代表的分类和X所表示的一致，而另一个不一致。</li><li>$max[d_{c_1}／d_{c_2},d_{c_2}／d_{c_1}]&lt;(1+￡)$且$min[d_{c_1}／d_{c_2},d_{c_2}／d_{c_1}]&gt;(1-￡)$</li></ol><p>如果上述条件满足，不妨设 $w_{c1}$与X代表的类别相同，算法将调整权向量$w_{c1},w_{c_2}$，使得 $w_{c1}$输入向量X的方向靠近，而 $w_{c_2}$远离输入向量．调整公式为：</p><ol><li>$W_{c_1}(new)=W_{c_1}(old)+\alpha(X—W_{c_1}(old))$</li><li>$W_{c_2}(new)=W_{c_2}(old)-\alpha(X—W_{c_2}(old))$</li></ol><h5 id="5-LVQ3算法改进"><a href="#5-LVQ3算法改进" class="headerlink" title="(5) LVQ3算法改进"></a>(5) LVQ3算法改进</h5><p>LVQ3也是同时考虑与输入向量X距离最近的两个权向量 $w_{c1},w_{c_2}$。当条件 $min[d_{c_1}／d_{c_2},d_{c_2}／d_{c_1}]&gt;(1-￡)(1+￡)$ 满足时 (￡的一个典型取值为0.2). 权向量将按照下述规则进行调整：</p><ul><li>如果 $w_{c1},w_{c_2}$两个权向量中有一个对应的分类与输入向量X一致，另一个不一致，则权向量的调整规则同 LVQ2.1</li><li>如果 $w_{c1},w_{c_2}$代表相同的分类，则权向量 $w_{c1},w_{c_2}$均采用公式 $W_{c}(new)=W_{c}(old)+\beta(X—W_{c}(old))$ 进行调整，其中$\beta=m\alpha, 0.1&lt;m&lt;0.5$</li></ul><p>LVQ3算法通过对标准LVQ算法学习过程的修改，可以使得在网络学习过程不断进行的同时，网络的权向量能够很好地反映输入空间的概率密度分布，并且防止权向量偏离最优的位置。</p><h5 id="6-G-LVQ"><a href="#6-G-LVQ" class="headerlink" title="(6) G-LVQ"></a>(6) G-LVQ</h5><p>G-LVQ将遗传算法应用于LVQ算法中，以克服LVQ算法本身的一些缺陷。</p><h4 id="2-1-4-高斯混合聚类-Mixture-of-Gaussian"><a href="#2-1-4-高斯混合聚类-Mixture-of-Gaussian" class="headerlink" title="2.1.4 高斯混合聚类(Mixture-of-Gaussian)"></a><a href="https://chengfeng96.com/blog/2019/05/17/%E8%81%9A%E7%B1%BB%E7%AC%94%E8%AE%B0/">2.1.4 高斯混合聚类(Mixture-of-Gaussian)</a></h4><p>高斯混合聚类(Mixture-of-Gaussian)采用概率模型来表达聚类原型.</p><ol><li>对于 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/0ae1a85663b488d8750034f314fead96.svg#id=JDBGA&amp;originHeight=13&amp;originWidth=13&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 维样本空间 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/4699152c8b2f628460172ab27174e1a5.svg#id=CWuXn&amp;originHeight=17&amp;originWidth=17&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 中的随机向量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/615142e29a0abd8b3367a4c758a49449.svg#id=iaNSt&amp;originHeight=18&amp;originWidth=13&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> ，若 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/615142e29a0abd8b3367a4c758a49449.svg#id=GYe95&amp;originHeight=18&amp;originWidth=13&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 服从高斯分布，则其概率密度函数为 ：<br><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/e6e6add27f1c70cea58a6996c58f9290.svg#id=WA0JY&amp;originHeight=59&amp;originWidth=525&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><br>其中 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/7236b04c3ae9e2fe1f55ac3bc9238d52.svg#id=BjlSD&amp;originHeight=25&amp;originWidth=193&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 为 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/0ae1a85663b488d8750034f314fead96.svg#id=FPC17&amp;originHeight=13&amp;originWidth=13&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 维均值向量， <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/27938747f32cbc4fe6955e67d8d45c38.svg#id=bgldE&amp;originHeight=17&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 是 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/3415f011eff324713afe399d1786fb13.svg#id=tkmVK&amp;originHeight=14&amp;originWidth=51&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的协方差矩阵。 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/615142e29a0abd8b3367a4c758a49449.svg#id=Nyny1&amp;originHeight=18&amp;originWidth=13&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的概率密度函数由参数 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/2f60157b69677e2409d7b75a4e7e93b8.svg#id=ikeYQ&amp;originHeight=23&amp;originWidth=37&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 决定。 </li><li>定义高斯混合分布： <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/c8de5f756081e9ff0cd9cd9b760c978a.svg#id=eL6Te&amp;originHeight=30&amp;originWidth=250&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。该分布由 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/b0f80a77b6404455ddb6f55b039e4105.svg#id=C7Zx8&amp;originHeight=17&amp;originWidth=19&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 个混合成分组成，每个混合成分对应一个高斯分布。其中: <ul><li><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/1cc9ee66ad19a06d6af2be1a9c20ec5c.svg#id=S9gMP&amp;originHeight=24&amp;originWidth=57&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 是第 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/92e437054d1e57ccf66e719e1ed2d224.svg#id=IoDZN&amp;originHeight=18&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 个高斯混合成分的参数。</li><li><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/0a569ee146c1c8961d05b2ab03ecc569.svg#id=tLcy8&amp;originHeight=21&amp;originWidth=61&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 是相应的混合系数，满足 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/da80875b7807995a5f59706235f3e093.svg#id=XCuoK&amp;originHeight=30&amp;originWidth=116&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。</li></ul></li><li>假设训练集 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/8c33c886b45490870d3ebe3746363b97.svg#id=x0P9j&amp;originHeight=23&amp;originWidth=192&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的生成过程是由高斯混合分布给出。<br>令随机变量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/6226f31bf6ddd7c4d7cc120a488f2f75.svg#id=zC8ZU&amp;originHeight=23&amp;originWidth=157&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 表示生成样本 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/615142e29a0abd8b3367a4c758a49449.svg#id=PrNRv&amp;originHeight=18&amp;originWidth=13&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的高斯混合成分序号， <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/fe69bad71d951e0818034cfb04f7570c.svg#id=ChYUZ&amp;originHeight=17&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的先验概率 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/e004a8f6dab99d296b91112533c9519f.svg#id=scUcD&amp;originHeight=23&amp;originWidth=137&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">。<br>生成样本的过程分为两步： <ul><li>首先根据概率分布 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/25dae5efcc84fef1d42af4729d28ab0f.svg#id=kt0V1&amp;originHeight=16&amp;originWidth=130&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 生成随机变量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/fe69bad71d951e0818034cfb04f7570c.svg#id=gbSYH&amp;originHeight=17&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。</li><li>再根据 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/fe69bad71d951e0818034cfb04f7570c.svg#id=c5kDH&amp;originHeight=17&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的结果，比如 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/6e3f9b310fc245216d1dd935ccec5553.svg#id=gkHqZ&amp;originHeight=18&amp;originWidth=54&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">， 根据概率 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f8e74c1578fb89763019f33b5ce97679.svg#id=vTTew&amp;originHeight=24&amp;originWidth=114&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 生成样本。</li></ul></li><li>根据贝叶斯定理， 若已知输出为 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=IkMqU&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，则 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/fe69bad71d951e0818034cfb04f7570c.svg#id=Fiwwv&amp;originHeight=17&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的后验分布为：<br><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/018a9976dedfc4fd92b71ea3ea81c577.svg#id=u1YAw&amp;originHeight=60&amp;originWidth=618&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><br>其物理意义为：所有导致输出为 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=NX9ak&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 的情况中， <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/6e3f9b310fc245216d1dd935ccec5553.svg#id=kl8so&amp;originHeight=18&amp;originWidth=54&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 发生的概率。 </li><li>当高斯混合分布已知时，高斯混合聚类将样本集 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/f81a08fd463c530d8b3e5b5790d35eb9.svg#id=iaiPb&amp;originHeight=18&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 划分成 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/b0f80a77b6404455ddb6f55b039e4105.svg#id=f6GJJ&amp;originHeight=17&amp;originWidth=19&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 个簇 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/9e0add8aaa3d30f78ba0d9faedcee18e.svg#id=ljIeH&amp;originHeight=23&amp;originWidth=195&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 。<br>对于每个样本 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=TeJXS&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> ，给出它的簇标记 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/4dc225dca196ee234da99867bd02f962.svg#id=UxDQN&amp;originHeight=21&amp;originWidth=19&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 为：<br><img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/dbf731e9b54a1ca6fc5990f2c15926b3.svg#id=qlz8U&amp;originHeight=35&amp;originWidth=260&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><br>即：如果 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/374d548f18d9f8ceb9f6b6b779a425ee.svg#id=GXHtw&amp;originHeight=22&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 最有可能是 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/6e3f9b310fc245216d1dd935ccec5553.svg#id=w37my&amp;originHeight=18&amp;originWidth=54&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> 产生的，则将该样本划归到簇 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/3a840486dde29f61a0ec8bccfa2c09ff.svg#id=HzXFB&amp;originHeight=21&amp;originWidth=25&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">。<br>这就是通过最大后验概率确定样本所属的聚类。 </li><li>现在的问题是，如何学习高斯混合分布的参数。由于涉及到隐变量 <img src="https://static.sitestack.cn/projects/huaxiaozhuan-ai/fe69bad71d951e0818034cfb04f7570c.svg#id=CC1Xo&amp;originHeight=17&amp;originWidth=15&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> ，可以采用<code>EM</code>算法求解。<br>具体求解参考<code>EM</code> 算法的章节部分。 </li></ol><h3 id="2-2-基于密度的聚类"><a href="#2-2-基于密度的聚类" class="headerlink" title="2.2 基于密度的聚类"></a>2.2 基于密度的聚类</h3><p><code>k-means</code>算法对于凸性数据具有良好的效果，能够根据距离来讲数据分为球状类的簇，但对于非凸形状的数据点，就无能为力了，当<code>k-means</code>算法在环形数据的聚类时，我们看看会发生什么情况。</p><p><img src="https://pic4.zhimg.com/80/v2-014e1a2055df95e067341cba1809a9af_1440w.jpg#id=ZVgLT&amp;originHeight=576&amp;originWidth=1440&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>从上图可以看到，<code>kmeans</code>聚类产生了错误的结果，这个时候就需要用到基于密度的聚类方法了。基于密度的聚类(Density-based Clustering)假设聚类结构能通过样本分布的紧密程度确定.密度聚类算法从样本密度的角度来考察样本之间的可连续性，并基于可连续样本不断扩展聚类簇以获得最终的聚类结果.</p><p>基于密度的聚类(Density-based Clustering)</p><ul><li><a href="https://en.wikipedia.org/wiki/DBSCAN"><strong>DBSCAN</strong></a> (Density-Based Spatial Clustering of Application with Noise)</li><li><a href="https://en.wikipedia.org/wiki/OPTICS_algorithm"><strong>OPTICS</strong></a> (Ordering Points To Identify the Clustering Structure)</li></ul><h4 id="2-2-1-DBSCAN"><a href="#2-2-1-DBSCAN" class="headerlink" title="2.2.1 DBSCAN"></a>2.2.1 DBSCAN</h4><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的空间聚类算法,它不需要定义簇的个数,而是将具有足够高密度的区域划分为簇,并在有噪声的数据中发现任意形状的簇,在此算法中将簇定义为密度相连的点的最大集合。</p><p>首先介绍几个概念，考虑集合<img src="https://www.zhihu.com/equation?tex=X%3D%5Cleft+%5C%7Bx%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...%2Cx%5E%7B%28n%29%7D%5Cright+%5C%7D#id=WvKOb&amp;originHeight=39&amp;originWidth=204&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，<img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon#id=cSBrZ&amp;originHeight=13&amp;originWidth=9&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">表示定义密度的邻域半径，设聚类的邻域密度阈值为<img src="https://www.zhihu.com/equation?tex=M#id=KESNV&amp;originHeight=17&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，有以下定义：</p><ul><li><img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon#id=tQJVL&amp;originHeight=13&amp;originWidth=9&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><strong>邻域(</strong><img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon#id=kjQhZ&amp;originHeight=13&amp;originWidth=9&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><strong>-neighborhood）</strong></li></ul><p><img src="https://www.zhihu.com/equation?tex=N_%7B%5Cvarepsilon+%7D%28x%29%3D%5Cleft+%5C%7By%5Cin++X%7Cd%28x%2C+y%29+%3C+%5Cvarepsilon+%5Cright+%5C%7D+%5C%5C#id=zSPxt&amp;originHeight=40&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><ul><li><strong>密度(desity)</strong><img src="https://www.zhihu.com/equation?tex=x#id=y738D&amp;originHeight=13&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">的密度为</li></ul><p><img src="https://www.zhihu.com/equation?tex=%5Crho+%28x%29%3D%5Cleft+%7C+N_%7B%5Cvarepsilon+%7D%28x%29%5Cright+%7C+%5C%5C#id=i6wfz&amp;originHeight=40&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><ul><li><strong>核心点(core-point)</strong><br>设<img src="https://www.zhihu.com/equation?tex=x%5Cin++X#id=qDZHW&amp;originHeight=17&amp;originWidth=49&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，若<img src="https://www.zhihu.com/equation?tex=%5Crho+%28x%29+%5Cgeq+M#id=kZEUo&amp;originHeight=23&amp;originWidth=79&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，则称<img src="https://www.zhihu.com/equation?tex=x#id=q1PI2&amp;originHeight=13&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">为<img src="https://www.zhihu.com/equation?tex=X#id=yArJ9&amp;originHeight=17&amp;originWidth=16&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">的核心点，记<img src="https://www.zhihu.com/equation?tex=X#id=EBfiT&amp;originHeight=17&amp;originWidth=16&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">中所有核心点构成的集合为<img src="https://www.zhihu.com/equation?tex=X_c#id=RgIvj&amp;originHeight=20&amp;originWidth=23&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，记所有非核心点构成的集合为<img src="https://www.zhihu.com/equation?tex=X_%7Bnc%7D#id=xFShV&amp;originHeight=20&amp;originWidth=31&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">。 </li><li><strong>边界点(border-point)</strong><br>若<img src="https://www.zhihu.com/equation?tex=x%5Cin++X_%7Bnc%7D#id=qI4A7&amp;originHeight=20&amp;originWidth=64&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，且<img src="https://www.zhihu.com/equation?tex=%5Cexists+y%5Cin++X#id=Ljp87&amp;originHeight=20&amp;originWidth=58&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，满足<br><img src="https://www.zhihu.com/equation?tex=y%5Cin++N_%7B%5Cvarepsilon+%7D%28x%29+%5Ccap+X_c+%5C%5C#id=FELj9&amp;originHeight=40&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><br>即<img src="https://www.zhihu.com/equation?tex=x#id=C3LHo&amp;originHeight=13&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">的<img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon#id=tUAlp&amp;originHeight=13&amp;originWidth=9&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">邻域中存在核心点，则称<img src="https://www.zhihu.com/equation?tex=x#id=JBGnC&amp;originHeight=13&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">为<img src="https://www.zhihu.com/equation?tex=X#id=oatxR&amp;originHeight=17&amp;originWidth=16&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">的边界点，记<img src="https://www.zhihu.com/equation?tex=X#id=OGj7O&amp;originHeight=17&amp;originWidth=16&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">中所有的边界点构成的集合为<img src="https://www.zhihu.com/equation?tex=X_%7Bbd%7D#id=Psyte&amp;originHeight=20&amp;originWidth=30&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">。<br>此外，边界点也可以这么定义：若<img src="https://www.zhihu.com/equation?tex=x%5Cin++X_%7Bnc%7D#id=J5RaA&amp;originHeight=20&amp;originWidth=64&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，且<img src="https://www.zhihu.com/equation?tex=x#id=sLlm4&amp;originHeight=13&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">落在某个核心点的<img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon#id=YlxYk&amp;originHeight=13&amp;originWidth=9&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">邻域内，则称<img src="https://www.zhihu.com/equation?tex=x#id=tMsk4&amp;originHeight=13&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">为<img src="https://www.zhihu.com/equation?tex=X#id=Sp9mo&amp;originHeight=17&amp;originWidth=16&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">的一个边界点，一个边界点可能同时落入一个或多个核心点的<img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon#id=B7oOd&amp;originHeight=13&amp;originWidth=9&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">邻域。 </li><li><strong>噪声点(noise-point)</strong><br>若<img src="https://www.zhihu.com/equation?tex=x#id=ME8Pu&amp;originHeight=13&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">满足<br><img src="https://www.zhihu.com/equation?tex=x%5Cin++X%2Cx+%5Cnotin+X_%7Bc%7D%E4%B8%94+x%5Cnotin+X_%7Bbd%7D+%5C%5C#id=M9jlO&amp;originHeight=41&amp;originWidth=600&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""><br>则称<img src="https://www.zhihu.com/equation?tex=x#id=g0C0n&amp;originHeight=13&amp;originWidth=11&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">为噪声点。<br>如下图所示，设<img src="https://www.zhihu.com/equation?tex=M%3D3#id=U2niB&amp;originHeight=17&amp;originWidth=54&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">，则A为核心点，B、C是边界点，而N是噪声点。 </li></ul><p><img src="https://pic1.zhimg.com/80/v2-7efff68d8eada2472ed0a0372fa0b914_1440w.jpg#id=OX7JL&amp;originHeight=330&amp;originWidth=536&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p>该算法的流程如下：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1guivub1phij60jc0mm0v802.jpg#id=YetGL&amp;originHeight=814&amp;originWidth=696&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p><strong>算法说明:</strong></p><ul><li>DBSCAN算法先任选数据集中的一个核心对象为 “种子 (seed)”,再由此出发确定相应的聚类簇；</li><li>第1-7行：先根据给定的邻域簇 $(c,MinPts)$ 找出所有核心对象；</li><li>第10-24行：以任一核心对象为出发点，找出由其密度可达的样本生成聚类簇，直到所有核心对象均被访问过为止.</li></ul><blockquote><p>一般来说，<code>DBSCAN</code>算法有以下几个特点：</p><ol><li>需要提前确定<img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon+#id=oe60x&amp;originHeight=13&amp;originWidth=9&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">和<img src="https://www.zhihu.com/equation?tex=M#id=SzpN5&amp;originHeight=17&amp;originWidth=20&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">值</li><li><strong>不需要提前设置聚类的个数</strong></li><li>对初值选取敏感，对噪声不敏感</li><li>对密度不均的数据聚合效果不好</li></ol></blockquote><h4 id="2-2-2-OPTICS"><a href="#2-2-2-OPTICS" class="headerlink" title="2.2.2 OPTICS"></a>2.2.2 OPTICS</h4><p>OPTICS (Ordering Points To Identify the Clustering Structure). OPTICS和DBSCAN聚类相同,都是基于密度的聚类,但是,OPTICS的好处在于可以处理不同密度的类,结果有点像基于连通性的聚类,不过还是有些区别的.</p><h3 id="2-3-层次聚类方法"><a href="#2-3-层次聚类方法" class="headerlink" title="2.3 层次聚类方法"></a>2.3 <strong>层次聚类方法</strong></h3><blockquote><p>前面介绍的几种算法确实可以在较小的复杂度内获取较好的结果，但是这几种算法却存在一个<code>链式效应</code>的现象，比如：A与B相似，B与C相似，那么在聚类的时候便会将A、B、C聚合到一起，但是如果A与C不相似，就会造成聚类误差，严重的时候这个误差可以一直传递下去。为了降低<code>链式效应</code>，这时候层次聚类就该发挥作用了。</p></blockquote><p>层次聚类(Hierarchical Clustering) 也称为基于连通性的聚类。这种算法试图在不同层次对数据进行划分，从而形成树形的聚类结构。</p><p>数据集的划分采用不同的策略会生成不同的层次聚类算法：</p><ul><li>“自底向上”的聚合策略<br>（<strong>Agglomerative</strong>）。每一个对象最开始都是一个 <code>cluster</code>，每次按一定的准则将最相近的两个 <code>cluster</code> 合并生成一个新的 <code>cluster</code>，如此往复，直至最终所有的对象都属于一个 <code>cluster</code>。这里主要关注此类算法。 <ul><li>AGNES(Agglomerative Nesting)</li></ul></li><li>“自顶向下”的分拆策略<br>（<strong>Divisive</strong>）。最开始所有的对象均属于一个 <code>cluster</code>，每次按一定的准则将某个 <code>cluster</code> 划分为多个 <code>cluster</code>，如此往复，直至每个对象均是一个 <code>cluster</code>。 </li></ul><p><img src="https://pic2.zhimg.com/80/v2-be2a7cf798fdc983e6521a68b1eb952d_1440w.jpg#id=eXMcp&amp;originHeight=1002&amp;originWidth=1440&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h4 id="2-3-1-AGNES"><a href="#2-3-1-AGNES" class="headerlink" title="2.3.1 AGNES"></a>2.3.1 AGNES</h4><p><strong>(1) 算法介绍</strong></p><p>AGNES(Agglomerative Nesting)是一种采用自底向上聚合策略的层次聚类算法，算法的步骤为：</p><ol><li>先将数据集中的每个样本当做是一个初始聚类簇;</li><li>然后在算法运行的每一步中找出距离最近的两个点(聚类簇)进行合并为一个聚类簇;</li><li>上述过程不断重复，直至所有的样本点合并为一个聚类簇或达到预设的聚类簇个数。 最终算法会产生一棵树，称为树状图(dendrogram), 树状图展示了数据点是如何合并的.</li></ol><p>这个算法的关键是如何计算两点之间以及两个聚类簇之间的距离</p><ol><li>如何计算两点之间的距离<strong>[距离矩阵(Metric)]</strong>：<br><img src="https://tva1.sinaimg.cn/large/008i3skNly1guqkyapbvgj616a0bq0ua02.jpg#id=eZQ7M&amp;originHeight=422&amp;originWidth=1522&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""> </li><li>如何计算两个聚类簇(集合)之间的距离<strong>[链接准则(Linkage criteria)]</strong>： </li></ol><ul><li>Complete-linkage clustering(全链接)</li><li>Single-linkage clustering(单链接)</li><li>Mean-linkage clustering(平均链接 UPGMA)</li><li>Centroid-linkage clustering(中心链接 UPGMC)</li><li><strong>Minimum energy clustering</strong></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1676455519982-21daf41c-04a7-4b8f-be17-f2fcf0cab8dc.png#averageHue=%231f1f1e&amp;clientId=u6ad9c9ba-2483-4&amp;from=paste&amp;height=381&amp;id=u0dbb165a&amp;name=image.png&amp;originHeight=762&amp;originWidth=1441&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=181796&amp;status=done&amp;style=none&amp;taskId=u52091656-f518-4dac-8a90-0edc08ba51e&amp;title=&amp;width=720.5" alt="image.png"></p><h2 id="3-聚类效果评价指标"><a href="#3-聚类效果评价指标" class="headerlink" title="3 聚类效果评价指标"></a>3 聚类效果评价指标</h2><blockquote><p><a href="https://blog.csdn.net/qq_42122496/article/details/106193859">这里给出三个聚类效果评价指标：互信息，标准化互信息，调整互信息(MI, NMI, AMI)</a></p></blockquote><h3 id="互信息-Mutual-information"><a href="#互信息-Mutual-information" class="headerlink" title="互信息(Mutual information)"></a>互信息(Mutual information)</h3><p>互信息的计算公式如下：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gw8woehiulj30qk0k077g.jpg#id=SZcTq&amp;originHeight=720&amp;originWidth=956&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h3 id="标准化互信息-NMI-Normalized-Mutual-Information"><a href="#标准化互信息-NMI-Normalized-Mutual-Information" class="headerlink" title="标准化互信息(NMI, Normalized Mutual Information)"></a>标准化互信息(NMI, Normalized Mutual Information)</h3><p>通常采用NMI和AMI来作为衡量聚类效果的指标。</p><p>标准化互信息的计算方法如下：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gw8wp9hjpdj30q20dwdid.jpg#id=K9LDB&amp;originHeight=500&amp;originWidth=938&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h3 id="调整互信息-AMI-Adjusted-Mutual-Information"><a href="#调整互信息-AMI-Adjusted-Mutual-Information" class="headerlink" title="调整互信息(AMI, Adjusted Mutual Information)"></a>调整互信息(AMI, Adjusted Mutual Information)</h3><p>调整互信息的计算要复杂一些，其计算方法如下：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gw8wq2hd05j30pu066jsa.jpg#id=lbsRf&amp;originHeight=222&amp;originWidth=930&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><p>Python中的 sklearn 库里有这三个指标的类，可以直接调用；</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs stylus">from sklearn<span class="hljs-selector-class">.metrics</span><span class="hljs-selector-class">.cluster</span> import entropy, mutual_info_score, normalized_mutual_info_score, adjusted_mutual_info_score<br><br>MI = lambda x, y: <span class="hljs-built_in">mutual_info_score</span>(x, y)<br>NMI = lambda x, y: <span class="hljs-built_in">normalized_mutual_info_score</span>(x, y, average_method=<span class="hljs-string">&#x27;arithmetic&#x27;</span>)<br>AMI = lambda x, y: <span class="hljs-built_in">adjusted_mutual_info_score</span>(x, y, average_method=<span class="hljs-string">&#x27;arithmetic&#x27;</span>)<br><br>A = <span class="hljs-selector-attr">[1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]</span><br>B = <span class="hljs-selector-attr">[1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 1, 1, 3, 3, 3]</span><br><span class="hljs-selector-id">#print</span>(<span class="hljs-built_in">entropy</span>(A))<br><span class="hljs-selector-id">#print</span>(<span class="hljs-built_in">MI</span>(A, B))<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(NMI(A, B)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(AMI(A, B)</span></span>)<br><br>C = <span class="hljs-selector-attr">[1, 1, 2, 2, 3, 3, 3]</span><br>D = <span class="hljs-selector-attr">[1, 1, 1, 2, 1, 1, 1]</span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(NMI(C, D)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(AMI(C, D)</span></span>)<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/75477709">用人话讲明白快速聚类kmeans</a> </li><li><a href="https://zhuanlan.zhihu.com/p/35959301">从零开始教你 KMeans 算法</a> </li><li><a href="https://zhuanlan.zhihu.com/p/34330242">如何正确使用「K均值聚类」？</a> </li><li><a href="https://www.biaodianfu.com/k-means-choose-k.html">K-Means算法之K值的选择</a> </li><li><a href="http://zhouchen.tech/2017/12/22/sklearn%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E5%88%86%E7%B1%BB%E3%80%81%E8%81%9A%E7%B1%BB%E3%80%81%E5%9B%9E%E5%BD%92%E5%92%8C%E9%99%8D%E7%BB%B4/">sklearn入门教程：分类、聚类、回归和降维</a> </li><li><a href="https://analyticsarora.com/k-means-for-beginners-how-to-build-from-scratch-in-python">K-means for Beginners: How to Build from Scratch in Python</a> </li><li><a href="https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/10.%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB.md">Introduction-NLP/chapter/10.文本聚类.md</a> </li><li><a href="https://zhuanlan.zhihu.com/p/104355127">常用聚类算法-知乎</a> </li><li><a href="http://rstudio-pubs-static.s3.amazonaws.com/202367_03e1ddfcbca74cba97c4a4be7bbabd48.html">Clustering.pdf</a> </li><li><a href="http://www.citisy.site/posts/57282.html">citisy的炼丹房</a> </li><li><a href="https://xueqiu.com/8566534281/139665190">LVQ学习向量量化算法原理以及代码实现-雪球</a> </li><li><a href="https://blog.csdn.net/changyuanchn/article/details/80427922">LVQ聚类算法</a> </li><li><a href="https://chengfeng96.com/blog/2019/05/17/%E8%81%9A%E7%B1%BB%E7%AC%94%E8%AE%B0/">机器学习基础（八）- 聚类笔记</a> </li><li><a href="http://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=14652">SOM算法、LVQ算法及其变体综述</a> </li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>文本聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阿里云es内部技术分享摘要</title>
    <link href="/2021/09/06/2021-09-06-%E9%98%BF%E9%87%8C%E4%BA%91%E6%90%9C%E7%B4%A2%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    <url>/2021/09/06/2021-09-06-%E9%98%BF%E9%87%8C%E4%BA%91%E6%90%9C%E7%B4%A2%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/</url>
    
    <content type="html"><![CDATA[<h2 id="Elasticsearch产品介绍与应用场景"><a href="#Elasticsearch产品介绍与应用场景" class="headerlink" title="Elasticsearch产品介绍与应用场景"></a>Elasticsearch产品介绍与应用场景</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6vodeee3j61i30u0afs02.jpg" alt="image-20210906141157193"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6vqzh7olj61ho0u0n6q02.jpg" alt="image-20210906141430014"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6vs7ws9ij61hw0u0gu202.jpg" alt="image-20210906141540813"></p><span id="more"></span><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6vuw14mpj61l60u0aiy02.jpg" alt="image-20210906141814202"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6vzeoqr4j61ke0u0gst02.jpg" alt="image-20210906142236249"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6w0mj476j61kg0u00zm02.jpg" alt="image-20210906142345991"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6w2sxslpj61hs0u07cb02.jpg" alt="image-20210906142551489"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6w3e6p7pj61ky0u00um02.jpg" alt="image-20210906142626246"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6w4inhenj61hs0u0q8v02.jpg" alt="image-20210906142730803"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6w724ug6j61l50u0tfv02.jpg" alt="image-20210906142956361"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6w7a3vntj61hj0u0tcg02.jpg" alt="image-20210906143010308"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wbi7htxj61hl0u0q8102.jpg" alt="image-20210906143413312"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wh3z18cj61hm0u0n3402.jpg" alt="image-20210906143936330"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6whzj8gtj61l70u0dik02.jpg" alt="image-20210906144027221"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wjcstiqj61ho0u0q4y02.jpg" alt="image-20210906144146456"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wjuhwhej61hi0u0n3h02.jpg" alt="image-20210906144214509"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wkf9gc6j61hc0u0gqp02.jpg" alt="image-20210906144247449"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wkwtoqqj61lq0u0gn302.jpg" alt="image-20210906144316224"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wltd1e7j61q00u0jyz02.jpg" alt="image-20210906144408307"></p><h2 id="阿里云ES集群运维最佳实践"><a href="#阿里云ES集群运维最佳实践" class="headerlink" title="阿里云ES集群运维最佳实践"></a>阿里云ES集群运维最佳实践</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wulbutpj61dc0u0wha02.jpg" alt="image-20210906145234327"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6wxbkd90j61al0u0n1y02.jpg" alt="image-20210906145511616"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6x1e8zw2j616b0u0adu02.jpg" alt="image-20210906145904593"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6xrzoiypj61610u0wh302.jpg" alt="image-20210906152440322"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6xvstziwj61b10u0whz02.jpg" alt="image-20210906152819905"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6xzq4xjxj61fc0re78o02.jpg" alt="image-20210906153206521"></p><h2 id="开放搜索算法工程化应用实践"><a href="#开放搜索算法工程化应用实践" class="headerlink" title="开放搜索算法工程化应用实践"></a>开放搜索算法工程化应用实践</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6ywjkt3uj61oq0u00zt02.jpg" alt="image-20210906160338448"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6yztnlz3j61lq0u0wkw02.jpg" alt="image-20210906160647548"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6z3fty76j61ec0u00x402.jpg" alt="image-20210906161016401"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6z6ji7vuj61rx0u0n2n02.jpg" alt="image-20210906161315371"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6zbo5olkj61nt0u079q02.jpg" alt="image-20210906161810909"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6zd3fzjej61fp0u0q9s02.jpg" alt="image-20210906161933085"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6zhess4xj61jk0u0dlx02.jpg" alt="image-20210906162341625"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6zlxaan1j61h20u078p02.jpg" alt="image-20210906162801473"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6zs1nb7tj61if0u0tdz02.jpg" alt="image-20210906163355185"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6zurxhhmj61fv0u0q7p02.jpg" alt="image-20210906163632999"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu6zz14knlj61l80u0afu02.jpg" alt="image-20210906164037715"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu71k9fzq4j61h00u0tfl02.jpg" alt="image-20210906173535366"></p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gu71o4engej61mk0u0dm502.jpg" alt="image-20210906173920626"></p>]]></content>
    
    
    
    <tags>
      
      <tag>阿里云es</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>降维方法总结</title>
    <link href="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <url>/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>在现实生活中很多机器学习问题有上千维，甚至上万维特征，这不仅影响了训练速度，通常还很难找到比较好的解。这样的问题成为<strong>维数灾难（curse of dimensionality）</strong></p><p>幸运的是，理论上降低维度是可行的。比如MNIST数据集大部分的像素总是白的，因此可以去掉这些特征；相邻的像素之间是高度相关的，如果变为一个像素，相差也并不大。</p><p>需要注意：降低维度肯定会损失一些信息，这可能会让表现稍微变差。因此应该先在原维度训练一次，如果训练速度太慢再选择降维。虽然有时候降为能去除噪声和一些不必要的细节，但通常不会，主要是能加快训练速度。</p><p><strong>降维除了能提高训练速度以外，还能用于数据可视化。</strong>把高维数据降到2维或3维，然后就能把特征在2维空间（3维空间）表示出来，能直观地发现一些规则。</p><h2 id="1-降维的主要方法"><a href="#1-降维的主要方法" class="headerlink" title="1.降维的主要方法"></a>1.降维的主要方法</h2><p>降维的方法主要为两种：<strong>projection 和 Manifold Learning</strong>。</p><h3 id="1-1-投影（Projection）"><a href="#1-1-投影（Projection）" class="headerlink" title="1.1 投影（Projection）"></a>1.1 投影（Projection）</h3><p>在大多数的真实问题，训练样例都不是均匀分散在所有的维度，许多特征都是固定的，同时还有一些特征是强相关的。因此所有的训练样例实际上可以投影在高维空间中的低维子空间中，下面看一个例子。</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648195969917-17bb0c87-ce35-4463-9441-0b0506e4400e.png" class="" title="image.png"><p>可以看到3维空间中的训练样例其实都分布在同一个2维平面，因此我们能够将所有样例都投影在2维平面。对于更高维的空间可能能投影到低维的子空间中。</p><p>然而投影（projection）不总是降维最好的方法在，比如许多情况下，空间可以扭转，如著名的瑞士卷（Swiss roll）数据。</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648196040951-1f0406fa-1043-477c-b9db-50c66c2717a2.png" class="" title="image.png"><p>如果简单的使用投影（project）降维（例如通过压平第3维），那么会变成如下左图的样子，不同类别的样例都混在了一起，而我们的预想是变成右下图的形式。</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648196100345-56959b49-39d0-459e-86ae-5b6de1491436.png" class="" title="image.png"><h3 id="1-2-流形学习（Manifold-Learning）"><a href="#1-2-流形学习（Manifold-Learning）" class="headerlink" title="1.2 流形学习（Manifold Learning）"></a>1.2 流形学习（Manifold Learning）</h3><blockquote><p>降维方法知多少？<a href="https://cloud.tencent.com/developer/article/1080918">https://cloud.tencent.com/developer/article/1080918</a></p></blockquote><p>从降维的角度来看，流形学习是除了核方法之外实现非线性降维的另一类重要手段。流形学习的一个基本假设是样本分布在一个潜在的流形之上，所以尽管输入空间的维数很高，但流形的内在维度一般不是很高。如图4所示，左图是三维空间中的一个S曲面流形，其内在维度是二维。中间图是对流形进行采样而来，右图是经流形学习算法局部线性嵌入把流形展在二维平面所得到的结果。</p><p>在等度规映射和局部线性嵌入之后，出现了一系列流形学习算法，主要包括拉普拉斯特征映射（Laplacian Eigenmaps，LE）、最大方差延展（Maximum Variance Unfolding，MVU）和局部正切空间对齐（Local TangentSpace Alignment， LTSA）等。相比传统线性降维算法，流形学习尽管能有效处理非线性问题，但多数流形算法存在计算效率低下、不能推广到Out-of-sample等问题。后续算法针对上述问题分别做出了许多改进，其中有代表性的是何（X.He）等人所提出的局部保持投影（Local Preserving Projection，LPP）。它实际上是拉普拉斯特征映射的线性化版本。在局部保持投影之外，出现了很多局部化的降维算法，这些方法中都利用了局部保持的思想，即在高维空间中相邻的样本在投影后的低维空间中也应该相邻，一般是通过k近邻（K-NearestNeighbor，k-NN）或ε邻域来度量近邻关系并用一个图来刻画，因而上述方法又可归为一类基于图的降维。之后，人们又将在支持向量机中关键的类间隔（Margin）思想引入其中，相应地发展出了近邻判别分析的降维算法（所谓类间隔是指类间的最小距离，直觉上，最大化该间隔能保证有好的判别性能）。与局部保持投影等无监督方法不同，近邻判别分析不仅综合了类别信息，而且通过同时构建类内及类间k近邻关系图定义出优化目标，进而优化该目标，以获得具有良好判别能力的数据降维。最近，严（S.Yan）等人提出的图嵌入的一般降维框架成为了一个典型的范例，它将上述多数降维方法纳入到该框架之中。在基于图的嵌入降维方法中，图的构建和参数的选取是其非常关键的步骤。</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648201775329-75435461-081a-4673-a2c2-4487a6aa86cb.png" class="" title="image.png"><p>下面介绍几种投影降维方法</p><h2 id="2-PCA-主成分分析"><a href="#2-PCA-主成分分析" class="headerlink" title="2.PCA 主成分分析"></a>2.PCA 主成分分析</h2><blockquote><p><a href="https://www.cnblogs.com/wj-1314/p/8032780.html">Python机器学习笔记：主成分分析（PCA）算法</a></p></blockquote><p>主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。</p><h3 id="2-1-原理："><a href="#2-1-原理：" class="headerlink" title="2.1 原理："></a>2.1 原理：</h3><p>PCA是一种把握事物主要性质的多元统计分析方法。它将原始变量变换为一小部分反映事物主要性质的变量（称之为主分量），从而将高维数据投影到了低维空间，并且保证投影后的低维数据在最小平方意义下最优地表征原有高维数据。</p><div class="table-container"><table><thead><tr><th><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1656001669878-683e2a0e-c607-4265-afb8-cb2faeba385e.jpeg" class="" title="主成分分析 1.jpg"></th><th><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1656001674355-b18523b2-2fae-4e84-b44f-3a05287976bc.jpeg" class="" title="主成分分析 2.jpg"></th></tr></thead><tbody><tr><td></td></tr></tbody></table></div><p>它具有如下性质：</p><ol><li>保留方差是最大的<ol><li>首先需要选择一个好的超平面。先看下图的例子，需要将2D降为1D，选择不同的平面得到右图不一样的结果，第1个投影以后方差最大，第3个方差最小，选择最大方差的一个感觉上应该是比较合理的，因为这样能保留更多的信息。</li><li>另外一种判断的方式是：通过最小化原数据和投影后的数据之间的均方误差。</li></ol></li><li>最终的重构误差（从变换后回到原始情况）是最小的</li></ol><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648196475472-a9a91bb5-b28f-4373-88f1-42bbd772740b.png" class="" title="image.png"><h3 id="2-2-算法步骤"><a href="#2-2-算法步骤" class="headerlink" title="2.2 算法步骤"></a>2.2 算法步骤</h3><p>主要步骤如下：</p><p>（1）计算数据样本的协方差矩阵；</p><p>（2）求解该协方差矩阵的特征向量，按照相应的特征值从大到小排序，选择排在前面的<strong>部分</strong>特征向量作为投影向量；</p><p>（3）将原高维数据投影到子空间中以达到降维的目的，即将数据转换到上述N个特征向量构建的新的空间中</p><p>补充资料：协方差矩阵计算公式：</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648199755739-a02c9a2d-f521-4b56-b82a-e6e50fbed83a.png" class="" title="image.png"><p>两个特征的协方差计算例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">Xi <span class="hljs-number">1.1</span> <span class="hljs-number">1.9</span> <span class="hljs-number">3</span><br>Yi <span class="hljs-number">5.0</span> <span class="hljs-number">10.4</span> <span class="hljs-number">14.6</span><br><br>E(X) = (<span class="hljs-number">1.1</span>+<span class="hljs-number">1.9</span>+<span class="hljs-number">3</span>)/<span class="hljs-number">3</span>=<span class="hljs-number">2</span><br>E(Y) = (<span class="hljs-number">5.0</span>+<span class="hljs-number">10.4</span>+<span class="hljs-number">14.6</span>)/<span class="hljs-number">3</span>=<span class="hljs-number">10</span><br>E(XY)=(<span class="hljs-number">1.1</span>×<span class="hljs-number">5.0</span>+<span class="hljs-number">1.9</span>×<span class="hljs-number">10.4</span>+<span class="hljs-number">3</span>×<span class="hljs-number">14.6</span>)/<span class="hljs-number">3</span>=<span class="hljs-number">23.02</span><br>Cov(X,Y)=E(XY)-E(X)E(Y)=<span class="hljs-number">23.02</span>-<span class="hljs-number">2</span>×<span class="hljs-number">10</span>=<span class="hljs-number">3.02</span><br></code></pre></td></tr></table></figure></p><ul><li>协方差矩阵的特征值和特征向量计算：</li></ul><p>一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648196677697-1b2a959e-c9f4-46ef-8ac1-8dcd4db5d35e.webp" class=""><p>其中A为方阵，v 是特征向量，λ是特征值。λ为特征向量 v 对应的特征值。<br>特征值分解是将一个矩阵分解为如下形式：</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648196685367-a789e9a7-f683-47cd-b660-55be81231ca7.webp" class=""><p>Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列。特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么</p><h3 id="2-3-代码实现"><a href="#2-3-代码实现" class="headerlink" title="2.3 代码实现"></a>2.3 代码实现</h3><p>numpy中实现PCA<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment">#读取文件</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loadDataSet</span>(<span class="hljs-params">fileName, delim=<span class="hljs-string">&#x27;\t&#x27;</span></span>):<br>    fr = <span class="hljs-built_in">open</span>(fileName)<br>    stringArr = [line.strip().split(delim) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fr.readlines()]<br>    datArr = [<span class="hljs-built_in">map</span>(<span class="hljs-built_in">float</span>,line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> stringArr]<br>    <span class="hljs-keyword">return</span> mat(datArr)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pca</span>(<span class="hljs-params">dataMat, topNfeat=n</span>):<br>    meanVals = np.mean(dataMat, axis=<span class="hljs-number">0</span>) <br>    <span class="hljs-comment">#（axis=0）按列求均值</span><br>    meanRemoved = dataMat - meanVals <br>    <span class="hljs-comment">#减去均值</span><br>    covMat = np.cov(meanRemoved, rowvar=<span class="hljs-number">0</span>)  <br>    <span class="hljs-comment">#计算协方差矩阵，rowvar为0，</span><br>    <span class="hljs-comment">#一行为一个样本，不为0一列为一个样本</span><br>    eigVals,eigVects = np.linalg.eig(mat(covMat))<br>    <span class="hljs-comment">#求特征值和特征向量,</span><br>    <span class="hljs-comment">#特征向量是按列放的，即一列代表一个特征向量 。</span><br>    <span class="hljs-comment">#eigVals以行向量形式存放特征值。</span><br>    <span class="hljs-comment">#eigVects存放特征向量，每一列代表一个特征向量。</span><br>    eigValInd = np.argsort(eigVals)            <br>    <span class="hljs-comment">#对特征值从小到大排序 ,函数argsort()返回从小到大排序的index</span><br>    eigValInd = eigValInd[:-(topNfeat+<span class="hljs-number">1</span>):-<span class="hljs-number">1</span>]  <br>    <span class="hljs-comment">#列表逆序以后，从头到位取前topNfeat个特征值index,</span><br>    <span class="hljs-comment">#即最大的n个特征值的index （python里面，list[a:b:c]代表从下标a开始到b，</span><br>    <span class="hljs-comment">#步长为c。list[::-1]可以看作是列表逆序）</span><br>    redEigVects = eigVects[:,eigValInd]       <br>    <span class="hljs-comment">#最大的n个特征值对应的特征向量  </span><br>    lowDDataMat = meanRemoved * redEigVects<br>    <span class="hljs-comment">#低维特征空间的数据 </span><br>    reconMat = (lowDDataMat * redEigVects.T) + meanVals<br>    <span class="hljs-comment">#把数据转换到新空间</span><br>    <span class="hljs-keyword">return</span> lowDDataMat, reconMat<br></code></pre></td></tr></table></figure><br>调用sklearn中的PCA</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1676455452673-306af021-4987-43c9-b566-3240d2537f30.png" class="" title="image.png"><p>sklearn.<strong>PCA参数说明：</strong></p><ul><li>n_components:<br>意义：PCA算法中所要保留的主成分个数n，也即保留下来的特征个数n<br>类型：int 或者 string，缺省时默认为None，所有成分被保留。</li><li>copy:<br>类型：bool，True或者False，默认为True。意义：表示是否在运行算法时，将原始训练数据复制一份。若为True，则运行PCA算法后，原始训练数据的值不会有任何改变，因为是在原始数据的副本上进行运算；若为False，则运行PCA算法后，原始训练数据的值会改，因为是在原始数据上进行降维计算。</li><li>whiten:<br>类型：bool，默认为False<br>意义：白化，使得每个特征具有相同的方差。就是对降维后的数据的每个特征进行归一化，一般不需要白化。</li></ul><p><strong>PCA对象的属性:</strong></p><ul><li>components_ ：返回具有最大方差的成分。</li><li>explained_variance_：所保留的n个成分各自的方差</li><li>explained_variance_ratio_：返回 所保留的n个成分各自的方差百分比。</li><li>n_components_：返回所保留的成分个数n。</li><li>noise variance：噪声方差大小</li><li>mean_:特征均值</li></ul><p><strong>PCA对象的方法：</strong></p><ul><li>fit(X,y=None)<br>fit()是scikit-learn中通用的方法。因为PCA是无监督学习算法，此处y等于None。<br>fit(X)，表示用数据X来训练PCA模型。</li><li>fit_transform(X)<br>用X来训练PCA模型，同时返回降维后的数据。</li><li>inverse_transform()<br>将降维后的数据转换成原始数据，X=pca.inverse_transform(newX)</li><li>transform(X)<br>将数据X转换成降维后的数据。</li></ul><h3 id="2-4-PCA小结"><a href="#2-4-PCA小结" class="headerlink" title="2.4 PCA小结"></a>2.4 PCA小结</h3><p>作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。</p><p>PCA算法的主要优点有：</p><ul><li>仅仅需要以方差衡量信息量，不受数据集以外的因素影响。 </li><li>各主成分之间正交，可消除原始数据成分间的相互影响的因素。</li><li>计算方法简单，主要运算是特征值分解，易于实现。</li></ul><p>PCA算法的主要缺点有：</p><ul><li>主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。</li><li>方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。</li></ul><h2 id="3-T-SNE（t-分布邻域嵌入算法）"><a href="#3-T-SNE（t-分布邻域嵌入算法）" class="headerlink" title="3. T-SNE（t-分布邻域嵌入算法）"></a>3. T-SNE（t-分布邻域嵌入算法）</h2><p>t-Distributed Stochastic Neighbor Embedding (t-SNE) ，简单来说是一种「降维」的计算方法，同时也可以用于展示（可视化）高维数据。</p><h3 id="3-1-基本原理"><a href="#3-1-基本原理" class="headerlink" title="3.1 基本原理"></a>3.1 基本原理</h3><p>SNE是通过仿射(affinitie)变换将数据点映射到概率分布上，主要包括两个步骤：</p><ol><li>SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。</li><li>SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</li></ol><p>我们看到t-SNE模型是非监督的降维，他跟kmeans等不同，他不能通过训练得到一些东西之后再用于其它数据（比如kmeans可以通过训练得到k个点，再用于其它数据集，而t-SNE只能单独的对数据做操作，也就是说他只有fit_transform，而没有fit操作）</p><h3 id="3-2-算法流程"><a href="#3-2-算法流程" class="headerlink" title="3.2 算法流程"></a>3.2 算法流程</h3><p>有空再补</p><h3 id="3-3-代码实现"><a href="#3-3-代码实现" class="headerlink" title="3.3 代码实现"></a>3.3 代码实现</h3><blockquote><p>参考《<a href="https://mortis.tech/2019/11/program_note/664/">Python – 如何使用t-SNE 进行降维</a>》</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> manifold, datasets<br><br><span class="hljs-comment">#Prepare the data</span><br>digits = datasets.load_digits(n_class=<span class="hljs-number">6</span>)<br>X, y = digits.data, digits.target<br>n_samples, n_features = X.shape<br>n = <span class="hljs-number">20</span>  <br>img = np.zeros((<span class="hljs-number">10</span> * n, <span class="hljs-number">10</span> * n))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>    ix = <span class="hljs-number">10</span> * i + <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        iy = <span class="hljs-number">10</span> * j + <span class="hljs-number">1</span><br>        img[ix:ix + <span class="hljs-number">8</span>, iy:iy + <span class="hljs-number">8</span>] = X[i * n + j].reshape((<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>plt.imshow(img, cmap=plt.cm.binary)<br>plt.xticks([])<br>plt.yticks([])<br>plt.show()<br><br><span class="hljs-comment">#t-SNE</span><br>X_tsne = manifold.TSNE(n_components=<span class="hljs-number">2</span>, init=‘random’, random_state=<span class="hljs-number">5</span>, verbose=<span class="hljs-number">1</span>).fit_transform(X)<br><br><span class="hljs-comment">#Data Visualization</span><br>x_min, x_max = X_tsne.<span class="hljs-built_in">min</span>(<span class="hljs-number">0</span>), X_tsne.<span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>)<br>X_norm = (X_tsne – x_min) / (x_max – x_min)  <span class="hljs-comment">#Normalize</span><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X_norm.shape[<span class="hljs-number">0</span>]):<br>    plt.text(X_norm[i, <span class="hljs-number">0</span>], X_norm[i, <span class="hljs-number">1</span>], <span class="hljs-built_in">str</span>(y[i]), color=plt.cm.Set1(y[i]), <br>             fontdict=&#123;‘weight’: ‘bold’, ‘size’: <span class="hljs-number">9</span>&#125;)<br>plt.xticks([])<br>plt.yticks([])<br>plt.show()<br></code></pre></td></tr></table></figure><p>TSNE()这个函数里各个参数的含义：</p><ul><li>n_components：你想要降成几维，一维填1、二维填2、三维填3，没有别的选择了。 </li><li>perplexity：你的资料量越大，就越大。预设是30，建议值是5-50，但你可以不要采纳。 </li><li>n_iter：你想要迭代的次数，预设是1000。 </li><li>init：最初投影，两个选择“random” 或是“pca” (t-SNE里面有PCA噢!) </li><li>verbose：要不要看训练过程 </li><li>random_state：控制随机数的生成 </li><li>method：不用改，除非你的数据量很小，才改成“exact” </li></ul><p>执行完之后应该会出现两张图：</p><div class="table-container"><table><thead><tr><th><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648200901363-0109c0c9-94c1-49c3-9564-19478afc91bc.png" class="" title="image.png"></th><th><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648200909064-be9f7a41-1a5e-459a-be5f-7b70992c9420.png" class="" title="image.png"></th></tr></thead><tbody><tr><td></td></tr></tbody></table></div><h3 id="3-4-PCA-vs-T-SNE"><a href="#3-4-PCA-vs-T-SNE" class="headerlink" title="3.4 PCA vs. T-SNE"></a>3.4 PCA vs. T-SNE</h3><p>PCA和T-SNE同为降维工具，主要区别在于：机制和原理不同</p><p>T-SNE 运行极慢，PCA 则相对较快；</p><p>因此通常来说，T-SNE只能用于展示（可视化）高维数据，由于速度慢常常先用 PCA 进行降维，再使用 tsne：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>data_pca = PCA(n_components=<span class="hljs-number">50</span>).fit_transform(data)<br>data_pca_tsne = TSNE(n_components=<span class="hljs-number">2</span>).fit_transform(data_pca)<br></code></pre></td></tr></table></figure><h2 id="4-MDS-多维标度法"><a href="#4-MDS-多维标度法" class="headerlink" title="4. MDS:多维标度法"></a>4. MDS:多维标度法</h2><p>MDS类似T-SNE，经常用于数据可视化，维度大多是时候是2-3，他在降维的同时尽可能保留样本间的相对距离。<br>具体介绍略….直接看代码…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> MDS<br>mds = manifold.MDS(n_components=<span class="hljs-number">3</span>)<br>Xtrans = mds.fit_transform(X)<br></code></pre></td></tr></table></figure><p>多维标度法解决的问题是：当n个对象（object）中各对对象之间的相似性（或距离）给定时，确定这些对象在低维空间中的表示，并使其尽可能与原先的相似性（或距离）“大体匹配”，使得由降维所引起的任何变形达到最小。</p><p>多维空间中排列的每一个点代表一个对象，因此点间的距离与对象间的相似性高度相关。也就是说，两个相似的对象由多维空间中两个距离相近的点表示，而两个不相似的对象则由多维空间两个距离较远的点表示。多维空间通常为二维或三维的欧氏空间，但也可以是非欧氏三维以上空间。</p><h2 id="5-SVD-矩阵分解-奇异值分解"><a href="#5-SVD-矩阵分解-奇异值分解" class="headerlink" title="5. SVD 矩阵分解/奇异值分解"></a>5. SVD 矩阵分解/奇异值分解</h2><blockquote><p><a href="https://www.6aiq.com/article/1586815086168">贝壳找房【语言模型系列】原理篇一：从 one-hot 到 Word2vec</a><br><a href="https://www.cnblogs.com/wj-1314/p/10304676.html">Python机器学习笔记：奇异值分解（SVD）算法</a><br>⭐️<a href="Singular Value Decomposition as Simply as Possible">Singular Value Decomposition as Simply as Possible</a></p></blockquote><h3 id="5-1-原理"><a href="#5-1-原理" class="headerlink" title="5.1. 原理"></a>5.1. 原理</h3><p>奇异值分解 (SVD) 是将矩阵 _A_ 进行分解，$A=UΣV^T$，其中 _A_ 是 _m_∗_n_ 的矩阵，_U_是 _m_∗_n_ 的矩阵，Σ 是 _m_∗_n_ 的矩阵，_V_ 是 _n_∗_n_ 的矩阵，如何求解得到这三个矩阵呢？</p><p>首先回顾一下特征值和特征向量的定义：</p><p>$Ax=λx$</p><p>其中，A 是 _m_∗_m_ 实对称矩阵，_x_ 是一个 _n_ 维向量，则称 _λ_ 是一个特征值，_x_ 是矩阵 _A_ 的特征值 _λ_ 对应的一个特征向量。如果我们求出了 _n_ 个特征值 $λ_1,λ_2…λ_n$及其对应的特征向量 $x_1,x_2…x_n$，如果这 _n_ 个特征向量线性无关，那么矩阵 _A_ 就可以表示为</p><p>$A=XΣX^{−1}$</p><p>其中，_X_ 是有 _n_ 个特征向量组成的 _n_∗_n_ 的矩阵，Σ 是由 _n_ 个特征值为主对角线的 _n_∗_n_ 维矩阵。一般会将 _X_ 进行标准化，即 $∣∣xi∣∣_2=1,x_ix_i^T=1,XX^T=I$，也就是得到了 $X^T=X^{−1}$，那么 _A_ 就可以表示为</p><p>$A=XΣX^T$</p><p>需要注意的是，直接进行特征分解的矩阵 _A_ 是方阵，而在 SVD 中的矩阵 _A_ 不一定是方阵，这时需要再做一些推导。</p><p>$AA^T=UΣV^TVΣ^TU^T$</p><p>由于 $V^TV=I$，所以</p><p>$AA^T=UΣ^2U^T$</p><p>因为$AA^T$是一个 _m_∗_m_ 的方阵，所以通过求解$AA^T$的特征向量，就可以得到矩阵 _U_。同理，通过求解$A^TA$的特征向量，就可以得到矩阵 _V_，通过对$AA^T$的特征值矩阵求平方根，就可以得到矩阵 _A_ 的奇异值矩阵 Σ。至此，我们得到了矩阵 A 的分解。</p><p>值得注意的是，奇异值是从大到小排列的，通常前 10% 甚至 1% 的奇异值的和就占了所有奇异值和的 99% 以上，奇异值在集合意义上代表了特征的权重，所以我们一般不会选择将整个 _U_ 矩阵作为词向量矩阵，而是选择前 k 维，最终的词向量表大小就是 _m_∗_k_ ,_m_ 是词表达小，_k_ 是词向量维度。</p><p>基于 SVD 的方法得到了稠密的词向量，相比于之前的 one-hot 有了一定提升，能够表现单词之间一定的语义关系，但是这种方法也存在很多问题：</p><ul><li>矩阵过于稀疏。因为在构建共现矩阵时，必然有很多词是没有共现的</li><li>分解共现矩阵的复杂度很高</li><li>高频无意义的词影响共现矩阵分解的效果，如 is，a</li><li>一词多义问题，即同一个词具有不同含义却被表示为一个向量，比如 bank 既有银行的意思，也有岸边的意思，但在这里被表示为统一的向量</li></ul><h3 id="5-2-使用"><a href="#5-2-使用" class="headerlink" title="5.2. 使用"></a>5.2. 使用</h3><p>numpy中调用方式和求特征值特征向量类似。实际上特征分解是一种特殊的奇异值分解，特征分解只能分解方阵，奇异值分解可以分解任意矩阵，pca中的特征分解通常会使用svd。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>U,Sigma,VT = linalg.svd(matrix)<br></code></pre></td></tr></table></figure></p><h3 id="5-3-降维"><a href="#5-3-降维" class="headerlink" title="5.3. 降维"></a>5.3. 降维</h3><p>利用SVD降维实际上是用来简化数据，使用了奇异值分解以后仅需保留着三个比较小的矩阵，就能表示原矩阵，不仅节省存储量，在计算的时候更是减少了计算量。SVD在信息检索（隐性语义索引）、图像压缩、推荐系统等等领域中都有应用。【TruncatedSVD（截断SVD）】sklearn中的pca就是使用svd分解，再选取三个在矩阵中间的对角矩阵中最大的一部分值，再还原这个矩阵。</p><h2 id="6-LDA-线性判别分析"><a href="#6-LDA-线性判别分析" class="headerlink" title="6. LDA 线性判别分析"></a>6. LDA 线性判别分析</h2><blockquote><p><a href="https://www.cnblogs.com/wj-1314/p/10234256.html">Python机器学习笔记：线性判别分析（LDA）算法</a></p></blockquote><p>PCA是一种无监督的数据降维方法，与之不同的是：LDA是一种有监督的数据降维方法。我们知道即使在训练样本上，我们提供了类别标签，在使用PCA模型的时候，我们是不利于类别标签的，而LDA在进行数据降维的时候是利用数据的类别标签提供的信息的。</p><p>从几何的角度来看，PCA和LDA都是将数据投影到新的相互正交的坐标轴上。只不过在投影的过程中他们使用的约束是不同的，也可以说目标是不同的。PCA是将数据投影到方差最大的几个相互正交的方向上，以期待保留最多的样本信息。样本的方差越大表示样本的多样性越好，在训练模型的时候，我们当然希望数据的差别越大越好。否则即使样本很多但是他们彼此相似或者相同，提供的样本信息将相同，相当于只有很少的样本提供信息是有用的。样本信息不足将导致模型性能不够理想。这就是PCA降维的目的：将数据投影到方差最大的几个相互正交的方向上。</p><p>由于PCA与LDA的动机不同，前者着眼于降维数据对原有高维数据保真度的优化，而后者更关心降维数据对不同类数据判别性的优化。因此对于相同的数据，它们求解出的投影方向（向量）也截然不同（见图1）。下图中，假设一组二维数据样本，分为两类（ClassⅠ与ClassⅡ），PCA选择方差最大的方向作为投影方向（图中以实线表示），目的是使得投影的数据能在最小平方意义下尽可能地表征原数据。这样做投影确实方差最大，但是是不是有其他问题。聪明的你发现了，这样做投影之后两类数据样本将混合在一起，将不再线性可分，甚至是不可分的。这对我们来说简直是地狱，本来线性可分的样本被我们亲手变得不再可分。而我们发现，如果使用图中虚线，向这条直线做投影即能使数据降维，同时还能保证两类数据仍然是线性可分的，这其实就是LDA的思想：不同类数据之间区别将被最大化，同类别样本之间的区别将达到最小化，从而显著提高了各类别之间的可分性。</p><img src="/2021/09/06/2021-09-06-%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1648201508319-732c41bd-bd60-488f-ab66-fc4413e56374.png" class="" title="image.png"><p>LDA将带有标签的数据降维，投影到低维空间同时满足三个条件：</p><ul><li>尽可能多的保留数据样本的信息（即选择最大的特征是对应的特征向量所代表的方向）。</li><li>寻找使样本尽可能好分的最佳投影方向。</li><li>投影后使得同类样本尽可能近，不同类样本尽可能远。</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><ul><li>可参考《<a href="https://blog.csdn.net/TiffanyRabbit/article/details/76445909">利用sklearn训练LDA主题模型及调参详解</a>》</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>【机器学习系统设计】<br>【机器学习—周志华】<br>【机器学习实战】<br><a href="http://blog.csdn.net/baimafujinji/article/details/79407478">详解多维标度法（MDS，Multidimensional scaling）</a><br>《<a href="https://www.jianshu.com/p/75e805ff247c">各类降维方法总结-简书</a>》</p>]]></content>
    
    
    
    <tags>
      
      <tag>降维</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【转载】深度学习中Dropout原理解析</title>
    <link href="/2021/08/30/2021-08-30-Dropout%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    <url>/2021/08/30/2021-08-30-Dropout%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>目录：</p><ol><li><p>Dropout简介</p><p>1.1 Dropout出现的原因</p><p>1.2 什么是Dropout</p></li><li><p>Dropout工作流程及使用</p><p>2.1 Dropout具体工作流程</p><p>2.2 Dropout在神经网络中的使用</p></li><li><p>为什么说Dropout可以解决过拟合</p></li><li><p>Dropout在Keras中源码分析</p></li></ol><span id="more"></span><h2 id="1-Dropout简介"><a href="#1-Dropout简介" class="headerlink" title="1. Dropout简介"></a><strong>1. Dropout简介</strong></h2><h2 id="1-1-Dropout出现的原因"><a href="#1-1-Dropout出现的原因" class="headerlink" title="1.1 Dropout出现的原因"></a><strong>1.1 Dropout出现的原因</strong></h2><p>在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。</p><p>过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。</p><p>综上所述，训练深度神经网络的时候，总是会遇到两大缺点：</p><p>（1）容易过拟合</p><p>（2）费时</p><p>Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。</p><h2 id="1-2-什么是Dropout"><a href="#1-2-什么是Dropout" class="headerlink" title="1.2 什么是Dropout"></a><strong>1.2 什么是Dropout</strong></h2><p>在2012年，Hinton在其论文《Improving neural networks by preventing co-adaptation of feature detectors》中提出Dropout。当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。</p><p>在2012年，Alex、Hinton在其论文《ImageNet Classification with Deep Convolutional Neural Networks》中用到了Dropout算法，用于防止过拟合。并且，这篇论文提到的AlexNet网络模型引爆了神经网络应用热潮，并赢得了2012年图像识别大赛冠军，使得CNN成为图像分类上的核心算法模型。</p><p>随后，又有一些关于Dropout的文章《Dropout:A Simple Way to Prevent Neural Networks from Overfitting》、《Improving Neural Networks with Dropout》、《Dropout as data augmentation》。</p><p>从上面的论文中，我们能感受到Dropout在深度学习中的重要性。那么，到底什么是Dropout呢？</p><p>Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。</p><p>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。</p><p><img src="https://pic2.zhimg.com/80/v2-5530bdc5d49f9e261975521f8afd35e9_1440w.jpg" alt="img">图1：使用Dropout的神经网络模型</p><h2 id="2-Dropout工作流程及使用"><a href="#2-Dropout工作流程及使用" class="headerlink" title="2. Dropout工作流程及使用"></a><strong>2. Dropout工作流程及使用</strong></h2><h2 id="2-1-Dropout具体工作流程"><a href="#2-1-Dropout具体工作流程" class="headerlink" title="2.1 Dropout具体工作流程"></a><strong>2.1 Dropout具体工作流程</strong></h2><p>假设我们要训练这样一个神经网络，如图2所示。</p><p><img src="https://pic3.zhimg.com/80/v2-a7b5591feb14da95d29103913b61265a_1440w.jpg" alt="img">图2：标准的神经网络</p><p>输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：</p><p>（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）</p><p><img src="https://pic3.zhimg.com/80/v2-24f1ffc4ef118948501eb713685c068a_1440w.jpg" alt="img">图3：部分临时被删除的神经元</p><p>（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。</p><p>（3）然后继续重复这一过程：</p><ul><li>恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）</li><li>从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。</li><li>对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。</li></ul><p>不断重复这一过程。</p><h2 id="2-2-Dropout在神经网络中的使用"><a href="#2-2-Dropout在神经网络中的使用" class="headerlink" title="2.2 Dropout在神经网络中的使用"></a><strong>2.2 Dropout在神经网络中的使用</strong></h2><p>Dropout的具体工作流程上面已经详细的介绍过了，但是具体怎么让某些神经元以一定的概率停止工作（就是被删除掉）？代码层面如何实现呢？</p><p>下面，我们具体讲解一下Dropout代码层面的一些公式推导及代码实现思路。</p><p>（1）在训练模型阶段</p><p>无可避免的，在训练网络的每个单元都要添加一道概率流程。</p><p><img src="https://pic3.zhimg.com/80/v2-543a000fcfe9778cd64c898c01743aae_1440w.jpg" alt="img">图4：标准网络和带有Dropout网络的比较</p><p>对应的公式变化如下：</p><ul><li>没有Dropout的网络计算公式：</li></ul><p><img src="https://pic1.zhimg.com/80/v2-11fd2a086d59490cf8121d90c4ec4e68_1440w.jpg" alt="img"></p><ul><li>采用Dropout的网络计算公式：</li></ul><p><img src="https://pic4.zhimg.com/80/v2-61933b0548270880aa5d5497ede5b383_1440w.jpg" alt="img"></p><p>上面公式中Bernoulli函数是为了生成概率r向量，也就是随机生成一个0、1的向量。</p><p>代码层面实现让某个神经元以概率p停止工作，其实就是让它的激活函数值以概率p变为0。比如我们某一层网络神经元的个数为1000个，其激活函数输出值为y1、y2、y3、……、y1000，我们dropout比率选择0.4，那么这一层神经元经过dropout后，1000个神经元中会有大约400个的值被置为0。</p><p><strong><em>注意：\</em></strong> 经过上面屏蔽掉某些神经元，使其激活值为0以后，我们还需要对向量y1……y1000进行缩放，也就是乘以1/(1-p)。如果你在训练的时候，经过置0后，没有对y1……y1000进行缩放（rescale），那么在测试的时候，就需要对权重进行缩放，操作如下。</p><p>（2）在测试模型阶段</p><p>预测模型的时候，每一个神经单元的权重参数要乘以概率p。</p><p><img src="https://pic4.zhimg.com/80/v2-335782876686a248b51ff739c7e9b1ff_1440w.jpg" alt="img">图5：预测模型时Dropout的操作</p><p>测试阶段Dropout公式：</p><p><img src="https://www.zhihu.com/equation?tex=w_%7Btest%7D%5E%7B%28l%29%7D%3DpW%5E%7B%28l%29%7D" alt="[公式]"></p><h2 id="3-为什么说Dropout可以解决过拟合？"><a href="#3-为什么说Dropout可以解决过拟合？" class="headerlink" title="3. 为什么说Dropout可以解决过拟合？"></a><strong>3. 为什么说Dropout可以解决过拟合？</strong></h2><p><strong>（1）取平均的作用：</strong> 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</p><p><strong>（2）减少神经元之间复杂的共适应关系：</strong> 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</p><p>（3）<strong>Dropout类似于性别在生物进化中的角色：</strong>物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。</p><h2 id="4-Dropout在Keras中的源码分析"><a href="#4-Dropout在Keras中的源码分析" class="headerlink" title="4. Dropout在Keras中的源码分析"></a><strong>4. Dropout在Keras中的源码分析</strong></h2><p>下面，我们来分析Keras中Dropout实现源码。</p><p>Keras开源项目GitHub地址为：</p><p><a href="https://link.zhihu.com/?target=https%3A//github.com/fchollet/keras/tree/master/keras">https://github.com/fchollet/keras/tree/master/keras</a></p><p>其中Dropout函数代码实现所在的文件地址：</p><p><a href="https://link.zhihu.com/?target=https%3A//github.com/fchollet/keras/blob/master/keras/backend/theano_backend.py">https://github.com/fchollet/keras/blob/master/keras/backend/theano_backend.py</a></p><p>Dropout实现函数如下：</p><p><img src="https://pic4.zhimg.com/80/v2-696f4b51d51ebc7fa1e50a03785ffc57_1440w.jpg" alt="img">图6：Keras中实现Dropout功能</p><p>我们对keras中Dropout实现函数做一些修改，让dropout函数可以单独运行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python3"># coding:utf-8<br>import numpy as np<br><br># dropout函数的实现<br>def dropout(x, level):<br>    if level &lt; 0. or level &gt;= 1: #level是概率值，必须在0~1之间<br>        raise ValueError(&#x27;Dropout level must be in interval [0, 1[.&#x27;)<br>    retain_prob = 1. - level<br><br>    # 我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样<br>    # 硬币 正面的概率为p，n表示每个神经元试验的次数<br>    # 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。<br>    random_tensor = np.random.binomial(n=1, p=retain_prob, size=x.shape) #即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了<br>    print(random_tensor)<br><br>    x *= random_tensor<br>    print(x)<br>    x /= retain_prob<br><br>    return x<br><br>#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  <br>x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)<br>dropout(x,0.4)<br></code></pre></td></tr></table></figure><p>函数中，x是本层网络的激活值。Level就是dropout就是每个神经元要被丢弃的概率。</p><p><strong><em>注意：\</em></strong> Keras中Dropout的实现，是屏蔽掉某些神经元，使其激活值为0以后，对激活值向量x1……x1000进行放大，也就是乘以1/(1-p)。</p><p><strong>思考：</strong>上面我们介绍了两种方法进行Dropout的缩放，那么Dropout为什么需要进行缩放呢？</p><p>因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的，用户可能认为模型预测不准。那么一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。</p><p><strong>总结：</strong></p><p>当前Dropout被大量利用于全连接网络，而且一般认为设置为0.5或者0.3，而在卷积网络隐藏层中由于卷积自身的稀疏化以及稀疏化的ReLu函数的大量使用等原因，Dropout策略在卷积网络隐藏层中使用较少。总体而言，Dropout是一个超参，需要根据具体的网络、具体的应用领域进行尝试。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Hinton G E, Srivastava N, Krizhevsky A, et al. Improving neural networks by preventing co-adaptation of feature detectors[J]. arXiv preprint arXiv:1207.0580, 2012.</li><li>Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</li><li>Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.</li><li>Srivastava N. Improving neural networks with dropout[J]. University of Toronto, 2013, 182.</li><li><p>Bouthillier X, Konda K, Vincent P, et al. Dropout as data augmentation[J]. arXiv preprint arXiv:1506.08700, 2015.</p></li><li><p><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/hjimce/article/details/50413257">深度学习（二十二）Dropout浅层理解与实现</a></p></li><li><p><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/stdcoutzyx/article/details/49022443">理解dropout</a></p></li><li><p>Dropout解决过拟合问题 - 晓雷的文章 - 知乎</p></li></ol><p><a href="https://zhuanlan.zhihu.com/p/23178423">https://zhuanlan.zhihu.com/p/23178423</a></p><ol><li><p><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/qunnie_yi/article/details/80128463">李理：卷积神经网络之Dropout</a></p></li><li><p><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/whiteinblue/article/details/37808623">Dropout原理，代码浅析</a></p></li><li><p><a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/tornadomeet/p/3258122.html%3F_t_t_t%3D0.09445037946091872">Deep learning：四十一(Dropout简单理解)</a></p></li></ol><p><a href="https://zhuanlan.zhihu.com/p/38200980">https://zhuanlan.zhihu.com/p/38200980</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Dropout</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从0开始知识蒸馏</title>
    <link href="/2021/07/27/2021-07-27-%E4%BB%8E0%E5%BC%80%E5%A7%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"/>
    <url>/2021/07/27/2021-07-27-%E4%BB%8E0%E5%BC%80%E5%A7%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/</url>
    
    <content type="html"><![CDATA[<blockquote><p>翻译自 <a href="https://keras.io/examples/vision/knowledge_distillation/#train-student-from-scratch-for-comparison">https://keras.io/examples/vision/knowledge_distillation/#train-student-from-scratch-for-comparison</a></p><p><a href="https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/knowledge_distillation.ipynb"><strong>View in Colab</strong></a>  •   <a href="https://github.com/keras-team/keras-io/blob/master/examples/vision/knowledge_distillation.py"><strong>GitHub source</strong></a></p><p>更多关于蒸馏和模型推理加速的知识可参考博客《<a href="https://mp.weixin.qq.com/s/Od_XK-tsFNdWzG1CXUkQqg">预训练模型参数量越来越大？这里有你需要的BERT推理加速技术指南</a>》</p></blockquote><h2 id="知识蒸馏简介"><a href="#知识蒸馏简介" class="headerlink" title="知识蒸馏简介"></a>知识蒸馏简介</h2><p>知识蒸馏是一个模型压缩的过程，其中训练一个小的（学生）模型来匹配一个大的预训练（教师）模型。通过最小化损失函数将知识从教师模型转移到学生，旨在匹配软化的教师逻辑和真实标签。</p><p>通过在 softmax 中应用“温度”缩放函数来软化对数，有效地平滑概率分布并揭示老师学到的类间关系。</p> <span id="more"></span><h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras<br><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br></code></pre></td></tr></table></figure><h2 id="构造-Distiller-类"><a href="#构造-Distiller-类" class="headerlink" title="构造 Distiller() 类"></a>构造 <code>Distiller()</code> 类</h2><p>自定义<code>Distiller()</code>类，覆盖<code>Model</code>方法<code>train_step</code>，<code>test_step</code>以及<code>compile()</code>。为了使用蒸馏器，我们需要：</p><ul><li>已经训好的教师模型</li><li>要训练的学生模型</li><li>关于学生预测和 ground-truth 之间差异的学生损失函数</li><li>A distillation loss function, along with a <code>temperature</code>, on the difference between the soft student predictions and the soft teacher labels</li><li>一个<code>alpha</code>因素加权学生和蒸馏损失</li><li>学生和（可选）指标的优化器来评估性能</li></ul><p>在该<code>train_step</code>方法中，我们执行教师和学生两者的 forward pass，计算<code>student_loss</code>和<code>distillation_loss</code>的加权损失（<code>alpha</code>与 <code>1 - alpha</code>），并执行 backward pass。Note: only the student weights are updated, and therefore we only calculate the gradients for the student weights.</p><p>在<code>test_step</code>方法中，我们在提供的数据集上评估学生模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Distiller</span>(keras.Model):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, student, teacher</span>):<br>        <span class="hljs-built_in">super</span>(Distiller, self).__init__()<br>        self.teacher = teacher<br>        self.student = student<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compile</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        optimizer,</span><br><span class="hljs-params">        metrics,</span><br><span class="hljs-params">        student_loss_fn,</span><br><span class="hljs-params">        distillation_loss_fn,</span><br><span class="hljs-params">        alpha=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">        temperature=<span class="hljs-number">3</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot; Configure the distiller.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            optimizer: Keras optimizer for the student weights</span><br><span class="hljs-string">            metrics: Keras metrics for evaluation</span><br><span class="hljs-string">            student_loss_fn: Loss function of difference between student</span><br><span class="hljs-string">                predictions and ground-truth</span><br><span class="hljs-string">            distillation_loss_fn: Loss function of difference between soft</span><br><span class="hljs-string">                student predictions and soft teacher predictions</span><br><span class="hljs-string">            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn</span><br><span class="hljs-string">            temperature: Temperature for softening probability distributions.</span><br><span class="hljs-string">                Larger temperature gives softer distributions.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(Distiller, self).<span class="hljs-built_in">compile</span>(optimizer=optimizer, metrics=metrics)<br>        self.student_loss_fn = student_loss_fn<br>        self.distillation_loss_fn = distillation_loss_fn<br>        self.alpha = alpha<br>        self.temperature = temperature<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_step</span>(<span class="hljs-params">self, data</span>):<br>        <span class="hljs-comment"># Unpack data</span><br>        x, y = data<br><br>        <span class="hljs-comment"># Forward pass of teacher</span><br>        teacher_predictions = self.teacher(x, training=<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br>            <span class="hljs-comment"># Forward pass of student</span><br>            student_predictions = self.student(x, training=<span class="hljs-literal">True</span>)<br><br>            <span class="hljs-comment"># Compute losses</span><br>            student_loss = self.student_loss_fn(y, student_predictions)<br>            distillation_loss = self.distillation_loss_fn(<br>                tf.nn.softmax(teacher_predictions / self.temperature, axis=<span class="hljs-number">1</span>),<br>                tf.nn.softmax(student_predictions / self.temperature, axis=<span class="hljs-number">1</span>),<br>            )<br>            loss = self.alpha * student_loss + (<span class="hljs-number">1</span> - self.alpha) * distillation_loss<br><br>        <span class="hljs-comment"># Compute gradients</span><br>        trainable_vars = self.student.trainable_variables<br>        gradients = tape.gradient(loss, trainable_vars)<br><br>        <span class="hljs-comment"># Update weights</span><br>        self.optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, trainable_vars))<br><br>        <span class="hljs-comment"># Update the metrics configured in `compile()`.</span><br>        self.compiled_metrics.update_state(y, student_predictions)<br><br>        <span class="hljs-comment"># Return a dict of performance</span><br>        results = &#123;m.name: m.result() <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.metrics&#125;<br>        results.update(<br>            &#123;<span class="hljs-string">&quot;student_loss&quot;</span>: student_loss, <span class="hljs-string">&quot;distillation_loss&quot;</span>: distillation_loss&#125;<br>        )<br>        <span class="hljs-keyword">return</span> results<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_step</span>(<span class="hljs-params">self, data</span>):<br>        <span class="hljs-comment"># Unpack the data</span><br>        x, y = data<br><br>        <span class="hljs-comment"># Compute predictions</span><br>        y_prediction = self.student(x, training=<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-comment"># Calculate the loss</span><br>        student_loss = self.student_loss_fn(y, y_prediction)<br><br>        <span class="hljs-comment"># Update the metrics.</span><br>        self.compiled_metrics.update_state(y, y_prediction)<br><br>        <span class="hljs-comment"># Return a dict of performance</span><br>        results = &#123;m.name: m.result() <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.metrics&#125;<br>        results.update(&#123;<span class="hljs-string">&quot;student_loss&quot;</span>: student_loss&#125;)<br>        <span class="hljs-keyword">return</span> results<br></code></pre></td></tr></table></figure><hr><h2 id="创建学生和教师模型"><a href="#创建学生和教师模型" class="headerlink" title="创建学生和教师模型"></a>创建学生和教师模型</h2><p>最初，我们创建了一个教师模型和一个较小的学生模型。这两个模型都是卷积神经网络，使用<code>Sequential()</code>，但可以是任何 Keras 模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Create the teacher</span><br>teacher = keras.Sequential(<br>    [<br>        keras.Input(shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)),<br>        layers.Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=<span class="hljs-string">&quot;same&quot;</span>),<br>        layers.LeakyReLU(alpha=<span class="hljs-number">0.2</span>),<br>        layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">&quot;same&quot;</span>),<br>        layers.Conv2D(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=<span class="hljs-string">&quot;same&quot;</span>),<br>        layers.Flatten(),<br>        layers.Dense(<span class="hljs-number">10</span>),<br>    ],<br>    name=<span class="hljs-string">&quot;teacher&quot;</span>,<br>)<br><br><span class="hljs-comment"># Create the student</span><br>student = keras.Sequential(<br>    [<br>        keras.Input(shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)),<br>        layers.Conv2D(<span class="hljs-number">16</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=<span class="hljs-string">&quot;same&quot;</span>),<br>        layers.LeakyReLU(alpha=<span class="hljs-number">0.2</span>),<br>        layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">&quot;same&quot;</span>),<br>        layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=<span class="hljs-string">&quot;same&quot;</span>),<br>        layers.Flatten(),<br>        layers.Dense(<span class="hljs-number">10</span>),<br>    ],<br>    name=<span class="hljs-string">&quot;student&quot;</span>,<br>)<br><br><span class="hljs-comment"># Clone student for later comparison</span><br>student_scratch = keras.models.clone_model(student)<br></code></pre></td></tr></table></figure><h2 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h2><p>用于训练教师和提炼教师的数据集是 <a href="https://keras.io/api/datasets/mnist/">MNIST</a>，该过程对于任何其他数据集都是等效的，例如<a href="https://keras.io/api/datasets/cifar10/">CIFAR-10</a>，具有合适的模型选择。学生和教师都在训练集上接受训练，并在测试集上进行评估。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Prepare the train and test dataset.</span><br>batch_size = <span class="hljs-number">64</span><br>(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()<br><br><span class="hljs-comment"># Normalize data</span><br>x_train = x_train.astype(<span class="hljs-string">&quot;float32&quot;</span>) / <span class="hljs-number">255.0</span><br>x_train = np.reshape(x_train, (-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))<br><br>x_test = x_test.astype(<span class="hljs-string">&quot;float32&quot;</span>) / <span class="hljs-number">255.0</span><br>x_test = np.reshape(x_test, (-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><h2 id="Train-the-teacher"><a href="#Train-the-teacher" class="headerlink" title="Train the teacher"></a>Train the teacher</h2><p>在知识蒸馏中，我们假设老师是经过培训和固定的。因此，我们首先以通常的方式在训练集上训练教师模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Train teacher as usual</span><br>teacher.<span class="hljs-built_in">compile</span>(<br>    optimizer=keras.optimizers.Adam(),<br>    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),<br>    metrics=[keras.metrics.SparseCategoricalAccuracy()],<br>)<br><br><span class="hljs-comment"># Train and evaluate teacher on data.</span><br>teacher.fit(x_train, y_train, epochs=<span class="hljs-number">5</span>)<br>teacher.evaluate(x_test, y_test)<br>Epoch <span class="hljs-number">1</span>/<span class="hljs-number">5</span><br><span class="hljs-number">1875</span>/<span class="hljs-number">1875</span> [==============================] - 248s 132ms/step - loss: <span class="hljs-number">0.2438</span> - sparse_categorical_accuracy: <span class="hljs-number">0.9220</span><br>Epoch <span class="hljs-number">2</span>/<span class="hljs-number">5</span><br><span class="hljs-number">1875</span>/<span class="hljs-number">1875</span> [==============================] - 263s 140ms/step - loss: <span class="hljs-number">0.0881</span> - sparse_categorical_accuracy: <span class="hljs-number">0.9738</span><br>Epoch <span class="hljs-number">3</span>/<span class="hljs-number">5</span><br><span class="hljs-number">1875</span>/<span class="hljs-number">1875</span> [==============================] - 245s 131ms/step - loss: <span class="hljs-number">0.0650</span> - sparse_categorical_accuracy: <span class="hljs-number">0.9811</span><br>Epoch <span class="hljs-number">5</span>/<span class="hljs-number">5</span><br> <span class="hljs-number">363</span>/<span class="hljs-number">1875</span> [====&gt;.........................] - ETA: <span class="hljs-number">3</span>:<span class="hljs-number">18</span> - loss: <span class="hljs-number">0.0555</span> - sparse_categorical_accuracy: <span class="hljs-number">0.9839</span><br></code></pre></td></tr></table></figure><h2 id="Distill-teacher-to-student"><a href="#Distill-teacher-to-student" class="headerlink" title="Distill teacher to student"></a>Distill teacher to student</h2><p>我们已经训练了教师模型，我们只需要初始化一个 <code>Distiller(student, teacher)</code>实例，<code>compile()</code>它具有所需的损失、超参数和优化器，并将教师提炼给学生。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Initialize and compile distiller</span><br>distiller = Distiller(student=student, teacher=teacher)<br>distiller.<span class="hljs-built_in">compile</span>(<br>    optimizer=keras.optimizers.Adam(),<br>    metrics=[keras.metrics.SparseCategoricalAccuracy()],<br>    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),<br>    distillation_loss_fn=keras.losses.KLDivergence(),<br>    alpha=<span class="hljs-number">0.1</span>,<br>    temperature=<span class="hljs-number">10</span>,<br>)<br><br><span class="hljs-comment"># Distill teacher to student</span><br>distiller.fit(x_train, y_train, epochs=<span class="hljs-number">3</span>)<br><br><span class="hljs-comment"># Evaluate student on test dataset</span><br>distiller.evaluate(x_test, y_test)<br>Epoch <span class="hljs-number">1</span>/<span class="hljs-number">3</span><br><span class="hljs-number">1875</span>/<span class="hljs-number">1875</span> [==============================] - 242s 129ms/step - sparse_categorical_accuracy: <span class="hljs-number">0.9761</span> - student_loss: <span class="hljs-number">0.1526</span> - distillation_loss: <span class="hljs-number">0.0226</span><br>Epoch <span class="hljs-number">2</span>/<span class="hljs-number">3</span><br><span class="hljs-number">1875</span>/<span class="hljs-number">1875</span> [==============================] - 281s 150ms/step - sparse_categorical_accuracy: <span class="hljs-number">0.9863</span> - student_loss: <span class="hljs-number">0.1384</span> - distillation_loss: <span class="hljs-number">0.0185</span><br>Epoch <span class="hljs-number">3</span>/<span class="hljs-number">3</span><br> <span class="hljs-number">399</span>/<span class="hljs-number">1875</span> [=====&gt;........................] - ETA: <span class="hljs-number">3</span>:<span class="hljs-number">27</span> - sparse_categorical_accuracy: <span class="hljs-number">0.9896</span> - student_loss: <span class="hljs-number">0.1300</span> - distillation_loss: <span class="hljs-number">0.0182</span><br></code></pre></td></tr></table></figure><h2 id="从头开始训练学生进行比较"><a href="#从头开始训练学生进行比较" class="headerlink" title="从头开始训练学生进行比较"></a>从头开始训练学生进行比较</h2><p>我们还可以在没有老师的情况下从头开始训练一个等效的学生模型，以评估通过知识蒸馏获得的性能提升。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Train student as doen usually</span><br>student_scratch.<span class="hljs-built_in">compile</span>(<br>    optimizer=keras.optimizers.Adam(),<br>    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),<br>    metrics=[keras.metrics.SparseCategoricalAccuracy()],<br>)<br><br><span class="hljs-comment"># Train and evaluate student trained from scratch.</span><br>student_scratch.fit(x_train, y_train, epochs=<span class="hljs-number">3</span>)<br>student_scratch.evaluate(x_test, y_test)<br>Epoch <span class="hljs-number">1</span>/<span class="hljs-number">3</span><br><span class="hljs-number">1875</span>/<span class="hljs-number">1875</span> [==============================] - 4s 2ms/step - loss: <span class="hljs-number">0.4731</span> - sparse_categorical_accuracy: <span class="hljs-number">0.8550</span><br>Epoch <span class="hljs-number">2</span>/<span class="hljs-number">3</span><br><span class="hljs-number">1875</span>/<span class="hljs-number">1875</span> [==============================] - 4s 2ms/step - loss: <span class="hljs-number">0.0966</span> - sparse_categorical_accuracy: <span class="hljs-number">0.9710</span><br>Epoch <span class="hljs-number">3</span>/<span class="hljs-number">3</span><br><span class="hljs-number">1875</span>/<span class="hljs-number">1875</span> [==============================] - 4s 2ms/step - loss: <span class="hljs-number">0.0750</span> - sparse_categorical_accuracy: <span class="hljs-number">0.9773</span><br><span class="hljs-number">313</span>/<span class="hljs-number">313</span> [==============================] - 0s 963us/step - loss: <span class="hljs-number">0.0691</span> - sparse_categorical_accuracy: <span class="hljs-number">0.9778</span><br><br>[<span class="hljs-number">0.06905383616685867</span>, <span class="hljs-number">0.9778000116348267</span>]<br></code></pre></td></tr></table></figure><p>如果教师接受了 5 个完整的 epochs 训练，而学生在这个教师身上被提炼了 3 个完整的 epochs，那么在这个例子中，与从头开始训练相同的学生模型相比，甚至与教师本身相比，都得到了性能提升。</p>]]></content>
    
    
    
    <tags>
      
      <tag>知识蒸馏</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>算法工程师工作心得</title>
    <link href="/2021/07/22/2021-07-22-%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%B7%A5%E4%BD%9C%E5%BF%83%E5%BE%97/"/>
    <url>/2021/07/22/2021-07-22-%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%B7%A5%E4%BD%9C%E5%BF%83%E5%BE%97/</url>
    
    <content type="html"><![CDATA[<p>转载自《<a href="https://mp.weixin.qq.com/s/rJkSGJNpMTRpqXumd8jPsw">心法利器[31] | 我的算法工程师日常</a>》</p><p>大家对算法工程师有这么几个误区：</p><ul><li>算法工程师每天就是调调包就好了，工资还赚的很多。</li><li>模型一训练，剩下的时间就能摸鱼了。</li><li>整天就是调模型就好了，脱离业务。</li></ul><p>其实这些误区都是围绕着模型来走的，很多人会以为算法只是在做模型，当然这个和很多媒体号聊的风向有关，但其实并非如此，模型工作只是占的很小的一个比例，大家一定要有这个预期。</p><h1 id="工作总结"><a href="#工作总结" class="headerlink" title="工作总结"></a>工作总结</h1><h2 id="大体工作有哪些"><a href="#大体工作有哪些" class="headerlink" title="大体工作有哪些"></a>大体工作有哪些</h2><p>算法工程师的主要职责就是负责一个或大或小的算法模块，对效果，对整体指标，对整个算法模块的可靠性稳定性负责的一个职能岗位，大到整个语音助手，小到一个技能的问答模块，我们就是搭建这样一个模块的工种，那么日常都有那些工作？</p><ul><li>需求和目标的沟通。</li><li>数据等资源的汇总和整理。</li><li>算法的落地实践和效果调优。</li><li>工程模块搭建。</li><li>技术沉淀和输出。</li><li><p>摸鱼和生活。</p><span id="more"></span></li></ul><h2 id="需求和目标的沟通"><a href="#需求和目标的沟通" class="headerlink" title="需求和目标的沟通"></a>需求和目标的沟通</h2><p>通常是和产品沟通，产品负责指挥整个产品的发展方向，对于我们而言就是和我们一起定义好一些目标和标准，例如”什么query应该被认定是天气“，更多的还有很多模糊的问题，最直接的其实就是一个“什么叫做好？”，这是个非常复杂的问题，很多交叉的内容需要判断该分给谁更合理，这个过程往往会比较困难，主要因为两者的信息不对称，产品不懂技术，技术不懂产品，这是问题的根源，而要解决这个问题还是就是要多沟通多交流，甚至不要等着别人来组织，而是自己主动把大家拉上讨论清楚。</p><p>越是新的项目，前期这块花的时间就会多很多，毕竟要同步大家的想法，把自己的想法完整、准确地告知对方，真的很难。</p><h2 id="数据等资源的汇总和整理"><a href="#数据等资源的汇总和整理" class="headerlink" title="数据等资源的汇总和整理"></a>数据等资源的汇总和整理</h2><p>我们都知道，算法研发是一个极度依赖数据的，对现象的分析，模型训练，效果评估，都需要大量的数据，为了构造这些数据，我们需要花费大量的经历去获取。</p><ul><li>通用的，开源的，可以直接下载。</li><li>部分在日志、埋点里的，那就要写sql拿。很多人所谓的“sql boy”的梗就来源于此。</li><li>没有标注的，需要和标注人员沟通合作，这里又是一波漫长的沟通和等待了。</li><li>啥都没有的，爬虫可能要试试，在合法的前提下。</li></ul><p>另外我们还要保障数据的正确性，数据量覆盖现实的情况，分布和口径是否一致等等，尤其是评测集，对这些问题的要求尤其高，有的时候数据分布不对，直接导致在线问题的出现，所以要小心谨慎。</p><p>为了评估好效果，为了能训练模型，而且数据处理其实非常繁琐而不具有通用型，所以时间消耗其实非常大。</p><h2 id="算法的落地实践和调优"><a href="#算法的落地实践和调优" class="headerlink" title="算法的落地实践和调优"></a>算法的落地实践和调优</h2><p>这才到了算法的落地实践和调优，也是大家最期待和喜欢的环节，但其实这个工作时间占比并不高。来看看都在干那些事</p><ul><li>各种方案的调研，这里包括看论文了。</li><li>技术方案的设计，包括模型的各种实验方案。</li><li>构造模型，进行实验。说实话，真正有用以后，一般不会太去动了，所以一般是初创的项目更可能有模型的任务。</li><li>效果调优，简单的、紧急的，一般就是用规则、词典来处理，一般不是到了天花板，很少会主动去处理基线模型的。</li><li>bad case分析，这个其实非常花时间，需要标注数据，同时一些归因之类的是需要思考的，不过我自己其实挺喜欢的，毕竟这其实是在积累自己对数据的理解，这是书上没有网上没有的东西。</li></ul><p>算法调优其实是一个很零散的流程，能拿到完整的效果优化时间一定要珍惜。这里也要和很多以为算法工程师只有调模型的人说一下，其实这块的工作很少。</p><h2 id="工程模块搭建"><a href="#工程模块搭建" class="headerlink" title="工程模块搭建"></a>工程模块搭建</h2><p>越是前期的项目，算法所需要承担的工程任务就越多，越是后期成熟的项目，算法可以专注算法的时间越多。因为项目初期，各种基础工作都没有，无论是工程本身还是各种数据，这会导致我们需要很多时间花在这里，尤其是工程（毕竟算法可以用规则哈哈哈），算法服务、日志，如果需要一些更新，那还有更新模块，甚至还包括一些预处理的工作（这里强烈建议大家自己构造一些框架、一些基础功能，是可以保留下来了）。</p><p>这个其实非常锻炼人的综合能力，很多事情自己能做很多，这个修炼是很有利于技术广度的，毕竟我们也不希望永远做算法吧，拓宽广度增加自己的综合能力还是挺关键的。</p><h2 id="技术沉淀和输出"><a href="#技术沉淀和输出" class="headerlink" title="技术沉淀和输出"></a>技术沉淀和输出</h2><p>要想进步，可以靠实践，但如果实践之后能总结和沉淀，就能让收获进一步，所以我自己本身会花大量的时间在这里，这也是我能持续成长最充足的动力。</p><ul><li>总结和复盘，总结本周自己的各种进展，有哪些做得好，那些不好，从而得到经验提升。</li><li>每天花点时间看看case，看看自己解决的情况，这个有利于看自己理解问题，找到问题点解决问题。</li><li>输出点东西，这就是我自己的必修课了，也就是大家每周看到的文章了。</li></ul><h2 id="摸鱼和生活"><a href="#摸鱼和生活" class="headerlink" title="摸鱼和生活"></a>摸鱼和生活</h2><p>高强度的工作正常人肯定受不了，稍微有些摸鱼肯定是非常幸福的，其实我非常建议大家也能找到自己的乐趣，我自己喜欢喝咖啡、打游戏之类的，这些事情能让自己从工作中快速跳出，清理大脑，其实非常有用。</p><h1 id="展望后续计划"><a href="#展望后续计划" class="headerlink" title="展望后续计划"></a>展望后续计划</h1><p>有所深入，例如我做NLP，尤其是NLU这块，那文本分类、NER、语义相似度之类的常见方法和前沿技术肯定都要了解，丰富自己的武器库，在解决各种问题的时候有足够高的效率，同时对对应任务的数据、常出现的问题要有比较深入的了解，自己一直保持一个习惯，就是每天都要看一些case，专题各有不同，在线的随机query、目前方案做不到的bad case、专题某个领域的专题样本等，所谓的经验其实就是见过一些问题，可以预判会出哪些问题然后知道怎么去解决。</p><p>有些广度，了解一些和自己所熟悉的方向接近的事情，甚至还有些深入。例如即使做的是NLP，因为自己做的搜索所以还是要对排序有些了解，如果自己做的对话，那对对话管理一套还是要了解的，技术上，则一些通用的存储和检索工具，如mysql、redis等，甚至到hive、spark一系列的技术，这里就不是局限在“学python还是学c++”的问题了，有时间、有需要随学随用，重要的是不给自己设限，不要认为这是XXX的工作所以我可以不了解，这就限制了自己了。</p><p>深度让自己能更好的解决一个问题，而广度让自己能解决更多问题。</p><p>列一下未来的修炼方案吧：</p><ul><li>论文的阅读是算法工程师的必修课，论文是要持续读的。</li><li>多了解业界的关注点。虽然要走自己的路，但是了解大家的关注点，互相学习启发才能进步。</li><li>保持交流和输出，互相碰撞、互相切磋是提升的重要手段。</li><li>会解决问题，能分析出一个目前效果不佳的原因并进行优化（绝对不是调参换模型那么简单！）</li><li>性能意识，知道自己用的方法性能是什么样的，是否符合需求。</li><li>内化，可以拿来，但是一定要懂，这是对自己负责，对未来要遇到的问题负责。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;mid=2247487108&amp;idx=1&amp;sn=564f163b68fa7b36d442530670a3efc6&amp;chksm=e882561adff5df0ce1d62c18c8fa1fd8dd61586094f3bf1b21e1c78d9bada70a44c96cbb094d&amp;scene=21#wechat_redirect">心法利器[30] | 算法新人如何在工作中成长</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>算法工程师</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker容器</title>
    <link href="/2021/07/14/2021-07-14-Docker%E5%AE%B9%E5%99%A8/"/>
    <url>/2021/07/14/2021-07-14-Docker%E5%AE%B9%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1677137511769-fa103057-da12-4497-9c88-adf482b1cd5f.png#averageHue=%23fefefe&amp;clientId=u136d91e2-d72b-4&amp;from=paste&amp;height=165&amp;id=uf3d996d4&amp;name=image.png&amp;originHeight=330&amp;originWidth=660&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=true&amp;size=35320&amp;status=done&amp;style=none&amp;taskId=u70dfb01f-1cc8-4685-a4ad-d4904cc47ac&amp;title=Docker%20%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B-%20%E9%98%AE%E4%B8%80%E5%B3%B0%E7%9A%84%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97&amp;width=330" alt="image.png" title="Docker 入门教程- 阮一峰的网络日志"><br><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1677138112349-419e4884-c264-430e-93ba-541e299eefca.png#averageHue=%23eeeed2&amp;clientId=u136d91e2-d72b-4&amp;from=paste&amp;height=337&amp;id=ub4e1eeb5&amp;name=image.png&amp;originHeight=674&amp;originWidth=1836&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=277838&amp;status=done&amp;style=none&amp;taskId=ud4d9db4e-19a6-4c68-ab18-4ac133e08e4&amp;title=&amp;width=918" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1677138126480-2669662e-e43a-4fd1-9c3a-9c23d4cdc281.png#averageHue=%23f0f0d3&amp;clientId=u136d91e2-d72b-4&amp;from=paste&amp;height=519&amp;id=u12f28aa2&amp;name=image.png&amp;originHeight=1038&amp;originWidth=1846&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=337339&amp;status=done&amp;style=none&amp;taskId=u2f5e8306-1c2b-4b6a-a09b-0ff90b7e71c&amp;title=&amp;width=923" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1677138137374-c89fb2cb-5483-44d8-a3a3-19caa49af5ab.png#averageHue=%23f0f0d4&amp;clientId=u136d91e2-d72b-4&amp;from=paste&amp;height=603&amp;id=u62db3551&amp;name=image.png&amp;originHeight=1206&amp;originWidth=1946&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=394705&amp;status=done&amp;style=none&amp;taskId=u4b367788-a495-4efd-bc84-31f6ed7d9ad&amp;title=&amp;width=973" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1677138152913-ae9474f0-2c28-4bb8-991c-53448a7368c5.png#averageHue=%23f0f0d3&amp;clientId=u136d91e2-d72b-4&amp;from=paste&amp;height=538&amp;id=ud269fea8&amp;name=image.png&amp;originHeight=1076&amp;originWidth=1894&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=349792&amp;status=done&amp;style=none&amp;taskId=ubc0f50e6-19e8-47d3-9da2-ad3b0fed2a2&amp;title=&amp;width=947" alt="image.png"></p><h2 id="Docker基本概念"><a href="#Docker基本概念" class="headerlink" title="Docker基本概念"></a>Docker基本概念</h2><ul><li>镜像（Image）</li></ul><p>Docker 镜像 是一个特殊的文件系统，<strong>除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）</strong>。镜像 不包含 任何动态数据，其内容在构建之后也不会被改变。</p><ul><li>容器（Container）</li></ul><p>镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样;<br>1、镜像是静态的定义，容器是镜像运行时的实体;<br>2、容器可以被创建、启动、停止、删除、暂停等。</p><ul><li>仓库（Repository）</li></ul><p>镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。一个 <strong>Docker Registry 中可以包含多个 仓库（Repository）</strong>；<strong>每个仓库可以包含多个 标签（Tag）；每个标签对应一个镜像</strong>。</p><h2 id="Docker-安装"><a href="#Docker-安装" class="headerlink" title="Docker 安装"></a>Docker 安装</h2><p>why?Because Docker 可以方便的安装深度学习环境（cuda+cudnn+tensorflow-gpu/pytorch），一键打包成镜像，解决不同电脑下软硬件的不同导致各种bug 满天飞的痛点！</p><h3 id="1-Docker的安装"><a href="#1-Docker的安装" class="headerlink" title="1. Docker的安装"></a>1. Docker的安装</h3><blockquote><p>安装链接：<a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/">https://docs.docker.com/install/linux/docker-ce/ubuntu/</a><br>跟着教程一路复制粘贴回车即可。<br>唯一的难点就是看懂英文的安装教程，看清楚段落层次结构。</p></blockquote><p>反正，最后如果你运行sudo docker run hello-world，可以跑通，看到：<br><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/8420697/1647329117433-b7dc5182-1290-402e-9a7c-0c9c01676262.jpeg#averageHue=%233a1d32&amp;clientId=u0e11c900-2139-4&amp;from=paste&amp;id=u26d6483a&amp;originHeight=318&amp;originWidth=733&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u1dbbee30-57c5-4ff6-b66e-134ea518dee&amp;title=" alt=""><br>就说明Docker已经被你成功安装了！</p><p>Docker 是服务器——客户端架构。命令行运行docker命令的时候，需要本机有 Docker 服务。如果这项服务没有启动，可以用下面的命令启动（<a href="https://docs.docker.com/config/daemon/systemd/">官方文档</a>）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">·<span class="hljs-comment"># service 命令的用法</span><br>$ sudo service docker start<br><br><span class="hljs-comment"># systemctl 命令的用法</span><br>$ sudo systemctl start docker<br></code></pre></td></tr></table></figure></p><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><ol><li>Nvidia-docker的安装<br>为何又蹦出来一个nvidia-docker？由于默认安装的<a href="https://so.csdn.net/so/search?q=docker&amp;spm=1001.2101.3001.7020">docker</a>都是基于cpu版本的，如果想要配合GPU进行一些简单的部署的话，则需要安装nvidia-docker来支持GPU加速。所以NVIDIA单独做了一个docker，来让docker镜像可以使用NVIDIA的gpu。</li></ol><p>链接：<a href="https://github.com/NVIDIA/nvidia-docker">https://github.com/NVIDIA/nvidia-docker</a></p><p>也是直接找对应的操作系统的命令，一行行复制粘贴回车就搞定了。<br>反正，最后当你运行docker run —runtime=nvidia —rm nvidia/cuda:9.0-base nvidia-smi时，如果看到：<br><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/8420697/1647329117442-cc95c5af-c2d7-498c-9163-2d7aaee7b553.jpeg#averageHue=%233a1d32&amp;clientId=u0e11c900-2139-4&amp;from=paste&amp;id=u1ae41082&amp;originHeight=357&amp;originWidth=721&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=uf3f10409-3edd-4b7c-8e0f-0c96a3ac025&amp;title=" alt=""><br>恭喜，安装成功了！</p><h3 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h3><p>PS: 深度学习镜像的安装<br>我这里使用镜像是<strong>deepo</strong>一款咱们中国人做出来的深度学习镜像，包含了现在多数流行的深度学习框架，而且版本也很新，所以我这个小白第一次就选择了这个。</p><p>链接：<a href="https://hub.docker.com/r/ufoym/deepo">https://hub.docker.com/r/ufoym/deepo</a></p><p>只要安装好了前面的docker和nvidia-docker，这里就很方便了。<br>直接通过命令docker pull ufoym/deepo就可以把各种框架都下载下来。但是这样比较大，费时较长，所以教程里面也提供了只安装其中某一种框架的方式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">docker pull ufoym/deepo:tensorflow<br></code></pre></td></tr></table></figure></p><p>另外，还提供了jupyter notebook版的镜像，我这里就是安装的这个，因为我日常基本都是使用jupyter notebook，这里贴一下我的命令：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sudo docker pull ufoym/deepo:<span class="hljs-built_in">all</span>-jupyter-py36-cu100<br></code></pre></td></tr></table></figure><br>这里的all-jupyter-py36-cu100也是deepo提供的jupyter notebook镜像的tag。<br>安装好之后，通过docker images命令，可以查看已经下载好的镜像：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/8420697/1647332304019-9bdbdc22-7ce1-405c-8a1a-082eac67e311.png#averageHue=%23373737&amp;clientId=uff743bcd-f367-4&amp;from=paste&amp;height=98&amp;id=ufd9021ff&amp;name=image.png&amp;originHeight=196&amp;originWidth=1156&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=60425&amp;status=done&amp;style=none&amp;taskId=u63545701-bc94-4a71-8350-41831b5dd93&amp;title=&amp;width=578" alt="image.png"><br>好了，该装的东西都装好了，下面进入操作部分了！</p><h3 id="PS-Docker-拉取镜像比较慢的解决方法"><a href="#PS-Docker-拉取镜像比较慢的解决方法" class="headerlink" title="PS: Docker 拉取镜像比较慢的解决方法"></a>PS: <a href="https://blog.csdn.net/uknow0904/article/details/105860129">Docker 拉取镜像比较慢的解决方法</a></h3><p>解决方法是  在/etc/docker文件夹下 修改daemon.json ,如果不存在这样的文件 新建一个即可.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">cd /etc/docker<br>sudo vim daemon.json<br></code></pre></td></tr></table></figure><br>然后编辑文件内容<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<br>    <span class="hljs-string">&quot;registry-mirrors&quot;</span>:[<br>        <span class="hljs-string">&quot;https://9cpn8tt6.mirror.aliyuncs.com&quot;</span>,<br>        <span class="hljs-string">&quot;https://registry.docker-cn.com&quot;</span><br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">sudo service docker restart<br>sudo systemctl status docker<br><br><span class="hljs-comment"># 以下命令还未尝试过</span><br>sudo systemctl enable docker <span class="hljs-comment"># 开机自动启动docker</span><br><br>sudo systemctl start docker <span class="hljs-comment"># 启动docker</span><br>sudo systemctl restart docker <span class="hljs-comment"># 重启dokcer</span><br>sudo systemctl daemon-reload<br></code></pre></td></tr></table></figure><br>重新试下docker pull 绝对速度飞起来~</p><h2 id="镜像入门篇"><a href="#镜像入门篇" class="headerlink" title="镜像入门篇"></a>镜像入门篇</h2><p>Docker 把应用程序及其依赖，打包在 image 文件里面。只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。</p><p>image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。</p><p>为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 Docker Hub 是最重要、最常用的 image 仓库。</p><h3 id="1、获取镜像"><a href="#1、获取镜像" class="headerlink" title="1、获取镜像"></a>1、获取镜像</h3><p>Docker 一般会将一些镜像放到 Docker Hub 上面，可以采用docker pull命令获取镜像：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">$ docker pull [选项] 镜像仓库地址:端口号/仓库名[:标签]<br></code></pre></td></tr></table></figure><br>实例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">docker pull ubuntu:<span class="hljs-number">18.04</span><br>docker pull nvidia/cuda:<span class="hljs-number">10.0</span>-base<br></code></pre></td></tr></table></figure><br>注：上面的命令中没有给出 Docker 镜像仓库地址，因此将会从 Docker Hub （docker.io）获取镜像。而镜像名称是 ubuntu:18.04，因此将会获取官方镜像 library/ubuntu 仓库中标签为 18.04 的镜像。docker pull 命令的输出结果最后一行给出了镜像的完整名称，即： docker.io/library/ubuntu:18.04。    </p><h3 id="2、列出所有镜像"><a href="#2、列出所有镜像" class="headerlink" title="2、列出所有镜像"></a>2、列出所有镜像</h3><p>可以 采用 以下命令 对想要的内容镜像内容进行查看<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 方式一 </span><br>$ docker image ls<br><span class="hljs-comment"># 方式二</span><br>$ docker images<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>output<br>REPOSITORY    TAG       IMAGE ID       CREATED       SIZE<br>ubuntu        latest    8e428cff54c8   <span class="hljs-number">2</span> weeks ago   <span class="hljs-number">72.9</span>MB<br>ubuntu        <span class="hljs-number">18.04</span>     3339fde08fc3   <span class="hljs-number">2</span> weeks ago   <span class="hljs-number">63.3</span>MB<br>hello-world   latest    d1165f221234   <span class="hljs-number">5</span> weeks ago   <span class="hljs-number">13.3</span>kB<br>alpine/git    latest    a939554ad0d0   <span class="hljs-number">7</span> weeks ago   <span class="hljs-number">25.1</span>MB<br></code></pre></td></tr></table></figure><br>注：在 上面内容中，我们可以查看 我们所下载的 镜像【REPOSITORY】，版本【TAG】，镜像 ID 【IMAGE ID】 ，镜像创建时间【CREATED】 和 大小【SIZE】</p><h3 id="3、删除镜像"><a href="#3、删除镜像" class="headerlink" title="3、删除镜像"></a>3、删除镜像</h3><p>我们可以采用以下 四种方式 删除镜像<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 删除 image 文件</span><br>$ docker image rm [imageName]<br></code></pre></td></tr></table></figure><br>注：一般可以用镜像的完整 ID，也称为 长ID，来删除镜像。使用脚本的时候可能会用长 ID，但是人工输入就太累了，所以更多的时候是用 短ID 来删除镜像。</p><h2 id="容器入门篇"><a href="#容器入门篇" class="headerlink" title="容器入门篇"></a>容器入门篇</h2><p>image 文件生成的容器实例，本身也是一个文件，称为容器文件。也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。</p><h3 id="新建容器"><a href="#新建容器" class="headerlink" title="新建容器"></a>新建容器</h3><p>docker container run 命令只在第一次运行镜像操作时使用，相当于执行了两步操作，将镜像放入容器中然后将容器启动；<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">docker run hello-world<br>docker run -it ubuntu bash<br></code></pre></td></tr></table></figure></p><h3 id="查看容器"><a href="#查看容器" class="headerlink" title="查看容器"></a>查看容器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查看所有容器</span><br>docker ps -a<br><br><span class="hljs-comment"># 查看后台运行的 5 个容器</span><br>docker ps -n <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><h3 id="删除容器"><a href="#删除容器" class="headerlink" title="删除容器"></a>删除容器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 删除容器文件</span><br>docker container rm container-<span class="hljs-built_in">id</span><br></code></pre></td></tr></table></figure><h3 id="启动-停止容器？"><a href="#启动-停止容器？" class="headerlink" title="启动/停止容器？"></a>启动/停止容器？</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 启动一个暂停的容器，使用该命令需要知道容器的id或者名字</span><br>docker start [-i] container-<span class="hljs-built_in">id</span><br><br><span class="hljs-comment"># 重启一个正在运行的容器</span><br>docker restart container-<span class="hljs-built_in">id</span><br><br><span class="hljs-comment"># 停止一个运行的容器(本质上是向该进程发送一个SIGTERM信号)</span><br>docker stop container-<span class="hljs-built_in">id</span><br><br><span class="hljs-comment"># 快速停止容器</span><br>docker kill container-<span class="hljs-built_in">id</span><br></code></pre></td></tr></table></figure><h3 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h3><h3 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h3><p>某些时候需要进入容器进行操作，可以使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令，原因是从这个 stdin 中 exit回到host端，不会导致容器的停止！</p><p><strong>① attach 命令：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">$ docker run -dit image-<span class="hljs-built_in">id</span> <br>$ docker container ls<br>$ docker attach container-<span class="hljs-built_in">id</span><br></code></pre></td></tr></table></figure><br><strong>② exec 命令：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">$ docker run -dit ubuntu <br>$ docker container ls<br>$ docker <span class="hljs-built_in">exec</span> -it 69d1 bash<br></code></pre></td></tr></table></figure></p><ul><li>-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上；</li><li>-i 则让容器的标准输入保持打开</li></ul><blockquote><p><strong>attach 和 exec 的区别：</strong></p><ol><li>attach直接进入容器启动命令的终端，不会启动新的进程；</li><li>exec则是在容器中打开新的终端，并且可以启动新的进程；</li><li>如果想直接在终端中查看命令的输出，用attach，其他情况使用exec；</li></ol></blockquote><h3 id="退出-离开容器"><a href="#退出-离开容器" class="headerlink" title="退出/离开容器"></a>退出/离开容器</h3><p>进入交互模式之后，怎么<strong>退出</strong>呢：</p><ul><li>想退出但是保持容器运行，按 CTRL+P &amp;&amp; CTRL+Q </li><li>退出，并关闭停止容器，按CTRL+D或者输入exit再回车</li></ul><h3 id="导出-导入容器"><a href="#导出-导入容器" class="headerlink" title="导出/导入容器"></a>导出/导入容器</h3><p><strong>导出容器：</strong>如果要导出本地某个容器，可以使用 <strong>docker export</strong> 命令，导出容器 1e560fca3906 快照到本地文件 ubuntu.tar<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">$ docker export 1e560fca3906 &gt; ubuntu.tar<br></code></pre></td></tr></table></figure><br><strong>导入容器快照：</strong>可以使用 docker import 从容器快照文件中再导入为镜像，以下实例将快照文件 ubuntu.tar 导入到镜像 test/ubuntu:v1:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">$ cat docker/ubuntu.tar | docker <span class="hljs-keyword">import</span> - test/ubuntu:v1<br></code></pre></td></tr></table></figure></p><h3 id="ps-Docker容器保存为镜像文件"><a href="#ps-Docker容器保存为镜像文件" class="headerlink" title="ps: Docker容器保存为镜像文件"></a>ps: Docker容器保存为镜像文件</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/348849578">https://zhuanlan.zhihu.com/p/348849578</a></p></blockquote><p>运行一个docker container，然后在其中进行定制化（安装、配置服务等）之后，将其打包成镜像，方便迁移至其他机器，快速搭建回之前的环境！！</p><p>方法 1：docker commit（方便）<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 将容器打包成镜像</span><br><span class="hljs-comment"># docker commit 容器id 镜像起名:版本号</span><br><span class="hljs-attribute">docker</span> commit d5944567401a mssql-<span class="hljs-number">2019</span>-with-cimb:<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><br><span class="hljs-comment"># 将镜像保存为本地文件</span><br><span class="hljs-comment"># docker save -o 本地文件名称 要打包的镜像名称:版本号</span><br><span class="hljs-attribute">docker</span> save -o mssql-<span class="hljs-number">2019</span>-with-cimb.tar mssql-<span class="hljs-number">2019</span>-with-cimb<br><br><span class="hljs-comment"># 从文件载入镜像</span><br><span class="hljs-attribute">docker</span> load --input 本地文件名称<br><span class="hljs-attribute">docker</span> images <br></code></pre></td></tr></table></figure></p><p>方法 2：docker build （推荐）<br>使用 Dockerfile 文件自动化制作 image<br><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs excel">docker build -f ./Dockerfile -<span class="hljs-built_in">t</span> 镜像名称<span class="hljs-symbol">:</span>版本号<br></code></pre></td></tr></table></figure></p><h2 id="实例：56"><a href="#实例：56" class="headerlink" title="实例：56"></a>实例：56</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">docker run -it -d -p <span class="hljs-number">10086</span>:<span class="hljs-number">10086</span>  --name nsx_cuda -v /data/ningshixian:/data/ningshixian 镜像<span class="hljs-built_in">id</span> /<span class="hljs-built_in">bin</span>/bash<br><br>nvidia-docker run -it -d -p <span class="hljs-number">10086</span>:<span class="hljs-number">10086</span> --name=nsx_cuda -v /data/ningshixian:/data/ningshixian 镜像<span class="hljs-built_in">id</span> /<span class="hljs-built_in">bin</span>/bash<span class="hljs-comment"># --ipc=host --runtime=nvidia</span><br></code></pre></td></tr></table></figure><p>注：</p><ul><li>-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上；</li><li>-i 交互式操作</li><li>-d 容器启动后会进入后台，启动完容器之后会停在host端；</li><li>-p 端口映射</li><li>—name 自命名启动一个容器 </li><li>-v /data/ningshixian:/data/ningshixian 可以将主机上的/data/ningshixian 地址挂载到容器里，并命名为/data/ningshixian 文件夹，这样这个文件夹的内容可以在容器和主机之间共享了。因为容器一旦关闭，容器中的所有改动都会清除，所以这样挂载一个地址可以吧容器内的数据保存到本地。</li><li>ubuntu: 基于ubuntu image创建 container</li><li>0bedd0dfd4cb 则是你安装的 nvidia/cuda:9.0-base 镜像的id</li><li>/bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。</li></ul><p>新建好容器之后，就可以进行自定义的环境配置了：</p><blockquote><p><a href="https://segmentfault.com/a/1190000022562739">解决Ubuntu终端下载速度过慢问题</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 进入容器后执行</span><br>apt-get update<br><span class="hljs-comment"># apt-get upgrade</span><br>apt-get install vim git<br><br>wget https://repo.anaconda.com/archive/Anaconda3-<span class="hljs-number">2020.11</span>-Linux-x86_64.sh<br>wget -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-<span class="hljs-number">2020.11</span>-Linux-x86_64.sh<br><br><span class="hljs-comment"># 进入下载目录...</span><br>sudo bash Anaconda3-<span class="hljs-number">2020.11</span>-Linux-x86_64.sh<br><span class="hljs-comment"># Anaconda安装过程...记得添加环境变量yes</span><br><br><span class="hljs-comment"># 换清华源</span><br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge <br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/<br><span class="hljs-comment"># 设置搜索时显示通道地址</span><br>conda config --<span class="hljs-built_in">set</span> show_channel_urls yes<br><br><span class="hljs-comment"># 创建keras环境</span><br>conda create -n nsx_env python=<span class="hljs-number">3.7</span><br><span class="hljs-comment"># 切换环境</span><br>conda activate nsx_env<br><span class="hljs-comment"># 在Ubuntu中提供编译c/c++的环境(可略)</span><br>apt-get install build-essential -y<br><span class="hljs-comment"># conda install cudatoolkit=10.1 cudnn=7.6.5</span><br>conda install cudatoolkit=<span class="hljs-number">10.0</span> cudnn=<span class="hljs-number">7.6</span><span class="hljs-number">.5</span><br><span class="hljs-comment"># 装完就完事了</span><br>写个程序跑一下，缺啥再pip<br></code></pre></td></tr></table></figure><h2 id="Docker-jupyter-notebook-服务-力荐"><a href="#Docker-jupyter-notebook-服务-力荐" class="headerlink" title="Docker jupyter notebook 服务 [力荐!]"></a>Docker jupyter notebook 服务 [力荐!]</h2><h3 id="1-如何创建自己的可以远程访问的容器："><a href="#1-如何创建自己的可以远程访问的容器：" class="headerlink" title="1.如何创建自己的可以远程访问的容器："></a>1.如何创建自己的可以远程访问的容器：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sudo nvidia-docker run -it -p <span class="hljs-number">7777</span>:<span class="hljs-number">8888</span> --ipc=host -v /data/ningshixian:/data/ningshixian --name nsx-notebook  90be7604e476<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li>-it为直接进入交互式</li><li>-p 7777:8888是把主机的7777端口映射到容器的8888端口</li><li>-ipc=host可以让容器与主机共享内存</li><li>还可以加一个—name xxxxx给容器定义一个个性化名字</li><li>-v /data/ningshixian:/data/ningshixian 可以将主机上的/data/ningshixian 地址挂载到容器里，并命名为/data/ningshixian 文件夹，这样这个文件夹的内容可以在容器和主机之间共享了。因为容器一旦关闭，容器中的所有改动都会清除，所以这样挂载一个地址可以吧容器内的数据保存到本地。</li><li>90be7604e476则是你安装的jupyter镜像的id，可以在刚刚docker images命令后面查看，当然你也可以直接写全名ufoym/deepo:all-py36-jupyter</li></ul><h3 id="2-创建了容器之后，我们可以进而启动jupyter-notebook："><a href="#2-创建了容器之后，我们可以进而启动jupyter-notebook：" class="headerlink" title="2.创建了容器之后，我们可以进而启动jupyter notebook："></a>2.创建了容器之后，我们可以进而启动jupyter notebook：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">jupyter notebook --no-browser --ip=<span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span> --allow-root --NotebookApp.token= --notebook-<span class="hljs-built_in">dir</span>=<span class="hljs-string">&#x27;/data/ningshixian&#x27;</span><br></code></pre></td></tr></table></figure><p>其中：</p><ul><li>—no-browser即不通过浏览器启动，—ip指定容器的ip，—allow-root允许root模型运行</li><li>—NotebookApp.token可以指定jupyter 登录密码，可以为空</li><li>—notebook-dir=’/data/ningshixian’ 指定jupyter的根目录</li></ul><h3 id="3-开启本地与服务器的端口映射，从而远程登录jupyter："><a href="#3-开启本地与服务器的端口映射，从而远程登录jupyter：" class="headerlink" title="3.开启本地与服务器的端口映射，从而远程登录jupyter："></a>3.开启本地与服务器的端口映射，从而远程登录jupyter：</h3><p>在<strong>本地机器</strong>上，执行如下命令：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ssh username@host-ip -L <span class="hljs-number">1234</span>:<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">7777</span><br></code></pre></td></tr></table></figure><br>这样，可以将本地的1234端口，映射到服务器的localhost的7777端口（即你前面创建jupyter容器时候的指定的服务器端口）<br>这样，你在本地电脑的浏览器里输入’localhost:1234’，即可登录到服务器上的jupyter notebook了！<br><img src="https://cdn.nlark.com/yuque/0/2022/png/8420697/1647330225395-79dbeb64-bfab-400e-a311-edf2bd0adc82.png#averageHue=%235d3f56&amp;clientId=u0e11c900-2139-4&amp;from=paste&amp;id=u7b4badf1&amp;originHeight=41&amp;originWidth=267&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u8c7af15a-41e9-43fc-8e16-6e24f49af82&amp;title=" alt=""><br><img src="https://cdn.nlark.com/yuque/0/2022/png/8420697/1647330225450-25a455f9-2d49-4b6a-99f0-72d5a939538a.png#averageHue=%23f9f8f7&amp;clientId=u0e11c900-2139-4&amp;from=paste&amp;id=u6d8ba384&amp;originHeight=341&amp;originWidth=628&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=uf4c617a7-a36a-48f1-85f4-9bc2a3ea142&amp;title=" alt=""><br><strong>既能远程访问高性能服务器，又可以像在本地一样便捷地操作</strong>，你说激动不激动你说激动不激动？</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/km1994/docker_study/blob/main/Introduction.md">【关于 docker 安装】那些你不知道的事</a></li><li><a href="https://github.com/km1994/docker_study/blob/main/image_container.md">【关于 docker 镜像与容器】那些你不知道的事</a></li><li><a href="https://www.runoob.com/docker/docker-container-usage.html">Docker 容器使用 | 菜鸟教程</a></li><li><a href="https://mp.weixin.qq.com/s/QHwOQyoijSAuHb35GJLznw"><del>史上讲解最好的 Docker 教程，从入门到精通（建议收藏的教程）</del></a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247489624&amp;idx=2&amp;sn=649f5b8e3a4318f57b69ee26095b5fb3&amp;chksm=ebb4208cdcc3a99a8c3859db0e349a9ca5d36baef1dd1cefd7128ca2b6265b46d8fb46ce4978&amp;scene=0&amp;xtrack=1#rd">⭐️Docker，救你于「深度学习环境配置」的苦海</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>大数据技术支持</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kafka消息队列</title>
    <link href="/2021/07/14/2021-07-14-Kafka%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    <url>/2021/07/14/2021-07-14-Kafka%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/</url>
    
    <content type="html"><![CDATA[<h2 id="文章目录"><a href="#文章目录" class="headerlink" title="文章目录"></a>文章目录</h2><p>1、Kafka 为实时日志流而生，要处理的并发和数据量非常大。可见，Kafka 本身就是一个高并发系统，它必然会遇到高并发场景下典型的三高挑战：高性能、高可用和高扩展。</p><p>2、为了简化实现的复杂度，Kafka 最终采用了很巧妙的消息模型：它将所有消息进行了持久化存储，让消费者自己各取所需，想取哪个消息，想什么时候取都行，只需要传递一个消息的 offset 进行拉取即可。</p><p>3、从宏观角度解析 Kafka 的整体架构：为了解决存储的扩展性问题，Kafka 对数据进行了水平拆分，引出了 Partition（分区），通过 Partition 的多副本冗余机制进行故障转移，确保了高可靠。</p><span id="more"></span><h2 id="Kafka-是什么？"><a href="#Kafka-是什么？" class="headerlink" title="Kafka 是什么？"></a>Kafka 是什么？</h2><p>我们先看看 Kafka 官网给自己下的定义：</p><ul><li>Apache Kafka is an open-source distributed event streaming platform.</li></ul><p>翻译成中文就是：Apache Kafka 是一个开源的分布式流处理平台。</p><p>Kafka 不是一个消息系统吗?为什么被称为分布式的流处理平台呢?这两者是一回事吗?</p><p>一定有读者会有这样的疑问，要解释这个问题，需要先从 Kafka 的诞生背景说起。</p><p>Kafka 最开始其实是 Linkedin 内部孵化的项目，在设计之初是被当做「数据管道」，用于处理以下两种场景：</p><ol><li><p>运营活动场景：记录用户的浏览、搜索、点击、活跃度等行为。</p></li><li><p>系统运维场景：监控服务器的 CPU、内存、请求耗时等性能指标。</p></li></ol><p>可以看到这两种数据都属于日志范畴，特点是：数据实时生产，而且数据量很大。</p><p>Linkedin 最初也尝试过用 ActiveMQ 来解决数据传输问题，但是性能无法满足要求，然后才决定自研 Kafka。</p><p>所以从一开始，Kafka 就是为实时日志流而生的。了解了这个背景，就不难理解 Kafka 与流数据的关系了，以及 Kafka 为什么在大数据领域有如此广泛的应用?也是因为它最初就是为解决大数据的管道问题而诞生的。</p><p>接着再解释下：为什么 Kafka 被官方定义成流处理平台呢?它不就提供了一个数据通道能力吗，怎么还和平台扯上关系了?</p><p>这是因为 Kafka 从 0.8 版本开始，就已经在提供一些和数据处理有关的组件了，比如：</p><ul><li>1、Kafka Streams：一个轻量化的流计算库，性质类似于 Spark、Flink。</li><li>2、Kafka Connect：一个数据同步工具，能将 Kafka 中的数据导入到关系数据库、Hadoop、搜索引擎中。</li></ul><p>可见 Kafka 的野心不仅仅是一个消息系统，它早就在往「实时流处理平台」方向发展了。</p><p>这时候，再回来看 Kafka 的官网介绍提到的 3 种能力，也不难理解了：</p><ul><li>1、数据的发布和订阅能力(消息队列)</li><li>2、数据的分布式存储能力(存储系统)</li><li>3、数据的实时处理能力(流处理引擎)</li></ul><p>这样，kafka 的发展历史和定义基本缕清了。当然，这个系列仅仅关注 Kafka 的前两种能力，因为这两种能力都和 MQ 强相关。</p><h2 id="从-Kafka的消息模型说起"><a href="#从-Kafka的消息模型说起" class="headerlink" title="从 Kafka的消息模型说起"></a>从 Kafka的消息模型说起</h2><p>理解了 Kafka 的定位以及它的诞生背景，接着我们分析下 Kafka 的设计思想。</p><p>上篇文章中我提到过：要吃透一个MQ，建议从「消息模型」这种最核心的理论层面入手，而不是一上来就去看技术架构，更不要直接进入技术细节。</p><p>所谓消息模型，可以理解成一种逻辑结构，它是技术架构再往上的一层抽象，往往隐含了最核心的设计思想。</p><p>下面我们尝试分析下 Kafka 的消息模型，看看它究竟是如何演化来的?</p><p>首先，为了将一份消息数据分发给多个消费者，并且每个消费者都能收到全量的消息，很自然的想到了广播。</p><p><a href="https://s2.51cto.com/oss/202103/28/3c3c798f32a199bee98dd1fc06031bc7.png-wh_600x-s_1818177541.png"><img src="2021-08-30-扒开Kafka的神秘面纱/3c3c798f32a199bee98dd1fc06031bc7.png-wh_600x-s_1818177541.png" alt="img"></a></p><p>紧接着问题出现了：来一条消息，就广播给所有消费者，但并非每个消费者都想要全部的消息，比如消费者 A 只想要消息1、2、3，消费者 B 只想要消息4、5、6，这时候该怎么办呢?</p><p><a href="https://s6.51cto.com/oss/202103/28/6af98ade85e67bc122d79086cc9a8a70.png-wh_600x-s_927003510.png"><img src="2021-08-30-扒开Kafka的神秘面纱/6af98ade85e67bc122d79086cc9a8a70.png-wh_600x-s_927003510.png" alt="img"></a></p><p>这个问题的关键点在于：MQ 不理解消息的语义，它根本无法做到对消息进行分类投递。</p><p>此时，MQ 想到了一个很聪明的办法：它将难题直接抛给了生产者，要求生产者在发送消息时，对消息进行逻辑上的分类，因此就演进出了我们熟知的 Topic 以及发布-订阅模型。</p><p><a href="https://s6.51cto.com/oss/202103/28/993357fa669dd44485c06a88ec7460d2.png-wh_600x-s_3839533348.png"><img src="2021-08-30-扒开Kafka的神秘面纱/993357fa669dd44485c06a88ec7460d2.png-wh_600x-s_3839533348.png" alt="img"></a></p><p>这样，消费者只需要订阅自己感兴趣的 Topic，然后从 Topic 中获取消息即可。</p><p>但是这样做了之后，仍然存在一个问题：假如多个消费者都对同一个 Topic 感兴趣(如下图中的消费者 C)，那又该如何解决呢?</p><p><a href="https://s2.51cto.com/oss/202103/28/596767d6f65503967a56724269ccd153.png-wh_600x-s_1818096394.png"><img src="2021-08-30-扒开Kafka的神秘面纱/596767d6f65503967a56724269ccd153.png-wh_600x-s_1818096394.png" alt="img"></a></p><p>如果采用传统的队列模式(单播)，那当一个消费者从队列中取走消息后，这条消息就会被删除，另外一个消费者就拿不到了。</p><p>这个时候，很自然又想到下面的解决方案：</p><p><a href="https://s3.51cto.com/oss/202103/28/2def175074ab13f75a5f477437224d54.png-wh_600x-s_3107869115.png"><img src="2021-08-30-扒开Kafka的神秘面纱/2def175074ab13f75a5f477437224d54.png-wh_600x-s_3107869115.png" alt="img"></a></p><p>也就是：当 Topic 每增加一个新的消费者，就「复制」一个完全一样的数据队列。</p><p>这样问题是解决了，但是随着下游消费者数量变多，将引发 MQ 性能的快速退化。尤其对于 Kafka 来说，它在诞生之初就是处理大数据场景的，这种复制操作显然成本太高了。</p><p>这时候，就有了 Kafka 最画龙点睛的一个解法：它将所有消息进行了持久化存储，由消费者自己各取所需，想取哪个消息，想什么时候取都行，只需要传递一个消息的 offset 即可。</p><p><a href="https://s3.51cto.com/oss/202103/28/8b9632a4eea00cef0ea6115fd151469c.png-wh_600x-s_393587047.png"><img src="2021-08-30-扒开Kafka的神秘面纱/8b9632a4eea00cef0ea6115fd151469c.png-wh_600x-s_393587047.png" alt="img"></a></p><p>这样一个根本性改变，彻底将复杂的消费问题又转嫁给消费者了，这样使得 Kafka 本身的复杂度大大降低，从而为它的高性能和高扩展打下了良好的基础。(这是 Kafka 不同于 ActiveMQ 和 RabbitMQ 最核心的地方)</p><p>最后，简化一下，就是下面这张图：</p><p><a href="https://s6.51cto.com/oss/202103/28/cd295f2dee49718d8fb098715ba78da7.png-wh_600x-s_528578466.png"><img src="2021-08-30-扒开Kafka的神秘面纱/cd295f2dee49718d8fb098715ba78da7.png-wh_600x-s_528578466.png" alt="img"></a></p><p>这就是 Kafka 最原始的消息模型。</p><p>这也间接解释了第二章节中：为什么官方会将 Kakfa 同时定义成存储系统的原因。</p><h2 id="Kafka架构设计的任督二脉"><a href="#Kafka架构设计的任督二脉" class="headerlink" title="Kafka架构设计的任督二脉"></a>Kafka架构设计的任督二脉</h2><p>下面我们再接着分析下：Kafka 究竟是如何解决存储问题的？</p><p>面对海量数据，单机的存储容量和读写性能肯定有限，大家很容易想到一种存储方案：对数据进行分片存储<strong>。</strong>这种方案在我们实际工作中也非常常见：</p><ol><li><p>比如数据库设计中，当单表的数据量达到几千万或者上亿时，我们会将它拆分成多个库或者多张表。</p></li><li><p>比如缓存设计中，当单个 Redis 实例的数据量达到几十个 G 引发性能瓶颈时，我们会将单机架构改成分片集群架构。</p></li></ol><p>类似的拆分思想在 HDFS、ElasticSearch 等中间件中都能看到。</p><p>Kafka 也不例外，它同样采用了这种水平拆分方案。在 Kafka 的术语中，拆分后的数据子集叫做 Partition（分区），各个分区的数据合集即全量数据。</p><p>我们再来看下 Kafka 中的 Partition 具体是如何工作的？举一个很形象的例子，如果我们把「Kafka」类比成「高速公路」：</p><ol><li><p>当大家听到京广高速的时候，知道这是一条从北京到广州的高速路，这是逻辑上的叫法，可以理解成 Kafka 中的 Topic（主题）。</p></li><li><p>一条高速路通常会有多个车道进行分流，每个车道上的车都是通往一个目的地的（属于同一个Topic），这里所说的车道便是 Partition。</p></li></ol><p>这样，一条消息的流转路径就如下图所示，先走主题路由，然后走分区路由，最终决定这条消息该发往哪个分区。</p><p><img src="2021-08-30-扒开Kafka的神秘面纱/640-20230424171952667" alt="图片"></p><p>其中分区路由可以简单理解成一个 Hash 函数，生产者在发送消息时，完全可以自定义这个函数来决定分区规则。如果分区规则设定合理，所有消息将均匀地分配到不同的分区中。</p><p>通过这样两层关系，最终在 Topic 之下，就有了一个新的划分单位：Partition。先通过 Topic 对消息进行逻辑分类，然后通过 Partition 进一步做物理分片，最终多个 Partition 又会均匀地分布在集群中的每台机器上，从而很好地解决了存储的扩展性问题。</p><p>因此，Partition 是 Kafka 最基本的部署单元。本文之所以将 Partition 称作 Kafka 架构设计的任督二脉，基于下面两点原因：</p><blockquote><p>1、Partition 是存储的关键所在，MQ「一发一存一消费」的核心流程必然围绕它展开。</p><p>2、Kafka 高并发设计中最难的三高问题都能和 Partition 关联起来。</p></blockquote><p>因此，以 Partition 作为根，能很自然地联想出 Kafka 架构设计中的各个知识点，形成可靠的知识体系。</p><p>下面，请大家继续跟着我的思路，以 Partition 为线索，对 Kafka 的宏观架构进行解析。</p><h2 id="Kafka的宏观架构设计"><a href="#Kafka的宏观架构设计" class="headerlink" title="Kafka的宏观架构设计"></a>Kafka的宏观架构设计</h2><p>接下来，我们再看看 Partition 的分布式能力究竟是如何实现的？它又是怎么和 Kafka 的整体架构关联起来的？</p><p>前面讲过 Partition 是 Topic 之下的一个划分单位，它是 Kafka 最基本的部署单元，它将决定 Kafka 集群的组织方式。</p><p>假设现在有两个 Topic，每个 Topic 都设置了两个 Partition，如果 Kafka 集群是两台机器，部署架构将会是下面这样：</p><p><img src="2021-08-30-扒开Kafka的神秘面纱/640-20230424172000709" alt="图片"></p><p>可以看到：同一个 Topic 的两个 Partition 分布在不同的消息服务器上，能做到消息的分布式存储了。但是对于 Kafka 这个高并发系统来说，仅存储可扩展还不够，消息的拉取也必须并行才行，否则会遇到极大的性能瓶颈。</p><p>那我们再看看消费端，它又是如何跟 Partition 结合并做到并行处理的？</p><p>从消费者来看，首先要满足两个基本诉求：</p><blockquote><p>1、广播消费能力：同一个 Topic 可以被多个消费者订阅，一条消息能够被消费多次。</p><p>2、集群消费能力：当消费者本身也是集群时，每一条消息只能分发给集群中的一个消费者进行处理。</p></blockquote><p>为了满足这两点要求，Kafka 引出了消费组的概念，每个消费者都有一个对应的消费组，组间进行广播消费，组内进行集群消费。此外，Kafka 还限定了：每个 Partition 只能由消费组中的一个消费者进行消费。</p><p>最终的消费关系如下图所示：假设主题 A 共有 4 个分区，消费组 2 只有两个消费者，最终这两个消费组将平分整个负载，各自消费两个分区的消息。</p><p><img src="2021-08-30-扒开Kafka的神秘面纱/640-20230424172004958" alt="图片"></p><p>如果要加快消息的处理速度，该如何做呢？也很简单，向消费组 2 中增加新的消费者即可，Kafka 将以 Partition 为单位重新做负载均衡。当增加到 4 个消费者时，每个消费者仅需处理 1 个 Partition，处理速度将提升两倍。</p><p>到这里，存储可扩展、消息并行处理这两个难题都解决了。但是高并发架构设计上，还遗留了一个很重要的问题：那就是高可用设计。</p><p>在 Kafka 集群中，每台机器都存储了一些 Partition，一旦某台机器宕机，上面的数据不就丢失了吗？</p><p>此时，你一定会想到对消息进行持久化存储，但是持久化只能解决一部分问题，它只能确保机器重启后，历史数据不丢失。但在机器恢复之前，这部分数据将一直无法访问。这对于高并发系统来说，是无法忍受的。</p><p>所以 Kafka 必须具备故障转移能力才行，当某台机器宕机后仍然能保证服务可用。</p><p>如果大家去分析任何一个高可靠的分布式系统，比如 ElasticSearch、Redis Cluster，其实它们都有一套多副本的冗余机制。</p><p>没错，Kafka 正是通过 Partition 的多副本机制解决了高可用问题。在 Kafka 集群中，每个 Partition 都有多个副本，同一分区的不同副本中保存的是相同的消息。</p><p>副本之间是 “一主多从” 的关系，其中 leader 副本负责读写请求，follower 副本只负责和 leader 副本同步消息，当 leader 副本发生故障时，它才有机会被选举成新的 leader 副本并对外提供服务，否则一直是待命状态。</p><p>现在，我假设 Kafka 集群中有 4 台服务器，主题 A 和主题 B 都有两个 Partition，且每个 Partition 各有两个副本，那最终的多副本架构将如下图所示：</p><p><img src="2021-08-30-扒开Kafka的神秘面纱/640-20230424172009069" alt="图片"></p><p>很显然，这个集群中任何一台机器宕机，都不会影响 Kafka 的可用性，数据仍然是完整的。</p><p>理解了上面这些内容，最后我们再反过来看下 Kafka 的整体架构：</p><p><img src="2021-08-30-扒开Kafka的神秘面纱/640-20230424172013042" alt="图片"></p><p>1、Producer：生产者，负责创建消息，然后投递到 Kafka 集群中，投递时需要指定消息所属的 Topic，同时确定好发往哪个 Partition。</p><p>2、Consumer：消费者，会根据它所订阅的 Topic 以及所属的消费组，决定从哪些 Partition 中拉取消息。</p><p>3、Broker：消息服务器，可水平扩展，负责分区管理、消息的持久化、故障自动转移等。</p><p>4、Zookeeper：负责集群的元数据管理等功能，比如集群中有哪些 broker 节点以及 Topic，每个 Topic 又有哪些 Partition 等。</p><p>很显然，在 Kafka 整体架构中，Partition 是发送消息、存储消息、消费消息的纽带。吃透了它，再去理解整体架构，脉络会更加清晰。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文以 Partition 为切入点，从宏观角度解析了 Kafka 的整体架构，再简单总结下本文的内容：</p><ol><li>Kafka 通过巧妙的模型设计，将自己退化成一个海量消息的存储系统。</li><li>为了解决存储的扩展性问题，Kafka 对数据进行了水平拆分，引出了 Partition（分区），这是 Kafka 部署的基本单元，同时也是 Kafka 并发处理的最小粒度。</li><li>对于一个高并发系统来说，还需要做到高可用，Kafka 通过 Partition 的多副本冗余机制进行故障转移，确保了高可靠。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://developer.51cto.com/art/202103/653775.htm">《吃透 MQ 系列》之扒开 Kafka 的神秘面纱</a></p><p><a href="https://mp.weixin.qq.com/s/I2O8OyQgrkUJIIDjVhmVcA">《吃透 MQ 系列》之打通 Kafka 的任督二脉</a></p>]]></content>
    
    
    <categories>
      
      <category>大数据技术支持</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>oss对象存储服务</title>
    <link href="/2021/07/14/2021-07-14-oss%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E6%9C%8D%E5%8A%A1/"/>
    <url>/2021/07/14/2021-07-14-oss%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E6%9C%8D%E5%8A%A1/</url>
    
    <content type="html"><![CDATA[<h2 id="oss-简介"><a href="#oss-简介" class="headerlink" title="oss 简介"></a>oss 简介</h2><p>对象存储服务（Object Storage Service，OSS）是一种海量、安全、低成本、高可靠的云存储服务，适合存放任意类型的文件。容量和处理能力弹性扩展，多种存储类型供选择，全面优化存储成本。</p><p>白话文解释就是将系统所要用的文件上传到云硬盘上，该云硬盘提供了文件下载、上传等一列服务，这样的服务以及技术可以统称为OSS，业内提供OSS服务的厂商很多，知名常用且成规模的蓝队云等。</p><h2 id="OSS的好处"><a href="#OSS的好处" class="headerlink" title="OSS的好处"></a>OSS的好处</h2><p>简单的HTTP API，包含所有主要操作系统和编程语言的客户端。只需支付使用的费用。对发布静态资产的内置支持允许您使用更少的服务器。一些对象存储提供内置的CDN集成，可以缓存资产以加快页面加载速度。可选的版本控制允许您检索旧版本的对象以从意外数据覆盖中恢复。可以轻松扩展对象存储服务，而无需额外的资源或体系结构更改。不需要支持硬盘驱动器和RAID阵列，因为所有这些都由存储提供商处理。使用数据对象存储元数据片段的能力将简化应用程序体系结构。</p><p>OSS非常适合存储静态资源，例如，用于存储用户定义的内容：图像和电影，存储备份文件和日志。</p><h2 id="简单使用示例"><a href="#简单使用示例" class="headerlink" title="简单使用示例"></a>简单使用示例</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs plain">import oss2<br><br>class oss(object):<br>    &quot;&quot;&quot;对象存储类，将模型传至阿里云端&quot;&quot;&quot;<br><br>    def __init__(self, access_key_id, access_key_secret, endpoint, bucket_name):<br>        self.auth = oss2.Auth(access_key_id, access_key_secret)<br>        self.bucket = oss2.Bucket(self.auth, endpoint, bucket_name)  # 连接OSS<br><br>    def put_file(self, file_path, oss_path):<br>        with open(&quot;&#123;&#125;&quot;.format(file_path), &quot;rb&quot;) as f:<br>            put_result = self.bucket.put_object(oss_path, f)<br>        if put_result.status == 200:<br>            # 若此时的status状态为200，则说明上传成功；<br>            print(&quot;put success&quot;)<br><br>    def get_file(self, file_path, oss_path):<br>        # param1:oss上bucket中的文件名<br>        # param2:保存在当地的文件路径+文件名<br>        get_result = self.bucket.get_object_to_file(oss_path, file_path)<br>        if get_result.status == 200:<br>            print(&quot;get success&quot;)<br>        else:<br>            print(&quot;get failed&quot;)<br><br><br>oss_server = oss(<br>    access_key_id=&quot;AccessKey&quot;),<br>    access_key_secret=&quot;AccessKeySecret&quot;),<br>    endpoint=&quot;EndPoint&quot;,<br>    bucket_name=&quot;Bucket&quot;,<br>)<br><br><br>def download_longfor_bert(pretrain_file, oss_get_path):<br>    &quot;&quot;&quot;获取OSS指定目录下的文件<br>    &quot;&quot;&quot;<br>    for obj in oss2.ObjectIterator(oss_server.bucket, prefix = oss_get_path, delimiter = &#x27;/&#x27;):<br>        # 通过is_prefix方法判断obj是否为文件夹。<br>        if obj.is_prefix():  # 判断obj为文件夹。<br>            print(&#x27;directory: &#x27; + obj.key)<br>        else:                # 判断obj为文件。<br>            print(&#x27;file: &#x27; + obj.key)<br>            file_name = str(obj.key).split(&#x27;/&#x27;)[-1]<br>            if file_name:<br>                oss_server.get_file(pretrain_file+file_name, obj.key)<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://help.aliyun.com/product/31815.html?spm=a2c4g.11174283.6.540.47ce4c07QFiKcz">https://help.aliyun.com/product/31815.html?spm=a2c4g.11174283.6.540.47ce4c07QFiKcz</a></p>]]></content>
    
    
    <categories>
      
      <category>大数据技术支持</category>
      
    </categories>
    
    
    <tags>
      
      <tag>oss</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>redis缓存</title>
    <link href="/2021/07/14/2021-07-14-redis%E7%BC%93%E5%AD%98/"/>
    <url>/2021/07/14/2021-07-14-redis%E7%BC%93%E5%AD%98/</url>
    
    <content type="html"><![CDATA[<h2 id="Redis-简介"><a href="#Redis-简介" class="headerlink" title="Redis 简介"></a>Redis 简介</h2><p>REmote DIctionary Server(Redis) 是一个由 Salvatore Sanfilippo 写的 key-value 存储系统，是跨平台的非关系型数据库。</p><p>Redis 与其他 key - value 缓存产品有以下三个特点：</p><ul><li>Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。</li><li>Redis不仅仅支持简单的key-value类型的数据，同时还提供字符串(String)、哈希(Hash)、列表(list)、集合(sets)和有序集合(sorted sets)等数据结构的存储。</li><li>Redis支持数据的备份，即master-slave模式的数据备份。</li></ul><h2 id="Redis-优势"><a href="#Redis-优势" class="headerlink" title="Redis 优势"></a>Redis 优势</h2><ul><li>性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。</li><li>丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。</li><li>原子 – Redis的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。</li><li>丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。</li></ul><span id="more"></span><h2 id="安装-redis-模块"><a href="#安装-redis-模块" class="headerlink" title="安装 redis 模块"></a>安装 redis 模块</h2><p>Python 要使用 redis，需要先安装 redis 模块：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cmake">sudo pip3 <span class="hljs-keyword">install</span> redis<br>或<br>sudo python setup.py <span class="hljs-keyword">install</span><br></code></pre></td></tr></table></figure><p>测试是否安装成功：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">&gt;&gt;&gt; import redis<br>&gt;&gt;&gt; r = redis.StrictRedis(<span class="hljs-attribute">host</span>=<span class="hljs-string">&#x27;localhost&#x27;</span>, <span class="hljs-attribute">port</span>=6379, <span class="hljs-attribute">db</span>=0)<br>&gt;&gt;&gt; r.<span class="hljs-built_in">set</span>(<span class="hljs-string">&#x27;foo&#x27;</span>, <span class="hljs-string">&#x27;bar&#x27;</span>)<br><span class="hljs-literal">True</span><br>&gt;&gt;&gt; r.<span class="hljs-built_in">get</span>(<span class="hljs-string">&#x27;foo&#x27;</span>)<br><span class="hljs-string">&#x27;bar&#x27;</span><br></code></pre></td></tr></table></figure><p>redis 提供两个类 Redis 和 StrictRedis, StrictRedis 用于实现大部分官方的命令，Redis 是 StrictRedis 的子类，用于向后兼用旧版本。</p><p>redis 取出的结果默认是字节，我们可以设定 <strong>decode_responses=True</strong> 改成字符串。</p><p><strong>实例</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import redis   # 导入redis 模块<br><br>r = redis.Redis(<span class="hljs-attribute">host</span>=<span class="hljs-string">&#x27;localhost&#x27;</span>, <span class="hljs-attribute">port</span>=6379, <span class="hljs-attribute">decode_responses</span>=<span class="hljs-literal">True</span>)  <br>r.<span class="hljs-built_in">set</span>(<span class="hljs-string">&#x27;name&#x27;</span>, <span class="hljs-string">&#x27;runoob&#x27;</span>)  # 设置 name 对应的值<br><span class="hljs-built_in">print</span>(r[<span class="hljs-string">&#x27;name&#x27;</span>])<br><span class="hljs-built_in">print</span>(r.<span class="hljs-built_in">get</span>(<span class="hljs-string">&#x27;name&#x27;</span>))  # 取出键 name 对应的值<br><span class="hljs-built_in">print</span>(type(r.<span class="hljs-built_in">get</span>(<span class="hljs-string">&#x27;name&#x27;</span>)))  # 查看类型<br></code></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs actionscript">runoob<br>runoob<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;str&#x27;</span>&gt;<br></code></pre></td></tr></table></figure><h2 id="连接redis-sentinel集群"><a href="#连接redis-sentinel集群" class="headerlink" title="连接redis sentinel集群"></a>连接redis sentinel集群</h2><p><a href="https://blog.csdn.net/u012887259/article/details/102425691">https://blog.csdn.net/u012887259/article/details/102425691</a></p><p>示例代码</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># import redis</span><br><span class="hljs-keyword">from</span> redis.sentinel import Sentinel<br> <br><span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">1、通过访问Sentinel服务的方式，获取redis的master、slave节点信息</span><br><span class="hljs-string">2、向master redis写入数据</span><br><span class="hljs-string">3、从slave redis读取数据</span><br><span class="hljs-string">&quot;</span><span class="hljs-string">&quot;&quot;</span><br> <br><span class="hljs-comment"># 连接哨兵服务器(主机名也可以用域名)</span><br>sentinel = Sentinel([(<span class="hljs-string">&#x27;192.168.196.129&#x27;</span>, 26379),<br>                     (<span class="hljs-string">&#x27;192.168.196.132&#x27;</span>, 26379)<br>             ],<br>                    <span class="hljs-attribute">socket_timeout</span>=0.5)<br> <br><span class="hljs-comment"># 获取主服务器地址</span><br>master = sentinel.discover_master(<span class="hljs-string">&#x27;mymaster&#x27;</span>)<br><span class="hljs-built_in">print</span>(master)<br><span class="hljs-comment"># 输出：(&#x27;192.168.196.132&#x27;, 6379)</span><br> <br> <br><span class="hljs-comment"># 获取从服务器地址</span><br>slave = sentinel.discover_slaves(<span class="hljs-string">&#x27;mymaster&#x27;</span>)<br><span class="hljs-built_in">print</span>(slave)<br><span class="hljs-comment"># 输出：[(&#x27;192.168.196.129&#x27;, 6379)]</span><br> <br> <br><span class="hljs-comment"># 获取主服务器进行写入</span><br>master = sentinel.master_for(<span class="hljs-string">&#x27;mymaster&#x27;</span>, <span class="hljs-attribute">socket_timeout</span>=0.5, <span class="hljs-attribute">password</span>=<span class="hljs-string">&#x27;newpwd&#x27;</span>, <span class="hljs-attribute">db</span>=0)<br>w_ret = master.<span class="hljs-built_in">set</span>(<span class="hljs-string">&#x27;foo&#x27;</span>, <span class="hljs-string">&#x27;bar&#x27;</span>)<br><span class="hljs-comment"># 输出：True</span><br> <br><span class="hljs-comment"># 获取从服务器进行读取（默认是round-roubin,随机从多个slave服务中读取数据）</span><br>slave = sentinel.slave_for(<span class="hljs-string">&#x27;mymaster&#x27;</span>, <span class="hljs-attribute">socket_timeout</span>=0.5, <span class="hljs-attribute">password</span>=<span class="hljs-string">&#x27;newpwd&#x27;</span>, <span class="hljs-attribute">db</span>=0)<br>r_ret = slave.<span class="hljs-built_in">get</span>(<span class="hljs-string">&#x27;foo&#x27;</span>)<br><span class="hljs-built_in">print</span>(r_ret)<br><span class="hljs-comment"># 输出：bar</span><br></code></pre></td></tr></table></figure><p>封装工具类方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> redis.sentinel <span class="hljs-keyword">import</span> Sentinel<br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">redisSentinelHelper</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,sentinel_list,service_name,password,db</span>):<br>        self.sentinel = Sentinel(sentinel_list,socket_timeout=<span class="hljs-number">0.5</span>)<br>        self.service_name = service_name<br>        self.password = password<br>        self.db = db<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_master_redis</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.sentinel.discover_master(self.service_name)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_slave_redis</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.sentinel.discover_slaves(self.service_name)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_key</span>(<span class="hljs-params">self,key,value</span>):<br>        master = self.sentinel.master_for(<br>            service_name=self.service_name,<br>            socket_timeout=<span class="hljs-number">0.5</span>,<br>            password=self.password,<br>            db=self.db<br>        )<br>        <span class="hljs-keyword">return</span> master.<span class="hljs-built_in">set</span>(key,value)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_key</span>(<span class="hljs-params">self,key</span>):<br>        slave = self.sentinel.slave_for(<br>            service_name=self.service_name,<br>            socket_timeout=<span class="hljs-number">0.5</span>,<br>            password=self.password,<br>            db=self.db<br>        )<br>        <span class="hljs-keyword">return</span> slave.get(key)<br> <br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_test</span>():<br>    <span class="hljs-comment"># redis info</span><br>    sentinel_list = [(<span class="hljs-string">&#x27;192.168.196.129&#x27;</span>, <span class="hljs-number">26379</span>),(<span class="hljs-string">&#x27;192.168.196.132&#x27;</span>, <span class="hljs-number">26379</span>)]<br>    password = <span class="hljs-string">&#x27;newpwd&#x27;</span><br>    db = <span class="hljs-number">0</span><br>    service_name = <span class="hljs-string">&#x27;mymaster&#x27;</span><br> <br>    <span class="hljs-comment"># create redis link</span><br>    rsh = redisSentinelHelper(sentinel_list=sentinel_list,password=password,service_name=service_name,db=db)<br> <br>    <span class="hljs-comment"># test set key : key1 test-insert-key1</span><br>    rsh.set_key(<span class="hljs-string">&#x27;key1&#x27;</span>,<span class="hljs-string">&#x27;test-insert-key1&#x27;</span>)<br> <br>    <span class="hljs-comment"># get key1</span><br>    <span class="hljs-built_in">print</span>(rsh.get_key(<span class="hljs-string">&#x27;key1&#x27;</span>))<br> <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    _test()<br></code></pre></td></tr></table></figure><p>运行如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim">D:\Python37\<span class="hljs-keyword">python3</span>.<span class="hljs-keyword">exe</span> D:/pythonProject/redis-test/test7.<span class="hljs-keyword">py</span><br><span class="hljs-keyword">b</span><span class="hljs-string">&#x27;test-insert-key1&#x27;</span><br></code></pre></td></tr></table></figure><p>其中，我没有把设置master节点写在初始化，而是在set key操作的时候才创建连接，主要是后续想要测试master节点变化的情况下，写入能够继续。</p><p>当然这样的话性能肯定不会很好，有很多可以根据实际情况修改的地方。</p><p><strong>测试：当master节点切换，能否自动连续写入</strong></p><p>当然，在sentinel执行master切点切换的过程，肯定会有些丢失，但是主要是要看切换之后，是否可以自动继续写入数据。</p><p><strong>首先编写一个循环写入的示例</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> redis.sentinel <span class="hljs-keyword">import</span> Sentinel<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">redisSentinelHelper</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,sentinel_list,service_name,password,db</span>):<br>        self.sentinel = Sentinel(sentinel_list,socket_timeout=<span class="hljs-number">0.5</span>)<br>        self.service_name = service_name<br>        self.password = password<br>        self.db = db<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_master_redis</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.sentinel.discover_master(self.service_name)<br>     <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_slave_redis</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.sentinel.discover_slaves(self.service_name)<br>     <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_key</span>(<span class="hljs-params">self,key,value</span>):<br>        master = self.sentinel.master_for(<br>            service_name=self.service_name,<br>            socket_timeout=<span class="hljs-number">0.5</span>,<br>            password=self.password,<br>            db=self.db<br>        )<br>        <span class="hljs-keyword">return</span> master.<span class="hljs-built_in">set</span>(key,value)<br>     <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_key</span>(<span class="hljs-params">self,key</span>):<br>        slave = self.sentinel.slave_for(<br>            service_name=self.service_name,<br>            socket_timeout=<span class="hljs-number">0.5</span>,<br>            password=self.password,<br>            db=self.db<br>        )<br>        <span class="hljs-keyword">return</span> slave.get(key)<br><br> <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_test</span>():<br>    <span class="hljs-comment"># redis info</span><br>    sentinel_list = [(<span class="hljs-string">&#x27;192.168.196.129&#x27;</span>, <span class="hljs-number">26379</span>),(<span class="hljs-string">&#x27;192.168.196.132&#x27;</span>, <span class="hljs-number">26379</span>)]<br>    password = <span class="hljs-string">&#x27;newpwd&#x27;</span><br>    db = <span class="hljs-number">0</span><br>    service_name = <span class="hljs-string">&#x27;mymaster&#x27;</span><br><br>    <span class="hljs-comment"># create redis link</span><br>    rsh = redisSentinelHelper(sentinel_list=sentinel_list,password=password,service_name=service_name,db=db)<br>     <br>    <span class="hljs-comment"># test set key : key1 test-insert-key1</span><br>    <span class="hljs-comment"># rsh.set_key(&#x27;key1&#x27;,&#x27;test-insert-key1&#x27;)</span><br>     <br>    <span class="hljs-comment"># get key1</span><br>    <span class="hljs-comment"># print(rsh.get_key(&#x27;key1&#x27;))</span><br>     <br>    <span class="hljs-comment"># loop set key</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">1000000</span>):<br>        rsh.set_key(<span class="hljs-string">&#x27;key&#x27;</span> + <span class="hljs-built_in">str</span>(i), i)<br>        <span class="hljs-built_in">print</span>(rsh.get_key(<span class="hljs-string">&#x27;key&#x27;</span> + <span class="hljs-built_in">str</span>(i)))<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    _test()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大数据技术支持</category>
      
    </categories>
    
    
    <tags>
      
      <tag>redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常用 Normalization 方法的总结与思考</title>
    <link href="/2021/07/09/2021-07-09-Normalization%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <url>/2021/07/09/2021-07-09-Normalization%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>转载自<a href="https://mp.weixin.qq.com/s/LFEtITz1cEXe00RZgp00PQ">https://mp.weixin.qq.com/s/LFEtITz1cEXe00RZgp00PQ</a></p><p>常用的Normalization方法主要有：Batch Normalization（BN，2015年）、Layer Normalization（LN，2016年）、Instance Normalization（IN，2017年）、Group Normalization（GN，2018年）。它们都是从激活函数的输入来考虑、做文章的，以不同的方式对<strong>激活函数的输入进行 Norm</strong> 的。</p><p>我们将输入的 <strong>feature map shape</strong> 记为<strong>[N, C, H, W]</strong>，其中N表示batch size，即N个样本；C表示通道数；H、W分别表示特征图的高度、宽度。这几个方法主要的区别就是在：</p><ol><li><p>BN是在batch上，对N、H、W做归一化，而保留通道 C 的维度。BN对较小的batch size效果不好。BN适用于固定深度的前向神经网络，如CNN，不适用于RNN；</p></li><li><p>LN在通道方向上，对C、H、W归一化，主要对RNN效果明显；</p></li><li><p>IN在图像像素上，对H、W做归一化，用在风格化迁移；</p></li><li><p>GN将channel分组，然后再做归一化。</p></li></ol><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8ZX1qZyJUGouuXic65BjgrYHybBCK9QY25bLqOib6hqPZbJM53eMeyZiaWOo5PZMI8wEpgMHS676TZw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p>每个子图表示一个特征图，其中N为批量，C为通道，（H，W）为特征图的高度和宽度。通过蓝色部分的值来计算均值和方差，从而进行归一化。</p><p><strong>如果把特征图<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkgdOA5FgYftAWCcfOh4bx460ycsqe3zfexsRDxDXQvh4UibBHL62iajvg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">比喻成一摞书，这摞书总共有 N 本，每本有 C 页，每页有 H 行，每行 有W 个字符。</strong></p><ol><li><p>BN 求均值时，相当于把这些书按页码一一对应地加起来（例如第1本书第36页，第2本书第36页……），再除以每个页码下的字符总数：N×H×W，因此可以把 BN 看成求“平均书”的操作（注意这个“平均书”每页只有一个字），求标准差时也是同理。</p></li><li><p>LN 求均值时，相当于把每一本书的所有字加起来，再除以这本书的字符总数：C×H×W，即求整本书的“平均字”，求标准差时也是同理。</p></li><li><p>IN 求均值时，相当于把一页书中所有字加起来，再除以该页的总字数：H×W，即求每页书的“平均字”，求标准差时也是同理。</p></li><li><p>GN 相当于把一本 C 页的书平均分成 G 份，每份成为有 C/G 页的小册子，求每个小册子的“平均字”和字的“标准差”。</p><span id="more"></span></li></ol><h1 id="一、-Batch-Normalization-BN"><a href="#一、-Batch-Normalization-BN" class="headerlink" title="一、 Batch Normalization, BN"></a><strong>一、 Batch Normalization, BN</strong></h1><p>论文链接：<a href="https://arxiv.org/pdf/1502.03167.pdf">https://arxiv.org/pdf/1502.03167.pdf</a></p><p><strong>为什么要进行BN呢？</strong></p><p>（1）在深度神经网络训练的过程中，通常以输入网络的每一个mini-batch进行训练，这样每个batch具有不同的分布，使模型训练起来特别困难。</p><p>（2）Internal Covariate Shift (ICS) 问题：在训练的过程中，激活函数会改变各层数据的分布，随着网络的加深，这种改变（差异）会越来越大，使模型训练起来特别困难，收敛速度很慢，会出现梯度消失的问题。</p><p><strong>BN的主要思想：</strong>针对每个神经元，<strong>使数据在进入激活函数之前，沿着通道计算每个batch的均值、方差，‘强迫’数据保持均值为0，方差为1的正态分布，</strong>避免发生梯度消失。具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道 …… 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值（注意是除以 N×H×W 而不是单纯除以 N，最后得到的是一个代表这个 batch 第1个通道平均值的数字，而不是一个 H×W 的矩阵）。求通道 1 的方差也是同理。对所有通道都施加一遍这个操作，就得到了所有通道的均值和方差。</p><p><strong>BN的使用位置</strong>：全连接层或卷积操作之后，激活函数之前。</p><p><strong>BN算法过程：</strong></p><ul><li>沿着通道计算每个batch的均值 <img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfklpoLNQqoJEHMFIsmzYEAX3yichGSzIXfcEKqA44ibLDkAclHBeLLrIzg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></li><li>沿着通道计算每个batch的方差 <img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfka85t5IDPyeECSup5IVBRTRPxyeNuibBibkg1ylGZz73LdXvG1jB6PsDw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></li><li>做归一化</li><li>加入缩放和平移变量<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkCUb2OCyJa9R1WxdiclP5mCfolUQIISsZjPGFScN9Mg3SrTzQ0QHIeMw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">和<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkB7cBCm1hd4ZW9rzcvXfXCvvB4b58zjClzNswC5M388CHwScn8RLyHg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8ZX1qZyJUGouuXic65BjgrYoAuSCl9f2dYKIHCNwZjxfiaQOuRPQg3ATKeLfoo9f3NhuWYj6OOFLOg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p>其中 是一个很小的正值，比如<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfk53bGbnsQdEgm3Ml5MsJxibjebstz4LjqGRP2nyCJywOfcmTdFxvt2mw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"> 。<strong>加入缩放和平移变量的原因是：保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。</strong> 这两个参数是用来学习的参数。</p><p><strong>BN的作用：</strong></p><p>（1）允许较大的学习率；</p><p>（2）减弱对初始化的强依赖性；</p><p>（3）保持隐藏层中数值的均值、方差不变，让数值更稳定，为后面网络提供坚实的基础；</p><p>（4）有轻微的正则化作用（相当于给隐藏层加入噪声，类似Dropout）。</p><p><strong>BN存在的问题：</strong></p><p>（1）每次是在一个batch上计算均值、方差，如果batch size太小，则计算的均值、方差不足以代表整个数据分布。</p><p>（2）batch size太大：会超过内存容量；需要跑更多的epoch，导致总训练时间变长；会直接固定梯度下降的方向，导致很难更新。</p><h1 id="二、-Layer-Normalization-LN"><a href="#二、-Layer-Normalization-LN" class="headerlink" title="二、 Layer Normalization, LN"></a><strong>二、 Layer Normalization, LN</strong></h1><p>论文链接：<a href="https://arxiv.org/pdf/1607.06450v1.pdf">https://arxiv.org/pdf/1607.06450v1.pdf</a></p><p>针对BN不适用于深度不固定的网络（sequence长度不一致，如RNN），LN对深度网络的某一层的所有神经元的输入按以下公式进行normalization操作。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8ZX1qZyJUGouuXic65BjgrYhOEWvjwIKf0e4XSibSAW7icIicX7OUbppaBRI8siaOjz0bytdOAiaT109xQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p>LN中同层神经元的输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。</p><p>对于特征图<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkvdnibSxvupA9hUaric4pG7sVP95iazlE9gP75Tdba2XiaTdETMuoGs4Mqg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"> ，LN 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。其均值和标准差公式为：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8ZX1qZyJUGouuXic65BjgrYVEaj6PGQDmOOPnoIC5walVquJlNqwb1uU2BgZJVZ2g6tsfWn4dicicxg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p>Layer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化。LN不依赖于batch size和输入sequence的长度，因此可以用于batch size为1和RNN中。LN用于RNN效果比较明显，但是在CNN上，效果不如BN。</p><h1 id="三、-Instance-Normalization-IN"><a href="#三、-Instance-Normalization-IN" class="headerlink" title="三、  Instance Normalization, IN"></a><strong>三、  Instance Normalization, IN</strong></h1><p>论文链接：<a href="https://arxiv.org/pdf/1607.08022.pdf">https://arxiv.org/pdf/1607.08022.pdf</a></p><p>IN针对图像像素做normalization，最初用于图像的风格化迁移。在图像风格化中，生成结果主要依赖于某个图像实例，feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格。所以对整个batch归一化不适合图像风格化中，因而对H、W做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。</p><p>对于<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfk0Z4QgIU1vZnRtIS0BMz56957S8uicTPsErAkzYbIL0EUb30BScxv7bQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">，IN 对每个样本的 H、W 维度的数据求均值和标准差，保留 N 、C 维度，也就是说，它只在 channel 内部求均值和标准差，其公式如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8ZX1qZyJUGouuXic65BjgrYSLr45qibX4ibTDqrQQtstOKCobVDcjias7rF6IZBttWa5lIFYLDJbtOkA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><h1 id="四、-Group-Normalization-GN"><a href="#四、-Group-Normalization-GN" class="headerlink" title="四、 Group Normalization, GN"></a><strong>四、 Group Normalization, GN</strong></h1><p>论文链接：<a href="https://arxiv.org/pdf/1803.08494.pdf">https://arxiv.org/pdf/1803.08494.pdf</a></p><p><strong>GN是为了解决BN对较小的mini-batch size效果差的问题。</strong>GN适用于占用显存比较大的任务，例如图像分割。对这类任务，可能 batch size 只能是个位数，再大显存就不够用了。而当 batch size 是个位数时，BN 的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN 也是独立于 batch 的，它是 LN 和 IN 的折中。</p><p><strong>GN的主要思想：</strong>在 channel 方向 group，然后每个 group 内做 Norm，计算<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkny1vKP8FFb4WtxYunxRwFFDklAWbicyVovfJewUdfmUQdQekPicDv5LQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"> 的均值和方差，这样就与batch size无关，不受其约束。</p><p><strong>具体方法：</strong>GN 计算均值和标准差时，把每一个样本 feature map 的 channel 分成 G 组，每组将有 C/G 个 channel，然后将这些 channel 中的元素求均值和标准差。各组 channel 用其对应的归一化参数独立地归一化。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8ZX1qZyJUGouuXic65BjgrYliatOb592F57xVweRCcKtbfdt6lgibU57fGnQTlk5VwB3Mzu3BjIiaiaNw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p>伪代码如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8ZX1qZyJUGouuXic65BjgrYHT7Z0pribIwctI01GvLh91GN1rFdLwjibmGx3QIunOSKmqmibVI4eJgbg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p>代码如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">def</span> GroupNorm(x, gamma, beta, G=<span class="hljs-number">16</span>):<br><br>    <span class="hljs-comment"># x_shape:[N, C, H, W]</span><br>    <span class="hljs-attribute">results</span> = <span class="hljs-number">0</span>.<br>    <span class="hljs-attribute">eps</span> = <span class="hljs-number">1</span>e-<span class="hljs-number">5</span><br>    <span class="hljs-attribute">x</span> = np.reshape(x, (x.shape[<span class="hljs-number">0</span>], G, x.shape[<span class="hljs-number">1</span>]/<span class="hljs-number">16</span>, x.shape[<span class="hljs-number">2</span>], x.shape[<span class="hljs-number">3</span>]))<br><br>    <span class="hljs-attribute">x_mean</span> = np.mean(x, axis=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>), keepdims=True)<br>    <span class="hljs-attribute">x_var</span> = np.var(x, axis=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>), keepdims=True0)<br>    <span class="hljs-attribute">x_normalized</span> = (x - x_mean) / np.sqrt(x_var + eps)<br>    <span class="hljs-attribute">results</span> = gamma * x_normalized + beta<br>    <span class="hljs-attribute">return</span> results<br></code></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h1><p>我们将feature map shape 记为[N, C, H, W]。如果把特征<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkXtYXDCS7CHAlp2wxgqibHZe1EgGZKOaHoLEEcDGpknvrb8SDtF67icPQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">比喻成一摞书，这摞书总共有 N 本，每本有 C 页，每页有 H 行，每行 有W 个字符。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8ZX1qZyJUGouuXic65BjgrYpG389r86ick0iaP42GlY1NgS9Dvp2RlVdHr9VSpNZTIROq7JSxsBlrUw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><ol><li><p>BN是在batch上，对N、H、W做归一化，而保留通道 C 的维度。BN 相当于把这些书按页码一一对应地加起来，再除以每个页码下的字符总数：N×H×W。</p></li><li><p>LN在通道方向上，对C、H、W归一化。LN 相当于把每一本书的所有字加起来，再除以这本书的字符总数：C×H×W。</p></li><li><p>IN在图像像素上，对H、W做归一化。IN 相当于把一页书中所有字加起来，再除以该页的总字数：H×W。</p></li><li><p>GN将channel分组，然后再做归一化。GN 相当于把一本 C 页的书平均分成 G 份，每份成为有 C/G 页的小册子，对每个小册子做Norm。</p></li></ol><p>另外，还需要注意它们的映射参数<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkCUb2OCyJa9R1WxdiclP5mCfolUQIISsZjPGFScN9Mg3SrTzQ0QHIeMw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">和<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkB7cBCm1hd4ZW9rzcvXfXCvvB4b58zjClzNswC5M388CHwScn8RLyHg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">的区别：对于 <strong>BN，IN，GN**</strong>， 其<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkCUb2OCyJa9R1WxdiclP5mCfolUQIISsZjPGFScN9Mg3SrTzQ0QHIeMw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">和<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkB7cBCm1hd4ZW9rzcvXfXCvvB4b58zjClzNswC5M388CHwScn8RLyHg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">都是维度等于通道数 C 的向<strong>**量。而对于 LN，其<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkCUb2OCyJa9R1WxdiclP5mCfolUQIISsZjPGFScN9Mg3SrTzQ0QHIeMw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">和<img src="https://mmbiz.qpic.cn/mmbiz_png/heS6wRSHVMn9a5uEF5qeEWGQHQweQrfkB7cBCm1hd4ZW9rzcvXfXCvvB4b58zjClzNswC5M388CHwScn8RLyHg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片">都是维度等于 normalized_shape 的矩阵。</strong></p><p>最后，<strong>BN 和 IN 可以</strong>设置参数：momentum和track_running_stats来获得在<strong>整体数据上更准确的均值和标准差。LN 和 GN 只能计算当前 batch 内数据的真实均值和标准差。</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Normalization方法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hook勾函数</title>
    <link href="/2021/06/30/2021-06-30-hook%E5%8B%BE%E5%87%BD%E6%95%B0/"/>
    <url>/2021/06/30/2021-06-30-hook%E5%8B%BE%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是Hook"><a href="#什么是Hook" class="headerlink" title="什么是Hook"></a>什么是Hook</h2><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy81bXQwZXd2OU9TMUFNanhPVWN4VTZjN1NOQ1RLbzdhOEFCd0JuOTBBUmVzT0U0UDJ3M3ZRWDZ0OFNEd2pGdlB3aWFlbDE3UE5nUGtJZVVqbjRnU05pYWljZy82NDA?x-oss-process=image/format,png" alt="img" style="zoom:50%;" /></p><p>钩子函数(hook function)，顾名思义，可以理解是一个挂钩，是指在执行函数和目标函数之间挂载的函数, 框架开发者给调用方提供一个point -挂载点, 至于挂载什么函数有我们调用方决定, 这样大大提高了灵活性</p><p>hook函数和我们常听到另外一个名称：回调函数（callback function）功能是类似的，可以按照同种模式来理解。</p> <span id="more"></span><h2 id="hook实现示例"><a href="#hook实现示例" class="headerlink" title="hook实现示例"></a>hook实现示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Runner</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self._hooks = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">register_hook</span>(<span class="hljs-params">self, hook</span>):<br>        self._hooks.append(hook)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call_hook</span>(<span class="hljs-params">self, fn_name</span>):<br>        <span class="hljs-keyword">for</span> hook <span class="hljs-keyword">in</span> self._hooks:<br>            <span class="hljs-built_in">getattr</span>(hook, fn_name)(self)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self</span>):<br>        self.a = <span class="hljs-number">10</span><br>        self.b = <span class="hljs-number">20</span><br>        self.call_hook(<span class="hljs-string">&#x27;before_train_epoch&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Done Epoch!&#x27;</span>)<br>        self.call_hook(<span class="hljs-string">&#x27;after_train_epoch&#x27;</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Hook</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">before_train_epoch</span>(<span class="hljs-params">self, runner</span>):<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">after_train_epoch</span>(<span class="hljs-params">self, runner</span>):<br>        <span class="hljs-keyword">pass</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AddHook</span>(<span class="hljs-title class_ inherited__">Hook</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">before_train_epoch</span>(<span class="hljs-params">self, runner</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;i am Add&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Add <span class="hljs-subst">&#123;runner.a&#125;</span> and <span class="hljs-subst">&#123;runner.b&#125;</span> equal <span class="hljs-subst">&#123;runner.a + runner.b&#125;</span>\n&#x27;</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MulHook</span>(<span class="hljs-title class_ inherited__">Hook</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">before_train_epoch</span>(<span class="hljs-params">self, runner</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;i am Mul&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Add <span class="hljs-subst">&#123;runner.a&#125;</span> and <span class="hljs-subst">&#123;runner.b&#125;</span> equal <span class="hljs-subst">&#123;runner.a * runner.b&#125;</span>\n&#x27;</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ExpHook</span>(<span class="hljs-title class_ inherited__">Hook</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">after_train_epoch</span>(<span class="hljs-params">self, runner</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;i am Exp&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Exp <span class="hljs-subst">&#123;runner.a&#125;</span> and <span class="hljs-subst">&#123;runner.b&#125;</span> equal <span class="hljs-subst">&#123;runner.a ** runner.b&#125;</span>\n&#x27;</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.runner = Runner()<br>        self.runner.register_hook(MulHook())<br>        self.runner.register_hook(ExpHook())<br>        self.runner.register_hook(AddHook())<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        self.runner.train()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    trainer = Trainer()<br>    trainer.run()<br></code></pre></td></tr></table></figure><h2 id="hook在开源框架中的应用"><a href="#hook在开源框架中的应用" class="headerlink" title="hook在开源框架中的应用"></a>hook在开源框架中的应用</h2><h3 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h3><p>在深度学习训练流程中，hook函数体现的淋漓尽致。</p><p>一个训练过程（不包括数据准备），会轮询多次训练集，每次称为一个epoch，每个epoch又分为多个batch来训练。流程先后拆解成：</p><ul><li>开始训练</li><li>训练一个epoch前</li><li>训练一个batch前</li><li>训练一个batch后</li><li>训练一个epoch后</li><li>评估验证集</li><li>结束训练</li></ul><p>这些步骤是穿插在训练一个batch数据的过程中，这些可以理解成是钩子函数，我们可能需要在这些钩子函数中实现一些定制化的东西，比如在<code>训练一个epoch后</code>我们要保存下训练的模型，在<code>结束训练</code>时用最好的模型执行下测试集的效果等等。</p><p>keras中是通过各种回调函数来实现钩子hook功能的。这里放一个callback的父类，定制时只要继承这个父类，实现你过关注的钩子就可以了。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs go">@keras_export(<span class="hljs-string">&#x27;keras.callbacks.Callback&#x27;</span>)<br>class Callback(object):<br>  <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;Abstract base class used to build new callbacks.</span><br><span class="hljs-string">  Attributes:</span><br><span class="hljs-string">      params: Dict. Training parameters</span><br><span class="hljs-string">          (eg. verbosity, batch size, number of epochs...).</span><br><span class="hljs-string">      model: Instance of `keras.models.Model`.</span><br><span class="hljs-string">          Reference of the model being trained.</span><br><span class="hljs-string">  The `logs` dictionary that callback methods</span><br><span class="hljs-string">  take as argument will contain keys for quantities relevant to</span><br><span class="hljs-string">  the current batch or epoch (see method-specific docstrings).</span><br><span class="hljs-string">  &quot;</span><span class="hljs-string">&quot;&quot;</span><br> <br>  def __init__(self):<br>    self.validation_data = None  # pylint: disable=g-missing-from-attributes<br>    self.model = None<br>    # Whether this Callback should only run on the chief worker in a<br>    # Multi-Worker setting.<br>    # TODO(omalleyt): Make this attr public once solution is stable.<br>    self._chief_worker_only = None<br>    self._supports_tf_logs = False<br> <br>  def set_params(self, params):<br>    self.params = params<br> <br>  def set_model(self, model):<br>    self.model = model<br> <br>  @doc_controls.for_subclass_implementers<br>  @generic_utils.<span class="hljs-keyword">default</span><br>  def on_batch_begin(self, batch, logs=None):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;A backwards compatibility alias for `on_train_batch_begin`.&quot;</span><span class="hljs-string">&quot;&quot;</span><br> <br>  @doc_controls.for_subclass_implementers<br>  @generic_utils.<span class="hljs-keyword">default</span><br>  def on_batch_end(self, batch, logs=None):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;A backwards compatibility alias for `on_train_batch_end`.&quot;</span><span class="hljs-string">&quot;&quot;</span><br> <br>  @doc_controls.for_subclass_implementers<br>  def on_epoch_begin(self, epoch, logs=None):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;Called at the start of an epoch.</span><br><span class="hljs-string">    Subclasses should override for any actions to run. This function should only</span><br><span class="hljs-string">    be called during TRAIN mode.</span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">        epoch: Integer, index of epoch.</span><br><span class="hljs-string">        logs: Dict. Currently no data is passed to this argument for this method</span><br><span class="hljs-string">          but that may change in the future.</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br> <br>  @doc_controls.for_subclass_implementers<br>  def on_epoch_end(self, epoch, logs=None):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;Called at the end of an epoch.</span><br><span class="hljs-string">    Subclasses should override for any actions to run. This function should only</span><br><span class="hljs-string">    be called during TRAIN mode.</span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">        epoch: Integer, index of epoch.</span><br><span class="hljs-string">        logs: Dict, metric results for this training epoch, and for the</span><br><span class="hljs-string">          validation epoch if validation is performed. Validation result keys</span><br><span class="hljs-string">          are prefixed with `val_`.</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br> <br>  @doc_controls.for_subclass_implementers<br>  @generic_utils.<span class="hljs-keyword">default</span><br>  def on_train_batch_begin(self, batch, logs=None):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;Called at the beginning of a training batch in `fit` methods.</span><br><span class="hljs-string">    Subclasses should override for any actions to run.</span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">        batch: Integer, index of batch within the current epoch.</span><br><span class="hljs-string">        logs: Dict, contains the return value of `model.train_step`. Typically,</span><br><span class="hljs-string">          the values of the `Model`&#x27;s metrics are returned.  Example:</span><br><span class="hljs-string">          `&#123;&#x27;loss&#x27;: 0.2, &#x27;accuracy&#x27;: 0.7&#125;`.</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # For backwards compatibility.<br>    self.on_batch_begin(batch, logs=logs)<br> <br>  @doc_controls.for_subclass_implementers<br>  @generic_utils.<span class="hljs-keyword">default</span><br>  def on_train_batch_end(self, batch, logs=None):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;Called at the end of a training batch in `fit` methods.</span><br><span class="hljs-string">    Subclasses should override for any actions to run.</span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">        batch: Integer, index of batch within the current epoch.</span><br><span class="hljs-string">        logs: Dict. Aggregated metric results up until this batch.</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # For backwards compatibility.<br>    self.on_batch_end(batch, logs=logs)<br>...<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了hook的概念和应用，并给出了python的实现细则。希望对比有帮助。总结如下：</p><ul><li>hook函数是流程中预定义好的一个步骤，没有实现</li><li>挂载或者注册时， 流程执行就会执行这个钩子函数</li><li>回调函数和hook函数功能上是一致的</li><li>hook设计方式带来灵活性，如果流程中有一个步骤，你想让调用方来实现，你可以用hook函数</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/pdcfighting/article/details/111243722">https://blog.csdn.net/pdcfighting/article/details/111243722</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>hook勾函数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Shadowsocks PAC模式自定义规则</title>
    <link href="/2021/06/23/2021-06-23-Shadowsocks%20PAC%E6%A8%A1%E5%BC%8F%E8%87%AA%E5%AE%9A%E4%B9%89%E8%A7%84%E5%88%99/"/>
    <url>/2021/06/23/2021-06-23-Shadowsocks%20PAC%E6%A8%A1%E5%BC%8F%E8%87%AA%E5%AE%9A%E4%B9%89%E8%A7%84%E5%88%99/</url>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://www.oldboard.tech/2021/03/02/shadowsocks-pac.html">https://www.oldboard.tech/2021/03/02/shadowsocks-pac.html</a></p><p>Shadowsocks使用PAC自动模式时，访问一个网站到底走不走代理，并不完全像我们期望的那样。这时候就需要手动进行干预。</p><h3 id="编辑PAC规则"><a href="#编辑PAC规则" class="headerlink" title="编辑PAC规则"></a>编辑PAC规则</h3><p>点击菜单栏shadowsocks图标，在下拉窗口里选择“代理设置-&gt;编辑PAC用户自定规则…”。</p> <span id="more"></span><p><img src="https://image.oldboard.tech/blog/shadowsocks-pac.jpg" alt="img"></p><p><img src="https://image.oldboard.tech/blog/shadowsocks-pacedit.png" alt="img"></p><h3 id="常用规则说明"><a href="#常用规则说明" class="headerlink" title="常用规则说明"></a>常用规则说明</h3><h4 id="标记"><a href="#标记" class="headerlink" title="* 标记"></a>* 标记</h4><p>通配符。*可以表示任何字符串，任何满足条件的都会走代理。<br>如：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-strong">*.example.com/*</span><br></code></pre></td></tr></table></figure><p>表示：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">https:<span class="hljs-regexp">//</span>www.example.com<br>https:<span class="hljs-regexp">//im</span>age.example.com<br>https:<span class="hljs-regexp">//im</span>age.example.com/abcd<br></code></pre></td></tr></table></figure><p>等，都会走代理。</p><h4 id="标记-1"><a href="#标记-1" class="headerlink" title="@@ 标记"></a>@@ 标记</h4><p>例外规则，任何满足 @@后面规则的地址，都不会走代理。<br>如：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">@@<span class="hljs-strong">*.example.com/\*</span><br></code></pre></td></tr></table></figure><p>表示：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">https:<span class="hljs-regexp">//</span>www.example.com<br>https:<span class="hljs-regexp">//im</span>age.example.com<br>https:<span class="hljs-regexp">//im</span>age.example.com/abcd<br></code></pre></td></tr></table></figure><p>等，都不会走代理。</p><h4 id="标记-2"><a href="#标记-2" class="headerlink" title="|| 标记"></a>|| 标记</h4><p>只匹配域名的结尾。<br>如：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">||example.com</span><br></code></pre></td></tr></table></figure><p>表示：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">http:<span class="hljs-regexp">//</span>example.com/abcd<br>https:<span class="hljs-regexp">//</span>example.com<br>ftp:<span class="hljs-regexp">//</span>example.com <br></code></pre></td></tr></table></figure><p>等，都会走代理。</p><h4 id="标记-3"><a href="#标记-3" class="headerlink" title="| 标记"></a>| 标记</h4><p>匹配地址的开头和结尾。<br>如：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">|https:<span class="hljs-regexp">//</span>example.com<br></code></pre></td></tr></table></figure><p>表示以 <a href="https://example.com">https://example.com</a> 开头的地址会走代理。</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">example.com<span class="hljs-string">|</span><br></code></pre></td></tr></table></figure><p>表示以 example.com 结尾的地址会走代理。</p><h4 id="标记-4"><a href="#标记-4" class="headerlink" title="! 标记"></a>! 标记</h4><p>注释。<br>如：</p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs diff"><span class="hljs-addition">! 这是一行注释</span><br><span class="hljs-addition">! ||example.com</span><br></code></pre></td></tr></table></figure><p>! 后面的内容表示注释，以!开头的规则也会无效。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p><img src="https://image.oldboard.tech/blog/shadowpaceditend.png" alt="img"></p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>每行只能写一条规则。修改PAC规则后，需要将shadowsocks关闭后重新打开，才会生效。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Shadowsocks</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习困难样本采样（Hard Mining）</title>
    <link href="/2021/05/13/2021-05-13-%E5%9B%B0%E9%9A%BE%E6%A0%B7%E6%9C%AC%E9%87%87%E6%A0%B7%EF%BC%88Hard%20Mining%EF%BC%89/"/>
    <url>/2021/05/13/2021-05-13-%E5%9B%B0%E9%9A%BE%E6%A0%B7%E6%9C%AC%E9%87%87%E6%A0%B7%EF%BC%88Hard%20Mining%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="深度学习困难样本采样（Hard-Mining）"><a href="#深度学习困难样本采样（Hard-Mining）" class="headerlink" title="深度学习困难样本采样（Hard Mining）"></a>深度学习困难样本采样（Hard Mining）</h1><p><img src="https://pic2.zhimg.com/80/v2-368ee3efb3a5f06d5c1517cef2f1d2dd_1440w.jpg" alt="img"></p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>hard negative mining顾名思义：negative，即负样本，其次是hard，说明是困难样本，也就是说在对负样本分类时候，loss比较大（label与prediction相差较大）的那些样本，也可以说是容易将负样本看成正样本的那些样本。</p><h2 id="要他有何用？"><a href="#要他有何用？" class="headerlink" title="要他有何用？"></a>要他有何用？</h2><blockquote><p>数据决定了上限，模型只是拟合上限</p></blockquote><p><strong>问题挑战</strong>：分布不一致问题。经典统计机器学习的基础假设是训练集和测试集分布一致，不一致的分布通常会导致模型学偏，训练集和验证集效果难以对齐。最直观的不一致是，<strong>大多训练集中只有正样本，没有负样本</strong>。</p><p>因此，为嵌入模型设计一个训练数据集，以便在该空间上高效有效地学习非常重要。</p><p>为了解决这个问题，hard mining是一个主要的方向，我们需要设计负采样策略来构造负样本，并尽可能使得采样出的负样本靠近测试集真实分布。</p><h2 id="采样策略"><a href="#采样策略" class="headerlink" title="采样策略"></a>采样策略</h2><p><a href="https://www.infoq.cn/article/x5qv90msfce0tkjkgbq3">KDD Cup 2020 多模态召回比赛季军方案与广告业务应用</a></p><p>在大多情况下，候选之间通常有着紧密的语义关联，需要在较细的属性粒度上对文本进行匹配的。<strong>适当增加训练数据中的强负例的难度，有助于提升模型效果</strong>。一般的做法是，从一个排序的候选段落中进行采样，越靠前的负例对模型来说难度越大。但是由于难以避免的漏标注情况，直接采样很大概率会引入假负例。</p><p>我们设计了如下表所示的四种采样策略来构建样本集。这四种策略中，随机采样得到的正负样本最容易被区分，难以区分相似的结果，需要更接近正样本的样本作为训练中的hard negative；在训练中，我们从基准模型出发，先在最简单的随机采样上训练基准模型，然后在更困难的按question标签采样、按 Query 的聚类采样的样本集上基于先前的模型继续训练，最后combination采样的样本集上训练。这样由易到难、由远到近的训练方式，有助于模型收敛到验证集分布上，在测试集上取得了更好的效果。</p><div class="table-container"><table><thead><tr><th>采样策略</th><th>描述</th><th>与验证集贴近程度</th></tr></thead><tbody><tr><td>随机采样</td><td>知识库中存在大量标准问和相似问的匹配对，将这些匹配对当作正样本；<br />随机抽取一条与标准问不匹配的标准问作为负样本，正负样本比例为 1:1</td><td>弱</td></tr><tr><td>1、按question类别采样</td><td><strong>将全部question按照其类别聚类</strong>；<br />对每个Query，在其对应的question所属的<strong>category_id</strong>中，<br />进行随机采样，作为负样本（same topic）；原始样本为正样本</td><td>中</td></tr><tr><td>2、按Query的聚类采样</td><td><strong>将全部Query按照Word Embedding进行聚类</strong>；<br />对每个question，在其对应的Query所属的聚类中，<br />进行随机采样，作为负样本；原始样本为正样本</td><td>较强</td></tr><tr><td><a href="https://www.aclweb.org/anthology/W17-7301">combination</a></td><td>50％负样本随机采样自同一类别，其余50％随机采样自其他类别；</td><td>较强</td></tr></tbody></table></div><p><a href="https://www.cnblogs.com/wanghui-garcia/p/14341485.html">简单地使用hard negative来训练的模型不能比使用random negative来训练的模型表现更好。</a>我们发现使用hardest例子并不是最好的策略。我们比较了不同排名位置的抽样，发现在排名<strong>101-500</strong>之间抽样的模型获得最好的recall。此外，在训练数据中存在easy negatives仍然是必要的，因为检索模型是在一个输入空间上操作的，该空间包含具有各种hard级别的数据，其中大多数是easy negatives。因此，<strong>在训练中混合random和hard的negative是有用的</strong>。增加easy negatives对hard negatives的比率将继续提高模型的召回， 在easy:hard=100:1的比例时趋于饱和结果。</p> <span id="more"></span><p>Trick：</p><p><a href="https://www.zhihu.com/question/324986054">focal loss解决hard sample mining。如果不知道怎么找准对应场景的困难样本，同时暂时也还在用交叉熵做二分类loss的话，可以无脑迁移focal loss。</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Hard Mining</tag>
      
      <tag>困难样本采样</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LambdaMART二次深入理解</title>
    <link href="/2021/04/20/2021-04-20-LambdaMART%E4%BA%8C%E6%AC%A1%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
    <url>/2021/04/20/2021-04-20-LambdaMART%E4%BA%8C%E6%AC%A1%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>原文： <a href="https://www.6aiq.com/forward?goto=https%3A%2F%2Fmedium.com%2Frocket-travel%2Fwhy-we-chose-lambdamart-for-our-hotel-ranking-model-45f84e22cec">https://medium.com/rocket-travel/why-we-chose-lambdamart-for-our-hotel-ranking-model-45f84e22cec</a></p><p>在本文中，我们将深入讨论我们选择的模型：LambdaMART:</p><ol><li>提升(简而言之)：利用梯度提升树的理论和实际效益</li><li>LambdaMART 是如何工作的？Pairwise 学习，NDCG 调整权重，解决表现偏差</li><li>将 LambdaMART 对“期望盈利能力”进行排序</li><li>关于特征工程的一些讨论</li><li>为什么不是隐语义模型？</li></ol><span id="more"></span><h1 id="LambdaMART-的优点"><a href="#LambdaMART-的优点" class="headerlink" title="LambdaMART 的优点"></a>LambdaMART 的优点</h1><p>我们选择 LambdaMART 的核心原因是：</p><ul><li><strong>灵活的数据输入</strong>。作为一个梯度提升树，LambdaMART 接受任何浮点数据作为输入，允许缺失值，不需要数据标准化。只有类别变量不能被自动转换为浮点数，但是具有低基数的变量可以映射到整数。因此，我们唯一不能包含的数据是酒店 id 和用户 id，因为它们的基数太高。</li><li><strong>强大的函数逼近能力</strong>。梯度提升树理论上是一个通用的函数逼近器。在实践中，这不会是真的，但梯度提升树的性能仍几乎无与伦比，只要特征集可以对于因变量可以形成有意义的描述。</li><li><strong>易于实现</strong>。<code>xgboost</code> 包有一个高度优化的 LambdaMART 实现，它允许我们用一行代码在几分钟内原型化模型。</li><li><strong>有意义的得分函数</strong>。下一节将解释，模型得分可以通过一个 softmax 或指数函数来表示一家酒店相对于另一家酒店的预订可能性。这让我们能够根据酒店的预期盈利能力对它们进行有意义的重新排序。</li><li><strong>可解释的预测</strong>。该模型做出的每一个预测都可以用 SHAP 值来解释，我们在前一篇文章中提到过。</li></ul><p>如果不首先理解 LambdaMART 在做什么，就很难扩展这些要点，所以我们将简要地介绍一下模型的结构。</p><h1 id="对-LambdaMART-了解的更清楚一些"><a href="#对-LambdaMART-了解的更清楚一些" class="headerlink" title="对 LambdaMART 了解的更清楚一些"></a>对 LambdaMART 了解的更清楚一些</h1><p><img src="https://img.6aiq.com/e/b6dfd3c8d0bc44ba8ece16e1b1cee901.webp" alt="img"></p><p>是时候把我的“去神秘化”也加到这一堆里了</p><p>因为 LambdaMART 方法经过了多年的改进，所以符号是不一致的，并且很大程度上集中在高效计算上，我们把这个问题留给了 <code>xgboost</code>。因此，我们的解释从头开始，重点放在对我们的用例很重要的核心思想上。</p><h2 id="LambdaMART-中的-MART，非常简短"><a href="#LambdaMART-中的-MART，非常简短" class="headerlink" title="LambdaMART 中的 MART，非常简短"></a>LambdaMART 中的 MART，非常简短</h2><p>在它的核心，LambdaMART 是一个损失函数(“Lambda”)附加到一个梯度提升森林(“MART” — 多重加法回归树)上。梯度提升森林通过训练一棵新树来预测之前出现的树的误差(损失函数的“梯度”)来改进它的预测。预测是通过将第一棵树的原始估计值与后续树的所有修正值相加而得出的。</p><p><img src="https://img.6aiq.com/e/2623eab681044da78fe6adb004ddf322.webp" alt="img"></p><p>提升森林中的每一棵新树都能在集成之前提前预测出误差。</p><p>由于树不会像线性回归那样对模型函数空间施加强大的结构，LambdaMART 实现良好排名的能力几乎完全取决于其“Lambda”误差的质量。由于这个原因，损失函数可以被详细的解释。</p><h2 id="LambdaMART-的损失"><a href="#LambdaMART-的损失" class="headerlink" title="LambdaMART 的损失"></a>LambdaMART 的损失</h2><p>Lambda 损失要求某些搜索结果的得分高于其他搜索结果。例如，预订的酒店应该比只被点击的酒店得分高，而被点击的酒店应该比完全被用户忽略的酒店得分高。使这些 <strong>pairwise</strong> 的比较是 Lambda 梯度函数的核心。</p><p>更精确地说，给定酒店和分数 I，酒店和分数 J，Lambda 梯度估计的酒店优于酒店的概率为<strong>σ</strong>(<em>J-I)</em>，其中<strong>σ</strong>是 sigmoid 函数。sigmoid 通常使用 e=2.7182818…，但是 Lambda 训练过程实际上对 sigmoid 的 base 是不变的。</p><p><img src="https://img.6aiq.com/e/449c87eed50b413f9d869dafe624d068.webp" alt="img"></p><p>◁ 运算符表示优于，比如，a◁b 表示 a 优于 b</p><p><img src="https://img.6aiq.com/e/b7aa64a71d1c4448af3dba67d0306be9.webp" alt="img"></p><p>σ(x)的图</p><p>_x_i_比 _x_j_的优先的概率代表什么？以下是几种解释：</p><ol><li>酒店_x_i_被点击或订购的次数比_x_j_大。</li><li>酒店_x_i_被选中的条件概率，假如只有_x_i_和_x_j_两个酒店，这两个酒店中只有一个被选中。</li></ol><p>我们希望将我们估算的 pairwise 偏好概率与我们排名中所有酒店对(<em>x_i, x_j</em>)的真实偏好概率之间的距离最小化：</p><p><img src="https://img.6aiq.com/e/5d7a139096634e26b659625a73e90f9c.webp" alt="img"></p><p>这对应的是关于 x_i 是否优于 x_j 的标准交叉熵损失。</p><p>使用这样的 pairwise 损失，我们就能训练出一个像样的模型。然而，我们可以考虑两个额外的效应来获得更好的性能：</p><p><strong>并不是所有的酒店对都是一样的</strong>：例如，我们并不关心排名第 340 位和第 341 位的酒店交换模式，而是关心排名第 1 位和第 341 位的酒店交换模式。如果上面的排名更正确，那么下面的一些错误排名是可以接受的。</p><p><strong>点击数据有偏差</strong>：排名高的结果更容易被观察到(<strong>表示偏差</strong>)，即使被观察到了，排名高的结果也更容易被点击(<strong>信任偏差</strong>)。考虑到这一点，排名较高的结果应该是有问题的。</p><p>我们依次来看。</p><h2 id="使用-NDCG-weighting-调整-listwise-损失"><a href="#使用-NDCG-weighting-调整-listwise-损失" class="headerlink" title="使用 NDCG-weighting 调整 listwise 损失"></a>使用 NDCG-weighting 调整 listwise 损失</h2><p>高排名的结果相比低排名的结果的重要性通过 NDCG 来进行度量。下面显示了 NDCG 的一个表达式，其中 IDCG 是排序所能达到的理想或最大 DCG， rel__i_是排在第 i 个的搜索结果的相关性标签值。</p><p><img src="https://img.6aiq.com/e/a7114e00297743c4b72cb05a08695d9e.webp" alt="img"></p><p>NDCG 是唯一的排名指标，可以区分出两个排序中哪个是更好的一个。除此之外，它对你的直觉几乎毫无用处。随着搜索结果数量的增加，搜索请求的预期 NDCG 收敛到 0，因此用不同的基数比较两个搜索请求的 NDCGs 是不合适的。</p><p>我们可以看到，当相关结果的排名较差时，DCG 分数会下降，因为贡献的分数被一个对数因子打折了。NDCG 为 99 个不相关的结果(相关标签为 0)和 1 个相关的结果(相关标签为 1)的搜索请求进行评分，下面是每一个可能的排序的相关结果图。</p><p><img src="https://img.6aiq.com/e/6b563927ef3748f98c86db98448ec2c6.webp" alt="img"></p><p>这个特殊的 NDCG 曲线对应于方程 1/log2(x+1)</p><p>给于高排名对比低排名对更高的权重，每一对(<em>x___i, x_j</em>)通过ΔNDCG 来加权，这个差值为 NDCG 分数和交换之后的 NDCG 分数之间的绝对差。</p><p>例如，如果相关结果排在第 1 位，它的 NDCG 值为 1，而如果与排在第 100 位的无关结果进行交换，NDCG 值为。1502。那一对排序(1, 100)的 NDCG 的变化为 |1-0.1502|=|0.8498|。</p><h2 id="使用反向加权来去除点击数据的偏差"><a href="#使用反向加权来去除点击数据的偏差" class="headerlink" title="使用反向加权来去除点击数据的偏差"></a>使用反向加权来去除点击数据的偏差</h2><p>表示偏差影响用户查看搜索结果的概率。如果一个搜索结果的排名靠后，那么用户看到它的几率就会降低。信任偏差会影响一个搜索结果被点击的概率，即使它被用户看过。虽然这是两种不同的效果，但它们都可以根据搜索结果的排名改变其被点击的概率。我们通过估计每个位置_i_上的两个量来衡量这种效果：</p><ul><li>本文地址：<a href="https://www.6aiq.com/article/1581480308811">为什么我们选择 LambdaMART 作为我们的酒店排序模型</a></li><li>本文版权归作者和<a href="https://www.6aiq.com/">AIQ</a>共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出</li></ul><ol><li><strong>正偏好比 p+(i)</strong>，搜索结果如果排在其他的位置相对于它在第 i 位的时候，点击率的比例。例如，如果 rank 1 是参考点，rank 2 的正偏好比例是 0.25，那么我们期望一个搜索结果在 rank 2 的时候被点击次数是排在 rank 1 的 25%。在这个例子里，第一位的正偏好比例一般是 1。</li><li><strong>负偏好比 p-(i)</strong>，或_倒数_是不相关的，如果它占据其他一些里程碑的排名。例如，如果 rank 1 是参考点，而 rank 2 的负偏好比是 4，那么我们就会认为未被点击的 rank 2 的搜索结果的不相关性是未被点击的 rank 1 的搜索结果的 4 倍。</li></ol><p>p+(<em>i</em>) 和 p-(<em>i</em>) 对于所有的排序位置 i 在 LambdaMART 训练过程中都是可迭代估计的，使用下面一组方程。其中，<em>L(i, j)</em>是原始（有偏差的）的 pairwise 损失函数，p* 这一项是在前一次迭代中估计出来的，然后对所有的查询 q 和搜索结果 j 加起来。</p><p><img src="https://img.6aiq.com/e/57bfe7acca344e08b6b0b119fc634a56.webp" alt="img"></p><p>表现偏差的影响可以通过简单地用损失除以偏差来逆转。这个修正给了我们一个一致的和无偏的 pairwise 的损失。</p><p><img src="https://img.6aiq.com/e/4984b4a4b96340e8af64134b21b30244.webp" alt="img"></p><h2 id="把所有的组合起来"><a href="#把所有的组合起来" class="headerlink" title="把所有的组合起来"></a>把所有的组合起来</h2><p>搜索结果_x_i_的 Lambda 梯度很简单，就是这些加权的 NDCG 全部加起来，就是对于每一对 (<em>x_i, x_j</em>)的修正了偏差的 pairwise 误差。注意，使用了相同的 label 的结果在交换的时候在 NDCG 上没有变化，所以，大部分的值是 0.方程如下：</p><p><img src="https://img.6aiq.com/e/1254bf38ce1444bf9eb6f4da1059b266.webp" alt="img"></p><h2 id="偏好概率的校准度量"><a href="#偏好概率的校准度量" class="headerlink" title="偏好概率的校准度量"></a>偏好概率的校准度量</h2><p>我之前说过 LambdaMART 估计了 i 优于 j 的偏好概率。这是一个相当强的声明，特别是当 loss 函数包含 NDCG-weighting 的时候。然而，下面的图表显示，该模型是根据经验进行了概率估计校准的。为了进行校准，酒店 i，模型估计有 p% 的概率优于某个酒店 j 被选中(在 x 轴上)，需要实际被选中的概率为 p%(在 y 轴上)。</p><p><img src="https://img.6aiq.com/e/fc70807825504c34b5c313e75660db1c.webp" alt="img"></p><p>原始 Lambda 论文还对使用 Lambda 梯度训练的神经网络模型参数的小扰动导致训练集上的期望 NDCG 较低进行了数值观察，表明该模型在拟合偏好概率的同时，联合最大化了 NDCG。同时做两件事情的能力都意味着 NDCG 最大化和最小化的目标偏好概率损失没有强烈的相互冲突，所以我们可以鱼（校准概率）和熊掌（NDCG 加权）兼得。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.6aiq.com/article/1581480308811">为什么我们选择 LambdaMART 作为我们的酒店排序模型</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LambdaMART</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多叉树treelib&amp;区间树intervaltree</title>
    <link href="/2021/03/04/2021-03-04-%E5%A4%9A%E5%8F%89%E6%A0%91&amp;%E5%8C%BA%E9%97%B4%E6%A0%91/"/>
    <url>/2021/03/04/2021-03-04-%E5%A4%9A%E5%8F%89%E6%A0%91&amp;%E5%8C%BA%E9%97%B4%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<p><a href="https://treelib.readthedocs.io/en/latest/">treelib 库</a>是一个 Python 的第三方库。这个库实现了一些多叉树相关的常用方法。</p><p><a href="https://github.com/chaimleib/intervaltree">intervaltree库</a>是一个可变的、自平衡的区间树。查询可以按点、按范围重叠或按范围包含。</p><span id="more"></span><h2 id="treelib-库"><a href="#treelib-库" class="headerlink" title="treelib 库"></a><a href="https://treelib.readthedocs.io/en/latest/">treelib 库</a></h2><p>用于构建多叉树</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#创建一棵多叉树</span><br><span class="hljs-comment">#show(): 将多叉树按树形结构展示输出</span><br><span class="hljs-keyword">from</span> treelib <span class="hljs-keyword">import</span> Tree, Node<br>tree = Tree()<br>tree.show()<br><span class="hljs-built_in">print</span>(tree.identifier)<br><br><span class="hljs-comment">#添加节点到多叉树中</span><br>tree.create_node(tag=<span class="hljs-string">&#x27;Node-5&#x27;</span>, identifier=<span class="hljs-string">&#x27;node-5&#x27;</span>, data=<span class="hljs-number">5</span>)<br>tree.create_node(tag=<span class="hljs-string">&#x27;Node-10&#x27;</span>, identifier=<span class="hljs-string">&#x27;node-10&#x27;</span>, parent=<span class="hljs-string">&#x27;node-5&#x27;</span>, data=<span class="hljs-number">10</span>)<br>tree.create_node(<span class="hljs-string">&#x27;Node-15&#x27;</span>, <span class="hljs-string">&#x27;node-15&#x27;</span>, <span class="hljs-string">&#x27;node-10&#x27;</span>, <span class="hljs-number">15</span>)<br>tree.show()<br><br>node = Node(tag=<span class="hljs-string">&#x27;Node-A&#x27;</span>, identifier=<span class="hljs-string">&#x27;node-A&#x27;</span>, data=<span class="hljs-string">&#x27;A&#x27;</span>)<br>tree.add_node(node, parent=<span class="hljs-string">&#x27;node-5&#x27;</span>)<br>tree.show()<br>              <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Node-5</span><br><span class="hljs-string">└── Node-10</span><br><span class="hljs-string">    └── Node-15</span><br><span class="hljs-string">    </span><br><span class="hljs-string">Node-5</span><br><span class="hljs-string">├── Node-10</span><br><span class="hljs-string">│   └── Node-15</span><br><span class="hljs-string">├── Node-A</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment">#节点的属性和方法</span><br><span class="hljs-built_in">print</span>(node)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;node id: &#x27;</span>, node.identifier)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;node tag:&#x27;</span>, node.tag)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;node data:&#x27;</span>, node.data)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;node is leaf: &#x27;</span>, node.is_leaf())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;node is root: &#x27;</span>, node.is_root())<br><br><span class="hljs-comment">#多叉树中的节点个数</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;tree len: &#x27;</span>, <span class="hljs-built_in">len</span>(tree))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;tree size:&#x27;</span>, tree.size())<br><br><span class="hljs-comment">#多叉树的深度和叶子节点</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;tree depth:&#x27;</span>, tree.depth())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;node-20 depth:&#x27;</span>, tree.depth(node=<span class="hljs-string">&#x27;node-20&#x27;</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;node-20 level:&#x27;</span>, tree.level(<span class="hljs-string">&#x27;node-20&#x27;</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;tree leaves:&#x27;</span>, tree.leaves())<span class="hljs-comment">#返回多叉树的所有叶节点</span><br><span class="hljs-built_in">print</span>(tree.paths_to_leaves())  <span class="hljs-comment">#返回根节点到每个叶节点的路径上的所有节点id</span><br><br><span class="hljs-comment">#返回多叉树中的节点</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;tree nodes:&#x27;</span>, tree.nodes)<br><span class="hljs-built_in">print</span>(tree.all_nodes())<br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> tree.all_nodes_itr():<br>    <span class="hljs-built_in">print</span>(node)<br><br><span class="hljs-comment">#多叉树转换成字典和保存到文件中</span><br><span class="hljs-built_in">print</span>(tree.to_dict())<br><span class="hljs-built_in">print</span>(tree.to_json())<br>tree.to_graphviz()<br>tree.save2file(<span class="hljs-string">&#x27;demo_tree.tree&#x27;</span>)<br></code></pre></td></tr></table></figure><p>应用：通过多叉树解决嵌套实体的问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Nodex</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tmp</span>):<br>        self.tmp = tmp<br>        <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_judge</span>(<span class="hljs-params">item_a, item_b</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;判断实体之间是否存在交集</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>    a1, b1 = item_a[<span class="hljs-string">&quot;char_offset&quot;</span>], item_a[<span class="hljs-string">&quot;char_offset&quot;</span>] + item_a[<span class="hljs-string">&quot;char_length&quot;</span>]<br>    a2, b2 = item_b[<span class="hljs-string">&quot;char_offset&quot;</span>], item_b[<span class="hljs-string">&quot;char_offset&quot;</span>] + item_b[<span class="hljs-string">&quot;char_length&quot;</span>]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">max</span>(a1, a2) &lt; <span class="hljs-built_in">min</span>(b1, b2):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>      <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_slot_dic_from_tree</span>(<span class="hljs-params">result_tmp</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;通过多叉树输出嵌套实体（非包含关系）的排列组合结果</span><br><span class="hljs-string">    result_tmp: 实体相关信息 ↑</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    tree1 = Tree()<br>    tree1.create_node(<span class="hljs-string">&quot;Root&quot;</span>, <span class="hljs-string">&quot;root&quot;</span>, data=Nodex(-<span class="hljs-number">1</span>))  <span class="hljs-comment">#  根节点</span><br>    root2leave = []<br>    <span class="hljs-keyword">if</span> result_tmp:<br>        result_tmp.sort(key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;char_offset&quot;</span>])<br>        <span class="hljs-comment"># 第一个new_node直接加入tree</span><br>        tree1.create_node(<span class="hljs-string">&quot;Child0&quot;</span>, <span class="hljs-string">&quot;child0&quot;</span>, parent=<span class="hljs-string">&quot;root&quot;</span>, data=Nodex(result_tmp[<span class="hljs-number">0</span>]))<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(result_tmp)):<br>            parent_idt_list = []<br>            <span class="hljs-comment"># 每个new_node与所有的叶子结点进行比较</span><br>            <span class="hljs-keyword">for</span> j, leaf <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tree1.leaves()):<br>                idt = leaf.identifier  <span class="hljs-comment"># 叶子结点标识符</span><br>                parent_idt = tree1.parent(idt).identifier  <span class="hljs-comment"># 父结点标识符</span><br>                node_a = Node(<br>                    tag=<span class="hljs-string">&quot;Child&quot;</span> + <span class="hljs-built_in">str</span>(i) + <span class="hljs-string">&quot;_&quot;</span> + <span class="hljs-built_in">str</span>(j),<br>                    identifier=<span class="hljs-string">&quot;Child&quot;</span> + <span class="hljs-built_in">str</span>(i) + <span class="hljs-string">&quot;_&quot;</span> + <span class="hljs-built_in">str</span>(j),<br>                    data=Nodex(result_tmp[i]),<br>                )<br>                <span class="hljs-comment"># 1、如果存在嵌套（冲突），则父节点增加新分支</span><br>                <span class="hljs-keyword">if</span> cross_judge(result_tmp[i], leaf.data.tmp):<br>                    <span class="hljs-comment"># 针对相同的new_node，父节点仅可增加一个新分支</span><br>                    <span class="hljs-keyword">if</span> parent_idt <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> parent_idt_list:<br>                        tree1.add_node(node_a, parent=parent_idt)<br>                        parent_idt_list.append(parent_idt)<br>                <span class="hljs-comment"># 2、否则，当前节点加深</span><br>                <span class="hljs-keyword">else</span>:<br>                    tree1.add_node(node_a, parent=idt)<br>        <span class="hljs-comment"># 最后返回【根节点→叶子结点】的所有路径，即所有不存在嵌套实体的排列组合结果</span><br>        <span class="hljs-keyword">for</span> path <span class="hljs-keyword">in</span> tree1.paths_to_leaves():<br>            item = [tree1.get_node(idt).data.tmp <span class="hljs-keyword">for</span> idt <span class="hljs-keyword">in</span> path[<span class="hljs-number">1</span>:]]<br>            root2leave.append(item)<br>    <span class="hljs-keyword">return</span> root2leave<br></code></pre></td></tr></table></figure><p>图示解决过程：</p><p><img src="/Users/ningshixian/NutstoreCloudBridge/坚果云相册/嵌套实体.jpg" alt="嵌套实体" style="zoom: 33%;" /></p><h2 id="intervaltree库"><a href="#intervaltree库" class="headerlink" title="intervaltree库"></a><a href="https://github.com/chaimleib/intervaltree">intervaltree库</a></h2><p>判断<strong>多个区间</strong>是否有重叠：利用intervaltree<strong>构建区间树</strong>，也即构建一个二叉排序树，树的叶节点是每个区间。同时，树的查询效率和树的深度是有关系的，所以构建一个相对平衡的二叉排序树也能进一步提高效率，比如红黑树。算法导论上介绍了这种数据结构，<strong>节点是区间的红黑树——区间树</strong>。</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> intervaltree <span class="hljs-keyword">import</span> Interval, IntervalTree</span><br><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">tree = IntervalTree()</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">iv = Interval(<span class="hljs-number">4</span>, <span class="hljs-number">7</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">tree.add(iv)</span><br><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">target_iv = Interval(<span class="hljs-number">5</span>, <span class="hljs-number">8</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">tree.overlaps(target_iv)</span><br>True<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">target_iv_2 = Interval(<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">tree.overlaps(target_iv_2)</span><br>False<br></code></pre></td></tr></table></figure><p>应用：如“今天8点到10点, 8个人”提取时间和人数时，idx=text.find(x)只能查找第一个时间“8”出现位置的索引，导致人数“8”的位置被覆盖了。解决：在发现实体区间有重叠时，循环执行 idx=text.find(x, idx+1)，直到-1跳出；</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment"># 寻找最适合的索引</span><br>interval_tree = IntervalTree()  <span class="hljs-comment"># 用于判断实体区间的重叠情况！</span><br>char_offset = <span class="hljs-keyword">text</span>.find(x.<span class="hljs-built_in">upper</span>())  <span class="hljs-comment"># first</span><br><span class="hljs-keyword">while</span> char_offset!=<span class="hljs-number">-1</span>:<br>    target_iv = Interval(char_offset, char_offset+<span class="hljs-built_in">len</span>(x))<br>    is_overlaps = bool(interval_tree.overlaps(target_iv))<br>    <span class="hljs-keyword">if</span> is_overlaps:    <span class="hljs-comment"># 有重叠，则继续找，直到-1跳出</span><br>        char_offset = <span class="hljs-keyword">text</span>.find(x.<span class="hljs-built_in">upper</span>(), char_offset+<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">else</span>:    <span class="hljs-comment"># 无重叠，直接跳出</span><br>        interval_tree.<span class="hljs-built_in">add</span>(target_iv)<br>        break<br><span class="hljs-keyword">if</span> char_offset==<span class="hljs-number">-1</span>:<br>    char_offset = <span class="hljs-keyword">text</span>.find(x.<span class="hljs-built_in">upper</span>())  <span class="hljs-comment"># first</span><br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/weixin_43790276/article/details/108248298">Python treelib库创建多叉树的用法介绍</a></p><p><a href="https://www.jb51.net/article/167331.htm">Python多叉树的构造及取出节点数据(treelib)的方法</a></p><p><a href="https://treelib.readthedocs.io/en/latest/index.html">treelib官方文档</a></p><p><a href="https://schecterdamien.github.io/2018/10/10/interval-tree/">判断多个区间是否有重叠</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>多叉树treelib</tag>
      
      <tag>区间树intervaltree</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CRF</title>
    <link href="/2021/02/22/2021-02-22-CRF%E5%9B%9E%E9%A1%BE/"/>
    <url>/2021/02/22/2021-02-22-CRF%E5%9B%9E%E9%A1%BE/</url>
    
    <content type="html"><![CDATA[<ul><li>什么是CRF？CRF用来干什么？</li><li>CRF模型公式？</li><li>特征函数是什么意思？特征模版什么意思？</li><li>CRF损失如何计算的？全部路径loss怎么算？</li><li>CRF的数据是如何生成的？</li><li>维特比算法没看懂，啥意思？</li><li>源代码怎么学习？道理我都懂，怎么用呢？</li><li>手撕BiLSTM + CRF </li></ul><span id="more"></span><h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><h3 id="【1】什么是CRF？CRF用来干什么？"><a href="#【1】什么是CRF？CRF用来干什么？" class="headerlink" title="【1】什么是CRF？CRF用来干什么？"></a>【1】什么是CRF？CRF用来干什么？</h3><p>该博客使用一个“照片分类”任务生动介绍了CRF在序列标注中的应用场景。</p><p>英文的地址为：<a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields</a></p><p>想象一下，贾斯汀·比伯(JustinBieber)生活中有一天会有一系列的快照，而你想用每个图像的活动(吃饭，睡觉，开车等等)来标记每个图像。你该怎样做呢？</p><p>一种方法是忽略快照的顺序性质，并构建一个图像分类器。例如，如果给予一个月的标签快照，您可能会发现早上6点拍摄的黑色图像往往是关于睡觉的，有很多鲜艳颜色的图像往往是关于跳舞，汽车图像是关于驾驶的，等等。</p><p>然而，通过忽略这个连续的方面，你失去了大量的信息。例如，如果您看到一张嘴巴的特写图片会发生什么？是关于唱歌还是饮食？如果你知道前面的图片是贾斯汀·比伯(Justin Bieber)的饮食或烹饪图片，那么这张图片更有可能是关于吃东西的。但是，如果前面的图像包含贾斯汀·比伯(Justin Bieber)唱歌或者跳舞，那么这个人可能也会演唱。</p><p>因此，为了提高labeller的精度，我们应该加入附近照片的标签，这正是条件随机场所做的。</p><p><strong>词性标注</strong></p><p>让我们进入一些更详细的部分，使用更常见的词性标注的例子。</p><p>在POS标记中，目标是用诸如ADJECTIVE，NOUN，PREPOSITION，VERB，ADVERB，ARTICLE等标签来标记一个句子(一个单词或一个令牌序列)。</p><p>例如，给出“Bob drank coffee at Starbucks”的句子，标签可能是“ob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN)”。</p><p>那么让我们建立一个条件随机场来为他们的词性标注句子。就像任何分类器一样，我们首先需要决定一组特征函数$ f_i $</p><p><strong>CRF中的特征函数</strong></p><p>在CRF中，每个特征函数都是一个输入的函数：</p><ul><li><p>一个句子 s</p></li><li><p>单词 i 在句子中的位置</p></li><li><p>当前单词的标签 $ l_i $</p></li><li><p>前一个词的标签 $ l_ {i-1} $</p></li></ul><p>输出的是一个实数值(尽管数字通常只是0或1)。</p><p>(注意：通过限制我们的特性只依赖当前和之前的标签，而不是整个句子中的任意标签，我实际上构建了一个<strong>linear-chain CRF</strong>的特殊情况。为了简单起见，在这篇文章中我将忽略 general CRFs)</p><p>例如，一个可能的特征函数可以测量我们怀疑当前单词应该被标记为形容词，因为前一个单词是“very”</p><p><strong>特征到概率</strong></p><p>接下来，将每个特征函数$ f_j $ 分配一个权重 $ \lambda_j $(我将在下面讨论如何从数据中学习这些权重)。给定句子s，现在我们可以通过在句子中的所有单词上加上权重特征来对s的标签进行评分：</p><p>$ score(l | s)= \sum_{j = 1} ^ m \sum_ {i = 1} ^ n \lambda_j f_j(s,i,l_i,l_ {i-1})$</p><p> (The first sum runs over each feature function $ j $, and the inner sum runs over each position $ i $ of the sentence.)</p><p>最后，我们可以通过指数化和规范化将这些分数转换成0到1之间的概率   $ p(l | s)$：</p><script type="math/tex; mode=display">p(l \mid s)=\frac{\exp [\operatorname{score}(l \mid s)]}{\sum_{l^{\prime}} \exp \left[\operatorname{score}\left(l^{\prime} \mid s\right)\right]}=\frac{\exp \left[\sum_{j=1}^{m} \sum_{i=1}^{n} \lambda_{j} f_{j}\left(s, i, l_{i}, l_{i-1}\right)\right]}{\sum_{l^{\prime}} \exp \left[\sum_{j=1}^{m} \sum_{i=1}^{n} \lambda_{j} f_{j}\left(s, i, l_{i}^{\prime}, l_{i-1}^{\prime}\right)\right]}</script><p><strong>示例特征函数</strong></p><p>那么这些功能函数是什么样的呢？ POS标记功能的例子可以包括：</p><ul><li><p>$ f_1(s，i，l_i，l_ {i-1})= 1 $ if $ l_i = $ ADVERB，and 第i个字以“-ly”结尾；否则为0。如果与此功能相关的重量$ \lambda_1 $大而正，那么这个特征实际上就是说，我们更喜欢将其中以-ly结尾的单词标记为ADVERB。</p></li><li><p>$f_2(s，i，l_i，l_ {i-1})= 1 $if $ i = 1 $，$ l_i = $ VERB，and 句子以问号结尾；否则为0。同样，如果与此特征相关联的权重$ \lambda_2 $是大的并且是肯定的，那么将VERB分配给问题中的第一个单词</p></li><li><p>$ f_3(s，i，l_i，l_ {i-1})= 1 $ if $ l_ {i-1} = $ ADJECTIVE and $ l_i = $NOUN；否则为0。同样，这个特征的正面权重意味着形容词往往被名词所覆盖。</p></li><li>$ f_4(s，i，l_i，l_ {i-1})= 1 $ if $ l_ {i-1} = $ PREPOSITION and $ l_i = $ PREPOSITION。这个函数的负权重$ \lambda_4 $将意味着介词不倾向于跟在介词后面，所以我们应该避免在这种情况发生的地方进行标记。</li></ul><p><strong>总结一下：为了建立一个条件随机场，你只需要定义一堆特征函数(可以依赖于整个句子，当前位置和附近的标签)，赋予它们权重，并将它们加在一起，必要时将其转换为概率。</strong></p><p>现在让我们退后一步，比较CRF和其他一些常见的机器学习技术。</p><p><strong>看起来像逻辑回归</strong>…</p><p>CRF概率的形式：$ p(l | s)= \frac {exp {\sum_ {j = 1} ^ m \sum_ {i = 1} ^ n f_j(s，i，l_i，l_ {i-1 }}]} {\sum_ {l’} exp [\sum_ {j = 1} ^ m \sum_ {i = 1} ^ nf_j(s，i，l’_i，l’_ {i-1}) ]} $ 可能看起来很熟悉。</p><p>这是因为CRF确实是逻辑回归的顺序版本： 而逻辑回归是分类的对数线性模型，CRF是顺序标签的对数线性模型。</p><p><strong>看起来像HMMs …</strong></p><p>回想一下，隐马尔可夫模型是词性标注的另一种模型。鉴于CRF将任何一组函数放在一起以获得标签分数，HMM采用<strong>生成方法</strong>来标记，定义：</p><p>   $ p(l,s)= p(l_1)\prod_i p(l_i | l_ {i-1})p(w_i| l_i)$</p><p> 其中：</p><ul><li><p>$ p(l_i | l_ {i-1})$ 是转移概率 (例如介词后跟一个名词的概率)</p></li><li><p>$ p(w_i | l_i)$  是发射概率 (例如，名词发出单词“爸爸”的概率)</p></li></ul><p>那HMM怎么和CRF比较？ CRF功能更强大 - 它们可以模拟HMM可以做的所有事情。One way of seeing this is as follows.</p><p>注意，HMM概率的对数是 $ \log p(l,s)= \log p(l_0)+ \sum_i \log p(l_i | l_ {i-1})+ \sum_i \log p(w_i | l_i)$。This has exactly the log-linear form of a CRF if we consider these log-probabilities to be the weights associated to binary transition and emission indicator features.</p><p>也就是说，我们可以构建一个与任何HMM等价的CRF通过：</p><ul><li><p>对于每个HMM转移概率$ p(l_i = y | l_ {i-1} = x)$，定义一组CRF转移特征，形式为$ f_ {x，y}(s,i,l_i,l_ {i -1})= 1 $如果$l_i = y $和$ l_ {i-1} = x $。给每个特征赋予$ w_ {x,y} = \log p(l_i = y | l_ {i-1} = x)$的权重</p></li><li><p>类似地，对于每个HMM发射概率$ p(w_i = z | l_ {i} = x)$，定义一组CRF发射特征的形式为$ g_ {x，y}(s,i,l_i,l_ {i -1})= 1 $如果$w_i = z $和$ l_i = x $。给每个特征赋予权重$w_ {x,z} = \log p(w_i = z |l_i = x)$</p></li></ul><p>因此，由CRF使用这些特征函数计算的得分$ p(l| s)$正好与由相关的HMM计算的得分成正比，因此每个HMM等同于一些CRF。</p><p>然而，通用报告格式也可以模拟更丰富的标签分布，主要有两个原因：</p><ul><li><p><strong>CRF可以定义更多的特征。</strong>尽管HMM本质上是本地的(因为它们被限制于二进制转换和发射特征函数，这迫使每个词只依赖于当前标签，而每个标签仅依赖于以前的标签)，CRF可以使用更多的全局特征。例如，上面POS标记器中的一个特征增加了标签的可能性，如果句子结尾包含问号，则将标签的第一个单词标记为VERB。</p></li><li><p><strong>CRF可以有任意的权重。</strong>而HMM的概率必须满足一定的约束(例如，$ 0 &lt;= p(w_i | l_i)&lt;= 1,\sum_w p(w_i = w | l_1)= 1)$，CRF的权重是不受限制的，$ \log p(w_i | l_i)$可以是任何想要的)。</p></li></ul><p><strong>学习权重</strong></p><p>让我们回到如何学习CRF中的特征权重的问题。一种方法是使用梯度下降 <strong>gradient descent</strong>。</p><p>假设我们有一堆训练examples (句子和相关的词性标签)。随机初始化我们的CRF模型的权重。要将这些随机初始化权重转换为正确的权重，对于每个训练示例：</p><ul><li><p>通过每个特征函数$ f_i $，并计算训练样例相对于$ \lambda_i $的梯度：</p><p>$ \frac{\partial}{\partial w_{i}} \log p(l \mid s)=\sum_{j=1}^{m} f_{i}\left(s, j, l_{j}, l_{j-1}\right)-\sum_{l^{\prime}} p\left(l^{\prime} \mid s\right) \sum_{j=1}^{m} f_{i}\left(s, j, l_{j}^{\prime}, l_{j-1}^{\prime}\right)$</p></li><li><p>请注意，梯度中的第一项是真实标签下特征$ f_i $的贡献，梯度中的第二项是当前模型下特征$ f_i $的预期贡献。这正是您期望渐变的形式。</p></li><li><p>在渐变方向上移动 $λ_i$：$\lambda_{i}=\lambda_{i}+\alpha\left[\sum_{j=1}^{m} f_{i}\left(s, j, l_{j}, l_{j-1}\right)-\sum_{l^{\prime}} p\left(l^{\prime} \mid s\right) \sum_{j=1}^{m} f_{i}\left(s, j, l_{j}^{\prime}, l_{j-1}^{\prime}\right)\right]$，其中α是学习率。</p></li><li><p>重复前面的步骤，直到达到一些停止条件(例如，更新低于某个阈值)。</p></li></ul><p>In other words, every step takes the difference between what we want the model to learn and the model’s current state, and moves λiλi in the direction of this difference.</p><p><strong>寻找最佳标签</strong></p><p>假设我们已经训练了我们的CRF模型，现在又出现了一个新句子。我们如何标记它？</p><p>最简单的方法是为每个可能的标签 $l$ 计算$ p(l | s)$，然后选择最大化这个概率的标签。然而，由于对于大小为k的标签集和长度为m的句子，存在$ k ^ m $个可能的标签，所以这种方法将不得不检查指数数量的标签。</p><p>一个更好的方法是认识到(线性链)CRFs满足一个最佳子结构属性（ <a href="http://en.wikipedia.org/wiki/Optimal_substructure">optimal substructure</a> ），允许我们使用动态规划算法来寻找最佳标签，类似于用于隐马尔科夫模型的维特比算法。</p><h3 id="【2】CRF模型公式？"><a href="#【2】CRF模型公式？" class="headerlink" title="【2】CRF模型公式？"></a>【2】CRF模型公式？</h3><p>这部分知识比较硬核，说实话目前还没有看到哪篇文章完整推导出CRF公式，大都走马观花，关键部分截图。<br>所以这里直接推荐看 李航的《统计学习》第二版，第11章， 11.2小结《条件随机场的定义与形式》。</p><h3 id="【3】特征函数是什么意思？特征模版什么意思？"><a href="#【3】特征函数是什么意思？特征模版什么意思？" class="headerlink" title="【3】特征函数是什么意思？特征模版什么意思？"></a>【3】特征函数是什么意思？特征模版什么意思？</h3><p>阅读过《统计学习》相关文章的读者会发现，条件概率<code>P(y|x)</code>中定义的特征函数<code>tk</code>,<code>sl</code>，还是令人迷惑的，数学公式过于抽象。<a href="https://www.zhihu.com/question/20279019">CRF模板</a> 这篇博文举例了特征函数和模版的使用。实际场景中，模版的定义可能多达几十个。</p><h3 id="【4】CRF损失如何计算的？全部路径loss怎么算？"><a href="#【4】CRF损失如何计算的？全部路径loss怎么算？" class="headerlink" title="【4】CRF损失如何计算的？全部路径loss怎么算？"></a>【4】CRF损失如何计算的？全部路径loss怎么算？</h3><p>在《统计学习》中，CRF的<code>P(y|x)</code>为规范化后的条件概率。规范化因子<code>Z(x)</code>,即全路径的计算，是一个难点。<a href="https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&amp;mid=2247488536&amp;idx=1&amp;sn=59726a10da833929960320fe4163ecee&amp;chksm=c0699c45f71e15538db0c6625b3ecb9cc0c78436a796b0650ebf327359fbe929d61945ecaa40&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1585896475288&amp;sharer_shareid=d7e53fb485600c389e7965f086d6336c&amp;rd2werd=1#wechat_redirect">CRF的全路径损失计算</a> 该文详细讲解了计算的过程。</p><h3 id="【5】CRF的数据是如何生成的？"><a href="#【5】CRF的数据是如何生成的？" class="headerlink" title="【5】CRF的数据是如何生成的？"></a>【5】CRF的数据是如何生成的？</h3><p>阅读过市面上很多关于CRF的文章，会发现很多文章对于训练数据的举例，似乎只有两三列。<code>word</code>,<code>POS</code>,<code>label</code>，这可能会对读者造成误解。实际场景下，会设计很多人工特征，添加到训练数据里，提升CRF模型的学习能力。</p><p>你以为的：</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">|<span class="hljs-string">token</span>|<span class="hljs-string">词性</span>|<span class="hljs-string">标签</span>|<br>|<span class="hljs-string">:---:</span>|<span class="hljs-string">:---:</span>|<span class="hljs-string">:---:</span>|<br>|<span class="hljs-string">北</span>|<span class="hljs-string">N</span>|<span class="hljs-string">B-LOC</span>|<br>|<span class="hljs-string">京</span>|<span class="hljs-string">N</span>|<span class="hljs-string">I-LOC</span>|<br></code></pre></td></tr></table></figure><p>实际的：</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">|<span class="hljs-string">token</span>|<span class="hljs-string">词性</span>|<span class="hljs-string">特征1</span>|<span class="hljs-string">特征2</span>|<span class="hljs-string">...</span>|<span class="hljs-string">标签</span>|<br>|<span class="hljs-string">:---:</span>|<span class="hljs-string">:---:</span>|<span class="hljs-string">:---:</span>|<span class="hljs-string">:---:</span>|<span class="hljs-string">:---:</span>|<span class="hljs-string">:---:</span>|<br>|<span class="hljs-string">北</span>|<span class="hljs-string">N</span>|<span class="hljs-string">..</span>|<span class="hljs-string">..</span>|<span class="hljs-string">..</span>|<span class="hljs-string">B-LOC</span>|<br>|<span class="hljs-string">京</span>|<span class="hljs-string">N</span>|<span class="hljs-string">..</span>|<span class="hljs-string">..</span>|<span class="hljs-string">..</span>|<span class="hljs-string">I-LOC</span>|<br></code></pre></td></tr></table></figure><p>人工设计的特征，可以有效提升CRF模型性能；但有可能会造成过拟合。 </p><h3 id="【6】维特比算法没看懂，啥意思？"><a href="#【6】维特比算法没看懂，啥意思？" class="headerlink" title="【6】维特比算法没看懂，啥意思？"></a>【6】维特比算法没看懂，啥意思？</h3><p>CRF的预测算法，《统计学习》书中介绍维特比算法，虽然举了一个例子，相对来说还是抽象了点。<a href="https://www.zhihu.com/question/20136144">如何理解viterbi算法</a> 该文形象的解释了维特比算法的原理以及求解过程。再回去过看《统计学习》就很好理解了。</p><h3 id="【7】源代码怎么学习？道理我都懂，怎么用呢？"><a href="#【7】源代码怎么学习？道理我都懂，怎么用呢？" class="headerlink" title="【7】源代码怎么学习？道理我都懂，怎么用呢？"></a>【7】源代码怎么学习？道理我都懂，怎么用呢？</h3><p>关于如何阅读源码，在博主的另一篇文章<a href="http://babycon.xyz/archives/手把手教你读源码bilstmcrf">手把手教你读源码——BiLSTM+CRF</a>，进行了详细的介绍。</p><h3 id="【8】BiLSTM-CRF"><a href="#【8】BiLSTM-CRF" class="headerlink" title="【8】BiLSTM + CRF ?"></a>【8】BiLSTM + CRF ?</h3><p>随着深度学习的普及，BiLSTM + CRF似乎成了很多序列标注任务的标配，这两个工具是如何结合起来的呢？参考这篇文章，手把手推理了<a href="https://zhuanlan.zhihu.com/p/97676647">手撕 BiLSTM-CRF</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://babycon.xyz/archives/c-r-f---bi-xu-dong">CRF，就这？</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>CRF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hashlib哈希分桶</title>
    <link href="/2020/12/23/2020-12-23-Hashlib%E5%93%88%E5%B8%8C%E5%88%86%E6%A1%B6/"/>
    <url>/2020/12/23/2020-12-23-Hashlib%E5%93%88%E5%B8%8C%E5%88%86%E6%A1%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>哈希加密算法应用非常广泛，包括数字签名，身份验证，操作检测，指纹，校验和（消息完整性检查），哈希表，密码存储等。</p><p>在密码学中，好的哈希算法应该满足以下两个条件：一是无法从哈希值解密原始消息；二是，更改原始消息的一个字节，哈希消息会发生非常大的变化。</p><p>哈希函数以<strong>可变长度的字节序列</strong>的作为输入，并将其转换为<strong>固定长度的序列</strong>。Hash算法特别的地方在于它是一种<code>单向算法</code>，用户可以通过Hash算法对目标信息生成一段特定长度的唯一的Hash值，却不能通过这个Hash值重新获得目标信息。</p> <span id="more"></span><h1 id="hash分桶操作"><a href="#hash分桶操作" class="headerlink" title="hash分桶操作"></a><a href="https://a358003542.github.io/articles/hashfen-tong-cao-zuo.html">hash分桶操作</a></h1><p>通常有两种常用的分桶方式：</p><ul><li>1.<strong>基于用户的分桶（User-based bucket）</strong>：这样的桶，是一个随机选定用户的集合。一种简单的方式是，使用一个hash函数，为每个user id生成一个hash值，选择一个特定的范围指向一个桶。例如：Ron Rivest设计的md5。</li><li>2.<strong>基于请求的分桶（Request-based bucket）</strong>：这样的桶，是一个随机选择的请求的集合。常用的做法是，为每个请求生成一个随机数，然后将对应指定范围的请求随机数指定到某个桶内。注意，在这样的桶中，在实验期间，同一个用户不同的访问，有可能属于不同的分桶。</li></ul><p>hashlib的md5算法作为实践级别的算法有一个很大的特点那就是输出的位数长度是固定的，这样很方面进行下一步的输出再分桶操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> string<br><span class="hljs-keyword">import</span> hashlib<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> add<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> reduce<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">md5</span>(<span class="hljs-params">key</span>):<br>    xx = hashlib.md5()  <span class="hljs-comment"># 导入md5算法</span><br>    xx.update(key.encode(<span class="hljs-string">&#x27;utf8&#x27;</span>)<br>    <span class="hljs-keyword">return</span> xx.hexdigest()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mapping</span>(<span class="hljs-params">hashkey, n=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-keyword">return</span> reduce(add, [<span class="hljs-built_in">ord</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> hashkey]) % n<br><br><span class="hljs-comment"># 进行分桶实现分流，制定不同的实验策略</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mapping2</span>(<span class="hljs-params">hashkey, n=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-keyword">return</span> hashkey[:<span class="hljs-number">1</span>] % n<br></code></pre></td></tr></table></figure><p>各个位数相加参考了 <a href="http://www.mathcs.emory.edu/~cheung/Courses/323/Syllabus/Map/hash.html">这个网站</a> ，但具体各个位数相加是否保证了分桶的均匀性，老实说我是不大确切的，如下做了一个简单的测试：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">data</span> = defaultdict(<span class="hljs-title">lambda</span>: 0)</span><br><br><br><span class="hljs-title">def</span> random_string_generator():<br>    <span class="hljs-class"><span class="hljs-keyword">data</span> = []</span><br>    random_length = random.randint(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>)<br>    for i <span class="hljs-keyword">in</span> range(random_length):<br>        x = random.choice(string.ascii_lowercase + string.digits + &#x27; &#x27;)<br>        <span class="hljs-class"><span class="hljs-keyword">data</span>.append(<span class="hljs-title">x</span>)</span><br>    return &#x27;&#x27;.join(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br><br><br><span class="hljs-title">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">100000</span>):<br>    s = random_string_generator()<br><br>    c = mapping(md5(s))<br>    print(f&#x27;<span class="hljs-string">&quot;&#123;s&#125;&quot;</span> mapping to bukket &#123;c&#125;&#x27;)<br><br>    <span class="hljs-class"><span class="hljs-keyword">data</span>[c] += 1</span><br><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br></code></pre></td></tr></table></figure><p>初步看来随机生成的各种各样的字符串最终分桶基本上是均匀的。虽然我知道md5算法是能够保证每个位数上的随机，但进行ord处理得到数字，相加再取模，是否也是保证均匀性的，我只能说这是猜的，需要严格的数学证明。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://a358003542.github.io/articles/hashfen-tong-cao-zuo.html">https://a358003542.github.io/articles/hashfen-tong-cao-zuo.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Hashlib</tag>
      
      <tag>哈希分桶</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pandas使用操作指南</title>
    <link href="/2020/12/22/2020-12-22-Pandas%E4%BD%BF%E7%94%A8%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/"/>
    <url>/2020/12/22/2020-12-22-Pandas%E4%BD%BF%E7%94%A8%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<p>数据帧(DataFrame)是二维数据结构，即数据以行和列的表格方式排列。</p><p>数据帧(DataFrame)的功能特点：潜在的列是不同的类型大小可变标记轴(行和列)可以对行和列执行算术运算</p><p>pandas中的DataFrame可以使用以下构造函数创建：</p><div class="table-container"><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>data</td><td>数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame</td></tr><tr><td>index</td><td>对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)</td></tr><tr><td>columns</td><td>列标签</td></tr><tr><td>dtype</td><td>每列的数据类型。</td></tr></tbody></table></div> <span id="more"></span><h1 id="常见使用操作汇总"><a href="#常见使用操作汇总" class="headerlink" title="常见使用操作汇总"></a>常见使用操作汇总</h1><h2 id="DataFrame对象"><a href="#DataFrame对象" class="headerlink" title="DataFrame对象"></a>DataFrame对象</h2><h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs prolog"># 通过列表创建<br>data = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]<br>df = pd.<span class="hljs-symbol">DataFrame</span>(data)<br><br>data = [[<span class="hljs-string">&#x27;Alex&#x27;</span>,<span class="hljs-number">10</span>],[<span class="hljs-string">&#x27;Bob&#x27;</span>,<span class="hljs-number">12</span>],[<span class="hljs-string">&#x27;Clarke&#x27;</span>,<span class="hljs-number">13</span>]]<br>df = pd.<span class="hljs-symbol">DataFrame</span>(data,columns=[<span class="hljs-string">&#x27;Name&#x27;</span>,<span class="hljs-string">&#x27;Age&#x27;</span>])<br><br># 从ndarrays/<span class="hljs-symbol">Lists</span>的字典来创建<br>data = &#123;<span class="hljs-string">&#x27;Name&#x27;</span>:[<span class="hljs-string">&#x27;Tom&#x27;</span>, <span class="hljs-string">&#x27;Jack&#x27;</span>, <span class="hljs-string">&#x27;Steve&#x27;</span>, <span class="hljs-string">&#x27;Ricky&#x27;</span>],<span class="hljs-string">&#x27;Age&#x27;</span>:[<span class="hljs-number">28</span>,<span class="hljs-number">34</span>,<span class="hljs-number">29</span>,<span class="hljs-number">42</span>]&#125;<br>df = pd.<span class="hljs-symbol">DataFrame</span>(data, index=[<span class="hljs-string">&#x27;rank1&#x27;</span>,<span class="hljs-string">&#x27;rank2&#x27;</span>,<span class="hljs-string">&#x27;rank3&#x27;</span>,<span class="hljs-string">&#x27;rank4&#x27;</span>])<br><br># 从列表创建数据帧<span class="hljs-symbol">DataFrame</span><br>data = [&#123;<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">2</span>&#125;,&#123;<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">10</span>, <span class="hljs-string">&#x27;c&#x27;</span>: <span class="hljs-number">20</span>&#125;]<br>df = pd.<span class="hljs-symbol">DataFrame</span>(data)<br><br># 从系列的字典来创建<br>d = &#123;<span class="hljs-string">&#x27;one&#x27;</span> : pd.<span class="hljs-symbol">Series</span>([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], index=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]),<br>      <span class="hljs-string">&#x27;two&#x27;</span> : pd.<span class="hljs-symbol">Series</span>([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>], index=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])&#125;<br>df = pd.<span class="hljs-symbol">DataFrame</span>(d)<br>df<br></code></pre></td></tr></table></figure><h3 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 查看行数和列数</span><br><span class="hljs-built_in">print</span>(df.shape)<br><br><span class="hljs-comment"># 查看前n行</span><br><span class="hljs-built_in">print</span>(df.head(5))<br><br><span class="hljs-comment"># 查看后n行</span><br><span class="hljs-built_in">print</span>(df.tail(5))<br><br><span class="hljs-comment"># 查看数值型列的汇总统计</span><br><span class="hljs-built_in">print</span>(df.describe())<br><br><span class="hljs-comment"># 查看列名</span><br>df.columns<br><span class="hljs-built_in">print</span>(df._stat_axis.values.tolist()) # 行名称<br><span class="hljs-built_in">print</span>(df.columns.values.tolist())<br><br><span class="hljs-comment"># 某一列的列值</span><br>df[<span class="hljs-string">&#x27;one&#x27;</span>]<br><br><span class="hljs-comment"># 第i行的第j列</span><br><span class="hljs-built_in">print</span>(df.loc[0, <span class="hljs-string">&quot;one&quot;</span>])   # 第1行的one列值<br><span class="hljs-built_in">print</span>(df.iloc[0, 0])   # 第1行的第一列值，同上<br><br><span class="hljs-comment"># 多行中的多列</span><br><span class="hljs-built_in">print</span>(df.loc[[2,3],[<span class="hljs-string">&#x27;one&#x27;</span>,<span class="hljs-string">&#x27;two&#x27;</span>]])  #选取指定的第2行和第3行，name和age列的数据<br><span class="hljs-built_in">print</span>(df.iloc[[2,3], [0,1]])  # 同上<br></code></pre></td></tr></table></figure><h3 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列添加</span><br><span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;three&#x27;</span>]=pd.Series([10,20,30],index=[<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>])<br><span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;four&#x27;</span>]=<span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;one&#x27;</span>]+<span class="hljs-built_in">df</span>[<span class="hljs-string">&#x27;two&#x27;</span>]<br><span class="hljs-built_in">print</span> (<span class="hljs-built_in">df</span>)<br><br><span class="hljs-comment"># 列删除</span><br>df.pop(<span class="hljs-string">&#x27;two&#x27;</span>)<br><span class="hljs-built_in">print</span> (<span class="hljs-built_in">df</span>)<br><br><span class="hljs-comment"># 行遍历&amp;切片</span><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">df</span>[1:3])<br><span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> df.iterrows():   <span class="hljs-comment"># 第一行的值</span><br>    <span class="hljs-built_in">print</span>(row)<br><br><span class="hljs-comment"># 附加行</span><br>df2 = pd.DataFrame([[5, 6], [7, 8]], columns=[<span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>])<br><span class="hljs-built_in">df</span> = df.append(df2)<br><span class="hljs-built_in">print</span> (<span class="hljs-built_in">df</span>)<br><br><span class="hljs-comment"># 删除行</span><br><span class="hljs-built_in">df</span> = df.drop(0)<br><span class="hljs-built_in">print</span> (<span class="hljs-built_in">df</span>)<br></code></pre></td></tr></table></figure><h3 id="数据处理利器：map"><a href="#数据处理利器：map" class="headerlink" title="数据处理利器：map"></a>数据处理利器：map</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#②使用函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gender_map</span>(<span class="hljs-params">x</span>):<br>    gender = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x == <span class="hljs-string">&quot;男&quot;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>    <span class="hljs-keyword">return</span> gender<br>    <br><span class="hljs-comment">#注意这里传入的是函数名，不带括号</span><br>data[<span class="hljs-string">&quot;gender&quot;</span>] = data[<span class="hljs-string">&quot;gender&quot;</span>].<span class="hljs-built_in">map</span>(gender_map)<br></code></pre></td></tr></table></figure><h3 id="数据处理利器：apply"><a href="#数据处理利器：apply" class="headerlink" title="数据处理利器：apply"></a>数据处理利器：apply</h3><p>可以接收各种各样的函数（Python内置的或自定义的），也可以同时处理多列数据；</p><p>axis=0代表操作对列columns进行，axis=1代表操作对行row进行</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-function">def <span class="hljs-title">BMI</span><span class="hljs-params">(series)</span>:</span><br><span class="hljs-function">    weight =</span> series[<span class="hljs-string">&quot;weight&quot;</span>]<br>    height = series[<span class="hljs-string">&quot;height&quot;</span>]/<span class="hljs-number">100</span><br>    BMI = weight/height**<span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> BMI<br><br># 沿着<span class="hljs-number">1</span>轴操作<br>data[<span class="hljs-string">&quot;BMI&quot;</span>] = data.<span class="hljs-built_in">apply</span>(BMI,axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>apply()在运算时实际上是一行一行遍历的，IO开销比较大，可以使用progress_apply()监视运行进度；</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs lua">def xx(a):<br>    <span class="hljs-keyword">return</span> a+<span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(data<span class="hljs-string">[[&#x27;height&#x27;, &#x27;weight&#x27;]]</span>))<br>data<span class="hljs-string">[[&#x27;height&#x27;, &#x27;weight&#x27;]]</span> = data<span class="hljs-string">[[&#x27;height&#x27;, &#x27;weight&#x27;]]</span>.progress_apply(xx, axis=<span class="hljs-number">1</span>)<br>data<br></code></pre></td></tr></table></figure><h3 id="数据处理利器：applymap"><a href="#数据处理利器：applymap" class="headerlink" title="数据处理利器：applymap"></a>数据处理利器：applymap</h3><p>applymap() 是与map() 方法相对应的专属于Dataframe对象的方法，可传入函数、字典等，作用于整个数据框中的每个位置的元素，<strong>返回结果的形状与元数据框 一致！</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lowerx</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(x, <span class="hljs-built_in">str</span>):<br>        <span class="hljs-keyword">return</span> x.lower() + <span class="hljs-string">&#x27;_&#x27;</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> x<br><br>data.applymap(lowerx)<br></code></pre></td></tr></table></figure><h2 id="过滤空值"><a href="#过滤空值" class="headerlink" title="过滤空值"></a>过滤空值</h2><p>常用于解决某行或某列值为NaN（空）的情况，很多时候我们要把这部分数据剔除，便于进一步的数据处理。</p><p>NaN由numpy产生，所以用python里的None无法判断pandas dataframe里的数据是否为空。本质上NaN不等于任何值，pandas的提供了相应的内置方法来处理该问题，如isnull()，notnull()</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text"># 获取字段column1值不为空的数据库集<br>df = df[df[&#x27;column1&#x27;].notnull()]<br><br># 获取字段column1值为空的数据库集<br>df = df[df[&#x27;column1&#x27;].isnull()]<br></code></pre></td></tr></table></figure><h2 id="常见数据过滤操作"><a href="#常见数据过滤操作" class="headerlink" title="常见数据过滤操作"></a>常见数据过滤操作</h2><p>通过比较值过滤</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs text"># 获取某列等于某值的数据集<br>df.loc[df[&#x27;column_name&#x27;] == some_value]<br><br># 某列不等于某值的数据集<br>df.loc[df[&#x27;column_name&#x27;] != some_value]<br><br># 某列的值属于某个迭代器的子值，如python &quot;target is in [&quot;v1&quot;, &quot;v2&quot;, &quot;v3&quot;]<br>df.loc[df[&#x27;column_name&#x27;].isin([&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;])]<br><br># 多个过滤条件，使用 &amp; 连接<br>df.loc[(df[&#x27;column_name&#x27;] &gt;= A) &amp; (df[&#x27;column_name&#x27;] &lt;= B)]<br># 一定要注意带圆括号，因为&amp;离A更近<br># df[&#x27;column_name&#x27;] &gt;= A &amp; df[&#x27;column_name&#x27;] &lt;= B 等同于df[&#x27;column_name&#x27;] &gt;= (A &amp; df[&#x27;column_name&#x27;]) &lt;= B<br><br># 否定条件（反条件），使用波浪号 &quot;~&quot;，不建议使用减号 &quot;-&quot;，减号在某些场合会失效<br>df.loc[~df[&#x27;column_name&#x27;] == some_value]<br>df.loc[~df[&#x27;column_name&#x27;].isin(some_values)]<br></code></pre></td></tr></table></figure><p>以……开始或以……结尾，语法与python标准库str对应的方法相同</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs text">&gt;&gt;&gt; s = pd.Series([&#x27;bat&#x27;, &#x27;Bear&#x27;, &#x27;cat&#x27;, np.nan])<br>&gt;&gt;&gt; s<br>0     bat<br>1    Bear<br>2     cat<br>3     NaN<br>dtype: object<br><br># 单值<br>&gt;&gt;&gt; s.str.startswith(&#x27;b&#x27;)<br>0     True<br>1    False<br>2    False<br>3      NaN<br>dtype: object<br><br>&gt;&gt;&gt; s.str.endswith(&#x27;t&#x27;)<br>0     True<br>1    False<br>2     True<br>3      NaN<br>dtype: object<br><br># 多值，使用元组<br>&gt;&gt;&gt; s.str.startswith((&#x27;b&#x27;, &#x27;c&#x27;))<br>0     True<br>1    False<br>2     True<br>3      NaN<br>dtype: object<br></code></pre></td></tr></table></figure><p>子字符串过滤，包含某值</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs text"># 初始化<br>df1 = pd.DataFrame(&#123;&#x27;col&#x27;: [&#x27;foo&#x27;, &#x27;foobar&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;, &#x27;FOo&#x27;]&#125;)<br>df1<br><br>      col<br>0     foo<br>1  foobar<br>2     bar<br>3     baz<br><br># 正则过滤，查找某列的值含有foo但不以foo结尾的<br>df1[df1[&#x27;col&#x27;].str.contains(r&#x27;foo(?!$)&#x27;)]<br><br>      col<br>1  foobar<br><br># 非正则过滤，regex默认为True，即默认使用正则<br>df1[df1[&#x27;col&#x27;].str.contains(&#x27;foo&#x27;, regex=False)]<br># same as df1[df1[&#x27;col&#x27;].str.contains(&#x27;foo&#x27;)] but faster.<br><br>      col<br>0     foo<br>1  foobar<br><br># 忽略大小写，case默认为True，即默认大小写敏感<br>df1[df1[&#x27;col&#x27;].str.contains(&#x27;foo&#x27;, regex=False, case=False)]<br><br>      col<br>0     foo<br>1  foobar<br>4     FOo<br></code></pre></td></tr></table></figure><h2 id="计算某列某个值出现的次数"><a href="#计算某列某个值出现的次数" class="headerlink" title="计算某列某个值出现的次数"></a>计算某列某个值出现的次数</h2><p>使用 shape 或 len</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs text">print df<br>  col1 education<br>0    a       9th<br>1    b       9th<br>2    c       8th<br><br>print df.education == &#x27;9th&#x27;<br>0     True<br>1     True<br>2    False<br>Name: education, dtype: bool<br><br>print df[df.education == &#x27;9th&#x27;]<br>  col1 education<br>0    a       9th<br>1    b       9th<br><br>print df[df.education == &#x27;9th&#x27;].shape[0]<br>2<br>print len(df[df[&#x27;education&#x27;] == &#x27;9th&#x27;])<br>2<br></code></pre></td></tr></table></figure><h2 id="获取某列的所有值"><a href="#获取某列的所有值" class="headerlink" title="获取某列的所有值"></a>获取某列的所有值</h2><p>使用tolist()或values.tolist()</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs text">import pandas as pd<br>df = pd.DataFrame(&#123;&#x27;a&#x27;:[1,3,5,7,4,5,6,4,7,8,9],<br>                   &#x27;b&#x27;:[3,5,6,2,4,6,7,8,7,8,9]&#125;)<br><br>&gt;&gt;&gt; df[&#x27;a&#x27;].tolist()<br>[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]<br><br>&gt;&gt;&gt; df[&#x27;a&#x27;].values.tolist()<br>[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]<br><br># 剔除重复项，使用drop_duplicates或set<br>&gt;&gt;&gt; df[&#x27;a&#x27;].drop_duplicates().values.tolist()<br>[1, 3, 5, 7, 4, 6, 8, 9]<br>&gt;&gt;&gt; list(set(df[&#x27;a&#x27;])) # as pointed out by EdChum<br>[1, 3, 4, 5, 6, 7, 8, 9]<br></code></pre></td></tr></table></figure><h2 id="基于其他列生成新的列"><a href="#基于其他列生成新的列" class="headerlink" title="基于其他列生成新的列"></a>基于其他列生成新的列</h2><p>使用apply方法映射处理数据的函数（操作类似map），同时注意axis=1的设置（按行处理）</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs text">data = [<br>    &#123;&#x27;ip&#x27;: &#x27;128.0.0.1&#x27;, &#x27;name&#x27;: &#x27;abc&#x27;, &quot;request_time&quot;: &quot;2019-07-04T01:04:23+08:00&quot;, &#x27;request_body&#x27;: &#x27;username=qwer&amp;fqwer&#x27;&#125;,<br>    &#123;&#x27;ip&#x27;: &#x27;128.0.0.1&#x27;, &#x27;name&#x27;: &#x27;c&#x27;, &quot;request_time&quot;: &quot;2019-07-04T00:04:23+08:00&quot;, &#x27;request_body&#x27;: &#x27;pwd=qwer&amp;fqwer&#x27;&#125;,<br>    &#123;&#x27;ip&#x27;: &#x27;128.0.0.1&#x27;, &quot;request_time&quot;: &quot;2019-07-04T07:04:23+08:00&quot;, &#x27;name&#x27;: &#x27;abc&#x27;, &#x27;request_body&#x27;: &#x27;pwd=qwer&amp;fqwer&#x27;&#125;,<br>    &#123;&#x27;ip&#x27;: &#x27;128.0.0.2&#x27;, &#x27;name&#x27;: &#x27;e&#x27;, &quot;request_time&quot;: &quot;2019-07-04T08:04:23+08:00&quot;, &#x27;request_body&#x27;: &#x27;pwd=qwer&amp;fqwer&#x27;&#125;,<br>    &#123;&#x27;ip&#x27;: &#x27;128.0.0.2&#x27;, &#x27;name&#x27;: &#x27;e&#x27;, &quot;request_time&quot;: &quot;2019-07-04T12:04:23+08:00&quot;, &#x27;request_body&#x27;: &#x27;username=qwer&amp;fqwer&#x27;&#125;,<br>    &#123;&#x27;ip&#x27;: &#x27;128.0.0.3&#x27;, &#x27;name&#x27;: &#x27;g&#x27;, &quot;request_time&quot;: &quot;2019-07-04T02:04:23+08:00&quot;, &#x27;request_body&#x27;: &#x27;username=qwer&amp;fqwer&#x27;&#125;<br>]<br><br><br>def check_user(row):<br>    if &#x27;name&#x27; in row[&#x27;request_body&#x27;]:<br>        return 1<br>    else:<br>        return 0<br><br><br>df = pd.DataFrame(data)<br>new_df = df.copy()<br>new_df.loc[:, &#x27;hit_user&#x27;] = df.apply(lambda row: check_user(row), axis=1)<br>print(new_df)<br><br>          ip name         request_body               request_time  hit_user<br>0  128.0.0.1  abc  username=qwer&amp;fqwer  2019-07-04T01:04:23+08:00         1<br>1  128.0.0.1    c       pwd=qwer&amp;fqwer  2019-07-04T00:04:23+08:00         0<br>2  128.0.0.1  abc       pwd=qwer&amp;fqwer  2019-07-04T07:04:23+08:00         0<br>3  128.0.0.2    e       pwd=qwer&amp;fqwer  2019-07-04T08:04:23+08:00         0<br>4  128.0.0.2    e  username=qwer&amp;fqwer  2019-07-04T12:04:23+08:00         1<br>5  128.0.0.3    g  username=qwer&amp;fqwer  2019-07-04T02:04:23+08:00         1<br></code></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/76938090">pandas常见实用操作</a></p><p><a href="https://zhuanlan.zhihu.com/p/100064394">https://zhuanlan.zhihu.com/p/100064394</a></p><p><a href="http://www.360doc.com/content/20/0202/23/7669533_889336554.shtml">http://www.360doc.com/content/20/0202/23/7669533_889336554.shtml</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python3中的LRU缓存机制</title>
    <link href="/2020/12/15/2020-12-15-Python3%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6lru_cache/"/>
    <url>/2020/12/15/2020-12-15-Python3%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6lru_cache/</url>
    
    <content type="html"><![CDATA[<p>缓存是一种将定量数据加以保存以备迎合后续获取需求的处理方式，旨在加快数据获取的速度。数据的生成过程可能需要经过计算，规整，远程获取等操作，如果是同一份数据需要多次使用，每次都重新生成会大大浪费时间。所以，如果将计算或者远程请求等操作获得的数据缓存下来，会加快后续的数据获取需求。</p><p>本文接下来会介绍Python3中的 <code>functools.lru_cache</code> 缓存机制以及对应的使用方法！</p> <span id="more"></span><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>先来一个简单的例子以了解缓存机制的概念：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> datetime<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyCache</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;缓存类&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 用字典结构以 kv 的形式缓存数据</span><br>        self.cache = &#123;&#125;<br>        <span class="hljs-comment"># 限制缓存的大小，因为缓存的空间有限</span><br>        <span class="hljs-comment"># 所以当缓存太大时，需要将旧的缓存舍弃掉</span><br>        self.max_cache_size = <span class="hljs-number">10</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__contains__</span>(<span class="hljs-params">self, key</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;根据该键是否存在于缓存当中返回 True 或者 False&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> key <span class="hljs-keyword">in</span> self.cache<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self, key</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;从缓存中获取数据&quot;&quot;&quot;</span><br>        data = self.cache[key]<br>        data[<span class="hljs-string">&quot;date_accessed&quot;</span>] = datetime.datetime.now()<br>        <span class="hljs-keyword">return</span> data[<span class="hljs-string">&quot;value&quot;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, key, value</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;更新该缓存字典，如果缓存太大则先删除最早条目&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.cache <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(self.cache) &gt;= self.max_cache_size:<br>            self.remove_oldest()<br>        self.cache[key] = &#123;<br>            <span class="hljs-string">&#x27;date_accessed&#x27;</span>: datetime.datetime.now(),<br>            <span class="hljs-string">&#x27;value&#x27;</span>: value<br>        &#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">remove_oldest</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;删除具备最早访问日期的输入数据&quot;&quot;&quot;</span><br>        oldest_entry = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> self.cache:<br>            <span class="hljs-keyword">if</span> oldest_entry <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                oldest_entry = key<br>                <span class="hljs-keyword">continue</span><br>            curr_entry_date = self.cache[key][<span class="hljs-string">&#x27;date_accessed&#x27;</span>]<br>            oldest_entry_date = self.cache[oldest_entry][<span class="hljs-string">&#x27;date_accessed&#x27;</span>]<br>            <span class="hljs-keyword">if</span> curr_entry_date &lt; oldest_entry_date:<br>                oldest_entry = key<br><br>        self.cache.pop(oldest_entry)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">size</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;返回缓存容量大小&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.cache)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 测试缓存功能</span><br>    cache = MyCache()<br>    cache.add(<span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-built_in">sum</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">100000</span>)))<br>    <span class="hljs-keyword">assert</span> cache.get(<span class="hljs-string">&quot;test&quot;</span>) == cache.get(<span class="hljs-string">&quot;test&quot;</span>)<br><br>    keys = [<br>        <span class="hljs-string">&#x27;red&#x27;</span>, <span class="hljs-string">&#x27;fox&#x27;</span>, <span class="hljs-string">&#x27;fence&#x27;</span>, <span class="hljs-string">&#x27;junk&#x27;</span>, <span class="hljs-string">&#x27;other&#x27;</span>, <span class="hljs-string">&#x27;alpha&#x27;</span>, <span class="hljs-string">&#x27;bravo&#x27;</span>, <span class="hljs-string">&#x27;cal&#x27;</span>,<br>        <span class="hljs-string">&#x27;devo&#x27;</span>, <span class="hljs-string">&#x27;ele&#x27;</span><br>    ]<br>    s = <span class="hljs-string">&#x27;abcdefghijklmnop&#x27;</span><br>    <span class="hljs-keyword">for</span> i, key <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(keys):<br>        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> cache:<br>            <span class="hljs-keyword">continue</span><br>        <span class="hljs-keyword">else</span>:<br>            value = <span class="hljs-string">&#x27;&#x27;</span>.join([random.choice(s) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>)])<br>            cache.add(key, value)<br><br>    <span class="hljs-keyword">assert</span> <span class="hljs-string">&quot;test&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> cache<br>    <span class="hljs-built_in">print</span>(cache.cache)<br></code></pre></td></tr></table></figure><p>以上示例仅简单的展示了缓存机制的原理，通过用键值对的方式将数据放到字典中，如果下次需要取值时可以直接到字典中获取。该示例在删除旧数据时的实现并不高效，实际应用中可以用别的方式实现。</p><p>在 Python 的 3.2 版本中，引入了一个非常优雅的缓存机制，即 <code>functool</code> 模块中的 <code>lru_cache</code> 装饰器，可以直接将函数或类方法的结果缓存住，后续调用则直接返回缓存的结果。<code>lru_cache</code> 原型如下：</p><blockquote><p>@functools.lru_cache(maxsize=None, typed=False)</p></blockquote><p>使用 functools 模块的 lur_cache 装饰器，可以缓存最多 maxsize 个此函数的调用结果，从而提高程序执行的效率，特别适合于耗时的函数。参数 <code>maxsize</code> 为最多缓存的次数，如果为 None，则无限制，设置为 2 的幂 时，性能最佳；如果 <code>typed=True</code>（注意，在 functools32 中没有此参数），则不同参数类型的调用将分别缓存，例如 f(3) 和 f(3.0)。</p><h1 id="LRU-算法介绍"><a href="#LRU-算法介绍" class="headerlink" title="LRU 算法介绍"></a>LRU 算法介绍</h1><p><strong>LRU (Least Recently Used，最近最少使用)</strong> 算法是一种缓存淘汰策略。其根据数据的历史访问记录来进行淘汰，核心思想是，“如果数据最近被访问过，那么将来被访问的几率也更高”。该算法最初为操作系统中一种内存管理的页面置换算法，主要用于找出内存中较久时间没有使用的内存块，将其移出内存从而为新数据提供空间。其原理就如以上的简单示例。</p><p>那么问题来了，为什么LRU能提高性能？其实这个问题描述本身是错误的——LRU并不总 是能提高性能的，任何实用的缓存算法都不行。LRU基于这样一个前提：越久没被访 问的数据，以后被访问到的概率也越小。比方说，如果你的程序需要<strong>周期性</strong>地处 理不同数据，用LRU可能只会带来周期性的缓存miss从而增加处理器或者IO负担而已， 反而拖慢程序执行速度。</p><p>说到处理器负担，因为LRU要跟踪数据的访问时间/存活时间，通常涉及查找或者哈希 操作，所以需要更多处理器资源，有时候会很可观——</p><h1 id="functools里的LRU实现"><a href="#functools里的LRU实现" class="headerlink" title="functools里的LRU实现"></a>functools里的LRU实现</h1><p>以下为一个简单的 lru_cache 的使用效果：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> functools import lru_cache<br><br>@lru_cache(<span class="hljs-attribute">maxsize</span>=32)<br>def <span class="hljs-built_in">add</span>(x, y):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;calculating: %s + %s&quot;</span> % (x, y))<br>    return x + y<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">add</span>(1, 2))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">add</span>(1, 2))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">add</span>(2, 3))<br></code></pre></td></tr></table></figure><p><code>@lru_cache(maxsize=32)</code> 中的 <code>maxsize</code> 参数就是缓存大小了，如果设成<code>None</code>，LRU逻辑会被禁用，变成一个 无限大的缓存；而如果设成<code>0</code>，缓存逻辑会被禁用，变成简单的调用次数统计。</p><p>输出结果：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">calculating</span>: <span class="hljs-number">1</span> + <span class="hljs-number">2</span><br><span class="hljs-attribute">3</span><br><span class="hljs-attribute">3</span><br><span class="hljs-attribute">calculating</span>: <span class="hljs-number">2</span> + <span class="hljs-number">3</span><br><span class="hljs-attribute">5</span><br></code></pre></td></tr></table></figure><p>从结果可以看出，当第二次调用 add(1, 2) 时，并没有真正执行函数体，而是直接返回缓存的结果。</p><h1 id="还有什么需要注意的？"><a href="#还有什么需要注意的？" class="headerlink" title="还有什么需要注意的？"></a>还有什么需要注意的？</h1><ol><li><p>目前的<code>lru_cache</code>是纯Python实现的。</p></li><li><p>底层数据结构是普通的<code>list</code>和<code>dict</code>. <code>list</code>用于实现LRU链表，<code>dict</code>用于 查找已缓存的数据。在缓存已满的情况下，每次调用被缓存的函数时，都要进行 两次字典查找操作和20次以内的列表访问。</p></li><li><p>对缓存的<strong>所有</strong>访问都是加了锁的，所以可以在多线程环境下使用。</p></li><li><p>被 <code>lru_cache</code> 装饰的函数会有 <code>cache_clear</code> 和 <code>cache_info</code> 两个方法，分别用于清除缓存和查看缓存信息。</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">add.cache_info()          <span class="hljs-comment"># 缓存信息</span></span><br>CacheInfo(hits=0, misses=0, maxsize=10, currsize=0)<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">add.cache_clear()         <span class="hljs-comment"># 清除所有缓存内容</span></span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">add.__wrapped__           <span class="hljs-comment"># 真正的 read_template 函数</span></span><br>&lt;function add at 0x7f9d0e9766a8&gt;<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span><br></code></pre></td></tr></table></figure><p>返回访问数（hits）、未访问数（misses）和当前缓存使用量（currsize）、最大容量（maxsize）</p></li><li><p><strong>缓存的使用场景：</strong></p><ul><li>在缓存期内，数据不会更改。</li><li>函数将始终为相同的参数返回相同的值（因此时间和随机对缓存没有意义）。</li><li>函数没有副作用。如果缓存被访问，则永远不会调用该函数，因此请确保不更改其中的任何状态。</li><li><strong>函数不返回不同的可变对象。例如，返回列表的函数不适合缓存，因为将要缓存的是对列表的引用，而不是列表内容</strong>（实际使用时，缓存的可变对象内容正确，不知道为啥？）</li></ul></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://kuanghy.github.io/2016/04/20/python-cache">Python 缓存机制与 functools.lru_cache</a></p><p><a href="https://blog.theerrorlog.com/simple-lru-cache-in-python-3-zh.html">Python3中的傻瓜式LRU缓存实现</a></p><p><a href="https://my.oschina.net/u/4260786/blog/4263791">如何让Python程序轻松加速，正确方法详解</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>缓存</tag>
      
      <tag>Cache</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rasa Core &amp; Rasa X 详解</title>
    <link href="/2020/12/14/2020-12-14-Rasa%20Core%20&amp;%20Rasa%20X%20%E8%AF%A6%E8%A7%A3/"/>
    <url>/2020/12/14/2020-12-14-Rasa%20Core%20&amp;%20Rasa%20X%20%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<ul><li>Rasa Core</li><li>Rasa X</li><li><p>Rasa Action Server</p><span id="more"></span></li></ul><h1 id="Rasa-Core"><a href="#Rasa-Core" class="headerlink" title="Rasa Core"></a>Rasa Core</h1><p><a href="https://jiangdg.blog.csdn.net/article/details/105434136">https://jiangdg.blog.csdn.net/article/details/105434136</a></p><p>Rasa框架提供的对话管理模块，即Dialog Management(DM)</p><p>它控制着人机对话的过程，是人机对话系统的重要组成部分。DM会根据NLU模块输出的语义表示执行对话状态的更新和追踪，并根据一定策略选择相应的候选动作。在对话过程中不断根据当前状态决定下一步应该采取的最优动作（如：提供结果，询问特定限制条件，澄清或确认需求等）</p><p><img src="https://flashgene.com/wp-content/uploads/2020/04/9d373f74b2cbb4925ff0cc2e7649db9e.jpg" alt="img"></p><p>DM有两个任务：</p><ol><li>对话状态维护（dialog state tracking, DST）</li></ol><p>对话状态是指记录了哪些槽位已经被填充、下一步该做什幺、填充什幺槽位，还是进行何种操作。用数学形式表达为，t+1 时刻的对话状态S(t+1)，依赖于之前时刻 t 的状态St，和之前时刻 t 的系统行为At，以及当前时刻 t+1 对应的用户行为O(t+1)。可以写成S(t+1)←St+At+O(t+1)。</p><ol><li>生成系统决策（dialog policy）</li></ol><p>根据 DST 中的对话状态（DS），产生系统行为（dialog act），决定下一步做什幺 dialog act 可以表示观测到的用户输入（用户输入 -&gt; DA，就是 NLU 的过程），以及系统的反馈行为（DA -&gt; 系统反馈，就是 NLG 的过程）</p><blockquote><p><strong>对于1，Rasa实现很简单，就是简单地基于策略的槽状态替换。</strong></p><p><strong>对于2，Rasa使用基于LSTM的排序学习，大体上是将当前轮用户意图、上一轮系统行为、当前槽值状态向量化，然后与所有系统行为做相似度学习，以此决定当前轮次的一个或多个系统行为</strong></p></blockquote><h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p><strong>一、动机</strong></p><p>当Rasa NLU识别到用户输入Message的意图后，Rasa Core对话管理模块将如何对其作出回应呢？</p><blockquote><p>答案：action</p></blockquote><p><strong>二、action 有哪些类别？</strong></p><ul><li>default actions</li><li>utter actions</li><li>custom actions</li></ul><p><strong>2.1 default actions</strong></p><ul><li>介绍：Rasa Core默认的一组actions，我们无需定义它们，直接可以story和domain中使用；</li><li>类别：<ul><li>action_listen：监听action，Rasa Core在会话过程中通常会自动调用该action；</li><li>action_restart：重置状态，比初始化Slots(插槽)的值等；</li><li>action_default_fallback：当Rasa Core得到的置信度低于设置的阈值时，默认执行该action；</li></ul></li></ul><p><strong>2.2 utter actions</strong></p><ul><li>介绍：以utter_为开头，仅仅用于向用户发送一条消息作为反馈的一类actions；</li><li>定义：只需要在domain.yml文件中的actions:字段定义以utter_为开头的action即可，而具体回复内容将被定义在templates:部分；</li></ul><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">actions</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">utter_answer_greet</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">utter_answer_goodbye</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">utter_answer_thanks</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">utter_introduce_self</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">utter_introduce_selfcando</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">utter_introduce_selffrom</span><br></code></pre></td></tr></table></figure><p><strong>2.3 custom actions</strong></p><p>即自定义action，允许开发者执行任何操作并反馈给用户，比如简单的返回一串字符串，或者控制家电、检查银行账户余额等等；</p><p><strong>三、自定义动作是如何工作</strong></p><p>当您的助手预测自定义动作时，Rasa服务器将<code>POST</code>使用json有效负载向动作服务器发送请求，其中包括预测动作的名称，对话ID，跟踪器的内容和域的内容。</p><p>当动作服务器完成自定义动作的运行时，它将返回<a href="https://rasa.com/docs/rasa/next/responses">响应</a>和<a href="https://rasa.com/docs/action-server/events">事件</a>的json。然后，Rasa服务器将响应返回给用户，并将事件添加到会话跟踪器。</p><p><strong>3.1 编写自定义Action</strong></p><p><code>Action Class</code>为任何自定义操作的基类。要定义自定义动作，请创建<code>Action</code>该类的子类并覆盖两个必需的方法<code>name</code>和<code>run</code>。当动作服务器<code>name</code>收到运行动作的请求时，它将根据其方法的返回值调用动作。</p><p>骨架自定义操作如下所示：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyCustomAction</span>(<span class="hljs-title class_">Action</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>) -&gt; <span class="hljs-title class_">Text</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;action_name&quot;</span><br><br>    async <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params"></span><br><span class="hljs-params">        <span class="hljs-variable language_">self</span>, dispatcher, <span class="hljs-symbol">tracker:</span> <span class="hljs-title class_">Tracker</span>, <span class="hljs-symbol">domain:</span> <span class="hljs-title class_">Dict</span>[<span class="hljs-title class_">Text</span>, <span class="hljs-title class_">Any</span>],</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-title class_">List</span>[<span class="hljs-title class_">Dict</span>[<span class="hljs-title class_">Text</span>, <span class="hljs-title class_">Any</span>]]:<br><br>        <span class="hljs-keyword">return</span> []<br></code></pre></td></tr></table></figure><ul><li><p>Action.name()    定义动作的名称。该方法返回的名称是您的机器人域中使用的名称。</p></li><li><p>Action.run()    参数解释：</p><ul><li><p>dispatcher –用于将消息发送回用户的调度程序。</p><ul><li>utter_message() 方法可用于将任何类型的响应返回给用户</li></ul><figure class="highlight nim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs nim">dispatcher.utter_message(text = <span class="hljs-string">&quot;Hey there&quot;</span>)<br>dispatcher.utter_message(image = <span class="hljs-string">&quot;&quot;</span>)<br>dispatcher.utter_message(json_message = <span class="hljs-meta">&#123;...&#125;</span>)<br>dispatcher.utter_message(<span class="hljs-keyword">template</span> = <span class="hljs-string">&quot;utter_greet&quot;</span>)<br>dispatcher.utter_message(<span class="hljs-keyword">template</span> = <span class="hljs-string">&quot;utter_greet_name&quot;</span>, name = <span class="hljs-string">&quot;Aimee&quot;</span>)<br>dispatcher.utter_messsage(buttons = [<br>                &#123;<span class="hljs-string">&quot;payload&quot;</span>: <span class="hljs-string">&quot;/affirm&quot;</span>, <span class="hljs-string">&quot;title&quot;</span>: <span class="hljs-string">&quot;Yes&quot;</span>&#125;,<br>                &#123;<span class="hljs-string">&quot;payload&quot;</span>: <span class="hljs-string">&quot;/deny&quot;</span>, <span class="hljs-string">&quot;title&quot;</span>: <span class="hljs-string">&quot;No&quot;</span>&#125;,<br>            ]<br></code></pre></td></tr></table></figure></li><li><p><a href="https://rasa.com/docs/action-server/sdk-tracker">tracker</a> –当前用户的状态跟踪器。可以通过<code>Tracker</code>属性和方法获取有关过去事件和会话当前状态的信息，如：访问插槽值 <code>tracker.get_slot(slot_name)</code>，最新的用户消息是<code>tracker.latest_message.text</code>以及任何其他 <code>rasa_sdk.Tracker</code>属性。具体请参阅<a href="https://rasa.com/docs/action-server/sdk-tracker">跟踪器</a>的<a href="https://rasa.com/docs/action-server/sdk-tracker">文档</a>。</p></li><li><p>domain–机器人的域</p></li></ul></li></ul><p><strong>3.2 Running a Rasa SDK Action Server</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">rasa <span class="hljs-built_in">run</span> actions<br></code></pre></td></tr></table></figure><h2 id="FormAction"><a href="#FormAction" class="headerlink" title="FormAction"></a>FormAction</h2><p><strong>一、动机</strong></p><p>在 Rasa Core 中，有时，我们需要执行 一个 Action 并 填充 多个 slots 时（eg：用户询问 天气时，Bot 需要知道 用户 想 查询的 是 哪一天（date-time），哪个地方（address），才能做出 正确 的 反馈），这个时候，需要怎么办么？</p><blockquote><p>答案：采用 FormAction 实现。</p></blockquote><p><strong>二、为什么 可以 采用 FormAction 实现呢？</strong></p><p>FormAction会遍历监管的所有slot，当发现相关的slot未被填充时，就会向用户主动发起询问，直到所有slot被填充完毕，才会执行接下来的业务逻辑。</p><p><strong>三、FormAction 的 工作流程</strong></p><ol><li>用户 输入 query;</li><li>FormAction 遍历监控所有 slots；</li><li>while 所有 slots 都被 填充？<ul><li>发现 相关 slot 未被 填充时，主动 向 用户 发起 询问；</li></ul></li><li>所有 slots 填充 完毕；</li><li>继续执行下一步 业务逻辑；</li></ol><p><strong>三、FormAction 怎么构建？</strong></p><p><strong>3.1 构建 story</strong></p><ul><li>注意：在 story 中，不仅需要考虑用户按照我们的设计准确的提供有效信息，而且还要考虑用户在中间过程改变要执行的意图情况或称输入无效信息，因为对于FormAction来说，如果无法获得预期的信息就会报错；</li><li>两种情况：<ul><li>情况一 happy path：</li><li>情况二 unhappy path：</li></ul></li><li>实例:</li></ul><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs stata">## happy path<br><span class="hljs-comment">* request_weather</span><br>    - weather_form<br>    - <span class="hljs-keyword">form</span>&#123;<span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;weather_form&quot;</span>&#125;  激活<span class="hljs-keyword">form</span><br>    - <span class="hljs-keyword">form</span>&#123;<span class="hljs-string">&quot;name&quot;</span>: null&#125;  使<span class="hljs-keyword">form</span>无效<br>    <br>## unhappy path<br><span class="hljs-comment">* request_weather</span><br>    - weather_form<br>    - <span class="hljs-keyword">form</span>&#123;<span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;weather_form&quot;</span>&#125;<br><span class="hljs-comment">* stop</span><br>    - utter_ask_continue<br><span class="hljs-comment">* deny</span><br>    - action_deactivate_form<br>    - <span class="hljs-keyword">form</span>&#123;<span class="hljs-string">&quot;name&quot;</span>: null&#125;<br></code></pre></td></tr></table></figure><blockquote><p>注：</p><ul><li>“* request_weather” : 意图</li><li>“- weather_form” ：form action，可以理解为 Core 针对意图 request_weather 所执行的 动作 Action；</li><li>“- form{“name”: “weather_form”}” ： 激活 form （这个 name 对应 action.py 中 Class WeatherForm 的 函数 name() 的返回值）；</li><li>“- action_deactivate_form”：默认 的 action，作用是用户可能在表单操作过程中改变主意，决定不继续最初的请求，我们使用这个default action来禁止(取消)表单，同时重置要请求的所有slots；</li></ul></blockquote><p><strong>3.2 在 定义域 domain.yml 中添加 form 字段</strong></p><ul><li>动机：定义域 domain.yml 类似于 配置文件，该文件中 配置 该项目 所需要 的 intents、slots、entities、actions、forms、responses；</li><li>目标：配置 form</li></ul><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">intents</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">request_weather</span><br><br><span class="hljs-attribute">forms</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">weather_form</span><br></code></pre></td></tr></table></figure><p><strong>3.3 配置 FormPolicy 策略模块</strong></p><ul><li>动机：前面 【<a href="https://github.com/km1994/nlp_paper_study/blob/master/dialogue_system_study/rasa/rasa系列/rasa_core_FormAction.md#为什么-可以-采用-formaction-实现呢">为什么可以采用 FormAction 实现呢？</a>】说了，FormAction 可以 持续 向用户 询问 未 填充 的 slot 的信息。那么，是什么 方法 使他具有该功能呢？</li><li>原因：主要 是 有 FormPolicy 方法</li><li>介绍：FormPolicy是MemoizationPolicy的扩展，用于处理(form)表单的填充事项；</li><li>思路：当一个FormAction被调用时，FormPolicy将持续预测表单动作，直到表单中的所有槽都被填满，然后再执行对应的FormAction；</li><li>如何 使用 FormActions ?<ul><li>在 config.yml 配置文件中配置</li></ul></li></ul><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">policies</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">name: EmbeddingPolicy</span><br>    <span class="hljs-attribute">epochs</span><span class="hljs-punctuation">:</span> <span class="hljs-string">100</span><br>    <span class="hljs-attribute">max_history</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">name: FallbackPolicy</span><br>    <span class="hljs-attribute">fallback_action_name</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&#x27;action_default_fallback&#x27;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">name: MemoizationPolicy</span><br>    <span class="hljs-attribute">max_history</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">name: FormPolicy</span><br></code></pre></td></tr></table></figure><p><strong>3.4 实现 WeatherForm 动作 Action</strong></p><ul><li>动机：看完上面内容，你是否有以下疑问：<ul><li><ol><li>意图 request_weather 怎么知道要执行哪些 动作 Action 呢？</li></ol></li><li><ol><li>FormPolicy 策略模块 怎么 知道 WeatherForm 需要 哪些 槽位呢？</li></ol></li></ul></li><li>答案：需要在 action.py 文件中 定义 WeatherForm 的相关 Action</li><li>样例代码：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># action weather_form</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WeatherForm</span>(<span class="hljs-title class_ inherited__">FormAction</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>(<span class="hljs-params">self</span>) -&gt; Text:<br>        <span class="hljs-string">&quot;&quot;&quot;Unique identifier of the form&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;weather_form&quot;</span><br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">required_slots</span>(<span class="hljs-params">tracker: Tracker</span>) -&gt; <span class="hljs-type">List</span>[Text]:<br>        <span class="hljs-string">&quot;&quot;&quot;A list of required slots that the form has to fill&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> [<span class="hljs-string">&quot;date-time&quot;</span>, <span class="hljs-string">&quot;address&quot;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">submit</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        dispatcher: CollectingDispatcher,</span><br><span class="hljs-params">        tracker: Tracker,</span><br><span class="hljs-params">        domain: <span class="hljs-type">Dict</span>[Text, <span class="hljs-type">Any</span>],</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>]:<br>        <span class="hljs-string">&quot;&quot;&quot;Define what the form has to do</span><br><span class="hljs-string">            after all required slots are filled&quot;&quot;&quot;</span><br>        address = tracker.get_slot(<span class="hljs-string">&#x27;address&#x27;</span>)<br>        date_time = tracker.get_slot(<span class="hljs-string">&#x27;date-time&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;action_default_fallback-&gt;address:<span class="hljs-subst">&#123;address&#125;</span>&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;action_default_fallback-&gt;date_time:<span class="hljs-subst">&#123;date_time&#125;</span>&quot;</span>)<br>        dispatcher.utter_message(<span class="hljs-string">&quot;正在为你查询 &#123;&#125; &#123;&#125;的天气 ing&quot;</span>.<span class="hljs-built_in">format</span>(address,date_time))<br>        <span class="hljs-keyword">return</span> [Restarted()]<br></code></pre></td></tr></table></figure><ul><li><p>代码解析：</p><ul><li>name()：定义 Action 名称;</li><li>required_slots(tracker: Tracker)：定义 需要 填充 slots;</li><li>submit()：执行函数，当 所有 槽位 填充完，通过该函数 进行 responses；</li></ul></li><li><p>流程解析：</p><ol><li><p>当form action第一被调用时，form就会被激活并进入FormPolicy策略模式；</p></li><li><p>每次执行form action，required_slots会被调用，当发现某个还未被填充时，会主动去调用形式为uter_ask_{slotname}的模板(注：定义在domain.yml的templates字段中)；</p></li><li><p>当所有slot被填充完毕，submit方法就会被调用，此时本次form操作完毕被取消激活；</p></li></ol></li></ul><h2 id="Stories"><a href="#Stories" class="headerlink" title="Stories"></a>Stories</h2><p><strong>二、动机</strong></p><p>在对话管理（DM）中需要通过学习 获取到 必要的知识。那么，DM 的 训练数据从何而来？训练数据的格式是怎么样的？</p><blockquote><p>答案： Stories</p></blockquote><p><strong>三、什么是 Stories？</strong></p><ul><li>介绍：Rasa 采用 Stories 作为一种训练 DM 模型的数据格式；</li><li>存放地址：该训练数据存放在 story.md 文件中。</li><li>说明：Stories 是用户和人工智能助手之间的对话的表示，转换为特定的格式，其中用户输入表示为相应的意图(和必要的实体)，而助手的响应表示为相应的操作名称。</li><li>该数据的格式如下所示：</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs dust"><span class="language-xml"><span class="hljs-comment">&lt;!-- ##表示story的描述，没有实际作用 --&gt;</span></span><br><span class="language-xml">## greet + location/price + cuisine + num people</span><br><span class="language-xml">* greet</span><br><span class="language-xml">   - utter_greet</span><br><span class="language-xml">* inform</span><span class="hljs-template-variable">&#123;&quot;location&quot;: &quot;rome&quot;, &quot;price&quot;: &quot;cheap&quot;&#125;</span><span class="language-xml"></span><br><span class="language-xml">   - action_on_it</span><br><span class="language-xml">   - action_ask_cuisine</span><br><span class="language-xml">* inform</span><span class="hljs-template-variable">&#123;&quot;cuisine&quot;: &quot;spanish&quot;&#125;</span><span class="language-xml"></span><br><span class="language-xml">   - action_ask_numpeople    </span><br><span class="language-xml">* inform</span><span class="hljs-template-variable">&#123;&quot;people&quot;: &quot;six&quot;&#125;</span><span class="language-xml"></span><br><span class="language-xml">   - action_ack_dosearch</span><br><span class="language-xml">  </span><br><span class="language-xml"><span class="hljs-comment">&lt;!-- Form Action--&gt;</span></span><br><span class="language-xml">## happy path </span><br><span class="language-xml">* request_weather</span><br><span class="language-xml">   - weather_form</span><br><span class="language-xml">   - form</span><span class="hljs-template-variable">&#123;&quot;name&quot;: &quot;weather_form&quot;&#125;</span><span class="language-xml"></span><br><span class="language-xml">   - form</span><span class="hljs-template-variable">&#123;&quot;name&quot;: null&#125;</span><br></code></pre></td></tr></table></figure><p><strong>四、Stories 主要分哪些部分呢？</strong></p><p><strong>4.1 用户输入 (User Messages)</strong></p><ul><li>“xx”:”yy” 对应的信息 为 用户输入消息，采用 NLU 管道输出的 intent 和 entities 来表示可能的输入，policies 根据 intent 和 entities 预测下一步 action；</li><li>类别：<ul><li>“* greet”：用户输入无 entity 的场景；</li><li>“* inform{“people”: “six”}”：用户输入包含 entity 的场景，响应这一类 intent 为 普通 action;</li><li>“* request_weather”：用户输入Message对应的intent为form action情况；</li></ul></li></ul><p><strong>4.2 动作 （Action）</strong></p><ul><li>“-”：要执行动作(Action)；</li><li>分类：<ul><li>utterance actions：在domain.yaml中定义以utter_为前缀，比如名为greet的意图，它的回复应为utter_greet；</li><li>custom actions：自定义动作，具体逻辑由我们自己实现，虽然在定义action名称的时候没有限制，但是还是建议以action_为前缀，比如名为inform的意图fetch_profile的意图，它的response可为action_fetch_profile；</li></ul></li></ul><p><strong>4.3 事件（Action）</strong></p><ul><li>“-”：要执行事件（Action）；</li><li>分类：<ul><li>槽值设置(SlotSet)：</li><li>激活/注销表单(Form)：</li></ul></li></ul><p><strong>4.3.1 Slot Events</strong></p><ul><li>作用：当我们在自定义Action中设置了某个槽值，那么我们就需要在Story中Action执行之后显著的将这个SlotSet事件标注出来，格式为- slot{“slot_name”: “value”}。</li><li>举例：我们在action_fetch_profile中设置了Slot名为account_type的值，代码如下：</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs kotlin">from rasa_sdk.actions <span class="hljs-keyword">import</span> Action<br>from rasa_sdk.events <span class="hljs-keyword">import</span> SlotSet<br><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FetchProfileAction</span>(Action):<br>    def name(self):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;fetch_profile&quot;</span><br><br>    def run(self, dispatcher, tracker, domain):<br>        url = <span class="hljs-string">&quot;http://myprofileurl.com&quot;</span><br>        <span class="hljs-keyword">data</span> = requests.<span class="hljs-keyword">get</span>(url).json<br>        <span class="hljs-keyword">return</span> [SlotSet(<span class="hljs-string">&quot;account_type&quot;</span>, <span class="hljs-keyword">data</span>[<span class="hljs-string">&quot;account_type&quot;</span>])]<br></code></pre></td></tr></table></figure><blockquote><p>注：需要在Story中执行action_fetch_profile之后，添加- slot{“account_type” : “premium”}。</p></blockquote><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs clean">## fetch_profile<br>* fetch_profile<br>   - action_fetch_profile<br>   - slot&#123;<span class="hljs-string">&quot;account_type&quot;</span> : <span class="hljs-string">&quot;premium&quot;</span>&#125;<br>   - utter_welcome_premium<br></code></pre></td></tr></table></figure><blockquote><p>如果您的自定义Action中将槽值重置为None，则对应的事件为-slot{“slot_name”: null}</p></blockquote><p><strong>4.3.2 Form Events</strong></p><ul><li>三种形式的表单事件(Form Events):<ul><li>Form Action事件:<ul><li>介绍：表单动作事件，是自定义Action的一种，用于一个表单操作</li><li>“- restaurant_form”</li></ul></li><li>Form activation事件：<ul><li>介绍：激活表单事件，当form action事件执行后，会立马执行该事件</li><li>“- form{“name”: “restaurant_form”}”</li></ul></li><li>Form deactivation事件:<ul><li>介绍：注销表单事件，作用与form activation相反</li><li>“- form{“name”: null}”</li></ul></li></ul></li></ul><figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs fsharp">## happy path<br><span class="hljs-operator">*</span> request_weather<br>    <span class="hljs-operator">-</span> weather_form<br>    <span class="hljs-operator">-</span> <span class="hljs-keyword">form</span>&#123;<span class="hljs-string">&quot;name&quot;</span><span class="hljs-operator">:</span> <span class="hljs-string">&quot;weather_form&quot;</span>&#125;<br>    <span class="hljs-operator">-</span> <span class="hljs-keyword">form</span>&#123;<span class="hljs-string">&quot;name&quot;</span><span class="hljs-operator">:</span> <span class="hljs-literal">null</span>&#125;<br></code></pre></td></tr></table></figure><p>总之，我们在构建Story时，可以说是多种多样的，因为设计的故事情节是多种多样的，这就意味着上述三种内容的组合也是非常灵活的。另外，在设计Story时Rasa还提供了<a href="https://rasa.com/docs/rasa/core/stories/#id9">Checkpoints </a>和<a href="https://rasa.com/docs/rasa/core/stories/#id9">OR statements</a>两种功能，来提升构建Story的灵活度，但是需要注意的是，东西虽好，但是不要太贪了，过多的使用不仅增加了复杂度，同时也会拖慢训练的速度。</p><h2 id="Domain"><a href="#Domain" class="headerlink" title="Domain"></a>Domain</h2><ul><li><p>Domain，描述了对话机器人应知道的所有信息</p><p>类似于“人的大脑”，存储了<strong>意图intents、实体entities、插槽slots、动作actions、响应、表单等</strong>信息。domain.yml文件组成结构如下：</p><p><img src="https://flashgene.com/wp-content/uploads/2020/04/e85b0bc1accb546be2a817523b5ee22c.png" alt="img"></p><ol><li><p>intents</p><p>意图的定义是在NLU样本中实现的，并且在每个意图下面我们需要枚举尽可多的样本用于训练，以达到Bot能够准确识别出我们输入的一句话到底想要干什幺</p></li><li><p>session_config</p><p>即会话配置，这部分的作用为配置一次会话(conversation session)是否有超时限制</p></li><li><p>slots</p><p>即插槽，它就像对话机器人的内存，它通过键值对的形式可用来收集存储用户输入的信息(实体)或者查询数据库的数据</p></li><li><p>entities</p><p>即实体，类似于输入文本中的关键字，需要在NLU样本中进行标注，然后Bot进行实体识别，并将其填充到Slot槽中，便于后续进行相关的业务操作</p></li><li><p>actions</p><p>当Rasa NLU识别到用户输入Message的意图后，Rasa Core对话管理模块就会对其作出回应，而完成这个回应的模块就是action</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">actions:<br>– utter<span class="hljs-emphasis">_answer_affirm</span><br><span class="hljs-emphasis">– utter_answer_deny</span><br><span class="hljs-emphasis">– utter_answer_</span>greet<br></code></pre></td></tr></table></figure></li><li><p>forms</p><p>即表单，该部分列举了在NLU样本中自定义了哪些表单动作 (Form Actions)</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">forms</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">weather_form</span><br></code></pre></td></tr></table></figure></li><li><p>responses</p><p>描述UtterActions具体的回复内容，并且每个UtterAction下可以定义多条信息，当用户发起一个意图，比如 “你好!”，就触发utter_answer_greet操作，Rasa Core会从该action的模板中自动选择其中的一条信息作为结果反馈给用户。除此之外，还可以在训练数据中存储Responses；或者自定义一个NLG服务来生成Responses。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">responses:</span><br>  utter_answer_greet:<br>    - <span class="hljs-keyword">text</span>: <span class="hljs-string">&quot;您好！请问我可以帮到您吗？&quot;</span><br>    - <span class="hljs-keyword">text</span>: <span class="hljs-string">&quot;您好！很高兴为您服务。请说出您要查询的功能？&quot;</span><br>    <br>  utter_ask_date_time:<br>    - <span class="hljs-keyword">text</span>: <span class="hljs-string">&quot;请问您要查询哪一天的天气？&quot;</span><br></code></pre></td></tr></table></figure></li></ol></li></ul><h2 id="Slots"><a href="#Slots" class="headerlink" title="Slots"></a>Slots</h2><ul><li><p>Slots</p><ul><li><p>Slots Type</p><ol><li>Text：文本</li><li>Boolean：布尔型</li><li>Categorical：多个值中的一个</li><li>Float：浮点数</li><li>List：</li><li>Unfeaturized：不影响对话的数据</li></ol></li><li><p>slots 填充方式</p><ul><li>通过initial_value字段为当前slot提供一个初始值</li><li>Slots Set from NLU <code>\* greet&#123;“name”: “Ali”&#125;</code></li><li>Slots Set By Clicking Buttons</li><li>Slots Set by Actions：在Custom Action中通过返回事件来填充Slots的值</li></ul></li><li><p>Slots值的两种获取方式</p><ul><li><p>Get Slot in responses</p><p>在domain.yaml的responses部分，可以通过{slotname}的形式获取槽值</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">responses</span><span class="hljs-punctuation">:</span><br><span class="hljs-punctuation"></span><br><span class="hljs-attribute">utter_greet</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">– text</span><span class="hljs-punctuation">:</span> <span class="hljs-string">“Hey, &#123;name&#125;. How are you?”</span><br></code></pre></td></tr></table></figure></li><li><p>Get Slot in Custom Action</p><p>通过Tracker，能够轻松获取整个对话信息，其中就包括Slot的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> rasa_sdk.actions <span class="hljs-keyword">import</span> Action<br><span class="hljs-keyword">from</span> rasa_sdk.events <span class="hljs-keyword">import</span> SlotSet<br><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FetchProfileAction</span>(<span class="hljs-title class_ inherited__">Action</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> “fetch_profile”<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self, dispatcher, tracker, domain</span>):<br>        <span class="hljs-comment"># 获取slot account_type的值</span><br>        account_type = tracker.get_slot(‘account_type’)<br>        <span class="hljs-keyword">return</span> []<br></code></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h2 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h2><p><strong>二、动机</strong></p><p>在对话管理（DM）需要根据 用户输入 作出 相应的响应，该过程就需要 制定合适 的策略。</p><blockquote><p>答案： Policies</p></blockquote><p><strong>三、Policies 是什么？</strong></p><ul><li><p>介绍：Policies 是 Rasa Core中的策略模块；</p></li><li><p>对应类：rasa_core.policies.Policy；</p></li><li><p>作用：使用合适的策略（Policy）来预测一次对话后要执行的行为（Actions）；</p></li><li><p>预测原理：衡量命中的哪些Policies哪个置信度高，由置信度高的Policy选择合适的Action执行。假如出现不同的Policy拥有相同的置信度，那么就由它们的优先级决定，即选择优先级高的Policy；</p></li><li><p>Rasa对提供的Policies进行了优先级排序，具体如下表：</p><p><img src="https://flashgene.com/wp-content/uploads/2020/04/68e68cbe6bf8a5e4f8bddb1f5eb5c2be.png" alt="img"></p></li></ul><p><strong>四、Policies 的 策略模块的作用？</strong></p><p><strong>4.1 Memoization Policy</strong></p><ul><li>作用：只记住(memorizes)训练数据中的对话。如果训练数据中存在这样的对话，那么它将以置信度为1.0预测下一个动作，否则将预测为None，此时置信度为0.0；</li><li>如何在策略配置文件config.yml文件中，配置MemoizationPlicy策略，其中，max_history(超参数)决定了模型查看多少个对话历史以决定下一个执行的action；</li></ul><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">policies:</span><br>   - name: <span class="hljs-string">&quot;MemoizationPolicy&quot;</span><br><span class="hljs-symbol">   max_history:</span> <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><blockquote><p>注：max_history值越大训练得到的模型就越大并且训练时间会变长，关于该值到底该设置多少，我们可以举这么个例子，比如有这么一个Intent：out_of_scope来描述用户输入的消息off-topic(离题)，当用户连续三次触发out_of_scope意图，这时候我们就需要主动告知用户需要向其提供帮助，如果要Rasa Core能够学习这种模型，max_history应该至少为3。story.md中表现如下：</p></blockquote><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ceylon">* <span class="hljs-keyword">out</span><span class="hljs-number">_</span><span class="hljs-keyword">of</span><span class="hljs-number">_</span>scope<br>   - utter<span class="hljs-number">_</span><span class="hljs-keyword">default</span><br>* <span class="hljs-keyword">out</span><span class="hljs-number">_</span><span class="hljs-keyword">of</span><span class="hljs-number">_</span>scope<br>   - utter<span class="hljs-number">_</span><span class="hljs-keyword">default</span><br>* <span class="hljs-keyword">out</span><span class="hljs-number">_</span><span class="hljs-keyword">of</span><span class="hljs-number">_</span>scope<br>   - utter<span class="hljs-number">_</span>help<span class="hljs-number">_m</span>essage<br></code></pre></td></tr></table></figure><p><strong>4.2 Keras Policy</strong></p><ul><li>作用：Keras框架中实现的神经网络来预测选择执行下一个action；</li><li>默认的框架：使用LSTM(Long Short-Term Memory，长短期记忆网络)算法，但是我们也可以重写KerasPolicy.model_architecture函数来实现自己的框架(architecture)。KerasPolicy的模型很简单，只是单一的LSTM+Dense+softmax，这就需要我们不断地完善自己的story来把各种情况下的story进行补充；</li><li>如何在策略配置文件config.yml文件中，配置KerasPolicy策略，其中，epochs表示训练的次数，max_history同上：</li></ul><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">policies</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">name: KerasPolicy</span><br>    <span class="hljs-attribute">epochs</span><span class="hljs-punctuation">:</span> <span class="hljs-string">100</span><br>    <span class="hljs-attribute">max_history</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5</span><br></code></pre></td></tr></table></figure><p><strong>4.3 Embedding Policy</strong></p><ul><li>动机：基于机器学习的对话管理能够学习复杂的行为以完成任务，但是将其功能扩展到新领域并不简单，尤其是不同策略处理不合作用户行为的能力，以及在学习新任务(如预订酒店)时，如何将完成一项任务(如餐厅预订)重新应用于该任务时的情况；</li><li><p>作用：循环嵌入式对话策略(Recurrent Embedding Dialogue Policy，REDP)，它通过将actions和对话状态嵌入到相同的向量空间(vector space)能够获得较好的效果，REDP包含一个基于改进的Neural Turing Machine的记忆组件和注意机制，在该任务上显著优于基线LSTM分类器；</p></li><li><p>配置EmbeddingPolicy参数：</p></li></ul><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">policies</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">name: EmbeddingPolicy</span><br>    <span class="hljs-attribute">epochs</span><span class="hljs-punctuation">:</span> <span class="hljs-string">100</span><br>    <span class="hljs-attribute">featurizer</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">name: FullDialogueTrackerFeaturizer</span><br>      <span class="hljs-attribute">state_featurizer</span><span class="hljs-punctuation">:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">name: LabelTokenizerSingleStateFeaturizer</span><br></code></pre></td></tr></table></figure><p><strong>4.4 Form Policy</strong></p><ul><li><p>作用：MemoizationPolicy的扩展，用于处理(form)表单的填充事项。当一个FormAction被调用时，FormPolicy将持续预测表单动作，直到表单中的所有槽都被填满，然后再执行对应的FormAction；</p></li><li><p>使用：需要实现FormAction，在<code>domain.yml</code>中指定，在<code>stories.md</code>中使用</p><p>当一个FormAction被调用时，FormPolicy将持续预测表单动作，直到表单中的所有槽都被填满，然后再执行对应的FormAction。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> rasa_sdk <span class="hljs-keyword">import</span> Tracker<br><span class="hljs-keyword">from</span> rasa_sdk.forms <span class="hljs-keyword">import</span> FormAction<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Dict</span>, Text, <span class="hljs-type">Any</span>, <span class="hljs-type">List</span><br><span class="hljs-keyword">from</span> rasa_sdk.executor <span class="hljs-keyword">import</span> CollectingDispatcher<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FacilityForm</span>(<span class="hljs-title class_ inherited__">FormAction</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>(<span class="hljs-params">self</span>) -&gt; Text:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;facility_form&quot;</span><br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">required_slots</span>(<span class="hljs-params">tracker: Tracker</span>) -&gt; <span class="hljs-type">List</span>[Text]:<br>        <span class="hljs-keyword">return</span> [<span class="hljs-string">&quot;facility_type&quot;</span>, <span class="hljs-string">&quot;location&quot;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">slot_mappings</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Dict</span>[Text, <span class="hljs-type">Any</span>]:<br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;facility_type&quot;</span>: self.from_entity(entity=<span class="hljs-string">&quot;facility_type&quot;</span>, intent=[<span class="hljs-string">&quot;inform&quot;</span>, <span class="hljs-string">&quot;search_provider&quot;</span>]),<br>                <span class="hljs-string">&quot;location&quot;</span>: self.from_entity(entity=<span class="hljs-string">&quot;location&quot;</span>, intent=[<span class="hljs-string">&quot;inform&quot;</span>, <span class="hljs-string">&quot;search_provider&quot;</span>])&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">submit</span>(<span class="hljs-params">self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: <span class="hljs-type">Dict</span>[Text, <span class="hljs-type">Any</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>]:<br>        location = tracker.get_slot(<span class="hljs-string">&#x27;location&#x27;</span>)<br>        facility_type = tracker.get_slot(<span class="hljs-string">&#x27;facility_type&#x27;</span>)<br>        dispatcher.utter_button_message(<span class="hljs-string">&quot;Here is home health agency near you&quot;</span>)<br>        <span class="hljs-keyword">return</span> []<br></code></pre></td></tr></table></figure><figure class="highlight ldif"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ldif"><span class="hljs-attribute">forms</span>:<br><span class="hljs-literal">-</span> facility_form<br></code></pre></td></tr></table></figure><p><strong>4.5 Mapping Policy</strong></p><ul><li>作用：M可用于直接将意图映射到要执行的action，从而实现被映射的action总会被执行，其中，这种映射是通过triggers属性实现的；</li><li>使用：无视之前对话，一旦触发意图就操作；映射是传递intent属性给<code>triggers</code>实现的，修改<code>domain.yml</code></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">intents:</span><br> <span class="hljs-bullet">-</span> <span class="hljs-attr">greet:</span> &#123;<span class="hljs-attr">triggers:</span> <span class="hljs-string">utter_goodbye</span>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>注：greet是意图；utter_goodbye是action。<br>一个意图最多只能映射到一个action，我们的机器人一旦收到映射意图的消息，它将执行对应的action。然后，继续监听下一条message。需要注意的是，对于上述映射，我们还需要要在story.md文件中添加如下样本，否则，任何机器学习策略都可能被预测的action_greet在dialouge历史中突然出现而混淆</p></blockquote><p><strong>4.6 Fallback Policy</strong></p><ul><li>作用：如果意图识别的置信度低于nlu_threshold，或者没有任何对话策略预测的action置信度高于core_threshold，FallbackPolicy将执行fallback action。通俗来说，就是我们的对话机器人意图识别和action预测的置信度没有满足对应的阈值，该策略将使机器人执行指定的默认action；</li><li>使用：例如用户问了让机器人理解不了的东西时需要回退</li></ul><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">policies</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">name: &quot;FallbackPolicy&quot;</span><br>    <span class="hljs-comment"># 意图理解置信度阈值</span><br>    <span class="hljs-attribute">nlu_threshold</span><span class="hljs-punctuation">:</span> <span class="hljs-string">0.3</span><br>    <span class="hljs-comment"># action预测置信度阈值</span><br>    <span class="hljs-attribute">core_threshold</span><span class="hljs-punctuation">:</span> <span class="hljs-string">0.3</span><br>    <span class="hljs-comment"># fallback action</span><br>    <span class="hljs-attribute">fallback_action_name</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&#x27;action_default_fallback&#x27;</span><br></code></pre></td></tr></table></figure><blockquote><p>注：其中，action_default_fallback是Rasa Core中的一个默认操作，它将向用户发送utter_default模板消息，因此我们需要确保在domain.yml文件中指定此模板；</p></blockquote><ul><li>自定义使用：在fallback_action_name字段自定义默认回复的action，比如my_fallback_cation，就可以这么改：</li></ul><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">policies:</span><br>  - name: <span class="hljs-string">&quot;FallbackPolicy&quot;</span><br><span class="hljs-symbol">    nlu_threshold:</span> <span class="hljs-number">0.4</span><br><span class="hljs-symbol">    core_threshold:</span> <span class="hljs-number">0.3</span><br><span class="hljs-symbol">    fallback_action_name:</span> <span class="hljs-string">&quot;my_fallback_action&quot;</span><br></code></pre></td></tr></table></figure><h2 id="Interactive-Learning"><a href="#Interactive-Learning" class="headerlink" title="Interactive Learning"></a>Interactive Learning</h2><p>虽然我们可以容易的人工构建story样本数据，但是往往会出现一些考虑不全，甚至出错等问题，基于此，Rasa Core框架为我们提供了一种交互式学习(Interactive Learning)来获得所需的样本数据。</p><ol><li>开启Action Server</li></ol><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs brainfuck"><span class="hljs-comment">python</span> <span class="hljs-literal">-</span><span class="hljs-comment">m rasa run actions</span> <span class="hljs-literal">--</span><span class="hljs-comment">port 5055</span> <span class="hljs-literal">--</span><span class="hljs-comment">actions actions</span> <span class="hljs-literal">--</span><span class="hljs-comment">debug</span><br></code></pre></td></tr></table></figure><ol><li>开启Interactive Learning</li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">python -m rasa interactive -m models<span class="hljs-regexp">/20200313-101055.tar.gz –endpoints configs/</span>endpoints.yml –config configs/config.yml<br><br>\<span class="hljs-comment"># 或者（没有已训练模型情况）</span><br><br>\<span class="hljs-comment"># rasa会先训练好模型，再开启交互式学习会话</span><br><br>python -m rasa interactive –data <span class="hljs-regexp">/data –domain configs/</span>domain.yml –endpoints configs<span class="hljs-regexp">/endpoints.yml –config configs/</span>config.yml<br></code></pre></td></tr></table></figure><p>分别执行(1)、(2)命令后，我们可以预设一个交互场景根据终端的提示操作即可。如果一个交互场景所有流程执行完毕，按Ctrl+C结束并选择Start Fresh进入下一个场景即可。当然Rasa还提供了可视化界面，以帮助你了解每个Story样本构建的过程，网址：<a href="http://localhost:5005/visualization.html。">http://localhost:5005/visualization.html。</a></p><h1 id="Rasa-X"><a href="#Rasa-X" class="headerlink" title="Rasa X"></a>Rasa X</h1><ul><li><p>layers on top of Rasa Open Source and helps you build a better assistant</p></li><li><p>is a free, closed source tool available to all developers</p></li><li>can be deployed anywhere, so your training data stays secure and proprietary</li></ul><p>Rasa X可以以本地模式安装，也可以使用Kubernetes / Openshift或Docker Compose安装在服务器上。请考虑以下哪种描述最能描述您的情况，并相应地选择一种安装方法。</p><h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p>如果以下任何一种描述您的用例，则最好的方法是：</p><ul><li>您正在开发自己的Rasa助手，并希望在用户界面中与其聊天并与他人共享</li><li>您是第一次浏览Rasa X，只想了解它在本地的工作方式</li></ul><p>在以下情况下，您将需要使用其他安装选项之一：</p><ul><li>您已经准备好部署助手并将其提供给最终用户</li></ul><p><a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/local-mode">本地模式安装指南</a></p><h2 id="服务器快速安装"><a href="#服务器快速安装" class="headerlink" title="服务器快速安装"></a>服务器快速安装</h2><p>这是在服务器或群集上安装Rasa X的最简单，最快的方法。如果您不确定选择哪种方法，请尝试这种方法。</p><p>如果以下任何一种描述您的用例，则最好的方法是：</p><ul><li>您正在安装要在生产中使用的Rasa X，并且不需要太多自定义</li><li>您是第一次浏览Rasa X，并想了解它在服务器上的工作方式</li></ul><p>在以下情况下，您将需要使用其他服务器安装选项之一：</p><ul><li>您的系统不符合<a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/quick-install-script#requirements">要求</a></li><li>除了<a href="https://rasa.com/docs/rasa-x/installation-and-setup/customize#server-quick-install">此方法可用的</a>定制之外，您还需要其他定制</li><li>您期望机器人有大量流量（数百个并发用户），并且需要扩展</li><li>您正在安装Rasa Enterprise</li></ul><p><a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/quick-install-script">服务器快速安装指南</a></p><h2 id="Docker撰写"><a href="#Docker撰写" class="headerlink" title="Docker撰写"></a>Docker撰写</h2><p>如果满足以下条件，则使用Docker Compose在服务器上安装Rasa X是一个不错的选择：</p><ul><li>您不能或不想使用Kubernetes / Openshift</li><li>您需要比快速安装脚本提供的更多自定义设置</li><li>您不会期望大量的用户流量（即数百个并发用户）</li></ul><p>安装脚本可用于满足<a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/docker-compose#requirements">硬件和操作系统要求的</a>服务器 。对于其他操作系统，您可以通过Docker Compose手动安装RasaX。</p><p><a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/docker-compose">Docker Compose安装指南</a></p><h2 id="注意❤❤❤"><a href="#注意❤❤❤" class="headerlink" title="注意❤❤❤"></a>注意❤❤❤</h2><p>启动 <code>rasa x</code> 的过程中，windows下可能会遇到如下问题</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">rasa.shared.exceptions.FileIOException: Failed <span class="hljs-built_in">to</span> <span class="hljs-built_in">read</span> <span class="hljs-built_in">file</span> <span class="hljs-string">&#x27;C:\Users\NINGSH~1\AppData\Local\Temp\tmpmqoghnak&#x27;</span>, could <span class="hljs-keyword">not</span> <span class="hljs-built_in">read</span> <span class="hljs-keyword">the</span> <span class="hljs-built_in">file</span> <span class="hljs-keyword">using</span> utf<span class="hljs-number">-8</span> <span class="hljs-built_in">to</span> decode <span class="hljs-keyword">it</span>. Please make sure <span class="hljs-keyword">the</span> <span class="hljs-built_in">file</span> is stored <span class="hljs-keyword">with</span> this encoding.<br></code></pre></td></tr></table></figure><p>解决：win10下需保证所有 XXX.yml 等配置文件不包含中文，或者是修改rasa\shared\utils\io.py代码：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs julia">def read_file(filename: <span class="hljs-built_in">Union</span>[<span class="hljs-built_in">Text</span>, Path], encoding: <span class="hljs-built_in">Text</span> = DEFAULT_ENCODING) -&gt; <span class="hljs-built_in">Any</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Read text from a file.&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">import</span> codecs<br>    <span class="hljs-keyword">try</span>:<br>        with codecs.open(filename, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=encoding) as f:<br>            <span class="hljs-keyword">return</span> f.read()<br>    except FileNotFoundError:<br>        raise FileNotFoundException(<br>            <span class="hljs-string">f&quot;Failed to read file, &quot;</span> <span class="hljs-string">f&quot;&#x27;&#123;os.path.abspath(filename)&#125;&#x27; does not exist.&quot;</span><br>        )<br>    except UnicodeDecodeError:<br>        <span class="hljs-keyword">try</span>:<br>            with codecs.open(filename, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=&#x27;gb2312&#x27;) as f:<br>                <span class="hljs-keyword">return</span> f.read()<br>        except <span class="hljs-built_in">Exception</span>:<br>            raise FileIOException(<br>                <span class="hljs-string">f&quot;Failed to read file &#x27;&#123;os.path.abspath(filename)&#125;&#x27;, &quot;</span><br>                <span class="hljs-string">f&quot;could not read the file using &#123;encoding&#125; to decode &quot;</span><br>                <span class="hljs-string">f&quot;it. Please make sure the file is stored with this &quot;</span><br>                <span class="hljs-string">f&quot;encoding.&quot;</span><br>            )<br></code></pre></td></tr></table></figure><h1 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title="遇到的坑"></a>遇到的坑</h1><ol><li>训练数据JSON格式实体entity为中文，报错<code>UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode characters</code>，entity命名应使用英文</li><li>根目录不要有文件<code>test.py</code>，启动API服务时会运行该文件，可能会报错</li><li>项目名不要带中文，启动Rasa X时可能报错</li><li>YAML文件涉及字符串最好用<code>&quot;</code>包围起来，特别为了调用Rasa API时。如<code>- payload: &quot;/inform&#123;\&quot;gender\&quot;: \&quot;男\&quot;&#125;&quot;</code></li><li>一个Form激活时输入了无关数据报错<code>Failed to extract slot xxx with action xxx_form</code>，可在Stories中添加<code>action_deactivate_form</code>停止</li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>400 多行代码！超详细 Rasa 中文聊天机器人开发指南<br><a href="https://flashgene.com/archives/110272.html">https://flashgene.com/archives/110272.html</a></p><p>Rasa中文聊天机器人开发指南</p><p><a href="https://jiangdg.blog.csdn.net/article/details/104328946">https://jiangdg.blog.csdn.net/article/details/104328946</a></p><p>Rasa入门——AI助手和聊天机器人</p><p><a href="https://blog.csdn.net/lly1122334/article/details/104041589">https://blog.csdn.net/lly1122334/article/details/104041589</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Rasa Core</tag>
      
      <tag>Rasa X</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rasa NLU 详解</title>
    <link href="/2020/12/14/2020-12-14-Rasa%20NLU%20%E8%AF%A6%E8%A7%A3/"/>
    <url>/2020/12/14/2020-12-14-Rasa%20NLU%20%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p><a href="#关于-rasa-nlu-那些你不知道的事">【关于 rasa-&gt;NLU 】那些你不知道的事</a></p><ul><li><a href="#目录">目录</a></li><li><a href="#一概况图">一、概况图</a></li><li><a href="#二动机">二、动机</a></li><li><a href="#三什么是-自然语言理解">三、什么是 自然语言理解？</a></li><li><a href="#四自然语言理解-的-工作-是什么">四、自然语言理解 的 工作 是什么？</a></li><li><a href="#五自然语言理解-三层次你知道么">五、自然语言理解 三层次，你知道么？</a></li><li><a href="#六自然语言理解-一般理解-你知道么">六、自然语言理解 一般理解 你知道么？</a></li><li><a href="#七nlu-训练数据如何准备">七、NLU 训练数据如何准备？</a><ul><li><a href="#71-nlu-训练样本数据格式">7.1 NLU 训练样本数据格式？</a></li><li><a href="#72-nlu-训练样本数据格式如何验证">7.2 NLU 训练样本数据格式如何验证？</a></li></ul></li><li><a href="#八rasa-nlu-components">八、Rasa NLU Components</a><ul><li><a href="#81-词向量资源word-vector-sources">8.1 词向量资源（Word Vector Sources）</a><ul><li><a href="#一mitienlp-做中文任务时一般选用做一个因为他是基于中文的词向量">（一）MitieNLP （做中文任务时，一般选用做一个，因为他是基于中文的词向量）</a></li><li><a href="#二spacynlp-英文">（二）SpacyNLP （英文）</a></li></ul></li><li><a href="#82-分词tokenizers">8.2 分词（Tokenizers）</a><ul><li><a href="#一whitespacetokenizer">（一）WhitespaceTokenizer</a></li><li><a href="#二jiebatokenizer">（二）JiebaTokenizer</a></li><li><a href="#三mitietokenizer">（三）MitieTokenizer</a></li><li><a href="#四spacytokenizer">（四）SpacyTokenizer</a></li><li><a href="#五converttokenizer">（五）ConveRTTokenizer</a></li></ul></li><li><a href="#83-文本特征化text-featurizers">8.3 文本特征化（Text Featurizers）</a><ul><li><a href="#一mitiefeaturizer">（一）MitieFeaturizer</a></li><li><a href="#二spacyfeaturizer">（二）SpacyFeaturizer</a></li><li><a href="#三convertfeaturizer">（三）ConveRTFeaturizer</a></li><li><a href="#四regexfeaturizer">（四）RegexFeaturizer</a></li><li><a href="#五countvectorsfeaturizer">（五）CountVectorsFeaturizer</a></li></ul></li><li><a href="#84-意图分类intent-classifiers">8.4 意图分类（Intent Classifiers）</a><ul><li><a href="#一mitieintentclassifier">（一）MitieIntentClassifier</a></li><li><a href="#二sklearnintentclassifier">（二）SklearnIntentClassifier</a></li><li><a href="#三embeddingintentclassifier">（三）EmbeddingIntentClassifier</a></li><li><a href="#四keywordintentclassifier">（四）KeywordIntentClassifier</a></li></ul></li><li><a href="#85-选择器selectors">8.5 选择器（Selectors）</a><ul><li><a href="#一response-selector">（一）Response Selector</a></li></ul></li><li><a href="#86-实体提取entity-extractors">8.6 实体提取（Entity Extractors）</a><ul><li><a href="#一mitieentityextractor">（一）MitieEntityExtractor</a></li><li><a href="#二spacyentityextractor">（二）SpacyEntityExtractor</a></li><li><a href="#三entitysynonymmapper">（三）EntitySynonymMapper</a></li><li><a href="#四crfentityextractor">（四）CRFEntityExtractor</a></li><li><a href="#五ducklinghttpextractor">（五）DucklingHTTPExtractor</a></li></ul></li></ul></li><li><a href="#九rasa-nlu-pipline">九、Rasa NLU Pipline</a><ul><li><a href="#91-动机">9.1 动机</a></li><li><a href="#92-介绍">9.2 介绍</a></li><li><a href="#93-使用template-pipline">9.3 使用Template Pipline</a><ul><li><a href="#一pretrained_embeddings_spacy">（一）pretrained_embeddings_spacy</a></li><li><a href="#二supervised_embeddings">（二）supervised_embeddings</a></li><li><a href="#三pretrained_embeddings_convert">（三）pretrained_embeddings_convert</a></li><li><a href="#四mitie">（四）MITIE</a></li></ul></li><li><a href="#94-使用custome-pipline">9.4 使用Custome Pipline</a><ul><li><a href="#一zh_jieba_mitie_sklearn">（一）zh_jieba_mitie_sklearn</a></li><li><a href="#二zh_crf_supervised_embeddings">（二）zh_crf_supervised_embeddings</a></li></ul></li></ul></li><li><a href="#参考资料">参考资料</a></li></ul><span id="more"></span></li></ul><h2 id="二、动机"><a href="#二、动机" class="headerlink" title="二、动机"></a>二、动机</h2><p>在人机对话中，首先需要 让 机器 理解 用户所说的 内容，才能 帮助 机器 知道用户意图，并作出 适当 的 反馈。那么 这个时候 需要怎么做呢？</p><blockquote><p>答案：自然语言理解（NLU）</p></blockquote><h2 id="三、什么是-自然语言理解？"><a href="#三、什么是-自然语言理解？" class="headerlink" title="三、什么是 自然语言理解？"></a>三、什么是 自然语言理解？</h2><ul><li>英文：（NLU，natural language understanding）</li><li>介绍：指机器能够理解执行人类所期望的某些语言功能，换句话说就是人与机器交流的桥梁；</li></ul><h2 id="四、自然语言理解-的-工作-是什么？"><a href="#四、自然语言理解-的-工作-是什么？" class="headerlink" title="四、自然语言理解 的 工作 是什么？"></a>四、自然语言理解 的 工作 是什么？</h2><ol><li>理解句子的正确次序规则和概念，又能理解不含规则的句子；</li><li>知道词的确切含义、形式、词类及构词法；</li><li>了解词的语义分类、词的多义性、词的歧义性；</li><li>指定和不定特性及所有特性；</li><li>问题领域的结构知识和实践概念；</li><li>语言的语气信息和韵律表现；</li><li>有关语言表达形式的文字知识；</li><li>论域的背景知识；</li></ol><h2 id="五、自然语言理解-三层次，你知道么？"><a href="#五、自然语言理解-三层次，你知道么？" class="headerlink" title="五、自然语言理解 三层次，你知道么？"></a>五、自然语言理解 三层次，你知道么？</h2><ul><li>三层次：<ul><li>词法分析：自然语言理解过程的第一层，它的性能直接影响到后面句法和语义分析的成果，主要包括自动分词、词性标注、中文命名实体标注三方面内容；</li><li>句法分析的目标是自动推导出句子的句法结构，实现这个目标首先要确定语法体系，不同的语法体系会产生不同的句法结构，常见语法体系有短语结构语法、依存关系语法；</li><li>语义分析就是指分析话语中所包含的含义，根本目的是理解自然语言。分为词汇级语义分析、句子级语义分析、段落／篇章级语义分析，即分别理解词语、句子、段落的意义。</li></ul></li></ul><p><img src="img/1532921162912e4eb2dd348.png" alt=""></p><blockquote><p>参考：<a href="https://www.aistudyblog.com/naturallanguage/20180731/13196.html">自然语言理解技术NLU</a></p></blockquote><h2 id="六、自然语言理解-一般理解-你知道么？"><a href="#六、自然语言理解-一般理解-你知道么？" class="headerlink" title="六、自然语言理解 一般理解 你知道么？"></a>六、自然语言理解 一般理解 你知道么？</h2><p><img src="img/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNjY3NDcxLTM3MzE1ZjdiYWFlZTc1ZjQuanBn.png" alt=""></p><blockquote><p>该图来源于【<a href="https://blog.csdn.net/weixin_41510260/article/details/99876405?utm_source=distribute.pc_relevant.none-task">自然语言处理(NLP)的一般处理流程</a>】，还提供了【<a href="https://naotu.baidu.com/file/f644044a8fb37fdba2d3d0bb4eb350e1?token=fd9855a9fc353aca">百度脑图查看点击链接</a>】。</p></blockquote><h2 id="七、NLU-训练数据如何准备？"><a href="#七、NLU-训练数据如何准备？" class="headerlink" title="七、NLU 训练数据如何准备？"></a>七、NLU 训练数据如何准备？</h2><h3 id="7-1-NLU-训练样本数据格式？"><a href="#7-1-NLU-训练样本数据格式？" class="headerlink" title="7.1 NLU 训练样本数据格式？"></a>7.1 NLU 训练样本数据格式？</h3><blockquote><p>注：将NLU训练数据保存到单独的文件或者多个文件的目录</p></blockquote><ul><li>markdown:<ul><li>Common Examples:（唯一必须，NLU模型核心，也是训练NLU模型基础）<ul><li>intent：某个意图，它应于某些text相对应；</li><li>text：用户自然语言文本，即用户Message（text中可以不包括实体，但如果包含需要用<a href="entityName">entityText</a>进行标志）；</li><li>entities：将要被提取的目标实体，我们需要在text文本中标出(如果该text存在实体的话)；</li></ul></li><li>synonyms：同义词，在实体提取时会统一被解析成同一个意思；<ul><li>举例说明：NLU 能够将（余额、话费、话费余额、账号余额）-映射-&gt; 余额；</li><li>注：<strong>需要pipeline中包含EntitySynonmMapper组件，才能使用</strong>；</li></ul></li><li>Regular Expression Features：<ul><li>介绍：有助于意图分类和实体提取，但是它并不参与实体和意图的定义，仅仅是提供规则来协助意图分类和实体提取，因此，在训练文本text中，该添加的实体和意图样本需要照样添加</li><li>举例：当需要用户输入的手机号实体时，我们可以再nlu.md文件中添加正则表达式特征支持，当用户输入的Message包含符合手机号正则表达式规则的内容时，Rasa可以更加容易地将其提取出来；</li><li>说明：phone_number表示的既不是实体名也不是意图名，它只是一个便于我们阅读的标志而已。除了实体识别，我们还可以编写符合意图分类的正则表达式，这里就不演示了。</li><li>对于<strong>实体提取</strong>来说，目前<strong>只有CRFEntityExtractor 实体提取器支持正则特征，像``MitieEntityExtractor和SpacyEntityExtractor目前还不支持</strong>；</li><li>对于<strong>意图分类器</strong>，<strong>目前均已支持正则特征</strong>。</li><li>注：<strong>需要pipeline中包含RegexFeaturizer组件，才能使用 正则特性</strong>；</li></ul></li><li>lookup tables：<ul><li>查找表有利于在加载训练数据时，生成与Regular Expression Features相同的正则特征。当在训练数据中提供查找表时，内容被组合成一个大型、不区分大小写的regex模式，该模式在训练示例中查找精确匹配。这些正则表达式匹配多个token，其处理与训练数据中直接指定的正则表达式模式相同。查找表可以包括在训练数据中，如果外部提供的数据必须要以换行进行分隔；</li><li>注：mobile_data_package表示实体名。为了查找表能够有效的被使用，训练数据中必须要有一些示例被匹配上。否则，模型不会使用查找表特征向查找表添加数据时必须小心，比如如果表中有误报或其他噪声，就会影响性能，因此请确保查找表包含干净的数据 【eg：查下<a href="mobile_data_package">腾讯视频流量包</a>被匹配时，才会去 查找 data/lookup_tables/DataPackage.txt 表】</li></ul></li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs s"># data/lookup_tables/DataPackage.txt 格式<br>腾讯视频流量包<br>...<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs s">&lt;!--Common Examples--&gt;<br>## intent:你的意图名称  <br>- text<br>## intent:request_phone_business<br>- 查个手机号<br>- 查电话号码[19800222425](phone_number)<br>- [余额](business)<br>- 查下[腾讯视频流量包](mobile_data_package)<br>- 你好！请帮我查询一下电话[12260618425](phone_number)的[账户余额](business)<br>- 帮我查个手机号[19860612222](phone_number)的[话费](business)<br>- 查下号码[19860222425](phone_number)的[抖音免流包](mobile_data_package)<br><br>&lt;!--synonyms--&gt;<br>## synonym:余额<br>- 余额<br>- 话费<br>...<br><br>&lt;!--Regular Expression Features--&gt;<br>## regex:phone_number<br>- ((\d&#123;3,4&#125;-)?\d&#123;7,8&#125;)|(((\+86)|(86))?(1)\d&#123;10&#125;)<br><br>## lookup: mobile_data_package<br>data/lookup_tables/DataPackage.txt<br></code></pre></td></tr></table></figure><ul><li>JSON:</li></ul><blockquote><p>markdown 类似</p></blockquote><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;rasa_nlu_data&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;common_examples&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>      <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;帮我查一下我的流量有多少&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;intent&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;request_search&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;entities&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>          <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;start&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;end&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">9</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;流量&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;entity&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;item&quot;</span><br>          <span class="hljs-punctuation">&#125;</span><br>        <span class="hljs-punctuation">]</span><br>      <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;regex_features&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><br>      <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;inform_package&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;pattern&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;套餐[0-9一二三四五六七八九十百俩两]+&quot;</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><br>      <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;inform_time&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;pattern&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;([0-9一二三四五六七八九十百俩两]+)月份?的?&quot;</span><br>    <span class="hljs-punctuation">&#125;</span><br>  <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;entity_synonyms&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><br>      <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;消费&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;synonyms&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;话费&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;钱&quot;</span><span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><br>  <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;lookup_tables&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><br>      <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;constellation&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;ookup&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;data/lookup_tables/constellation.txt&quot;</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><br>  <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#125;</span><br>  <br><br></code></pre></td></tr></table></figure><h3 id="7-2-NLU-训练样本数据格式如何验证？"><a href="#7-2-NLU-训练样本数据格式如何验证？" class="headerlink" title="7.2 NLU 训练样本数据格式如何验证？"></a>7.2 NLU 训练样本数据格式如何验证？</h3><ul><li>动机：对于 domian.yml、NLU data和Story data，如何检查这些文件是否有错误呢？</li><li>使用命令：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs s">python -m rasa data validate<br></code></pre></td></tr></table></figure><blockquote><p>参数说明：</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs s">usage: rasa data validate [-h] [-v] [-vv] [--quiet] [--fail-on-warnings]<br>                        [-d DOMAIN] [--data DATA]<br><br>optional arguments:<br>  -h, --help            show this help message and exit<br>  --fail-on-warnings    Fail validation on warnings and errors. If omitted<br>                        only errors will result in a non zero exit code.<br>                        (default: False)<br>  -d DOMAIN, --domain DOMAIN<br>                        Domain specification (yml file). (default: domain.yml)<br>  --data DATA           Path to the file or directory containing Rasa data.<br>                        (default: data)<br><br>Python Logging Options:<br>  -v, --verbose         Be verbose. Sets logging level to INFO. (default:<br>                        None)<br>  -vv, --debug          Print lots of debugging statements. Sets logging level<br>                        to DEBUG. (default: None)<br>  --quiet               Be quiet! Sets logging level to WARNING. (default:<br>                        None)<br></code></pre></td></tr></table></figure><ul><li>代码：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">from</span> rasa <span class="hljs-keyword">import</span> utils<br><span class="hljs-keyword">from</span> rasa.core.validator <span class="hljs-keyword">import</span> Validator<br><br>logger = logging.getLogger(__name__)<br><br>utils.configure_colored_logging(<span class="hljs-string">&#x27;DEBUG&#x27;</span>)<br><br>validator = Validator.from_files(domain_file=<span class="hljs-string">&#x27;domain.yml&#x27;</span>,<br>                                 nlu_data=<span class="hljs-string">&#x27;data/nlu_data.md&#x27;</span>,<br>                                 stories=<span class="hljs-string">&#x27;data/stories.md&#x27;</span>)<br><br>validator.verify_all()<br></code></pre></td></tr></table></figure><h2 id="八、Rasa-NLU-Components"><a href="#八、Rasa-NLU-Components" class="headerlink" title="八、Rasa NLU Components"></a>八、Rasa NLU Components</h2><h3 id="8-1-词向量资源（Word-Vector-Sources）"><a href="#8-1-词向量资源（Word-Vector-Sources）" class="headerlink" title="8.1 词向量资源（Word Vector Sources）"></a>8.1 词向量资源（Word Vector Sources）</h3><h4 id="（一）MitieNLP-（做中文任务时，一般选用做一个，因为他是基于中文的词向量）"><a href="#（一）MitieNLP-（做中文任务时，一般选用做一个，因为他是基于中文的词向量）" class="headerlink" title="（一）MitieNLP （做中文任务时，一般选用做一个，因为他是基于中文的词向量）"></a>（一）MitieNLP （做中文任务时，一般选用做一个，因为他是基于中文的词向量）</h4><p><img src="img/20200923075311.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;MitieNLP&quot;<br>  # 语言模型<br>  model: &quot;data/total_word_feature_extractor_zh.dat&quot;<br></code></pre></td></tr></table></figure><h4 id="（二）SpacyNLP-（英文）"><a href="#（二）SpacyNLP-（英文）" class="headerlink" title="（二）SpacyNLP （英文）"></a>（二）SpacyNLP （英文）</h4><p><img src="img/20200923075505.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;SpacyNLP&quot;<br>  # 指定语言模型<br>  model: &quot;en_core_web_md&quot;<br>  # 设定在检索单词向量时，这将决定单词的大小写是否相关<br>  # 当为false时，表示不区分大小写。比如`hello` and `Hello`<br>  # 检索到的向量是相同的。<br>  case_sensitive: false<br></code></pre></td></tr></table></figure><h3 id="8-2-分词（Tokenizers）"><a href="#8-2-分词（Tokenizers）" class="headerlink" title="8.2 分词（Tokenizers）"></a>8.2 分词（Tokenizers）</h3><h4 id="（一）WhitespaceTokenizer"><a href="#（一）WhitespaceTokenizer" class="headerlink" title="（一）WhitespaceTokenizer"></a>（一）WhitespaceTokenizer</h4><p><img src="img/20200923075716.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;WhitespaceTokenizer&quot;<br>  # 指定是否大小写敏感，默认true为敏感<br>  case_sensitive: false<br></code></pre></td></tr></table></figure><h4 id="（二）JiebaTokenizer"><a href="#（二）JiebaTokenizer" class="headerlink" title="（二）JiebaTokenizer"></a>（二）JiebaTokenizer</h4><p><img src="img/20200923075852.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;JiebaTokenizer&quot;<br>  # 指定自定义词典<br>  dictionary_path: &quot;path/to/custom/dictionary/dir&quot;<br></code></pre></td></tr></table></figure><h4 id="（三）MitieTokenizer"><a href="#（三）MitieTokenizer" class="headerlink" title="（三）MitieTokenizer"></a>（三）MitieTokenizer</h4><p><img src="img/20200923080230.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;MitieTokenizer&quot;<br></code></pre></td></tr></table></figure><h4 id="（四）SpacyTokenizer"><a href="#（四）SpacyTokenizer" class="headerlink" title="（四）SpacyTokenizer"></a>（四）SpacyTokenizer</h4><p><img src="img/20200923080341.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;SpacyTokenizer&quot;<br></code></pre></td></tr></table></figure><h4 id="（五）ConveRTTokenizer"><a href="#（五）ConveRTTokenizer" class="headerlink" title="（五）ConveRTTokenizer"></a>（五）ConveRTTokenizer</h4><p><img src="img/20200923080521.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;ConveRTTokenizer&quot;<br></code></pre></td></tr></table></figure><h3 id="8-3-文本特征化（Text-Featurizers）"><a href="#8-3-文本特征化（Text-Featurizers）" class="headerlink" title="8.3 文本特征化（Text Featurizers）"></a>8.3 文本特征化（Text Featurizers）</h3><h4 id="（一）MitieFeaturizer"><a href="#（一）MitieFeaturizer" class="headerlink" title="（一）MitieFeaturizer"></a>（一）MitieFeaturizer</h4><p><img src="img/20200923080657.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;MitieFeaturizer&quot;<br></code></pre></td></tr></table></figure><h4 id="（二）SpacyFeaturizer"><a href="#（二）SpacyFeaturizer" class="headerlink" title="（二）SpacyFeaturizer"></a>（二）SpacyFeaturizer</h4><p><img src="img/20200923080941.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;SpacyFeaturizer&quot;<br></code></pre></td></tr></table></figure><h4 id="（三）ConveRTFeaturizer"><a href="#（三）ConveRTFeaturizer" class="headerlink" title="（三）ConveRTFeaturizer"></a>（三）ConveRTFeaturizer</h4><p><img src="img/20200923081101.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;ConveRTFeaturizer&quot;<br></code></pre></td></tr></table></figure><h4 id="（四）RegexFeaturizer"><a href="#（四）RegexFeaturizer" class="headerlink" title="（四）RegexFeaturizer"></a>（四）RegexFeaturizer</h4><p><img src="img/20200923081214.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;RegexFeaturizer&quot;<br></code></pre></td></tr></table></figure><h4 id="（五）CountVectorsFeaturizer"><a href="#（五）CountVectorsFeaturizer" class="headerlink" title="（五）CountVectorsFeaturizer"></a>（五）CountVectorsFeaturizer</h4><p><img src="img/20200923081332.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;CountVectorsFeaturizer&quot;<br>  &quot;use_shared_vocab&quot;: False,<br>  analyzer: &#x27;word&#x27; <br>  token_pattern: r&#x27;(?u)\b\w\w+\b&#x27;<br>  strip_accents: None  <br>  stop_words: None <br>  min_df: 1 <br>  max_df: 1.0 <br>  min_ngram: 1  <br>  max_ngram: 1  <br>  max_features: None  <br>  lowercase: true<br>  OOV_token: None  <br>  OOV_words: []  <br></code></pre></td></tr></table></figure><h3 id="8-4-意图分类（Intent-Classifiers）"><a href="#8-4-意图分类（Intent-Classifiers）" class="headerlink" title="8.4 意图分类（Intent Classifiers）"></a>8.4 意图分类（Intent Classifiers）</h3><h4 id="（一）MitieIntentClassifier"><a href="#（一）MitieIntentClassifier" class="headerlink" title="（一）MitieIntentClassifier"></a>（一）MitieIntentClassifier</h4><p><img src="img/20200923081653.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;MitieIntentClassifier&quot; <br></code></pre></td></tr></table></figure><ul><li>输出格式：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;intent&quot;: &#123;&quot;name&quot;: &quot;greet&quot;, &quot;confidence&quot;: 0.98343&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="（二）SklearnIntentClassifier"><a href="#（二）SklearnIntentClassifier" class="headerlink" title="（二）SklearnIntentClassifier"></a>（二）SklearnIntentClassifier</h4><p><img src="img/20200923081836.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;SklearnIntentClassifier&quot;<br>  # 指定SVM训练时要尝试的参数<br>  # 通过运行超参数搜索，以找到最佳的参数集<br>  C: [1, 2, 5, 10, 20, 100]<br>  # 指定C-SVM使用的内核<br>  # 它与GridSearchCV中的“C”超参数一起使用<br>  kernels: [&quot;linear&quot;]<br></code></pre></td></tr></table></figure><ul><li>输出格式：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;intent&quot;: &#123;&quot;name&quot;: &quot;greet&quot;, &quot;confidence&quot;: 0.78343&#125;,<br>    &quot;intent_ranking&quot;: [<br>        &#123;<br>            &quot;confidence&quot;: 0.1485910906220309,<br>            &quot;name&quot;: &quot;goodbye&quot;<br>        &#125;,<br>        &#123;<br>            &quot;confidence&quot;: 0.08161531595656784,<br>            &quot;name&quot;: &quot;restaurant_search&quot;<br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="（三）EmbeddingIntentClassifier"><a href="#（三）EmbeddingIntentClassifier" class="headerlink" title="（三）EmbeddingIntentClassifier"></a>（三）EmbeddingIntentClassifier</h4><p><img src="img/20200923082013.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;EmbeddingIntentClassifier&quot;<br># Embedding算法的控制参数非常多<br># 具体参照官方文档，这里以指定训练次数为例<br>  epochs: 500<br></code></pre></td></tr></table></figure><ul><li>输出格式：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;intent&quot;: &#123;&quot;name&quot;: &quot;greet&quot;, &quot;confidence&quot;: 0.8343&#125;,<br>    &quot;intent_ranking&quot;: [<br>        &#123;<br>            &quot;confidence&quot;: 0.385910906220309,<br>            &quot;name&quot;: &quot;goodbye&quot;<br>        &#125;,<br>        &#123;<br>            &quot;confidence&quot;: 0.28161531595656784,<br>            &quot;name&quot;: &quot;restaurant_search&quot;<br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="（四）KeywordIntentClassifier"><a href="#（四）KeywordIntentClassifier" class="headerlink" title="（四）KeywordIntentClassifier"></a>（四）KeywordIntentClassifier</h4><p><img src="img/20200923082133.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;KeywordIntentClassifier&quot;<br>  case_sensitive: True<br></code></pre></td></tr></table></figure><ul><li>输出格式：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;intent&quot;: &#123;&quot;name&quot;: &quot;greet&quot;, &quot;confidence&quot;: 1.0&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="8-5-选择器（Selectors）"><a href="#8-5-选择器（Selectors）" class="headerlink" title="8.5 选择器（Selectors）"></a>8.5 选择器（Selectors）</h3><h4 id="（一）Response-Selector"><a href="#（一）Response-Selector" class="headerlink" title="（一）Response Selector"></a>（一）Response Selector</h4><p><img src="img/20200923082314.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;KeywordIntentClassifier&quot; <br>  # 算法支持很多参数配置，详情见文档<br>  case_sensitive: True<br></code></pre></td></tr></table></figure><ul><li>输出格式：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;text&quot;: &quot;What is the recommend python version to install?&quot;,<br>    &quot;entities&quot;: [],<br>    &quot;intent&quot;: &#123;&quot;confidence&quot;: 0.6485910906220309, &quot;name&quot;: &quot;faq&quot;&#125;,<br>    &quot;intent_ranking&quot;: [<br>        &#123;&quot;confidence&quot;: 0.6485910906220309, &quot;name&quot;: &quot;faq&quot;&#125;,<br>        &#123;&quot;confidence&quot;: 0.1416153159565678, &quot;name&quot;: &quot;greet&quot;&#125;<br>    ],<br>    &quot;response_selector&quot;: &#123;<br>      &quot;faq&quot;: &#123;<br>        &quot;response&quot;: &#123;&quot;confidence&quot;: 0.7356462617, &quot;name&quot;: &quot;Supports 3.5, 3.6 and 3.7, <br>                     +&quot;recommended version is 3.6&quot;&#125;,<br>        &quot;ranking&quot;: [<br>            &#123;&quot;confidence&quot;: 0.7356462617, &quot;name&quot;: &quot;Supports 3.5, 3.6 and 3.7, <br>             +&quot;recommended version is 3.6&quot;&#125;,<br>            &#123;&quot;confidence&quot;: 0.2134543431, &quot;name&quot;: &quot;You can ask me about how <br>             +&quot;to get started&quot;&#125;<br>        ]<br>      &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="8-6-实体提取（Entity-Extractors）"><a href="#8-6-实体提取（Entity-Extractors）" class="headerlink" title="8.6 实体提取（Entity Extractors）"></a>8.6 实体提取（Entity Extractors）</h3><p><img src="img/20200923082503.png" alt=""></p><h4 id="（一）MitieEntityExtractor"><a href="#（一）MitieEntityExtractor" class="headerlink" title="（一）MitieEntityExtractor"></a>（一）MitieEntityExtractor</h4><p><img src="img/20200923082623.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;MitieEntityExtractor&quot;<br></code></pre></td></tr></table></figure><ul><li>输出格式：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;entities&quot;: [&#123;&quot;value&quot;: &quot;New York City&quot;,<br>                  &quot;start&quot;: 20,<br>                  &quot;end&quot;: 33,<br>                  &quot;confidence&quot;: null,<br>                  &quot;entity&quot;: &quot;city&quot;,<br>                  &quot;extractor&quot;: &quot;MitieEntityExtractor&quot;&#125;]<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="（二）SpacyEntityExtractor"><a href="#（二）SpacyEntityExtractor" class="headerlink" title="（二）SpacyEntityExtractor"></a>（二）SpacyEntityExtractor</h4><p><img src="img/20200923082716.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;SpacyEntityExtractor&quot;<br>  # dimensions to extract<br>  dimensions: [&quot;PERSON&quot;, &quot;LOC&quot;, &quot;ORG&quot;, &quot;PRODUCT&quot;]<br></code></pre></td></tr></table></figure><ul><li>输出格式：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;entities&quot;: [&#123;&quot;value&quot;: &quot;New York City&quot;,<br>                  &quot;start&quot;: 20,<br>                  &quot;end&quot;: 33,<br>                  &quot;entity&quot;: &quot;city&quot;,<br>                  &quot;confidence&quot;: null,<br>                  &quot;extractor&quot;: &quot;SpacyEntityExtractor&quot;&#125;]<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="（三）EntitySynonymMapper"><a href="#（三）EntitySynonymMapper" class="headerlink" title="（三）EntitySynonymMapper"></a>（三）EntitySynonymMapper</h4><p><img src="img/20200923082819.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;EntitySynonymMapper&quot;<br></code></pre></td></tr></table></figure><ul><li>训练数据与实体提取示例：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs s">[&#123;<br>  &quot;text&quot;: &quot;I moved to New York City&quot;,<br>  &quot;intent&quot;: &quot;inform_relocation&quot;,<br>  &quot;entities&quot;: [&#123;&quot;value&quot;: &quot;nyc&quot;,<br>                &quot;start&quot;: 11,<br>                &quot;end&quot;: 24,<br>                &quot;entity&quot;: &quot;city&quot;,<br>               &#125;]<br>&#125;,<br>&#123;<br>  &quot;text&quot;: &quot;I got a new flat in NYC.&quot;,<br>  &quot;intent&quot;: &quot;inform_relocation&quot;,<br>  &quot;entities&quot;: [&#123;&quot;value&quot;: &quot;nyc&quot;,<br>                &quot;start&quot;: 20,<br>                &quot;end&quot;: 23,<br>                &quot;entity&quot;: &quot;city&quot;,<br>               &#125;]<br>&#125;]<br></code></pre></td></tr></table></figure><blockquote><p>说明：在上述例子中，该组件将实体New York City和NYC映射到nyc。即使消息包含NYC，实体提取将返回nyc。当该组件更改现有实体时，它将自己附加到该实体的处理器列表中。</p></blockquote><h4 id="（四）CRFEntityExtractor"><a href="#（四）CRFEntityExtractor" class="headerlink" title="（四）CRFEntityExtractor"></a>（四）CRFEntityExtractor</h4><p><img src="img/20200923082958.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;CRFEntityExtractor&quot;<br>  features: [[&quot;low&quot;, &quot;title&quot;], [&quot;bias&quot;, &quot;suffix3&quot;], [&quot;upper&quot;, &quot;pos&quot;, &quot;pos2&quot;]]<br>  # 决定是否使用BILOU_flag<br>  BILOU_flag: true<br>  # 在训练前将该参数设定给sklearn_crfcuite.CRF tagger<br>  max_iterations: 50<br>  # 指定L1正则化系数<br>  # 在训练前将该参数设定给sklearn_crfcuite.CRF tagger<br>  L1_c: 0.1<br>  # 指定L2正则化系数<br>  # 在训练前将该参数设定给sklearn_crfcuite.CRF tagger<br>  L2_c: 0.1<br></code></pre></td></tr></table></figure><ul><li>实体提取示例：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;entities&quot;: [&#123;&quot;value&quot;:&quot;New York City&quot;,<br>                  &quot;start&quot;: 20,<br>                  &quot;end&quot;: 33,<br>                  &quot;entity&quot;: &quot;city&quot;,<br>                  &quot;confidence&quot;: 0.874,<br>                  &quot;extractor&quot;: &quot;CRFEntityExtractor&quot;&#125;]<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="（五）DucklingHTTPExtractor"><a href="#（五）DucklingHTTPExtractor" class="headerlink" title="（五）DucklingHTTPExtractor"></a>（五）DucklingHTTPExtractor</h4><p><img src="img/20200923083143.png" alt=""></p><ul><li>configs.yml 配置</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;DucklingHTTPExtractor&quot;<br>  # duckling server的url<br>  url: &quot;http://localhost:8000&quot;<br>  # 指定提取哪些维度，即实体类型<br>  dimensions: [&quot;time&quot;, &quot;number&quot;, &quot;amount-of-money&quot;, &quot;distance&quot;]<br>  # 配置语言环境<br>  locale: &quot;de_DE&quot;<br>  # 指定时区<br>  timezone: &quot;Europe/Berlin&quot;<br>  # 访问ducking server超时时间<br>  timeout : 3<br></code></pre></td></tr></table></figure><ul><li>实体提取示例：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs s">&#123;<br>    &quot;entities&quot;: [&#123;&quot;end&quot;: 53,<br>                  &quot;entity&quot;: &quot;time&quot;,<br>                  &quot;start&quot;: 48,<br>                  &quot;value&quot;: &quot;2017-04-10T00:00:00.000+02:00&quot;,<br>                  &quot;confidence&quot;: 1.0,<br>                  &quot;extractor&quot;: &quot;DucklingHTTPExtractor&quot;&#125;]<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="九、Rasa-NLU-Pipline"><a href="#九、Rasa-NLU-Pipline" class="headerlink" title="九、Rasa NLU Pipline"></a>九、Rasa NLU Pipline</h2><h3 id="9-1-动机"><a href="#9-1-动机" class="headerlink" title="9.1 动机"></a>9.1 动机</h3><ul><li>动机：前面介绍了怎么多 组件(Component)，问题来了：怎么多 招式，我们需要怎么搭配 才能 治敌呢？</li><li>解答：本节将继续讲解如何使用这些组件将准备好的样本数据(nlu.md)训练得到NLU模型</li></ul><h3 id="9-2-介绍"><a href="#9-2-介绍" class="headerlink" title="9.2 介绍"></a>9.2 介绍</h3><p>在Rasa NLU模块中，提供了一种名为Pipline(管道)配置方式，传入的消息(Message)通过管道中一系列组件处理后得到最终的模型。管道(Pipline)由多个组件(Component)构成，每个组件有各自的功能，比如实体提取、意图分类、响应选择、预处理等，这些组件在管道中一个接着一个的执行，每个组件处理输入并创建输出，并且输出可以被该组件之后管道中任何组件使用。当然，有些组件只生成管道中其他组件使用的信息，有些组件生成Output属性，这些Output属性将在处理完成后返回。下图为”pipeline”: [“Component A”, “Component B”, “Last Component”]训练时调用顺序：</p><p><img src="img/20200923083606" alt=""></p><p>在Rasa NLU模块中，已为我们提供了几种模板(Template) Pipline，比如pretrained_embeddings_spacy、supervised_embeddings等，每一种Pipline组件构成不同，可以根据训练数据的特性选择使用。当然，Pipline的配置非常的灵活，我们可以自定义Pipline中的组件，实现不同特性的Pipline。</p><h3 id="9-3-使用Template-Pipline"><a href="#9-3-使用Template-Pipline" class="headerlink" title="9.3 使用Template Pipline"></a>9.3 使用Template Pipline</h3><h4 id="（一）pretrained-embeddings-spacy"><a href="#（一）pretrained-embeddings-spacy" class="headerlink" title="（一）pretrained_embeddings_spacy"></a>（一）pretrained_embeddings_spacy</h4><p>在config.yaml文件中配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;en&quot;<br>pipeline: &quot;pretrained_embeddings_spacy&quot;<br></code></pre></td></tr></table></figure><p> 当然，上述配置等价于：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;en&quot;<br><br>pipeline:<br>- name: &quot;SpacyNLP&quot;        # 预训练词向量        <br>- name: &quot;SpacyTokenizer&quot;  # 文本分词器          <br>- name: &quot;SpacyFeaturizer&quot; # 文本特征化  <br>- name: &quot;RegexFeaturizer&quot; # 支持正则表达式  <br>- name: &quot;CRFEntityExtractor&quot; # 实体提取器  <br>- name: &quot;EntitySynonymMapper&quot; # 实体同义词映射  <br>- name: &quot;SklearnIntentClassifier&quot; # 意图分类器 <br></code></pre></td></tr></table></figure><blockquote><p>  pretrained_embeddings_spacy管道使用GloVe或 fastText的预训练词向量，因此，它的优势在于当你有一个训练样本如I want to buy apples，Rasa会预测意图为get pears。因为模型已经知道“苹果”和“梨”是非常相似的。如果没有足够大的训练数据，这一点尤其有用。</p></blockquote><h4 id="（二）supervised-embeddings"><a href="#（二）supervised-embeddings" class="headerlink" title="（二）supervised_embeddings"></a>（二）supervised_embeddings</h4><p>在config.yaml文件中配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;en&quot;<br>pipeline: &quot;supervised_embeddings&quot;<br></code></pre></td></tr></table></figure><p> 当然，上述配置等价于：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;en&quot;<br><br>pipeline:<br>- name: &quot;WhitespaceTokenizer&quot;   # 分词器<br>- name: &quot;RegexFeaturizer&quot;       # 正则<br>- name: &quot;CRFEntityExtractor&quot; # 实体提取器<br>- name: &quot;EntitySynonymMapper&quot; # 同义词映射<br>- name: &quot;CountVectorsFeaturizer&quot;  # featurizes文本基于词<br>- name: &quot;CountVectorsFeaturizer&quot;  # featurizes文本基于n-grams character，保留词边界 <br>  analyzer: &quot;char_wb&quot;<br>  min_ngram: 1<br>  max_ngram: 4<br>- name: &quot;EmbeddingIntentClassifier&quot;  # 意图分类器<br></code></pre></td></tr></table></figure><blockquote><p>  supervised_embeddings 管道不使用任何的预训练词向量或句向量，而是针对自己的数据集特别做的训练。它的优势是面向自己特定数据集的词向量(your word vectors will be customised for your domain)，比如，在通用英语中，单词“balance” (平衡)与单词 “symmetry”(对称)意思非常相近，而与单词”cash”意思截然不同。但是，在银行领域(domain)，“balance”与”cash”意思相近，而supervised_embeddings训练得到的模型就能够捕捉到这一点。该pipline不需要任何指定的语言模型，因此适用于任何语言，当然，需要指定对应的分词器。比如默认使用WhitespaceTokenizer，对于中文可以使用Jieba分词器等等，也就是该Pipline的组件是可以自定义的。</p></blockquote><h4 id="（三）pretrained-embeddings-convert"><a href="#（三）pretrained-embeddings-convert" class="headerlink" title="（三）pretrained_embeddings_convert"></a>（三）pretrained_embeddings_convert</h4><p>在config.yaml文件中配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;en&quot;<br><br>pipeline: &quot;pretrained_embeddings_convert&quot;<br></code></pre></td></tr></table></figure><p> 当然，上述配置等价于：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;en&quot;<br><br>pipeline:<br>- name: &quot;ConveRTTokenizer&quot;<br>- name: &quot;ConveRTFeaturizer&quot;<br>- name: &quot;EmbeddingIntentClassifier&quot;<br></code></pre></td></tr></table></figure><blockquote><p>  pretrained_embeddings_convert使用预训练的句子编码模型ConveRT以抽取用户输入句子的整体向量表征。该pipeline使用ConveRT模型抽取句子表征，并将句子表征输入到EmbeddingIntentClassifier以进行意图分类。使用pretrained_embeddings_convert的好处是不独立地处理用户输入句子中的每个词，而是为完整的句子创建上下文向量表征。比如，句子can I book a car?Rasa 会预测意图为I need a ride from my place。由于这两个示例的上下文向量表征已经非常相似，因此对它们进行分类的意图很可能是相同的。如果没有足够大的训练数据，这也很有用。需要注意的是，由于ConveRT模型仅在英语语料上进行训练，因此只有在训练数据是英语时才能够使用该pipeline。</p></blockquote><h4 id="（四）MITIE"><a href="#（四）MITIE" class="headerlink" title="（四）MITIE"></a>（四）MITIE</h4><p>在config.yaml文件中配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;en&quot;<br><br># 1. 使用SklearnIntentClassifier意图分类器<br># 这里的模型为英文<br><br>pipeline:<br>- name: &quot;MitieNLP&quot;       # 预训练词向量<br>  model: &quot;data/total_word_feature_extractor.dat&quot;<br>- name: &quot;MitieTokenizer&quot;  # 分词器<br>- name: &quot;MitieEntityExtractor&quot; # 实体提取器<br>- name: &quot;EntitySynonymMapper&quot; # 同义词映射<br>- name: &quot;RegexFeaturizer&quot; # 正则<br>- name: &quot;MitieFeaturizer&quot; # 特征化<br>- name: &quot;SklearnIntentClassifier&quot; # 意图分类器<br><br># 2. 使用MitieIntentClassifier意图分类器<br># 数据量大的时候，训练非常慢(不推荐)<br><br># pipeline:<br># - name: &quot;MitieNLP&quot;<br>#   model: &quot;data/total_word_feature_extractor.dat&quot;<br># - name: &quot;MitieTokenizer&quot;<br># - name: &quot;MitieEntityExtractor&quot;<br># - name: &quot;EntitySynonymMapper&quot;<br># - name: &quot;RegexFeaturizer&quot;<br># - name: &quot;MitieIntentClassifier&quot;<br></code></pre></td></tr></table></figure><blockquote><p>  Rasa NLU模块支持在Pipline中使用Mitie，但是在使用前需要训练词向量，然后使用MitieNLP组件指定。MITIE后端对于小型数据集执行得很好，但是如果数据量超过几百个示例，则训练可能需要很长时间。Rasa官网不建议使用它，因为mitie支持在将来的版本中可能会被弃用。</p></blockquote><h3 id="9-4-使用Custome-Pipline"><a href="#9-4-使用Custome-Pipline" class="headerlink" title="9.4 使用Custome Pipline"></a>9.4 使用Custome Pipline</h3><h4 id="（一）zh-jieba-mitie-sklearn"><a href="#（一）zh-jieba-mitie-sklearn" class="headerlink" title="（一）zh_jieba_mitie_sklearn"></a>（一）zh_jieba_mitie_sklearn</h4><p>在config.yaml文件中配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;zh&quot;<br><br>pipeline:<br>- name: &quot;MitieNLP&quot; # 使用中文词向量模型<br>  model: &quot;data/total_word_feature_extractor_zh.dat&quot;<br>- name: &quot;JiebaTokenizer&quot; # 使用jieba分词<br>- name: &quot;MitieEntityExtractor&quot;<br>- name: &quot;EntitySynonymMapper&quot;<br>- name: &quot;RegexFeaturizer&quot;<br>- name: &quot;MitieFeaturizer&quot;<br>- name: &quot;SklearnIntentClassifier&quot;<br></code></pre></td></tr></table></figure><p> 当然，上述配置等价于：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs s">Received user message &#x27;&quot;广州明天的天气怎么样&quot;&#x27; with <br>intent <br>&#x27;&#123;&#x27;name&#x27;: &#x27;request_weather&#x27;, &#x27;confidence&#x27;: 0.5182071733645418&#125;&#x27; <br>and entities <br>&#x27;[&#123;&#x27;entity&#x27;: &#x27;address&#x27;, &#x27;value&#x27;: &#x27;广州&#x27;, &#x27;start&#x27;: 1, &#x27;end&#x27;: 3, <br>&#x27;confidence&#x27;: None, &#x27;extractor&#x27;: &#x27;MitieEntityExtractor&#x27;&#125;, <br>&#123;&#x27;entity&#x27;: &#x27;date-time&#x27;, &#x27;value&#x27;: &#x27;明天&#x27;, &#x27;start&#x27;: 3, &#x27;end&#x27;: 5, &#x27;confidence&#x27;: <br>     None,&#x27;extractor&#x27;: &#x27;MitieEntityExtractor&#x27;&#125;]&#x27;<br>confidence&#x27;: None, &#x27;extractor&#x27;: &#x27;MitieEntityExtractor&#x27;&#125;<br>     ]&#x27;<br></code></pre></td></tr></table></figure><blockquote><p>  由于Rasa NLU模块提供的模板Pipline主要适用于英文，假如我们需要训练中文NLU模型的话，就需要使用中文分词器，比如jieba分词器，因此，我们修改MITIE Pipline将分词器改为Jieba，并修改MitieNLP预训练词向量模型为中文模型，其他不变，如MitieEntityExtractor，SklearnIntentClassifier等。根据NLU识别结果可知，输入文本经过处理后输出的intent和entities，从而可知，intent意图识别和entities实体识别是相互独立的。</p></blockquote><h4 id="（二）zh-crf-supervised-embeddings"><a href="#（二）zh-crf-supervised-embeddings" class="headerlink" title="（二）zh_crf_supervised_embeddings"></a>（二）zh_crf_supervised_embeddings</h4><p>在config.yaml文件中配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs s">language: &quot;zh&quot;<br><br>pipeline:<br>- name: &quot;JiebaTokenizer&quot;  # 使用jieba分词<br>- name: &quot;RegexFeaturizer&quot;<br>- name: &quot;CRFEntityExtractor&quot;<br>- name: &quot;EntitySynonymMapper&quot;<br>- name: &quot;CountVectorsFeaturizer&quot;<br>- name: &quot;CountVectorsFeaturizer&quot;<br>  analyzer: &quot;char_wb&quot;<br>  min_ngram: 1<br>  max_ngram: 4<br>- name: &quot;EmbeddingIntentClassifier&quot;<br></code></pre></td></tr></table></figure><ul><li>NLU识别结果示例1：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs s">Received user message &#x27;&quot;广州明天的天气怎么样&quot;&#x27; with <br>intent <br>&#x27;&#123;&#x27;name&#x27;: &#x27;request_weather&#x27;, &#x27;confidence&#x27;: 0.9965207576751709&#125;&#x27; <br>and entities <br>&#x27;[&#123;&#x27;start&#x27;: 1, &#x27;end&#x27;: 3, &#x27;value&#x27;: &#x27;广州&#x27;, &#x27;entity&#x27;: &#x27;address&#x27;,<br>&#x27;confidence&#x27;: 0.4974091477686857, &#x27;extractor&#x27;: &#x27;CRFEntityExtractor&#x27;&#125;, <br>&#123;&#x27;start&#x27;: 3, &#x27;end&#x27;: 5, &#x27;value&#x27;: &#x27;明天&#x27;, &#x27;entity&#x27;: &#x27;date-time&#x27;, <br>     &#x27;confidence&#x27;: 0.8807040793780636, &#x27;extractor&#x27;: &#x27;CRFEntityExtractor&#x27;&#125;]&#x27;<br></code></pre></td></tr></table></figure><ul><li>NLU识别结果示例2：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs s">Received user message &#x27;&quot;查下138383834381的账户余额&quot;&#x27; with <br>intent <br>&#x27;&#123;&#x27;name&#x27;: &#x27;request_phone_business&#x27;, &#x27;confidence&#x27;: 0.9994893074035645&#125;&#x27; <br>and entities <br>&#x27;[&#123;&#x27;start&#x27;: 3, &#x27;end&#x27;: 15, &#x27;value&#x27;: &#x27;138383834381&#x27;, &#x27;entity&#x27;: &#x27;phone_number&#x27;, <br>        &#x27;confidence&#x27;: 0.5848492378103071, &#x27;extractor&#x27;: &#x27;CRFEntityExtractor&#x27;&#125;,<br>&#123;&#x27;start&#x27;: 16, &#x27;end&#x27;: 20, &#x27;value&#x27;: &#x27;余额&#x27;, &#x27;entity&#x27;: &#x27;business&#x27;,<br>         &#x27;confidence&#x27;: 0.9023286498337025, &#x27;extractor&#x27;: &#x27;CRFEntityExtractor&#x27;, <br>         &#x27;processors&#x27;: [&#x27;EntitySynonymMapper&#x27;]&#125;]&#x27;<br></code></pre></td></tr></table></figure><blockquote><p>注：该Pipline修改自模板管道supervised_embeddings，由于该模板默认支持英文，为了实现支持中文，我们将分词器由WhitespaceTokenizer改为JiebaTokenizer，其他配置不变。经过测试可知，在意图分类方面，CountVectorsFeaturizer、EmbeddingIntentClassifier组合意图提取置信度高于MitieFeaturizer、SklearnIntentClassifier组合；在实体提取方面。CRFEntityExtractor也优于MitieEntityExtractor。另外，supervised_embeddings不需要任何指定的语言模型，因此适用于任何语言，并且完全依赖于训练数据，因此训练得到的模型拥有更好的适应性，训练的时间也非常快。但是，目前我遇到的有一点就是，有可能在训练数据不足时，在实体提取时可能会出现无法提取到实体的问题，当然，这只是我的推测，有待进一步验证。</p><p>当然，除了对已有的模板Pipline进行重新组合，我们完全可以自定义Pipline中的组件，定制你想要的功能和改进每个环节，这或许就是Rasa的优秀之处，非常灵活。比如，我们只希望支持实体识别，不做意图分类，那么我们可以这样自定义一个Pipline：</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs s">pipeline:<br>- name: &quot;SpacyNLP&quot;<br>- name: &quot;CRFEntityExtractor&quot;<br>- name: &quot;EntitySynonymMapper&quot;<br></code></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://jiangdg.blog.csdn.net/article/details/104530994">Rasa中文聊天机器人开发指南(2)：NLU篇</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Rasa NLU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rasa安装以及项目初尝试</title>
    <link href="/2020/12/14/2020-12-14-Rasa%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%A1%B9%E7%9B%AE%E5%88%9D%E5%B0%9D%E8%AF%95/"/>
    <url>/2020/12/14/2020-12-14-Rasa%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%A1%B9%E7%9B%AE%E5%88%9D%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<ul><li>Rasa简介</li><li>Rasa安装<ul><li>Windows系统下的环境要求</li><li>NLU 管道依赖项<ul><li>首选spaCy</li><li>第二个选择:MITIE</li></ul></li></ul></li><li>Rasa教程<ul><li>1.创建新的项目</li><li>2.构建NLU样本</li><li>3.构建Core样本</li><li>4.定义域</li><li>5.定义你的模型配置</li><li>6.训练NLU和CORE模型</li><li>7.配置Http和Action</li><li>8.和你的助手谈谈</li><li>9.启动web服务</li></ul></li><li>Rasa命令备忘单</li><li><p>Rasa架构</p><span id="more"></span></li></ul><h1 id="Rasa简介"><a href="#Rasa简介" class="headerlink" title="Rasa简介"></a>Rasa简介</h1><p><em>Rasa</em>是一个开源机器学习框架，用于构建上下文AI助手和聊天机器人。Rasa Open Source有两个主要模块：</p><ul><li><a href="https://rasa.com/docs/rasa/nlu/about/">NLU</a>：实现<code>意图识别</code>和<code>槽值</code>提取，它把用户的输入转换为结构化数据；</li><li><a href="https://rasa.com/docs/rasa/core/about/">Core</a>：是一个对话管理平台，用于预测、决定下一步做什么；</li></ul><p><a href="https://rasa.com/docs/rasa-x/">Rasa X</a>是一个工具，可帮助您构建、改进和部署由Rasa框架提供支持的AI Assistants</p><h1 id="Rasa安装"><a href="#Rasa安装" class="headerlink" title="Rasa安装"></a>Rasa安装</h1><ul><li>创建环境</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">$ conda create -n rasa <span class="hljs-attribute">python</span>=3.6<br></code></pre></td></tr></table></figure><ul><li>激活环境</li></ul><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-meta"><span class="hljs-keyword">$conda</span> activate rasa</span><br></code></pre></td></tr></table></figure><ul><li>Rasa 的推荐安装方式是通过<code>pip</code>:</li></ul><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs axapta">pip --<span class="hljs-literal">default</span>-timeout=<span class="hljs-number">500</span> --extra-<span class="hljs-keyword">index</span>-url https:<span class="hljs-comment">//pypi.rasa.com/simple install -U rasa-x</span><br></code></pre></td></tr></table></figure><p>这将同时安装Rasa和Rasa X。如果你不想使用Rasa X,只需要运行<code>pip --default-timeout=500 install -U rasa</code></p><p>在安装的过程中有可能会出现超时中断，可以尝试多执行几次上面的命令</p><h2 id="Windows系统下的环境要求"><a href="#Windows系统下的环境要求" class="headerlink" title="Windows系统下的环境要求"></a>Windows系统下的环境要求</h2><p>确保安装了Microsoft vc++编译器，这样python就可以编译任何依赖项。你可以从<a href="https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/?rr=https%3A%2F%2Frasa.com%2Fdocs%2Frasa%2Fuser-guide%2Finstallation%2F">Visual Studio</a>获得编译器。下载安装程序并在列表中选择vc++构建工具。</p><p>VS 2017社区版（Community）下载地址：</p><ul><li><a href="https://pan.baidu.com/s/1jJXyRMA">百度网盘</a>，密码: ub6c</li></ul><p>在单个XX中，需要勾选“<code>用于 CMake 的 Visual C++ 工具</code>”。安装完毕后，将如下路径添加到环境变量PATH中，再重启电脑使之生效；</p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs livescript">C:<span class="hljs-string">\Program</span> Files (x86)<span class="hljs-string">\Microsoft</span> Visual Studio<span class="hljs-string">\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin</span><br><br>C:<span class="hljs-string">\Program</span> Files (x86)<span class="hljs-string">\Microsoft</span> Visual Studio <span class="hljs-number">14.0</span><span class="hljs-string">\VC\bin</span><br></code></pre></td></tr></table></figure><h2 id="NLU-管道依赖项"><a href="#NLU-管道依赖项" class="headerlink" title="NLU 管道依赖项"></a>NLU 管道依赖项</h2><p>Rasa NLU有用于识别意图和实体的不同组件，其中大多数都有一些额外的依赖项。</p><p>当你训练NLU模型时，Rasa将检查是否安装了所有必需的依赖项，并告诉你缺少哪一个依赖项。<a href="http://rasachatbot.com/1_Installation/">选择管道</a>的页面将帮助你选择要使用的管道。</p><h3 id="首选spaCy"><a href="#首选spaCy" class="headerlink" title="首选spaCy"></a>首选spaCy</h3><p>你可以用以下命令安装:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pip</span> install rasa[spacy]==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">python</span> -m spacy download zh_core_web_md<br></code></pre></td></tr></table></figure><p>若要使用spacy 中文模型，需要通过代理下载并安装：</p><ul><li><p><a href="https://github.com/explosion/spacy-models/releases/zh_core_web_md-2.3.1">spacy 中文模型下载地址</a></p></li><li><p>spacy 中文模型安装<br>（模型版本2.3.1支持spacy&gt;=2.3.0,&lt;2.4.0）：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pip</span> install zh_core_web_md-<span class="hljs-number">2</span>.<span class="hljs-number">3</span>.<span class="hljs-number">1</span>.tar.gz<br></code></pre></td></tr></table></figure><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">pip install https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/explosion/</span>spacy-models<span class="hljs-regexp">/releases/</span>download<span class="hljs-regexp">/zh_core_web_md-2.3.1/</span>zh_core_web_md-<span class="hljs-number">2.3</span>.<span class="hljs-number">1</span>.tar.gz<br></code></pre></td></tr></table></figure></li></ul><h3 id="第二个选择-MITIE"><a href="#第二个选择-MITIE" class="headerlink" title="第二个选择:MITIE"></a>第二个选择:MITIE</h3><p>MITIE后端对于小型数据集执行得很好，但是如果你有数百个示例，那么训练可能会花费很长时间。我们可能会在未来弃用MITIE后端。</p><p>下载<a href="https://github.com/mit-nlp/MITIE">MITIE源码</a>和中文词向量模型<a href="https://pan.baidu.com/s/1kNENvlHLYWZIddmtWJ7Pdg">total_word_feature_extractor_zh.dat(密码：p4vx)</a>，这里需要将该模型拷贝到创建的python项目data目录下(可任意位置)，后面训练NLU模型时用到；</p><p>Mitie源码安装:</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> build<br><span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> install<br></code></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>待安装完毕后，使用如下命令查看详情</p><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dart">pip <span class="hljs-keyword">show</span> rasa<br></code></pre></td></tr></table></figure><h1 id="Rasa项目初尝试"><a href="#Rasa项目初尝试" class="headerlink" title="Rasa项目初尝试"></a>Rasa项目初尝试</h1><h2 id="1-创建新的项目"><a href="#1-创建新的项目" class="headerlink" title="1.创建新的项目"></a>1.创建新的项目</h2><p>第一步是创建一个新的Rasa项目。要做到这一点，运行下面的代码:</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">rasa</span> init --<span class="hljs-literal">no</span>-prompt<br></code></pre></td></tr></table></figure><p><code>rasa init</code>命令创建rasa项目所需的所有文件，并根据一些示例数据训练一个简单的机器人。如果你省略了<code>——no-prompt</code>参数，将会询问你一些关于项目设置的问题。这将创建以下文件:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">.<br>├── __init__<span class="hljs-selector-class">.py</span><br>├── actions<span class="hljs-selector-class">.py</span><br>├── config<span class="hljs-selector-class">.yml</span><br>├── credentials<span class="hljs-selector-class">.yml</span><br>├── data<br>│   ├── nlu<span class="hljs-selector-class">.md</span><br>│   └── stories<span class="hljs-selector-class">.md</span><br>├── domain<span class="hljs-selector-class">.yml</span><br>├── endpoints<span class="hljs-selector-class">.yml</span><br>└── models<br>    └── &lt;timestamp&gt;<span class="hljs-selector-class">.tar</span>.gz<br></code></pre></td></tr></table></figure><p>文件说明如下：</p><div class="table-container"><table><thead><tr><th>文件名称</th><th>作用说明</th></tr></thead><tbody><tr><td><strong>init</strong>.py</td><td>帮助python查找操作的空文件</td></tr><tr><td>actions.py</td><td>为你的自定义操作编写代码</td></tr><tr><td>config.yml ‘*’</td><td>配置NLU和Core模型</td></tr><tr><td>credentials.yml</td><td>连接到其他服务的详细信息</td></tr><tr><td>data/nlu.md ‘*’</td><td>你的NLU训练数据</td></tr><tr><td>data/stories.md ‘*’</td><td>你的故事</td></tr><tr><td>domain.yml ‘*’</td><td>你的助手的域</td></tr><tr><td>endpoints.yml</td><td>接到fb messenger等通道的详细信息</td></tr><tr><td>models/.tar.gz</td><td>你的初始模型</td></tr></tbody></table></div><p>最重要的文件用“<code>*</code>”标记。你将在本教程中了解所有这些文件。</p><h2 id="2-构建NLU样本"><a href="#2-构建NLU样本" class="headerlink" title="2.构建NLU样本"></a>2.构建NLU样本</h2><p>Rasa助手的第一个部分是NLU模型。NLU代表自然语言理解，这意味着将用户消息转换为结构化数据。要使用Rasa做到这一点，你需要提供一些训练示例，展示Rasa应该如何理解用户消息，然后通过展示的这些示例来训练模型。</p><p>查看并添加NLU模型的训练样本数据（即intent）：<code>data/nlu.md</code>。以添加【点外卖】为例：</p><ul><li>确定 意图 名称：request_takeway；</li><li>确定需要的 槽位：refreshments、tea、address、phone_number；</li><li>确定 用户 query；</li><li>确定 当 某个 槽位 为空时，Bot 通过回复 什么获取 对应 槽位值</li></ul><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-bullet">-</span> <span class="hljs-string">intent: request_takeway</span><br>  <span class="hljs-attribute">examples</span><span class="hljs-punctuation">:</span> <span class="hljs-string">|</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">点份外卖</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">点一份[肠粉](refreshments)，一杯[茉莉花茶](tea)，送到[南山区](address)，电话号码为[13025240602](phone_number)</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">麻烦送一份[蛋糕](refreshments)到[南山区](address)</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">...</span><br><br><span class="hljs-bullet">-</span> <span class="hljs-string">intent: request_weather</span><br>  <span class="hljs-attribute">examples</span><span class="hljs-punctuation">:</span> <span class="hljs-string">|</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">天气</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">查询天气</span><br></code></pre></td></tr></table></figure><p>Rasa的工作是预测用户向助手发送新消息时的正确意图。你可以在<a href="http://rasachatbot.com/2_Rasa_Tutorial/">训练数据格式</a>中找到数据格式的所有细节。</p><h2 id="3-构建Core样本"><a href="#3-构建Core样本" class="headerlink" title="3.构建Core样本"></a>3.构建Core样本</h2><p>在这个阶段，你将教会你的助手如何回复你的信息。这称为对话管理(dialogue management)，由你的Core模型来处理。</p><p>Core模型以训练“故事”的形式从真实的会话数据中学习。故事是用户和助手之间的真实对话。</p><p>在 NLU 识别出 query 的意图和槽位之后，需要确定其所触发的对应事件！</p><p>下面是一个简单对话的例子 <code>stories.md</code></p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">stories</span><span class="hljs-punctuation">:</span><br><span class="hljs-punctuation"></span><br><span class="hljs-bullet">-</span> <span class="hljs-string">story: greet</span><br>  <span class="hljs-attribute">steps</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">intent: greet</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">action: utter_greet</span><br></code></pre></td></tr></table></figure><h2 id="4-定义域"><a href="#4-定义域" class="headerlink" title="4.定义域"></a>4.定义域</h2><p>领域文件：<code>domain.yml</code></p><p>域相当于AI助手的大脑，定义了助手所处的环境:它应该期望得到什么用户输入、它应该能够预测什么操作、如何响应以及存储什么信息。</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">intents</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">greet</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">request_weather</span><br><br><span class="hljs-attribute">slots</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-attribute">date-time</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-attribute">type</span><span class="hljs-punctuation">:</span> <span class="hljs-string">unfeaturized</span><br>  <span class="hljs-attribute">address</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-attribute">type</span><span class="hljs-punctuation">:</span> <span class="hljs-string">unfeaturized</span><br><br><span class="hljs-attribute">entities</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">date-time</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">address</span><br><br><span class="hljs-attribute">actions</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">utter_greet</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">action_default_fallback</span><br><br><span class="hljs-attribute">forms</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">weather_form</span><br><br><span class="hljs-attribute">responses</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-attribute">utter_greet</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">text: &quot;Hey! How are you?&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">text: &quot;您好！请问我可以帮到您吗？&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">text: &quot;您好！很高兴为您服务。请说出您要查询的功能？&quot;</span><br>  <br><span class="hljs-attribute">session_config</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-attribute">session_expiration_time</span><span class="hljs-punctuation">:</span> <span class="hljs-string">60</span><br>  <span class="hljs-attribute">carry_over_slots_to_new_session</span><span class="hljs-punctuation">:</span> <span class="hljs-string">true</span><br></code></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>解释说明</th><th></th></tr></thead><tbody><tr><td>intents</td><td>你希望用户说的话</td></tr><tr><td>actions</td><td>你的助手能做的和能说的</td></tr><tr><td>templates</td><td>你的助手可以说的东西的模板字符串</td></tr></tbody></table></div><ul><li><p>Rasa Core 工作机制：</p><p>在对话的每个步骤中选择正确的操作来执行。在本例中，我们的操作只是向用户发送一条消息。这些简单的话语操作是从域中以utter_开头的操作。助手将根据templates部分中的模板返回一条消息。</p></li></ul><h2 id="5-定义你的模型配置"><a href="#5-定义你的模型配置" class="headerlink" title="5.定义你的模型配置"></a>5.定义你的模型配置</h2><p>配置文件：config.yml</p><p>配置文件定义了模型将使用的NLU和Core组件。你可以在<a href="http://rasachatbot.com/2_Rasa_Tutorial/">这里</a>了解不同的NLU管道。</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs xquery">language: <span class="hljs-string">&quot;zh&quot;</span><br><br>pipeline:<br>-<span class="hljs-built_in"> name</span>: <span class="hljs-string">&quot;MitieNLP&quot;</span><br>  model: <span class="hljs-string">&quot;data/total_word_feature_extractor_zh.dat&quot;</span><br>-<span class="hljs-built_in"> name</span>: <span class="hljs-string">&quot;JiebaTokenizer&quot;</span><br>  dictionary_path: <span class="hljs-string">&quot;data/dict&quot;</span><br>-<span class="hljs-built_in"> name</span>: <span class="hljs-string">&quot;MitieEntityExtractor&quot;</span><br>-<span class="hljs-built_in"> name</span>: <span class="hljs-string">&quot;EntitySynonymMapper&quot;</span><br>-<span class="hljs-built_in"> name</span>: <span class="hljs-string">&quot;RegexFeaturizer&quot;</span><br>-<span class="hljs-built_in"> name</span>: <span class="hljs-string">&quot;MitieFeaturizer&quot;</span><br>-<span class="hljs-built_in"> name</span>: <span class="hljs-string">&quot;SklearnIntentClassifier&quot;</span><br><br>policies:<br>  -<span class="hljs-built_in"> name</span>: TEDPolicy<br>    epochs: <span class="hljs-number">100</span><br>    max_history: <span class="hljs-number">5</span><br>  -<span class="hljs-built_in"> name</span>: FallbackPolicy<br>    fallback_action_name: <span class="hljs-string">&#x27;action_default_fallback&#x27;</span><br>  -<span class="hljs-built_in"> name</span>: MemoizationPolicy<br>    max_history: <span class="hljs-number">5</span><br>  -<span class="hljs-built_in"> name</span>: FormPolicy<br></code></pre></td></tr></table></figure><p><code>language</code>和<code>pipeline</code>键指定应该如何构建NLU模型。<code>policies</code>键定义Core模型将使用的策略。</p><h2 id="6-训练NLU和CORE模型"><a href="#6-训练NLU和CORE模型" class="headerlink" title="6.训练NLU和CORE模型"></a>6.训练NLU和CORE模型</h2><p>每当我们添加新的NLU或Core数据，或更新域或配置时，我们都需要根据示例故事和NLU数据重新训练一个神经网络。为此，运行下面的命令。该命令将调用Rasa Core和NLU训练函数，并将训练后的模型存储到<code>models/</code>目录中。该命令只会在数据或配置发生更改时自动对不同的模型部件进行重新训练。</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">训练模型：rasa train<br>数据增强：rasa train <span class="hljs-comment">--augmentation 20</span><br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python -m rasa train --config configs/config.yml --domain configs/domain.yml --data data/<br></code></pre></td></tr></table></figure><h2 id="7-配置Http和Action"><a href="#7-配置Http和Action" class="headerlink" title="7.配置Http和Action"></a>7.配置Http和Action</h2><p>要在生产中运行AI助手，请在<code>credentials.yml</code>中配置所需的<a href="http://rasachatbot.com/9_Running_Rasa_with_Docker/">消息和语音通道</a>。</p><p>当我们需要通过Http的形式访问Rasa Server时，就需要在该文件中配置<code>rest:</code>。rest通道将为您提供一个rest端点（即Rasa Server），用于向其发送消息，响应该请求将发送回bots消息。根据<a href="https://rasa.com/docs/rasa/user-guide/connectors/your-own-website/#rest-channels">这个文档</a>的说明，当我们请求Rasa Server的URL应为：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">http:<span class="hljs-regexp">//</span>rasaServerIP:rasaServerPort<span class="hljs-regexp">/webhooks/</span>rest/webhook<br></code></pre></td></tr></table></figure><p>如果希望rasa server(<code>注：指rasa core</code>)能够连接到其他web，我们可以再<code>endpoints.yml</code>这个文件中进行配置，比如为了<strong>实现custom action</strong>，我们就需要在该文件中对action server进行配置，又比如我们将nlu模块放到其他的web项目中，就需要在该文件中配置nlu server等等。endpoints.yml文件内容如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta"># 指定action server的url</span><br><span class="hljs-meta"># 当然，也可以将action server单独实现在一个web server项目中</span><br><span class="hljs-meta"># 那么这个url为&quot;https:<span class="hljs-comment">//yourWebIp:yourWebPort/webhook“</span></span><br>action_endpoint:<br> url: <span class="hljs-string">&quot;http://localhost:5055/webhook&quot;</span><br> <br><span class="hljs-meta"># 配置nlu(单独创建一个web项目):</span><br><span class="hljs-meta">#  url: &quot;http:<span class="hljs-comment">//10.0.0.153:5000/&quot;</span></span><br></code></pre></td></tr></table></figure><p>当Rasa NLU识别到用户输入Message的意图后，Rasa Core对话管理模块就会对其作出回应，而完成这个回应的模块就是action。</p><h2 id="8-和你的助手谈谈"><a href="#8-和你的助手谈谈" class="headerlink" title="8.和你的助手谈谈"></a>8.和你的助手谈谈</h2><p>恭喜你! 🚀 你刚刚建立了一个完全由机器学习驱动的助手。 下一步就是尝试一下!</p><p>运行以下命令在本地终端与助手对话：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">rasa shell<br>rasa <span class="hljs-built_in">int</span>eractive<br></code></pre></td></tr></table></figure><p>你也可以用Rasa X来收集更多的对话以提高你的助手: 尝试安装<a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/local-mode">Rasa X</a></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 仅支持和兼容ujson 1.35, 下载ujson 1.35的wheel文件</span><br><span class="hljs-attribute">pip</span> install ujson-<span class="hljs-number">1</span>.<span class="hljs-number">35</span>-cp37-cp37m-win_amd64.whl<br><span class="hljs-comment"># </span><br><span class="hljs-attribute">pip</span> install rasa-x --extra-index-url https://pypi.rasa.com/simple<br></code></pre></td></tr></table></figure><p><a href="https://forum.rasa.com/t/error-failed-building-wheel-for-ujson-while-installing-rasa-x-using-pip/31377/4">https://forum.rasa.com/t/error-failed-building-wheel-for-ujson-while-installing-rasa-x-using-pip/31377/4</a></p><h2 id="9-启动web服务"><a href="#9-启动web服务" class="headerlink" title="9.启动web服务"></a>9.启动web服务</h2><p><strong>（1）启动Rasa服务</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">启动rasa服务</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">该服务实现自然语言理解(NLU)和对话管理(Core)功能</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">注：该服务的--port默认为5005，如果使用默认则可以省略</span><br>python -m rasa run --port 5005 --endpoints configs/endpoints.yml --credentials configs/credentials.yml --debug<br></code></pre></td></tr></table></figure><p><strong>（2）启动Custom Action服务</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Then start action manager</span><br>rasa run actions<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">启动action服务</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">注：该服务的--port默认为5055，如果使用默认则可以省略</span><br>Python -m rasa run actions --port 5055 --actions actions --debug <br></code></pre></td></tr></table></figure><p><strong>（3）创建、启动server.py</strong></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">uvicorn api<span class="hljs-selector-class">.main</span>:app <span class="hljs-attr">--reload</span> <span class="hljs-attr">--port</span> <span class="hljs-number">8089</span><br></code></pre></td></tr></table></figure><p><strong>（4）效果演示</strong></p><p>当<strong>Rasa Server</strong>、<strong>Action Server</strong>和<strong>Server.py</strong>运行后，在浏览器输入测试：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">http:<span class="hljs-regexp">//</span><span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">8088</span>/ai?content=你好<br></code></pre></td></tr></table></figure><h1 id="Rasa命令行-备忘单"><a href="#Rasa命令行-备忘单" class="headerlink" title="Rasa命令行 备忘单"></a>Rasa命令行 备忘单</h1><p>命令行界面(CLI)为你提供易于记忆的常见任务命令。</p><div class="table-container"><table><thead><tr><th>命令</th><th>作用说明</th></tr></thead><tbody><tr><td>rasa init</td><td>使用示例训练数据，操作和配置文件创建新项目</td></tr><tr><td>rasa train</td><td>使用你的NLU数据和故事训练模型，在<code>./model</code>中保存训练的模型</td></tr><tr><td>rasa interactive</td><td>启动交互式学习会话，通过聊天创建新的训练数据</td></tr><tr><td>rasa shell</td><td>加载已训练的模型，并让你在命令行上与助手交谈</td></tr><tr><td>rasa run</td><td>使用已训练的的模型启动Rasa服务。有关详细信息，请参阅<a href="http://rasachatbot.com/3_Command_Line_Interface/">运行服务</a>文档</td></tr><tr><td>rasa run actions</td><td>使用Rasa SDK启动操作服务</td></tr><tr><td>rasa visualize</td><td>可视化故事</td></tr><tr><td>rasa data validate</td><td>验证域文件，NLU数据或故事数据中是否存在任何错误</td></tr><tr><td>rasa test</td><td>使用你的测试NLU数据和故事测试已训练的Rasa模型</td></tr><tr><td>rasa data split nlu</td><td>根据指定的百分比执行NLU数据的拆分</td></tr><tr><td>rasa data convert nlu</td><td>在不同格式之间转换NLU训练数据</td></tr><tr><td>rasa x</td><td>在本地启动Rasa X</td></tr><tr><td>rasa -h</td><td>显示所有可用命令</td></tr></tbody></table></div><p>具体介绍，可以查看 <a href="http://rasachatbot.com/3_Command_Line_Interface/">Rasa 命令行界面</a></p><h1 id="Rasa架构"><a href="#Rasa架构" class="headerlink" title="Rasa架构"></a>Rasa架构</h1><p>此图显示了使用Rasa构建的助手如何响应消息的基本步骤：</p><p><img src="https://img-blog.csdnimg.cn/20200215170601829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZHJFeHBlcnQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li>用户输入Message，送入Rasa NLU</li><li>NLU识别Message中的”意图(intent)“和提取所有”实体”(entity)数据</li><li>Tracker用于跟踪对话状态，输出Embedding<br>用户意图的Embedding<br>上一步系统动作的Embedding<br>实体的Embedding</li><li>Policy记录Tracker对象的当前状态，并选择执行相应的action</li><li>Tracker记录系统行为action，下一次提供给Policy使用</li><li>返回消息给用户</li></ol><h1 id="在Docker上运行Rasa"><a href="#在Docker上运行Rasa" class="headerlink" title="在Docker上运行Rasa"></a>在Docker上运行Rasa</h1><p><a href="http://rasachatbot.com/9_Running_Rasa_with_Docker/">http://rasachatbot.com/9_Running_Rasa_with_Docker/</a></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>Rasa中文聊天机器人开发指南</p><p><a href="https://jiangdg.blog.csdn.net/article/details/104328946">https://jiangdg.blog.csdn.net/article/details/104328946</a></p><p><a href="https://github.com/jiangdongguo/ChitChatAssistant">RASA中文聊天机器人Github地址：ChitChatAssistant</a></p><p><a href="http://rasachatbot.com/">Rasa 聊天机器人中文官方文档|磐创AI</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Rasa</tag>
      
      <tag>聊天机器人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A/B 测试（A/B test）</title>
    <link href="/2020/12/09/2020-12-09-AB%20%E6%B5%8B%E8%AF%95%EF%BC%88AB%20test%EF%BC%89/"/>
    <url>/2020/12/09/2020-12-09-AB%20%E6%B5%8B%E8%AF%95%EF%BC%88AB%20test%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="A-B-测试（A-B-test）"><a href="#A-B-测试（A-B-test）" class="headerlink" title="A/B 测试（A/B test）"></a><a href="https://www.cnblogs.com/HuZihu/p/11178068.html">A/B 测试（A/B test）</a></h1><ul><li>什么是A/B测试</li><li>A/B测试的本质</li><li>A/B测试的两种做法</li><li>A/B测试的步骤</li><li>基于hash的AB测试</li><li><p>其他常见问题</p><span id="more"></span></li></ul><h3 id="什么是A-B测试？"><a href="#什么是A-B测试？" class="headerlink" title="什么是A/B测试？"></a>什么是A/B测试？</h3><p>A/B 测试是一种产品优化的方法，为同一个优化目标制定两个方案（比如两个页面），让一部分用户使用A 方案（称为控制组或对照组），同时另一部分用户使用 B 方案（称为变化组或试验组），统计并对比不同方案的转化率、点击量、留存率等指标，以判断不同方案的优劣并进行决策。</p><h3 id="A-B测试的本质："><a href="#A-B测试的本质：" class="headerlink" title="A/B测试的本质："></a>A/B测试的本质：</h3><p>A/B测试中是用对照版本和试验版本这两个样本的数据来对两个总体是否存在差异进行检验，所以其本质是使用假设检验中的独立样本t检验 。</p><p>零假设为：试验版本的总体参数与对照版本的总体参数无显著差异。</p><p>备择假设为：试验版本的总体参数与对照版本的总体参数有显著差异。</p><h3 id="A-B测试的两种做法："><a href="#A-B测试的两种做法：" class="headerlink" title="A/B测试的两种做法："></a>A/B测试的两种做法：</h3><p>1，连续测试（Consecutive Testing）：对所有用户第一阶段投放A版本，第二阶段投放B版本。</p><p>优点是容易部署和跟踪，不需要分流用户；缺点是结果没有同步测试那么准确，因为随着时间的推移，在测试过程中可能发生很多不可控的变化。</p><p>2，同步测试（Synchronous Testing）：在同一时间段，将用户分流到不同方案，一部分用户使用A版本，一部分用户使用B版本。</p><p>优点结果比连续测试准确；缺点是实现比较复杂，需要选择有效的试验组，制定分流方案。</p><p>一般来说，A/B测试都是采用同步测试（Synchronous Testing）的方法。</p><h3 id="A-B测试的步骤："><a href="#A-B测试的步骤：" class="headerlink" title="A/B测试的步骤："></a>A/B测试的步骤：</h3><p>1，通过数据分析，找到现有产品中可能存在的问题，针对性地提出产品优化的方案并进行假设。例子：“假设把注册流程中的图片校验码方式，改成短信校验码的方式，注册转化率可能会提升”。</p><p>2，确立优化目标和比较指标。设立可量化的、可以落实到某一个具体功能点的、可实施的小目标。比如：“通过优化注册流程，将注册转化率提升20%”。</p><p>3，设计优化版本并完成开发。</p><p>4，确定测试时长。</p><p>5，确定分流方案（每个测试版本的分流比例）。</p><p>6，按照分流比例开放线上流量进行测试。</p><p>7，收集实验数据进行有效性判断和效果判断。</p><p>8，根据试验结果有以下几种可能：①发布新版本；②调整分流比例继续测试；③在未达成目标效果的情况下继续优化迭代方案，重新开发并上线试验。</p><p><strong>鹅厂实践</strong></p><p><img src="https://ningshixian.github.io/resources/images/abtest.png" alt=""></p><h3 id="基于hash的AB测试"><a href="#基于hash的AB测试" class="headerlink" title="基于hash的AB测试"></a>基于hash的AB测试</h3><p>在AB测试中需要将用户随机的分成两组，通过对每个用户唯一id<strong>做hash运算，并对hash值对2取模</strong>，便可以将用户分成0，1两组（分桶测试便是hahs对n取模）。</p><p>每组采用不同的策略，并定义一个策略Id，跟随不同的策略埋点，在最后回收策略Id，最后通过策略id分析ab两种策略的好坏。</p><h3 id="其他常见问题："><a href="#其他常见问题：" class="headerlink" title="其他常见问题："></a>其他常见问题：</h3><p>1，测试时长应该设定多久？</p><p>测试的时长不宜过短。用户进入到新方案中，很可能因为好奇而表现得更加活跃，但随着时间的推移，逐渐趋于冷静，数据表现回到本该有的水平，如果实验观察期设置的过早，则容易得出错误的结论。适应期的长短通常以足量用户参与试验后的2到3天为宜。适应期过后的试验时间长短除了需要考察样本量外，还需要参考用户的行为周期，譬如说电商用户的购买行为有较强的周期规律，周末的购买量与工作日会有显著差异，这时测试的周期需要能够覆盖一个完整的周期，也就是应该大于1周。</p><p>但是测试时间也不宜太长，因为A/B测试是对线上多个版本的测试，这也就意味着线上系统需要同时维护多个可用的版本，长时间的A/B测试无疑加大了系统的复杂性。</p><p>2，应该怎样对用户进行分流？</p><p>分流也就是抽样，应该保证同时性、同质性、唯一性、稳定性。</p><p>①同时性：分流应该是同时进行的。</p><p>②同质性：分出的用户群在各维度的特征都应相似。可以基于用户的设备特征（例如手机机型、操作系统版本号、手机语言等）和用户的其他标签（例如性别、年龄、新老用户、会员等级等）进行分群，每一个A/B测试都可以选定特定的用户群进行试验。</p><p>③唯一性：即要求用户不被重复计入测试。</p><p>④稳定性：每次用户都应被分到相同的实验版本，这样可以保证用户体验的一致性，保证用户能够在适应新版本的情况下有稳定的表现。</p><p>3，什么是A/A测试？</p><p>A/A测试将分给原始版本的流量再次划分，分出的两组流量分别给两个相同的原始版本进行测试。A/A测试用来评估两个实验组是否是处于相同的水平，是为了测试埋点、分流、实验统计的正确性，增加A/B测试的结论可信度。如果AA实验的结果不存在显著差异，那么可以认为实验结果是有效的，进而可以对新老版本的实验结果进行进一步的判断。</p><p>4，A/B测试只能同时测试2个方案吗？</p><p>A/B测试不是只能测试A方案和B方案，实际上一个测试可以包含A/B/C/D/E/……多个版本，但是要保证是单变量的测试，比如按钮的颜色—-赤/橙/黄/绿/青/蓝/紫，那么这七个方案可以同时做A/B测试，但如果某方案在旁边新增了另一个按钮，即便实验结果产生了显著差异，我们也无法判断这种差异的成因究竟是什么。</p><p>5，为什么要A/B测试</p><ul><li>产品优化<strong>依靠经验主义</strong>，不能保证新的产品版本一定会有业绩提升</li><li>重大产品功能<strong>很难决策</strong>，不确定哪个方案效果最优</li><li><strong>后验成本高</strong>，如果改版失败，业绩损失无法挽回</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://zhuanlan.zhihu.com/p/68019926">https://zhuanlan.zhihu.com/p/68019926</a></p><p><a href="http://www.appadhoc.com/blog/ab-test-in-dianrong/">http://www.appadhoc.com/blog/ab-test-in-dianrong/</a></p><p><a href="https://cloud.tencent.com/developer/article/1496302">https://</a><a href="https://cloud.tencent.com/developer/article/1496302">cloud.tencent.com/developer/article/1496302</a></p><p><a href="https://www.zhihu.com/question/20045543">https://www.zhihu.com/question/20045543</a></p><p><a href="https://blog.leapoahead.com/2015/08/27/introduction-to-ab-testing/">https://blog.leapoahead.com/2015/08/27/introduction-to-ab-testing/</a></p><p><a href="https://www.cnblogs.com/typeck/p/11808187.html">https://www.cnblogs.com/typeck/p/11808187.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>AB Test</tag>
      
      <tag>AB测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典算法重温 Aho-Corasick automaton（转载）</title>
    <link href="/2020/12/01/2020-12-01-%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E9%87%8D%E6%B8%A9AC%E8%87%AA%E5%8A%A8%E6%9C%BA%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
    <url>/2020/12/01/2020-12-01-%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E9%87%8D%E6%B8%A9AC%E8%87%AA%E5%8A%A8%E6%9C%BA%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://carlos9310.github.io/2020/01/01/Aho-Corasick/">https://carlos9310.github.io/2020/01/01/Aho-Corasick/</a></p><p>Aho–Corasick automaton 算法(简称AC自动机算法)是由Alfred V. Aho和Margaret J.Corasick于1975年在贝尔实验室发明的多<strong>模</strong>(<strong>模式串</strong>)匹配算法。即<strong>给定多个模式串和一个文本串，求解多模串在文本串中存在的情况(包括是否存在、存在几次、存在于哪些位置等)。</strong></p> <span id="more"></span><h2 id="单模匹配"><a href="#单模匹配" class="headerlink" title="单模匹配"></a>单模匹配</h2><p>在介绍AC自动机这种多模匹配算法前，先回顾下单模匹配问题，即给定一个文本串和一个模式串，求解模式串在文本串中的匹配情况。</p><h3 id="朴素匹配"><a href="#朴素匹配" class="headerlink" title="朴素匹配"></a>朴素匹配</h3><p>最直接的想法是暴力(Brute Force)匹配，即将文本串的第一个字符与模式串的第一个字符进行匹配，若相等则继续比较文本串的第二个字符与模式串的第二个字符。若不等，则比较目标串的第二个字符与模式串的第一个字符，依次比较下去，直到得到最后的匹配结果。相关代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 每次匹配失败时，文本串T的指针回退到开始匹配位置的下一个位置，模式串P的指针回退到初始位置，然后重新开始匹配</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">bfMatch</span>(<span class="hljs-params">T,P</span>):<br>    tLen,pLen = <span class="hljs-built_in">len</span>(T),<span class="hljs-built_in">len</span>(P)<br>    indexs = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tLen - pLen + <span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(pLen):<br>            <span class="hljs-keyword">if</span> T[i+j] == P[j]:<br>                <span class="hljs-keyword">if</span> j == pLen - <span class="hljs-number">1</span>:<br>                    indexs.append(i)<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> indexs<br>T=<span class="hljs-string">&#x27;ushershe&#x27;</span> <br>P=<span class="hljs-string">&#x27;he&#x27;</span> <br><span class="hljs-built_in">print</span>(bfMatch(T,P))<br></code></pre></td></tr></table></figure><p>上述匹配过程存在重复匹配，KMP算法优化了上述匹配过程。<strong>在匹配失败时，文本串的指针不需要回退。</strong></p><h3 id="KMP"><a href="#KMP" class="headerlink" title="KMP"></a>KMP</h3><p>与朴素匹配不同，KMP算法在匹配到某个字符失败时，文本串的匹配指针不会回退，模式串则根据<strong>部分匹配表(也叫next数组)</strong> 向右滑动一定距离后继续与上次在文本串中不匹配的位置进行匹配，若仍不匹配，则继续根据部分匹配表向右滑动模式串，重复上述不匹配–滑动的过程，当匹配指针指到模式串的初始位置依然不匹配，则模式串向右滑动一位，文本串的匹配指针向前移动一位；若匹配，则继续匹配其他位置的字符。当匹配指针连续匹配的字符数与模式串的长度相等，则匹配完成。形象图解可参考<a href="http://www.ruanyifeng.com/blog/2013/05/Knuth–Morris–Pratt_algorithm.html">字符串匹配的KMP算法</a>。相应代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 匹配过程中，模式串P中每个待匹配的字符与文本串T中的字符对齐，即匹配指针相同，但两个字符串的下标不同</span><br><span class="hljs-comment"># 部分匹配表是针对模式串构建的</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">kmpMatch</span>(<span class="hljs-params">T,P</span>):<br>    tLen,pLen = <span class="hljs-built_in">len</span>(T),<span class="hljs-built_in">len</span>(P)<br>    Next = partialMatchTable(P)<br>    q = <span class="hljs-number">0</span> <span class="hljs-comment"># 模式串P的下标</span><br>    indexs = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tLen):<br>        <span class="hljs-keyword">while</span> q &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> P[q] != T[i]:<br>            q = Next[q-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> P[q] == T[i]:<br>            q += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> q == pLen:<br>            indexs.append(i-pLen+<span class="hljs-number">1</span>)<br>            q=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">return</span> indexs<br></code></pre></td></tr></table></figure><p>部分匹配表中的数值是指<strong>某个子串的前缀和后缀的最长共有元素的长度。</strong> 其有两种构建方式。一种是手动法，详见<a href="http://www.ruanyifeng.com/blog/2013/05/Knuth–Morris–Pratt_algorithm.html">字符串匹配的KMP算法</a>。相关代码如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs vim"># 手动法求部分匹配表<br>def partialMatchTable(<span class="hljs-keyword">p</span>): # 也叫<span class="hljs-keyword">next</span>数组<br>    prefix,suffix = <span class="hljs-keyword">set</span>(),<span class="hljs-keyword">set</span>()<br>    pLen = <span class="hljs-built_in">len</span>(<span class="hljs-keyword">p</span>)<br>    <span class="hljs-keyword">Next</span> = [<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,pLen):<br>        prefix.<span class="hljs-built_in">add</span>(<span class="hljs-keyword">p</span>[:i]) <br>        suffix = &#123;<span class="hljs-keyword">p</span>[<span class="hljs-keyword">j</span>:i+<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> <span class="hljs-keyword">j</span> in <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,i+<span class="hljs-number">1</span>)&#125;<br>        common_len = <span class="hljs-built_in">len</span>((prefix &amp; suffix <span class="hljs-built_in">or</span> &#123;<span class="hljs-string">&#x27;&#x27;</span>&#125;).<span class="hljs-keyword">pop</span>())<br>#         <span class="hljs-keyword">print</span>(<span class="hljs-keyword">p</span>[:i+<span class="hljs-number">1</span>],prefix,suffix,common_len)<br>        <span class="hljs-keyword">Next</span>.<span class="hljs-keyword">append</span>(common_len) <br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">Next</span><br><span class="hljs-keyword">p</span>=<span class="hljs-string">&#x27;ababaca&#x27;</span><br>partialMatchTable(<span class="hljs-keyword">p</span>)<br></code></pre></td></tr></table></figure><p>另一种是程序法，模式串针对自己的前后缀的匹配。详见<a href="https://blog.csdn.net/qingdujun/article/details/85281936">KMP算法：线性时间O(n)字符串匹配算法</a>中的部分匹配表部分。相关代码如下：</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs autoit"><span class="hljs-meta"># 由模式串生成的部分匹配表，其存储的是前缀尾部 的位置。有前缀尾部 = next(后缀尾部)，</span><br><span class="hljs-meta"># 当后缀之后q不匹配时，通过查询部分匹配表，确定前缀尾部的位置k,然后将前缀滑动过来与后缀对齐，继续后续匹配工作</span><br><span class="hljs-meta"># 程序法计算部分匹配表 </span><br>def partialMatchTable(p):<br>    pLen = len(p)<br>    <span class="hljs-keyword">Next</span> = [<span class="hljs-number">0</span>]<br>    k = <span class="hljs-number">0</span> <span class="hljs-meta"># 模式串nP的下标</span><br>    <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,pLen): <span class="hljs-meta"># 文本串nT的下标</span><br>        <span class="hljs-keyword">while</span> k &gt; <span class="hljs-number">0</span> <span class="hljs-literal">and</span> p[k] != p[q]:<br>            k = <span class="hljs-keyword">Next</span>[k<span class="hljs-number">-1</span>]<br>        <span class="hljs-keyword">if</span> p[k] == p[q]:<br>            k += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">Next</span>.append(k)<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">Next</span><br>p=<span class="hljs-string">&#x27;ababaca&#x27;</span><br>partialMatchTable(p)<br></code></pre></td></tr></table></figure><h2 id="Trie"><a href="#Trie" class="headerlink" title="Trie"></a>Trie</h2><p>Trie又叫前缀树或字典树，是一种多叉树结构。Trie这个术语来源于re<strong>trie</strong>val(检索)，其是一种用于快速检索的数据结构。其核心思想是利用字符串的<strong>公共前缀</strong>最大限度地减少不必要的字符串比较，提高查询(检索)效率，缺点是内存消耗大。</p><p>Trie树的基本性质：</p><ul><li>根节点不包含字符，除根节点外的每一个子节点都包含一个字符</li><li>从根节点到某一个节点，路径上经过的字符连起来为该节点对应的字符串</li><li>每个节点的所有子节点包含的字符互不相同</li></ul><p>应用场景</p><ul><li>前缀匹配(自动补全)：返回所有<strong>前缀相同</strong>的字符串</li><li>词频统计：将每个节点是否构成单词的标志位改成构成单词的数量</li><li>字典序排序：将所有待排序集合逐个加入到Trie中，然后按照先序遍历输出所有值</li><li>分词</li><li>检索</li></ul><h2 id="多模匹配–AC自动机"><a href="#多模匹配–AC自动机" class="headerlink" title="多模匹配–AC自动机"></a>多模匹配–AC自动机</h2><p>有了上述KMP和Trie的背景知识后，对AC自动机会有更加清晰的认识。</p><p>AC自动机<strong>首先将多模串构建(与Trie树的构建类似)为确定有限状态自动机(DFA)，然后按照文本串中的字符顺序依次接收字符，并发生状态转移。</strong>【状态中缓存了如下三种情况下的跳转与输出：1.按字符转移成功，但不是模式串的结尾。即成功转移到另一个状态，对应success/goto；2.按字符转移成功，是模式串的结尾。即命中一个模式串，对应emits/output；<strong>3.按字符转移失败，此时跳转到一个特定的节点，对应failure。从根节点到这个特定的节点的路径恰好是失败前的文本的一部分，类似KMP算法中利用部分匹配表来加速模式串的滑动从而减少重复匹配】</strong></p><p>上述匹配过程只需扫描一遍文本串，其时间复杂度为O(n),与模式串的数量和长度无关。<strong>AC自动机可简单看成是在Trie树上通过KMP来实现多模串的匹配。其中Trie树负责状态转移，KMP负责减少重复匹配。</strong></p><p><strong>补充：AC自动机中fail路径的构建</strong></p><hr><p>AC自动机的构建虽然与Trie树的构建类似，但其fail路径(<strong>本质是一种回溯，避免重复匹配</strong>)是AC自动机中特有的。具体构建(<strong>从离根节点由近及远的节点逐步构建</strong>)逻辑为(<strong>每个节点都有一条发出的fail路径</strong>)：</p><ul><li>1.如果自己是根节点，则指向自己</li><li>2.如果自己的父节点是根节点，则指向根节点</li><li>3.<strong>找到自己父节点fail路径指向的节点，如果这个节点可以正常接收自己的输入字符，那么就指向这个节点接收自己输入字符后所指向的那个节点</strong></li><li>4.如果自己父节点fail路径指向的节点不满足，就按第3步的判断，检查自己父节点的父节点的fail路径指向的节点</li><li>5.一直父节点、父节点、父节点这样的回溯，直到根结点还没找到就指向根节点</li></ul><p>以经典的ushers为例，模式串是he、she、his、hers，文本为“ushers”。构建的自动机如图：</p><p><img src="https://carlos9310.github.io/assets/images/nlp/algo/ac_auto.png" alt="png"></p><p>其中实线部分是一颗Trie树，虚线部分为各节点的fail路径。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www.ruanyifeng.com/blog/2013/05/Knuth–Morris–Pratt_algorithm.html">字符串匹配的KMP算法</a></li><li><a href="https://blog.csdn.net/qingdujun/article/details/85281936">KMP算法：线性时间O(n)字符串匹配算法</a></li><li><a href="https://benarvintec.com/2018/11/26/算法学习之Aho-Corasick/">算法学习之Aho-Corasick</a></li><li><a href="https://blog.csdn.net/lemon_tree12138/article/details/49335051">深入理解Aho-Corasick自动机算法</a></li><li><a href="https://www.hankcs.com/program/algorithm/implementation-and-analysis-of-aho-corasick-algorithm-in-java.html">Aho-Corasick算法的Java实现与分析</a></li><li><a href="https://www.cnblogs.com/wenzhixin/p/9448045.html">Aho-Corasick automaton（AC自动机）解析及其在算法竞赛中的典型应用举例</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Aho-Corasick</tag>
      
      <tag>AC自动机</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>超参数优化框架Optuna</title>
    <link href="/2020/11/13/2020-11-13-%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E6%A1%86%E6%9E%B6Optuna/"/>
    <url>/2020/11/13/2020-11-13-%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E6%A1%86%E6%9E%B6Optuna/</url>
    
    <content type="html"><![CDATA[<h1 id="超参数优化框架Optuna"><a href="#超参数优化框架Optuna" class="headerlink" title="超参数优化框架Optuna"></a>超参数优化框架Optuna</h1><p>本文介绍超参数优化框架Optuna，Optuna是一种自动超参优化框架，专为机器学习而设计。它具有命令式，按运行定义的用户API。使用Optuna编写的代码具有很高的模块性，Optuna的用户可以动态构建超参数的搜索空间。</p><p>它目前支持的部分库有(全部请参见<a href="https://optuna.org/">官网</a>)：</p><ul><li>XGBoost</li><li>LightGBM</li><li>Sklearn</li><li>Keras</li><li>TensorFlow</li><li>tf.keras</li><li>MXNet</li><li>PyTorch</li><li><p>FastAI</p><span id="more"></span></li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> optuna<br></code></pre></td></tr></table></figure><h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h2><p>添加两个函数<code>define_model</code>和<code>objective</code>就能将Optuna框架零感地插入到我们现有的代码中去。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> optuna<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim, nn<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string"># 这个定义的model只是个示例，实际并没什么用</span><br><span class="hljs-string">class model(nn.Module):</span><br><span class="hljs-string"></span><br><span class="hljs-string">    def __init__(self, input_size, output_size, hidden_size=200, dropout=p):</span><br><span class="hljs-string">        super(model, self).__init__()</span><br><span class="hljs-string">        self.embedding = nn.Embedding(num_embeddings=input_size,</span><br><span class="hljs-string">                                      embedding_dim=hidden_size)</span><br><span class="hljs-string">        self.linear = nn.Linear(in_features=hidden_size, out_features=output_size)</span><br><span class="hljs-string">        self.dropout = nn.Dropout(dropout)</span><br><span class="hljs-string">    def forward(self, x):</span><br><span class="hljs-string">    x = self.dropout(self.embedding(x))</span><br><span class="hljs-string">        outputs = self.linear(x)</span><br><span class="hljs-string">        return outputs</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">define_model</span>(<span class="hljs-params">trial</span>):<br>    <span class="hljs-comment"># 在100到200之间搜索hidden_size</span><br>    hidden_size = trial.suggest_int(<span class="hljs-string">&#x27;hidden_size&#x27;</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span>)<br>    <span class="hljs-comment"># 在0.2到0.5之间搜索dropout rate</span><br>    p = trial.suggest_uniform(<span class="hljs-string">&#x27;dropout&#x27;</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>)<br>    <span class="hljs-comment"># 假设vocab_size, output_size都已经定义了</span><br>    m = model(input_size=vocab_size, output_size=output_size, <br>              hidden_size=hiddensize, dropout=p)<br>    <span class="hljs-keyword">return</span> m<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">objective</span>(<span class="hljs-params">trial</span>):<br>    <span class="hljs-comment"># 尝试不同的optimizer</span><br>    optimizer_name = trial.suggest_categorical(<span class="hljs-string">&#x27;optimizer&#x27;</span>, <br>                                               [<span class="hljs-string">&#x27;Adam&#x27;</span>, <span class="hljs-string">&#x27;RMSprop&#x27;</span>, <span class="hljs-string">&#x27;SGD&#x27;</span>])<br>    <span class="hljs-comment"># 搜索学习率</span><br>    lr = trial.suggest_uniform(<span class="hljs-string">&#x27;lr&#x27;</span>, <span class="hljs-number">1e-5</span>, <span class="hljs-number">1e-1</span>)<br>    m = define_model(trial)<br>    optimizer = <span class="hljs-built_in">getattr</span>(optim, optimizer_name)(m.parameters(), lr=lr)<br>    <span class="hljs-comment"># 这里省略了run函数，内部应该将数据喂给model训练，训练完成后在验证集上测试，计算并返回acc</span><br>    acc = run(m, optimizer=optimizer)<br>    <span class="hljs-keyword">return</span> acc<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 创建一个学习实例，因为objective返回的评价指标是acc，因此目标是最大化，如果是loss就该是minimize</span><br>    study = optuna.create_study(direction=<span class="hljs-string">&#x27;maximize&#x27;</span>)<br>    <span class="hljs-comment"># n_trials代表搜索100种，n_jobs是并行搜索的个数，-1代表使用所有的cpu核心</span><br>    study.optimize(objective, n_trials=<span class="hljs-number">100</span>, n_jobs=-<span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Number of finished trials: &#x27;</span>, <span class="hljs-built_in">len</span>(study.trials))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Best trial:&#x27;</span>)<br>    trial = study.best_trial<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;  Value: &#x27;</span>, trial.value)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;  Params: &#x27;</span>)<br>    <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> trial.params.items():<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;    &#123;&#125;: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(key, value))<br></code></pre></td></tr></table></figure><p>这里只是简单地展示了一下如何在pytorch中使用optuna，以此来解放调参师的双手，更详细的使用请参见官方网站。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://tianhongzxy.top/2020/02/06/optuna/">https://tianhongzxy.top/2020/02/06/optuna/</a></p><p><a href="https://optuna.readthedocs.io/zh_CN/latest/tutorial/10_key_features/002_configurations.html">https://optuna.readthedocs.io/zh_CN/latest/tutorial/10_key_features/002_configurations.html</a></p><p><a href="https://github.com/optuna/optuna/tree/master/examples">https://github.com/optuna/optuna/tree/master/examples</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>Optuna</tag>
      
      <tag>超参数优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FAISS向量化检索方法</title>
    <link href="/2020/11/13/2020-11-16-FAISS%E5%90%91%E9%87%8F%E5%8C%96%E6%A3%80%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    <url>/2020/11/13/2020-11-16-FAISS%E5%90%91%E9%87%8F%E5%8C%96%E6%A3%80%E7%B4%A2%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p><img src="https://ningshixian.github.io/resources/images/FAISS+SBERT.png" alt=""></p><ul><li>语义搜索介绍</li><li>经典的向量化检索方法</li><li>FAQ检索方法</li><li>SBERT +Faiss 语义搜索引擎</li><li><p>FAQ Web服务</p><span id="more"></span></li></ul><h1 id="语义搜索介绍"><a href="#语义搜索介绍" class="headerlink" title="语义搜索介绍"></a>语义搜索介绍</h1><p>语义搜索是一种信息检索系统，其重点是句子的含义，而不是常规的关键字匹配。基于关键词的搜索引擎通常会遇到以下问题:</p><ul><li><p>复杂查询或具有双重含义的单词。</p></li><li><p>长查询，如论文摘要或博客中的一段。</p></li><li><p>不熟悉某个领域术语的用户或想要进行探索性搜索的用户。</p></li></ul><p>基于向量(也称为语义)的搜索引擎通过使用最先进的语言模型找到文本查询的数字表示，在高维向量空间中对它们进行索引，并度量查询向量与索引文档的相似程度，从而解决了这些缺陷。</p><p>在本文中，我将讨论如何使用SOTA句子嵌入（<a href="https://arxiv.org/pdf/1908.10084.pdf">句子转换器</a>）和<a href="https://github.com/facebookresearch/faiss">FAISS</a>来实现最小语义搜索引擎，实现从海量文章中求topk相似文章。</p><h1 id="经典的向量化检索方法"><a href="#经典的向量化检索方法" class="headerlink" title="经典的向量化检索方法"></a>经典的向量化检索方法</h1><p>向量相似度检索，即根据一个向量Q从海量的向量库中寻找TopK个与Q最相似或者距离最近的向量，其在工业中有着广泛的应用场景，比如图像检索、文本语义检索以及推荐系统中基于User与Item的Embedding向量召回等。在生产环境中，被查找的向量库往往是海量，甚至超过了内存的限制，而且面临着高并发与低延迟的需求。当前涌现出了一系列高质量的向量化工具。</p><p><strong>1、Gensim</strong></p><p><strong>Gensim</strong>是 <strong>Radim Řehůřek开源</strong>的一个主题建模、文本向量化计算工具库，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达，支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，提供了针对向量的多种操作，如相似度计算，信息检索等一些常用任务的API接口，如找到与一个词相似度最高的词语集合，比较两个词语之间的相似度值。</p><p><strong>地址：<a href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a></strong></p><p>Gensim的提供了wordvec模块提供了cbow和skipgram两种词向量训练接口，用户可以通过训练自有语料来得到特定的向量文件。因此，我们一方面可以直接使用该向量文件实现检索操作，也可以预先将预先得到的embedding【如DeepWalk、Node2vec得到的向量，根据TFIDF得到的文本向量，从其他开源渠道下载得到的向量等】按照gensim所规定的格式【一般是文件首行为词表大小、空格、向量维度，第二行至最后一行为每个词、空格、以空格连接的各维度向量】，调用该工具完成加载和使用，<strong>实验表明，</strong>gensim加载模型耗时很长，会将所有的词向量加载进入内存，占用内存很大，most_similar函数耗时较长。</p><p><strong>2、Annoy</strong></p><p>Annoy是<strong>Spotify开源</strong>的一个用于近似最近邻查询的C++/Python工具，在 Spotify 使用它进行音乐推荐。Annoy对内存使用进行了优化，索引可以在硬盘保存或者加载，提供欧式距离，曼哈顿距离，余弦距离，汉明距离，內积距离等距离的度量方法，可以使用 Annoy 对 word2vec 等向量建立索引。不过，Annoy仅支持树结构的索引类型，且不支持批量插入和查询，仅支持一种索引类型，单步查询速度快，另外，annoy中向量的item-id只接受非负数，如果自己的数据不符合要求需要自己维护一份映射。</p><p><strong>地址：<a href="https://github.com/spotify/annoy">https://github.com/spotify/annoy</a></strong></p><p><strong>3、\</strong>FAISS*<em>*</em></p><p>FAISS 是 <strong>Facebook AI 开源</strong>的针对聚类和相似性搜索库，是当前使用较为广泛的一个框架，用 C ++ 编写的，带有 Python / numpy 的完整封装，支持 c++ 与 python 调用。支持多种索引方式以及CPU和GPU计算，Faiss 支持多种向量检索方式，包括内积、欧氏距离等，同时支持精确检索与模糊搜索，并使用 GPU 来获得更高的内存带宽和计算吞吐量。不过，Faiss本身只是一个能够单机运行的支持各种向量检索模型的机器学习算法基础库，不支持分布式实时索引和检索，同时也不支持标量字段的存储和索引等功能。</p><p><strong>地址：<a href="https://github.com/facebookresearch/faiss">https://github.com/facebookresearch/faiss</a></strong></p><p><strong>4、SPTAG</strong></p><p>SPTAG(空间分区树和图)是<strong>微软开源</strong>的BING搜索算法库，作为一种分布式近似最近邻域搜索（ANN）库，可用于大规模矢量搜索场景提供高质量矢量的索引构建，搜索和分布式在线服务。SPTAG内置L2 距离或余弦距离来计算向量之间的相似度，并提供KD-Tree 和相对邻域图（SPTAG-KDT）、以及平衡 k-means 树和相对邻域图（SPTAG-BKT）两种搜索算法。前者在指数构建成本方面能够有效降低成本，后者则在非常高维数据中保持较高的搜索精度。</p><p><strong>地址：<a href="https://github.com/microsoft/SPTAG">https://github.com/microsoft/SPTAG</a></strong></p><p><strong>5、Vearch</strong></p><p>Vearch 是由<strong>京东开源</strong>的一个分布式向量搜索系统，考虑到开发及可扩展性，vearch 中的 Master，Router 和 PS 均采用 GO 语言编写。出于性能考虑，核心的存储检索引擎 gamma 基于 faiss 采用 c++ 语言实现， 提供了快速的向量检索功能，以及类似 Elasticsearch 的 Restful API 可以方便地对数据及表结构进行管理查询等工作。</p><p>此外，为满足实际业务场景需要，Vearch 还提供了算法插件服务模块，通过选择默认的 VGG，Resnet 或自定义算法模型等，能够提供端到端的图像检索，视频流智能监控等业务应用场景的实现。</p><p><strong>地址：<a href="https://github.com/vearch/vearch">https://github.com/vearch/vearch</a></strong></p><p>6、Milvus</p><p>Milvus 是一款<strong>国产开源</strong>的、针对海量特征向量的相似性搜索引擎。Milvus能够很好地应对海量向量数据，它集成了目前在向量相似性计算领域的几个开源库，并针对性做了定制，支持结构化查询、多模查询等业界比较急需的功能，并支持cpu、gpu、arm等多种类型的处理器，能够PC（16GB内存）上实现 1 亿级向量（数据来自SIFT1billion）的搜索。</p><p>在实际实验中发现，与FAISS相比，Milvus多平台通用，mac，windows和linux都是支持的，可以通过docker部署，在平台通用性上好了不少，并且支持Java，c，c++和python等多种编程语言。值得注意的是，Milvus 专门开通了训练营们，对了解向量数据库的操作及各种应用场景做了索引，例如如何进行 Milvus 性能测评，搭建智能问答机器人、推荐系统、以图搜图系统、分子式检索系统。</p><p><strong>地址：<a href="https://milvus.io/">https://milvus.io/</a></strong></p><h1 id="FAQ检索方法"><a href="#FAQ检索方法" class="headerlink" title="FAQ检索方法"></a>FAQ检索方法</h1><h2 id="关键字检索（倒排索引）"><a href="#关键字检索（倒排索引）" class="headerlink" title="关键字检索（倒排索引）"></a>关键字检索（倒排索引）</h2><p>传统召回模块<strong>基于关键字检索</strong></p><ul><li>计算关键字在问题集中的 <a href="https://en.wikipedia.org/wiki/Tf–idf">TF-IDF</a> 以及<a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a> 得分，并建立<strong>倒排索引表</strong></li><li>第三方库 <a href="https://whoosh.readthedocs.io/en/latest/index.html">ElasticSearch</a>，<a href="https://lucene.apache.org/pylucene/">Lucene</a>，<a href="https://whoosh.readthedocs.io/en/latest/index.html">Whoosh</a></li></ul><p>以Elasticsearch为例。Elasticsearch使用标记器将文档分割成标记(即有意义的文本单位)，这些标记映射到数字序列，并用于构建反向索引。</p><ul><li>反向索引: 与检查每个文档是否包含查询词不同，反向索引使我们能够查找一个词并检索包含该词的所有文档列表。</li><li><p>同时，Elasticsearch用一个高维加权向量表示每个索引文档，其中每个不同的索引项是一个维度，它们的值(或权重)是用TF-IDF计算的。</p></li><li><p>在搜索过程中，使用相同的TF-IDF管道将查询转换为向量，文档d对查询q的VSM得分为加权查询向量V(q)和V(d)的余弦相似度。</p></li></ul><h2 id="向量检索（语义召回）"><a href="#向量检索（语义召回）" class="headerlink" title="向量检索（语义召回）"></a>向量检索（语义召回）</h2><p>随着语义表示模型的增强、预训练模型的发展，基于 BERT 向量的<strong>语义检索</strong>得到广泛应用</p><ul><li>对候选问题集合进行向量编码，得到 <strong>corpus 向量矩阵</strong></li><li>当用户输入 query 时，同样进行编码得到 <strong>query 向量表示</strong></li><li>然后进行语义检索（矩阵操作，KNN，FAISS）</li></ul><p>针对小规模 FAQ 问题集直接计算 query 和 corpus 向量矩阵的<strong>余弦相似度</strong>，从而获得 topk 候选问题</p><p><strong>句向量获取解决方案</strong></p><div class="table-container"><table><thead><tr><th>Python Lib</th><th>Framework</th><th>Desc</th><th>Example</th></tr></thead><tbody><tr><td><a href="https://github.com/hanxiao/bert-as-service">bert-as-serivce</a></td><td>TensorFlow</td><td>高并发服务调用，支持 fine-tune，较难拓展其他模型</td><td><a href="https://github.com/hanxiao/bert-as-service#getting-started">getting-started</a></td></tr><tr><td><a href="https://www.sbert.net/index.html">Sentence-Transformers</a></td><td>PyTorch</td><td>接口简单易用，支持各种模型调用，支持 fine-turn（单GPU）</td><td><a href="https://www.sbert.net/docs/quickstart.html#quickstart">using-Sentence-Transformers-model</a> <a href="https://github.com/UKPLab/sentence-transformers/issues/184#issuecomment-607069944">using-Transformers-model</a></td></tr><tr><td>🤗 <a href="https://github.com/huggingface/transformers/">Transformers</a></td><td>PyTorch</td><td>自定义程度高，支持各种模型调用，支持 fine-turn（多GPU）</td><td><a href="https://www.sbert.net/docs/usage/computing_sentence_embeddings.html#sentence-embeddings-with-transformers">sentence-embeddings-with-Transformers</a></td></tr></tbody></table></div><h1 id="SBERT-Faiss-语义搜索引擎"><a href="#SBERT-Faiss-语义搜索引擎" class="headerlink" title="SBERT +Faiss 语义搜索引擎"></a>SBERT +Faiss 语义搜索引擎</h1><p>使用 <code>Sentence transformers</code> 对用户 query 进行向量表示，借助 <code>Faiss</code> 对问题集做相似性召回和打分，最后对 topk 结果进行精排序。</p><h2 id="SBERT是啥"><a href="#SBERT是啥" class="headerlink" title="SBERT是啥"></a>SBERT是啥</h2><p>SBERT（Sentence transformers）是一个框架或者说是一系列的模型，用于将句子或段落表示成密集向量。由于BERT在这些任务中表现不佳，这些模型是 transformer 网络（BERT，RoBERTa等）专门针对语义文本相似性任务进行了微调。以下是STS基准测试中不同模型的性能：</p><p><img src="https://ningshixian.github.io/resources/images/sbert排行榜.jpeg" alt=""></p><p>我们可以看到，Sentence transformer models 很大程度上优于其他模型。</p><p>但是，如果您通过<a href="https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark">带有代码</a>和<a href="https://gluebenchmark.com/leaderboard">GLUE</a>的<a href="https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark">论文</a>查看排行榜，则会看到许多90以上的模型。那为什么我们需要Sentence transformers？</p><p>在这些模型中，语义文本相似性被视为回归任务。这意味着每当我们需要计算两个句子之间的相似性得分时，我们都需要将它们一起传递到模型中，然后模型输出它们之间的数字得分。尽管这对于基准测试非常有效，但对于实际使用案例却无法很好地扩展，这就是原因。</p><ol><li>当您需要搜索超过10k个文档时，您将需要执行10k个单独的推理计算，不可能分别计算嵌入并仅计算余弦相似度。参见作者的<a href="https://github.com/UKPLab/sentence-transformers/issues/405#issuecomment-689397806">解释</a>。</li><li>在两个文档之间共享最大序列长度（模型一次可以获取的单词/令牌的总数），这会导致由于分块而使表示形式被稀释</li></ol><h2 id="Faiss是啥"><a href="#Faiss是啥" class="headerlink" title="Faiss是啥"></a>Faiss是啥</h2><blockquote><p>它可对矢量化数据进行索引并对其进行高效搜索（建立向量索引，使用 k-nearest-neighbor 召回）</p></blockquote><p><a href="https://github.com/facebookresearch/faiss">Faiss</a>是Facebook AI团队开源的针对聚类和相似性搜索的开源库，为稠密向量提供高效相似度搜索服务，支持十亿级别向量的搜索，是目前最为成熟的近似近邻搜索库之一。Faiss提供了多种索引类型如L2距离，向量內积等，详细介绍可参考<a href="https://github.com/facebookresearch/faiss/wiki/Faiss-indexes">Faiss Indexes</a>，我们可以针对不同大小的向量集和聚类算法选择合适的索引类型。</p><p>Faiss 可以根据以下 factors 提供不同的 indexes 方式：</p><ul><li>search time</li><li>search quality</li><li>memory used per index vector</li><li>training time</li><li>need for external data for unsupervised training</li></ul><p>因此，选择正确的 indexes 将是这些因素之间的权衡。</p><h2 id="详细步骤（附代码）"><a href="#详细步骤（附代码）" class="headerlink" title="详细步骤（附代码）"></a>详细步骤（附代码）</h2><p>Faiss 总体使用过程可以分为三步：</p><ol><li>构建训练数据（以矩阵形式表达）</li><li>挑选合适的 Index （Faiss 的核心部件），将训练数据 add 进 Index 中。</li><li>Search，也就是搜索，得到最后结果</li></ol><h3 id="1-加载SBERT进行矢量化"><a href="#1-加载SBERT进行矢量化" class="headerlink" title="1.加载SBERT进行矢量化"></a>1.加载SBERT进行矢量化</h3><p>首先，让我们安装和加载所需的库</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs elm">!pip install faiss-cpu<br>!pip install -<span class="hljs-type">U</span> sentence-transformers<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> faiss<br><span class="hljs-keyword">import</span> time<br><span class="hljs-title">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer<br></code></pre></td></tr></table></figure><p>然后，加载数据集</p><p>我使用了来自Kaggle的数据集，其中包含十七年来发布的新闻头条。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vbnet">data = [<span class="hljs-comment">&#x27;This framework generates embeddings for each input sentence&#x27;,</span><br>    <span class="hljs-comment">&#x27;Sentences are passed as a list of string.&#x27;, </span><br>    <span class="hljs-comment">&#x27;The quick brown fox jumps over the lazy dog.&#x27;]</span><br></code></pre></td></tr></table></figure><p>加载预训练的模型并执行推理</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">model</span> = SentenceTransformer(<span class="hljs-string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)<br><span class="hljs-attr">encoded_data</span> = model.encode(data, show_progress_bar=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h3 id="2-Faiss索引数据集"><a href="#2-Faiss索引数据集" class="headerlink" title="2.Faiss索引数据集"></a>2.Faiss索引数据集</h3><p>通过参考<a href="https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index">指南，</a>我们可以根据用例选择不同的索引选项。</p><p>让我们定义索引并向其中添加数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell">index = faiss.IndexIDMap(faiss.IndexFlatIP(768))<br>index.add_with_ids(encoded_data, np.array(range(0, len(data))))<br><br>d, nlist = 768, 1000    # 聚类中心的个数<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-comment"># 精确的内积搜索，对归一化向量计算余弦相似度（不太准？）</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">faiss.normalize_L2(encoded_data)   <span class="hljs-comment"># 归一化</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">index = faiss.IndexFlatIP(d)  <span class="hljs-comment"># 内积建立索引</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">index.add(encoded_data)  <span class="hljs-comment"># 添加矩阵</span></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-comment"># 精确的L2距离搜索</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">index = faiss.IndexFlatL2(d)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">print</span>(index.is_trained)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">index.add(encoded_data)</span> <br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">print</span>(index.ntotal)  <span class="hljs-comment"># 查看建立索引的向量数目</span></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-string">倒排文件检索</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-string">为了加速查询，可以把数据集切分成多个，采用基于Multi-probing(best-bin KD树变体）的分块方法。</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-string">这便是IndexIVFFlat，它需要另一个索引来记录倒排列表。</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-string">&quot;</span><span class="hljs-string">&quot;&quot;</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">quantizer = faiss.IndexFlatIP(d)   <span class="hljs-comment"># 建立一个量化器</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">index.train(encoded_data)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">index.add(encoded_data)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">print</span>(index.ntotal)</span><br></code></pre></td></tr></table></figure><p>序列化索引</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">faiss.write<span class="hljs-constructor">_index(<span class="hljs-params">index</span>, &#x27;<span class="hljs-params">abc_news</span>&#x27;)</span><br></code></pre></td></tr></table></figure><p>然后可以将序列化的索引导出，迁移至托管搜索引擎的任何计算机中！</p><p>反序列化索引</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">index</span> = faiss.read_index(<span class="hljs-string">&#x27;abc_news&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="3-执行语义相似度搜索"><a href="#3-执行语义相似度搜索" class="headerlink" title="3.执行语义相似度搜索"></a>3.执行语义相似度搜索</h3><p>首先让我们构建一个包装函数进行搜索</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">def <span class="hljs-keyword">search</span>(query):<br>   t=<span class="hljs-type">time</span>.time()<br>   query_vector = model.encode([query])<br>   k = <span class="hljs-number">5</span><br>   top_k = <span class="hljs-keyword">index</span>.<span class="hljs-keyword">search</span>(query_vector, k)<br>   print(<span class="hljs-string">&#x27;totaltime: &#123;&#125;&#x27;</span>.format(<span class="hljs-type">time</span>.time()-t))<br>   <span class="hljs-keyword">return</span> [data[_id] <span class="hljs-keyword">for</span> _id <span class="hljs-keyword">in</span> top_k[<span class="hljs-number">1</span>].tolist()[<span class="hljs-number">0</span>]]<br></code></pre></td></tr></table></figure><p>执行搜索</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">query</span>=str(input())<br><span class="hljs-attribute">results</span>=search(query)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;results :&#x27;</span>)<br><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\t&#x27;</span>,result)<br></code></pre></td></tr></table></figure><h3 id="4-索引更新"><a href="#4-索引更新" class="headerlink" title="4.索引更新"></a>4.索引更新</h3><p><a href="https://juejin.im/post/6844903935044501511">https://juejin.im/post/6844903935044501511</a></p><p>每隔2分钟创建一个更新索引的task，要求更新所有handler对应的索引，worker接收到task之后从生产数据库上下载对应的主问题，下载对应的向量文件，加载向量构建索引，然后将索引序列化到文件，Faiss server提供gRPC接口来接收celery的通知(调用)，接到通知后直接加载索引文件更新索引。</p><h1 id="FAQ-Web服务"><a href="#FAQ-Web服务" class="headerlink" title="FAQ Web服务"></a>FAQ Web服务</h1><h2 id="Web-API"><a href="#Web-API" class="headerlink" title="Web API"></a>Web API</h2><ul><li>Web 框架选择<ul><li><a href="https://flask.palletsprojects.com/">Flask</a> + Gunicorn + gevent + nginx ，进程管理（崩溃自动重启）（uwsgi 同理，gunicorn 更简单）</li><li>:fire: <strong><a href="https://fastapi.tiangolo.com/">FastAPI</a></strong> + uvicorn（崩溃自动重启），最快的Python Web框架（实测的确比 Flask 快几倍）</li></ul></li><li>cache 缓存机制（保存最近的query对应的topic，命中后直接返回）<ul><li>Flask 相关<ul><li><a href="https://github.com/sh4nks/flask-caching">flask-caching</a> （默认缓存500，超时300秒），使用 set/get 进行数据操作；项目来源于 <a href="https://github.com/pallets/werkzeug">pallets/werkzeug</a> （werkzeug 版本0.4以后弃用 cache）</li></ul></li><li>Python 3.2 以上自带（FastAPI 中可使用）<ul><li>:fire: <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache"><strong>functools.lru_cache()</strong></a> （默认缓存128，lru策略），装饰器，缓存函数输入和输出</li></ul></li></ul></li></ul><h2 id="Locust-压力测试"><a href="#Locust-压力测试" class="headerlink" title="Locust 压力测试"></a>Locust 压力测试</h2><p><a href="https://community.jiguang.cn/article/464443">https://community.jiguang.cn/article/464443</a></p><p><a href="https://debugtalk.com/post/head-first-locust-user-guide/">https://debugtalk.com/post/head-first-locust-user-guide/</a></p><p>使用 <a href="https://locust.io/">Locust</a> 编写压力测试脚本</p><ul><li><p>运行命令说明</p><blockquote><p>总共 100 个模拟用户，启动时每秒递增 10 个，压力测试持续 3 分钟</p></blockquote><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">locust</span>  -f locust_test.py  --host=http://<span class="hljs-number">127.0.0.1:8889</span>/module --headless -u <span class="hljs-number">100</span> -r <span class="hljs-number">10</span> -t <span class="hljs-number">3</span>m<br></code></pre></td></tr></table></figure></li><li><p>:hourglass: 配置 <strong>4核8G CPU</strong> （6层小模型占用内存约 700MB）</p><ul><li>小服务器上 <strong>bert-as-service</strong> 服务非常不稳定（tensorflow各种报错）， 效率不如简单封装的 <strong>TransformersEncoder</strong></li><li><strong>FastAPI</strong> 框架速度远胜于 <strong>Flask</strong>，的确堪称最快的 Python Web 框架</li><li><strong>cache</strong> 的使用能够大大提高并发量和响应速度（最大缓存均设置为<strong>500</strong>）</li><li>最终推荐配置 :fire: <strong>TransformersEncoder + FastAPI + functools.lru_cache</strong></li></ul></li></ul><h1 id="最后的想法"><a href="#最后的想法" class="headerlink" title="最后的想法"></a>最后的想法</h1><p>这是一个基本的实现，在语言模型部分和索引部分上仍然需要做很多工作。根据使用情况，数据大小和可用的计算能力，应从不同的索引选项中选择合适的索引选项。此外，本文使用的句子嵌入只是在某些公共数据集上进行微调，在特定领域的数据集上对其进行微调将改善嵌入效果，从而改善搜索结果。</p><p>很多场景下，基于关键字的倒排索引召回结果已经足够，可以考虑综合基于关键字和基于向量的召回方法，参考知乎语义检索系统 <a href="http://arxiv.org/abs/2008.03917">Beyond Lexical: A Semantic Retrieval Framework for Textual SearchEngine</a></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><em>[1] Nils Reimers和Iryna Gurevych。“</em><a href="https://arxiv.org/pdf/2004.09813.pdf"><em>使用知识提炼使多语言的单语言句子嵌入成为多语言</em></a><em>。” arXiv（2020）：2004.09813。</em></p><p>[2] Johnson，Jeff和Douze，Matthijs和J {\’e} gou，Herv {\’e} <em>。“</em><a href="https://arxiv.org/abs/1702.08734">使用GPU进行十亿规模的相似性搜索</a><em>”</em> arXiv预印本arXiv：1702.08734 <em>。</em></p><p><a href="https://zhuanlan.zhihu.com/p/107241260">Fiass - 常见问题总结</a></p><p><a href="https://blog.csdn.net/flyfish1986/article/details/108012151">Faiss 相似度搜索使用余弦相似性</a></p><p><a href="[https://blog.khay.site/2020/09/06/FAQ%E4%B9%8B%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E5%90%91%E9%87%8F%E8%AF%AD%E4%B9%89%E6%A3%80%E7%B4%A2%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/](https://blog.khay.site/2020/09/06/FAQ之基于BERT的向量语义检索解决方案/">FAQ之基于BERT的向量语义检索解决方案</a>)</p><p><a href="http://mp.163.com/article/FR52N3PU0531D9VR.html">使用Sentence Transformers和Faiss构建语义搜索引擎</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>faiss</tag>
      
      <tag>向量化检索</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>✨工程实践指导原则</title>
    <link href="/2020/10/22/2020-10-22-%E2%9C%A8%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%AF%BC%E5%8E%9F%E5%88%99/"/>
    <url>/2020/10/22/2020-10-22-%E2%9C%A8%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%AF%BC%E5%8E%9F%E5%88%99/</url>
    
    <content type="html"><![CDATA[<h1 id="✨工程实践方法论"><a href="#✨工程实践方法论" class="headerlink" title="✨工程实践方法论"></a>✨工程实践方法论</h1><ol><li>一个优秀的机器学习实践者需要知道：<ul><li>存在哪些算法以及这些算法为何有效的原理</li><li>如何针对具体应用挑选一个合适的算法</li><li>如何监控算法，并根据实验反馈改进机器学习系统</li></ul></li><li>实际开发过程中，实践者需要决定：是否收集更多的数据、是否需要增加/降低模型容量、是否需要添加/删除正则化项、是否需要改进模型的优化算法、是否需要改进模型的近似推断….这些都需要大量的时间</li><li>在实际应用中，正确使用一个普通算法通常要比草率的使用一个不清楚的算法要效果更好</li><li><p>正确应用一个算法需要掌握一些相当简单的方法论：</p><ul><li>确定目标：使用什么样的误差度量（是准确率还是召回率？），并为此误差度量确定目标（我们期望达到什么样的效果：如召回率大于95%？）<ul><li>这些目标和误差度量取决于该应用为了解决什么问题</li></ul></li><li>尽快建立一个端到端的工作流程，包括计算出合适的性能度量（如：计算出召回率）</li><li>搭建系统，并确定性能瓶颈。<ul><li>检查哪个部分的性能差于预期</li><li>检查差于预期的原因：是否过拟合、欠拟合、还是数据或者软件缺陷造成的</li></ul></li><li>根据具体观察反复进行增量式改动：如收集新数据、调整超参数、改进算法</li></ul><span id="more"></span></li></ol><h2 id="一、性能度量"><a href="#一、性能度量" class="headerlink" title="一、性能度量"></a>一、性能度量</h2><ol><li>确定目标，即使用什么误差度量是第一步<ul><li>因为误差度量将指导接下来的所有工作</li><li>同时我们也能够了解大概能得到什么级别的目标性能</li></ul></li><li>对大多数应用而言，不可能实现绝对零误差<ul><li>即使有无限的训练数据，并且恢复了真正的概率分布，但是由于输入特征可能无法包含输出变量的完整信息、或者系统本质上是个随机系统，则仍然产生了误差</li><li>当然实际上我们也不可能有无限的训练数据</li></ul></li><li>通常我们需要收集更多的数据。但是我们需要在收集更多数据的成本，与进一步减少误差的价值之间权衡</li><li>通常我们需要对错误率定一个 baseline 从而判断预测得好坏<ul><li>对于学术界，我们可以将先前公布的基准结果作为  baseline </li><li>在工业界，我们从安全的、能带来性价比、或者能吸引用户的角度来确定错误率的 baseline </li></ul></li><li>性能度量：表明从哪个角度来度量算法的性能<ul><li>常见的有：精度<code>precision</code>，召回率<code>recall</code>以及 <code>PR curve</code>，<code>ROC</code>曲线，<code>F-score</code>等</li><li>还有一种：覆盖率。它是机器学习系统能够产生响应的样本占所有样本的比例<ul><li>一个系统可以拒绝处理任何样本，从而达到 100% 的精度。但是覆盖率为 0%</li></ul></li><li>也可以从专业角度考量：点击率、用户满意度调查等等</li><li>最重要的是：确定使用哪个性能度量</li></ul></li></ol><h2 id="二、默认的基准模型"><a href="#二、默认的基准模型" class="headerlink" title="二、默认的基准模型"></a>二、默认的基准模型</h2><ol><li>根据问题的复杂性，项目开始时可能无需使用深度学习<ul><li>如果只需要正确选择几个线性权重就能解决问题，那么项目开始可以使用一个简单的统计模型，如逻辑回归</li></ul></li><li>如果问题属于 “AI-完全”类型的，如对象识别、语音识别等，那么项目开始于一个合适的深度学习模型，则效果会比较好</li><li>根据数据结构选择一类合适的模型：<ul><li>如果是以固定大小的向量作为输入的有监督学习，那么可以使用全连接的前馈网络</li><li>如果输入已知的拓扑结构（如图像），则可以使用卷积网络</li><li>如果输入或者输出是一个序列，则使用门控循环网络（LSTM 或者 GRU）</li></ul></li><li>刚开始时可以使用某些分段线性单元：如 <code>ReLU</code>或者其扩展</li><li>可以选择具有衰减学习率以及动量的<code>SGD</code>作为优化算法<ul><li>常见的衰减方法有：<ul><li>衰减到固定最低学习率的线性衰减</li><li>指数衰减</li><li>每次发生验证错误停滞时将学习率降低 2-10 倍的衰减策略</li></ul></li><li>另一种优化选择是 <code>Adam</code>算法</li></ul></li><li><code>batch normalization</code>对优化性能有着显著的影响，特别是对于卷积网络和具有<code>sigmoid</code>非线性函数的网络而言<ul><li>最初的基准中，可以忽略<code>batch normalization</code></li><li>当优化似乎出现问题时，应立即使用<code>batch normalization</code></li></ul></li><li>除非训练集包含数千万或者更多的样本，否则项目一开始就应该包含一些温和的正则化<ul><li>建议采用早停策略</li><li>建议采用<code>dropout</code>策略，它也兼容很多模型以及许多正则化项</li><li><code>batch normalization</code> 有时可以降低泛化误差，此时可以省略<code>dropout</code>策略。因为用于 <code>normalize</code>的统计量估计本身就存在噪音</li></ul></li><li>如果我们的任务和另一个被广泛研究的任务相似，那么通过复制之前研究中已知的性能良好的模型和算法，可能会得到很好的效果<ul><li>你可以从该任务中复制一个训练好的模型。如从<code>ImageNet</code>上训练好的卷积网络的特征来解决其他计算机视觉任务</li></ul></li><li>对于是否使用无监督学习，和特定领域有关<ul><li>对于某些领域，如自然语言处理，能大大受益于无监督学习技术</li><li>在其他领域，目前无监督学习并没有带来好处</li><li>如果你所处理的应用，无监督学习是非常重要的，那么将其包含在第一个端到端的基准中；否则只有在解决无监督问题时，才第一次尝试使用无监督学习</li></ul></li></ol><h2 id="三、决定是否收集更多数据"><a href="#三、决定是否收集更多数据" class="headerlink" title="三、决定是否收集更多数据"></a>三、决定是否收集更多数据</h2><ol><li>建立第一个端到端的系统后，就可以度量算法的性能并决定如何改进算法<ul><li>很多新手忍不住尝试很多不同的算法来改进</li><li>实际上，收集更多的数据往往比改进学习算法要有用的多</li></ul></li><li>决定是否需要收集更多数据的标准：<ul><li>首先：确定训练集上的性能可否接受。如果模型在训练集上的性能就很差，那么没必要收集更多的数据。<ul><li>此时可以尝试增加更多的网络层</li><li>或者每层增加更多的隐单元从而增加模型的规模</li><li>也可以尝试调整学习率等超参数来改进算法</li><li>如果这些都不行，则说明问题可能源自训练数据的质量：数据包含太多噪声，或者数据未能包含预测输出所需要的正确输入。此时我们需要重新开始收集干净的数据，或者收集特征更丰富的数据</li></ul></li><li>如果模型在训练集上的性能可以接受，那么我们开始度量测试集上的性能。</li><li>如果测试集上的性能也可以接受，则任务完成</li><li>如果测试集上的性能比训练集的要差得多，则收集更多的数据时最有效的解决方案之一</li><li>此时主要考虑三个要素：<ul><li>收集更多数据的代价和可行性</li><li>其他方法降低测试误差的代价和可行性</li><li>增加数据数量能否显著提升测试集性能</li></ul></li></ul></li><li>如果增加数据数量代价太大，那么一个替代的方案是降低模型规模，或者改进正则化（调整超参数、或者加入正则化策略）<ul><li>如果调整正则化参数之后，训练集性能和测试集性能之间的差距还是无法接受，则只能收集更多的数据</li></ul></li><li>当决定增加数据数量时，还需要确定收集多少数据。可以绘制曲线来显式训练集规模和泛化误差之间的关系<ul><li>根据曲线的延伸，可以预测还需要多少训练数据来到达一定的性能</li><li>通常加入小比例的样本不会对泛化误差产生显著的影响。因此建议在对数尺度上考虑训练集的大小，以及增加的数据数量</li></ul></li><li>如果收集更多的数据是不可行的（成本太高无法实现，或者无法改进泛化误差），那么改进泛化误差的唯一方法是：改进学习算法本身<ul><li>这是属于研究领域，并不是对实践者的建议</li></ul></li></ol><h2 id="四、选择超参数"><a href="#四、选择超参数" class="headerlink" title="四、选择超参数"></a>四、选择超参数</h2><ol><li>大部分深度学习算法都有许多超参数来控制不同方面的算法表现<ul><li>有的超参数会影响算法运行的时间和存储成本</li><li>有的超参数会影响学习到的模型质量，以及在新输入上推断正确结果的能力</li></ul></li><li>有两种选择超参数的方法：<ul><li>手动选择。手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化</li><li>自动选择。自动选择超参数算法不需要你了解超参数做了什么以及机器学习模型如何才能取得零号的泛化，但是它往往需要更高的计算成本</li></ul></li></ol><h3 id="4-1-手动调整超参数"><a href="#4-1-手动调整超参数" class="headerlink" title="4.1 手动调整超参数"></a>4.1 手动调整超参数</h3><ol><li><p>手动设置超参数：</p><p>我们必须了解超参数、训练误差、泛化误差、计算资源（内存和运行时间） 之间的关系。这要求我们切实了解一个学习算法有效容量的基本概念</p></li><li><p>手动搜索超参数的任务是：在给定运行时间和内存预算范围的条件下，最小化泛化误差</p><ul><li>我们不讨论超参数对于运行时间和内存的影响，因为它们高度依赖于平台</li></ul></li><li><p>手动搜索超参数的主要目标是：调整模型的有效容量以匹配任务的复杂性</p><ul><li>模型的有效容量受限于三个因素：<ul><li>模型的表示容量。更多的网络层、每层更多的隐单元的模型具有更大的容量（能表达更复杂的函数）</li><li>学习算法成功最小化训练模型代价函数的能力</li><li>训练过程正则化模型的程度</li></ul></li><li>模型的表示容量并不是越高越好。如果无法找到合适的代价函数来最小化训练代价、或者正则化项排除了某些合适的函数，那么即使模型的表达能力再强，也无法学习出合适的函数。</li></ul></li><li><p>如果以超参数为自变量，泛化误差为因变量。那么会在的曲线通常会表现为 U 形</p><ul><li>在某个极端情况下，超参数对应着低容量（并不是超参数越小，模型容量越低；也可能是相反的情况）。此时泛化误差由于训练误差较大而很高。这就是欠拟合</li><li>在另一个极端情况下，超参数对应着高容量，此时泛化误差也很大。这就是过拟合<ul><li>过拟合中，泛化误差较大的原因是：虽然此时训练误差较小，但是训练误差和测试误差之间的差距较大。</li></ul></li><li>最优的模型容量位于曲线中间的某个位置</li></ul></li><li><p>对于某些超参数，当超参数值太大时，会发生过拟合。如中间层隐单元的数量，数量越大，模型容量越高，也更容易发生过拟合。</p><p>对于某些超参数，当超参数值太小时，也会发生过拟合。如 正则化的权重系数，系数为0，表示没有正则化，此时很容易过拟合。</p></li><li><p>并不是每个超参数都对应着完整的 U 形曲线</p><ul><li>很多超参数是离散的，如中间层隐单元的数量，或者 <code>maxout</code>单元中线性片段的数目</li><li>有些超参数甚至是二值的。如是否决定对输入特征进行标准化这个布尔值的超参数</li><li>有些超参数可能有最小值或者最大值</li></ul></li><li><p>学习率可能是最重要的超参数</p><ul><li><p>如果你只有时间来调整一个超参数，那么就调整学习率</p></li><li><p>相比其他超参数，学习率以一种更复杂的方式控制模型的有效容量：</p><ul><li>当学习率大小适当时，模型的有效容量最高</li><li>当学习率过大时，梯度下降可能会不经意地增加而非减少训练误差。在理想的二次情况下，如果学习率是最佳值的两倍时，会发生这种情况</li><li>当学习率太小时，训练不仅会很慢，还有可能永久停留在一个很高的训练误差。对于这种情况我们知之甚少（但是我们可以知道这种情况不会发生在一个凸损失函数中）</li></ul></li><li><p>学习率关于训练误差具有 U 形曲线。泛化误差也是类似的 U 形曲线，但是正则化作用在学习率过大或者过小处比较复杂</p><p><img src="http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_practical/man_search.png" alt="man_search"></p></li></ul></li><li><p>调整学习率以外的其他参数时，需要同时监测训练误差和测试误差，从而判断模型是否过拟合或者欠拟合，然后适当调整其容量</p><ul><li>如果训练集错误率大于目标错误率（这个根据任务背景人工指定），那么只能增加模型容量以改进模型。但是这增加了模型的计算代价</li><li>如果测试集错误率大于目标错误率，则有两个方法：<ul><li>如果训练误差较小（这说明模型容量较大），则表明测试误差取决于训练误差与测试误差之间的差距。要减少这个差距，我们可以改变正则化超参数，以减少有效的模型容量</li><li>如果训练误差较大（者说明模型容量较小），那么也只能增加模型容量以改进模型</li></ul></li></ul></li><li><p>通常最佳性能来自于正则化很好的大规模模型，如使用<code>Dropout</code>的神经网络</p></li><li><p>大部分超参数可以推论出是否增加或者减少模型容量，部分示例如下：</p><p>|    超参数    | 容量何时增加 |                             原因                             |                           注意事项                           |<br>| :—————: | :—————: | :—————————————————————————————: | :—————————————————————————————: |<br>|  隐单元数量  |     增加     |              增加隐单元数量会增加模型的表示能力              | 几乎模型每个操作所需要的时间和内存代价都会随隐单元数量的增加而增加 |<br>|    学习率    |   调至最优   | 不正确的学习率，不管是太高还是太低都会由于优化失败而导致低的有效容量的模型 |                                                              |<br>|  卷积核宽度  |     增加     |              增加卷积核宽度会增加模型的参数数量              | 较宽的卷积核导致较窄的输出尺寸，除非使用隐式零填充来减少此影响，否则会降低模型容量。较宽的卷积核需要更多的内存来存储参数，并增加运行时间 |<br>|  隐式零填充  |     增加     |           在卷积之前隐式添加零能保持较大尺寸的表示           |               大多数操作的时间和内存代价会增加               |<br>| 权重衰减系数 |     降低     |          降低权重衰减系数使得模型参数可以自由地变大          |                                                              |<br>| dropout 比率 |     降低     |     较少地丢弃单元可能更多的让单元彼此“协力”来适应训练集     |                                                              |</p></li><li><p>手动调整超参数时不要忘记最终目标：提升测试集性能</p><ul><li>加入正则化只是实现这个目标的一种方法</li><li>如果训练误差很低，也可以通过收集更多的训练数据来减少泛化误差。如果训练误差太大，则收集更多的训练数据就没有意义。</li><li>实践中的一种暴力方法是：不断提高模型容量和训练集的大小。这种方法增加了计算代价，只有在拥有充足的计算资源时才可行</li></ul></li></ol><h3 id="4-2-自动超参数优化算法"><a href="#4-2-自动超参数优化算法" class="headerlink" title="4.2 自动超参数优化算法"></a>4.2 自动超参数优化算法</h3><ol><li>理想的学习算法应该是只需要输入一个数据集，然后就可以输出学习的函数而不需要人工干预调整超参数<ul><li>一些流行的算法如逻辑回归、支持向量机，其流行的部分原因是：这类算法只需要调整一到两个超参数，而且性能也不错</li><li>某些情况下，神经网络只需要调整少量的超参数就能达到不错的性能；但是大多数情况下需要调整更多的超参数</li></ul></li><li>原则上可以开发出封装了学习算法的超参数优化算法，并自动选择其超参数<ul><li>超参数优化算法往往有自己的超参数（如每个超参数的取值范围），这将问题变得更复杂</li><li>我们可以人工选择参数优化算法这一级的超参数，因为这一级的超参数通常更容易选择</li></ul></li></ol><h3 id="4-3-网格搜索"><a href="#4-3-网格搜索" class="headerlink" title="4.3 网格搜索"></a>4.3 网格搜索</h3><ol><li>当只有三个或者更少的超参数时，常见的超参数搜索方法是：网格搜索<ul><li>对于每个超参数，选择一个较小的有限值集合去搜索</li><li>然后这些超参数笛卡尔乘积得到多组超参数</li><li>网格搜索使用每一组超参数训练模型，挑选验证集误差最小的超参数作为最好的超参数</li></ul></li><li>如何确定搜索集合的范围？<ul><li>如果超参数是数值，则搜索集合的最小、最大元素可以基于先前相似实验的经验保守地挑选出来</li><li>如果超参数是离散的，则直接使用离散值</li></ul></li><li>通常网格搜索会在对数尺度下挑选合适的值</li><li>通常重复进行网格搜索时，效果会更好。假设在集合 <code>&#123;-1,0,1&#125;</code>上网格搜索超参数 <ul><li>如果找到的最佳值是 1，那么说明我们低估了  的取值范围。此时重新在 <code>&#123;1,2,3&#125;</code> 上搜索</li><li>如果找到的最佳值是 0，那么我们可以细化搜索范围以改进估计。此时重新在 <code>&#123;-0.1,0,0.1&#125;</code> 上搜索</li></ul></li><li>网格搜索的一个明显问题时：计算代价随着超参数数量呈指数级增长。<ul><li>如果有  个超参数，每个最多取  个值，那么所需的试验数将是   。虽然我们可以并行试验，但是指数级增长的计算代价仍然不可行</li></ul></li></ol><h3 id="4-4-随机搜索"><a href="#4-4-随机搜索" class="headerlink" title="4.4 随机搜索"></a>4.4 随机搜索</h3><ol><li>随机搜索是一种可以替代网格搜索的方法，它编程简单、使用方便、能更快收敛到超参数的良好取值：<ul><li>首先为每个超参数定义一个边缘分布，如伯努利分布（对应着二元超参数）或者对数尺度上的均匀分布（对应着正实值超参数）</li><li>然后假设超参数之间相互独立，从各分布中抽样出一组超参数。</li><li>使用这组超参数训练模型</li><li>经过多次抽样 -&gt; 训练过程，挑选验证集误差最小的超参数作为最好的超参数</li></ul></li><li>随机搜索的优点：<ul><li>不需要离散化超参数的值，也不需要限定超参数的取值范围。这允许我们在一个更大的集合上进行搜索</li><li>当某些超参数对于性能没有显著影响时，随机搜索相比于网格搜索指数级地高效，它能更快的减小验证集误差</li></ul></li><li>与网格搜索一样，我们通常会基于前一次运行结果来重复运行下一个版本的随机搜索</li><li>随机搜索比网格搜索更快的找到良好超参数的原因是：没有浪费的实验<ul><li>在网格搜索中，两次实验之间只会改变一个超参数 （假设为 ）的值，而其他超参数的值保持不变。如果这个超参数 的值对于验证集误差没有明显区别，那么网格搜索相当于进行了两个重复的实验</li><li>在随机搜索中，两次实验之间，所有的超参数值都不会相等（因为每个超参数的值都是从它们的分布函数中随机采样而来）。因此不大可能会出现两个重复的实验</li><li>如果 超参数与泛化误差无关，那么不同的 值：<ul><li>在网格搜索中，不同  值、相同的其他超参数值，会导致大量的重复实验</li><li>在随机搜索中，其他超参数值每次也都不同，因此不大可能出现两个重复的实验（除非所有的超参数都与泛化误差无关）</li></ul></li></ul></li></ol><h3 id="4-5-基于模型的超参数优化"><a href="#4-5-基于模型的超参数优化" class="headerlink" title="4.5 基于模型的超参数优化"></a>4.5 基于模型的超参数优化</h3><ol><li>超参数搜索问题可以转化为一个优化问题：<ul><li>决策变量是超参数</li><li>优化的代价是超参数训练出来的模型在验证集上的误差</li><li>在简化的设定下，可以计算验证集上可导的误差函数关于超参数的梯度，然后基于这个梯度进行更新</li></ul></li><li>实际上，大多数问题中，超参数的梯度是不可用的<ul><li>一方面可能是因为因为高额的计算代价和存储成本。因为你需要计算非常多的超参数才能获得一系列的超参数的导数（这就要求你运行非常多轮次的算法）</li><li>另一方面可能是因为验证集误差在超参数上本质不可导（如超参数时离散值的情况）</li></ul></li><li>为了弥补超参数梯度的缺失，我们可以使用贝叶斯回归模型来估计每个超参数的验证集误差的期望和该期望的不确定性（贝叶斯回归模型不需要使用梯度）<ul><li>目前无法明确的确定：贝叶斯超参数优化是否有效。实际上，它有时表现得像一个人类专家，但是有时候又发生灾难的失误。目前该方法还不够成熟或可靠</li></ul></li><li>大部分超参数优化算法比随机搜索更复杂，并且具有一个共同的缺点：在获取任何有效的超参数信息之前，你必须完整运行整个训练过程<ul><li>这种做法是相当低效的。实际上人类手动搜索之前，可以很早就判断某些超参数组合是否是完全病态的</li></ul></li></ol><h2 id="五、调试策略"><a href="#五、调试策略" class="headerlink" title="五、调试策略"></a>五、调试策略</h2><ol><li><p>当一个机器学习系统效果不好时，很难判断效果不好的原因是算法本身，还是算法实现错误。由于各种原因，机器学习系统很难调试</p><ul><li>大多数情况下，我们不能提前知道算法的行为<ul><li>实际上，使用机器学习的出发点是：机器学习会发现一些我们无法发现的有用的行为</li><li>如果我们在一个新的分类任务上训练一个神经网络，它达到了 5% 的测试误差。我们无法直接知道：这是期望的结果，还是次优的结果？</li></ul></li><li>大部分机器学习模型有多个自适应的部分<ul><li>如果某个部分失效了，那么其他部分仍然可以自适应这种状况，并获得大致可接受的性能</li><li>比如在多层神经网络中，假设我们在梯度下降中对偏置更新时犯了一个错误  （错误原因：没有使用梯度。其中  为学习率）。这个错误导致偏置在学习过程中不断减小。但是只是检查模型输出的话，这个错误并不是显而易见的。因为权重   可以自适应的补偿偏置  的错误</li></ul></li></ul></li><li><p>大部分神经网络的调试策略都是解决上述两个难点的一个或者两个</p><ul><li>我们可以设计一种足够简单的情况，能够提前得到正确结果，判断模型预测是否与之相符</li><li>我们可以设计一个测试（类似于 <code>UnitTest</code>），独立检查神经网络实现的各个部分</li></ul></li><li><p>一些重要的调试方法如下：</p><ul><li>可视化计算中模型的行为：直接观察机器学习模型运行机器任务，有助于确定其达到的量化性能数据是否看上去合理<ul><li>仅仅评估模型性能可能是最具破坏性的错误之一，因为它会使你在系统出问题时误以为系统运行良好</li></ul></li><li>可视化那些最严重的错误：大多数模型能够输出运行任务时的某种置信度（如基于<code>softmax</code>函数输出层的分类器给每个类分配了一个概率）。如果某个样本被错误的分类，而且其分类置信度很低，那么可以检查这些样本的问题</li><li>根据训练和测试误差检测软件：通常我们很难确定底层软件是否正确实现，而测试和训练误差提供了一些线索<ul><li>如果训练误差较低，而测试误差较高，那么很可能是由于算法过拟合。也有可能是软件错误，导致测试误差未能正确地度量</li><li>如果训练误差和测试误差都很高，那么很难确定是由于软件错误，还是由于算法模型的欠拟合。此时需要进一步测试，如下所述</li></ul></li><li>监控激活函数值和梯度的直方图：可视化激活函数值和梯度的统计量往往是有用的。<ul><li>隐单元的激活函数值告诉我们：该单元是否饱和，或者它们饱和的频率如何（如多久关闭一次，是否永远关闭，以及采用双曲正切单元时饱和的程度）</li><li>梯度的统计量告诉我们：梯度是否快速的增长或者快速的消失</li><li>参数的梯度的量级和参数的量级也有意义：我们希望参数在一个小批量更新中变化的幅度是参数量值的 1% 这样的级别（而不是 50% 或者 0.0001%，这两者要么导致参数移动太快，要么太慢）</li><li>如果数据是稀疏的（如自然语言），则有些参数可能是很少更新的，检测时需要注意</li></ul></li></ul></li><li><p>许多深度学习算法为每一步的结果产生了某种保证。其中包括：</p><ul><li><p>目标函数值确保在算法的迭代步中不会增加</p></li><li><p>某些变量的导数在算法的每一步中都是 0</p></li><li><p>所有变量的梯度在收敛时，会变为 0</p><blockquote><p>由于计算机存储浮点数的舍入误差，这些条件可能会有误差</p></blockquote></li></ul><p>如果违反了这些保证，那么一定是软件错误</p></li></ol><h3 id="5-1-梯度检查"><a href="#5-1-梯度检查" class="headerlink" title="5.1 梯度检查"></a>5.1 梯度检查</h3><ol><li>比较反向传播导数和数值导数：如果我们正在使用一个软件框架或者库，那么必须定义 <code>bprop</code>方法，常见的错误原因是未能正确的实现梯度表达。验证该错误的一个方法是：比较实现的自动求导和通过有限差分计算的导数</li></ol><p>   或者采用中心差分法：</p><p>   其中 必须足够大，从而确保不会由于有限精度问题产生舍入误差</p><ul><li><p>如果梯度是雅克比矩阵（输出为向量，输入也是向量），则我们可以多次使用有限差分法来评估所有的偏导数</p></li><li><p>通常推荐使用中心差分法，因为根据泰勒展开，有：</p></li></ul><pre><code class="hljs"> 使用中心差分法的误差更小。</code></pre><ol><li><p>理论上进行梯度检查很简单，就是把解析的梯度和数值计算的梯度进行比较。但是实际操作过程中，这个过程复杂且容易出错。</p></li><li><p>使用中心化公式来计算数值梯度。</p><p>通常我们计算数值梯度时，采用：</p></li></ol><p>   其中 为一个很小的数字，在实践过程中近似为 。但是实践中证明，使用中心化公式效果更好：</p><ul><li>缺点：在检查梯度的每个维度时，需要计算两次损失函数</li><li>优点：梯度的数值求解结果会准确的多。中心化公式的近似误差为 ，而前一个公式的近似误差为 </li></ul><ol><li>使用相对误差来比较。对于数值梯度 和解析梯度 ，通常都是比较其相对误差而不是绝对误差来判断数值计算得到的梯度是否正确：</li></ol><ul><li>如果相对误差 ，则通常意味着求解梯度可能出错</li><li>如果相对误差 ，则对于有不可导点的目标函数时OK的，但是对于使用了<code>tanh/softmax</code>的目标函数，还是有点高</li><li>如果相对误差 ，则求解梯度结果正确</li><li>因为网络越深，相对误差就越高。因此对于一个 10 层网络的输入数据做梯度检查，那么  的相对误差可能就OK了，因为误差一直在累积</li></ul><ol><li><p>使用双精度类型。当使用单精度浮点数来进行梯度检查时，即使梯度计算过程正确，相对误差也会很高（如 ）</p></li><li><p>梯度检查时，一个导致不准确的原因是：不可导点。</p><ul><li>不可导点是目标函数不可导的部分，由于 relu 等函数的引入导致。</li><li>解决方法是：使用更少的数据点。<ul><li>因为数据点越少，不可导点就越少，所以在计算有限差值近似时，横跨不可导点的几率就越小。</li><li>另外如果你的梯度检查对 2-3 个数据点都有效，那么基本上对于整个 batch 数据进行梯度检查也没有问题。所以用少量的数据点，能让梯度检查更迅速有效</li></ul></li></ul></li><li><p>谨慎设置步长 </p><ul><li>并不是  越小越好。因为当  特别小时，可能会遇到数值精度问题。如果梯度检查不通过，可以尝试将  调到  或者  </li></ul></li><li><p>建议让网络预热一小段时间，等到损失函数开始下降之后在进行梯度检查</p><ul><li>第一次迭代就开始梯度检查的危险在于：此时可能处于不正常的边界情况，从而掩盖了梯度没有正确实现的事实。因为梯度检查是在参数空间中的一个特定的、单独的点上进行的。即使在该点上检查成功，也不能保证梯度实现是正确的。</li></ul></li><li><p>不要让正则化吞没数据</p><ul><li>通常损失函数为：数据损失+正则化损失之和。某些情况下，损失函数的梯度主要来源于正则化部分，此时就会掩盖掉数据损失梯度的不正确实现</li><li>建议先关掉正则化对数据损失做单独检查，然后对正则化做单独检查</li></ul></li><li><p>关闭 dropout 和数据 augmentation</p><ul><li>梯度检查时，关闭网络中任何不确定的效果的操作，如随机 dropout，随机数据扩展。不然它们会在计算数值梯度时导致巨大误差</li></ul></li><li><p>检查少量的维度。由于梯度可以有上百万的参数，此时只能检查其中一些维度然后假设其他维度是正确的</p><ul><li>确保在所有不同的参数中都抽取一部分来梯度检查。比如  这些权重矩阵中，每个矩阵抽取一部分来检查。</li><li>不要将所有参数拼接成一个巨大的向量，然后从这个向量中随机抽取一些维度来检查</li></ul></li><li><p>只有在调试过程中进行梯度检验。不要在训练过程中执行梯度检验，因为非常耗计算资源。</p></li></ol><h3 id="5-2-学习过程检查"><a href="#5-2-学习过程检查" class="headerlink" title="5.2 学习过程检查"></a>5.2 学习过程检查</h3><ol><li><p>拟合极小的数据集：</p><p>当训练集上有很大的误差时，我们需要确定问题是欠拟合，还是软件错误。尝试对一个小数据子集进行训练，然后确保能达到 0 的损失值。</p><ul><li>通常即使是极小模型也能保证很好的拟合一个足够小的数据集。如只有一个样本的分类数据，它可以通过正确设置输出层的偏置来拟合</li><li>如果分类器不能正确标定一个单样本组成的训练集、自编码器无法成功再现一个单独的样本、生成模型无法一致的生成一个单独的样本，那么很有可能是由于软件错误</li><li>这种测试可以推广到少量样本的小数据集上，不一定是只有一个样本的数据集</li><li>进行这个训练时，最好让正则化强度为0，不然它会阻止得到0的损失</li><li>如果不能通过这个检验，那么训练过程有问题。此时进行整个数据集的训练是没有意义的；如果能通过这个检验，那么也不保证训练过程没问题</li></ul></li><li><p>在训练神经网络时，应该跟踪多个重要数值</p><ul><li>这些数值输出的图表是观察训练进程的一个窗口，是直观理解不同超参数设置效果的工具</li><li>通常 轴都是以周期 epoch 为单位，它衡量了训练中每个样本数据都被观察过次数的期望<ul><li>一个周期表示每个样本数据都被观察过了一次</li><li>不要使用迭代次数。因为迭代次数与数据的 batch size 有关。而 batch size 可以任意设置</li></ul></li></ul></li><li><p>第一个要跟踪的数值就是损失函数</p><p><img src="http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_practical/training_loss.png" alt="training_loss"></p><p>从左图中可见：</p><ul><li>过低的学习率（蓝色曲线）导致算法的损失函数下降近似线性的</li><li>稍高一些的学习率（红色曲线） 会导致算法的损失函数看起来呈几何指数下降</li><li>更高一些的学习率（绿色曲线）会让损失函数下降的更快，但是它使得损失函数停留在一个较高的水平位</li><li>异常高的学习率（黄色曲线）会让损失函数上升</li></ul><p>从右图可见：</p><ul><li>损失函数值曲线看起来比较合理，但是 batch size 可能有点小。因为损失值的噪音很大</li><li>损失值的震荡程度和 batch size 有关。当 batch size=1 时，震荡会相对较大；而 batch size 为整个数据集大小时，震荡最小。因为每个梯度更新都是单调的优化损失函数</li></ul></li><li><p>在训练分类器时，第二个要跟踪的就是验证集和训练集的准确率。从该图标可以获知模型过拟合的程度</p><p><img src="http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_practical/training_loss2.png" alt="training_loss2"></p><ul><li>训练集准确率和验证集准确率之间的空隙指明了模型过拟合的程</li><li>蓝色的验证集曲线相对于训练集，验证集的准确率低了很多。这说明模型有很强的过拟合。此时应该增大正则化强度或者收集更多的数据</li><li>绿色的验证集曲线和训练集曲线如影随形，说明你的模型容量还不够大，发生了欠拟合。此时应该通过增加参数数量让模型更大一些。</li></ul></li><li><p>另一个应该跟踪的数值是：权重更新的相对值</p><p>假设权重为 ，更新的增量为 。 为学习率。那么权重更新的相对值为：</p></li></ol><ul><li>这里的梯度是一个更新的梯度。如果多个 batch ，则每个 batch 更新一次就计算一次</li><li>这个比例  应该在  左右。如果更低，说明学习率可能太小；如果更高，说明学习率可能太高</li></ul><ol><li><p>一个不正确的初始化可能让学习过程变慢，甚至停止。这个问题可以比较简单的诊断出来：</p><ul><li>输出网络中所有层的激活数据和梯度分布的柱状图</li><li>如果看到任何奇怪的分布，都不是好兆头。如；对于使用 tanh 的神经元，我们应该看到激活数据的值在整个 [-1,1] 区间都有分布。如果看到神经元的输出都是0，或者都在饱和部分，那么就有问题</li></ul></li><li><p>如果数据是图像像素数据，那么把第一层特征可视化会有帮助：</p><p><img src="http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_practical/training_img.png" alt="training_img"></p><ul><li>左图的特征充满了噪音，这暗示网络可能出现问题：网络没有收敛 、学习率设置不当、正则化惩罚的权重过低</li><li>右图特征不错，平滑、干净、种类繁多。说明训练过程良好</li></ul></li></ol><h2 id="六、示例：数字识别系统"><a href="#六、示例：数字识别系统" class="headerlink" title="六、示例：数字识别系统"></a>六、示例：数字识别系统</h2><ol><li>项目开始于性能度量的选择以及这些度量的期望。<ul><li>总的原则是：度量的选择要符合项目的业务目标</li><li>因为地图只有高准确率时才有价值，所以该系统要求达到 98% 的准确率（达到人类水平）</li><li>为了达到这个级别的准确率，系统牺牲了覆盖率。因此在保持准确率 98% 的前提下，覆盖率成了项目的主要性能度量</li></ul></li><li>选择基准系统：对于视觉任务而言，基准系统是带有 <code>ReLU</code>单元的卷积网络</li></ol><h2 id="七、数据预处理"><a href="#七、数据预处理" class="headerlink" title="七、数据预处理"></a>七、数据预处理</h2><ol><li><p>常见数据预处理方式：（红色的线指出各维度的数值范围）</p><p><img src="http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_practical/data_process1.PNG" alt="data_process1"></p><ul><li><p>均值减法：对数据中每个独立特征减去均值，集合上理解为：在每个维度上，都将数据云的中心迁移到原点</p></li><li><p>归一化：将所有维度都做归一化：</p><ul><li>第一种做法：先对数据做零中心化处理，然后每个维度都除以标准差</li><li>第二种做法：每个维度都做归一化，使得每个维度最大和最小值都是 1 和 -1</li></ul></li><li><p>PCA 降维：取得数据的主成分，可以对数据去除相关性</p></li><li><p>白化<code>whitening</code>：先对数据进行旋转（旋转的矩阵就是 SVD 分解中的 U矩阵），然后对每个维度除以特征值（为防止分母为0，通常加一个很小的值作为平滑系数）来对数值范围进行归一化</p><ul><li>如果数据服从多变量的高斯分布，则白化之后，数据的分布是一个均值为零，且协方差相等的矩阵</li><li>该变换的缺点是：可能会放大数据中的噪声。因为它将所有维度都拉伸到相同的维度，这包括了那些大多数是噪声的维度。这个问题可以采用更强的平滑系数来解决</li></ul><p><img src="http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_practical/data_process2.PNG" alt="data_process2"></p></li><li><p>实际在神经网络中，并不会采用 PCA 和白化。</p></li></ul></li><li><p>任何预处理策略都只能在训练集的数据上进行，然后再应用到验证集或测试集上。</p><ul><li>如数据均值：首先分成训练集、验证集、测试集，从训练集中求数据的均值。然后训练集、验证集、测试集中的数据减去这个均值。 而不是减去测试集均值或者验证集均值</li></ul></li><li><p>激活函数：当前推荐使用 ReLU 激活函数</p></li><li><p>Batch Normalization：让数据在通过激活函数之前，添加一个 batch normalization</p></li></ol><h2 id="八、变量初始化"><a href="#八、变量初始化" class="headerlink" title="八、变量初始化"></a>八、变量初始化</h2><h3 id="8-1-权重初始化"><a href="#8-1-权重初始化" class="headerlink" title="8.1 权重初始化"></a>8.1 权重初始化</h3><ol><li><p>权重一定不能全零初始化。因为这会导致神经元在前向传播中计算出同样的输出，然后在反向传播中计算出同样的梯度，从而进行同样的权重更新。这就产生了大量对称性神经元。</p></li><li><p>通常采用小随机数初始化，通过这样来打破对称性。至于使用高斯分布还是均匀分布，对结果影响很小。</p><ul><li><p>之所以用小的随机数，是因为：如果网络中存在<code>tanh</code> 或者 <code>sigmoid</code> 激活函数，或者网络的输出层为<code>sigmoid</code> 等单元，则它们的变量值必须很小。</p><p>如果使用较大的随机数，则很有可能这些单元会饱和，使得梯度趋近于零。这意味着基于梯度下降的算法推进的很慢，从而学习很慢。</p></li><li><p>如果网络中不存在<code>sigmoid/tanh</code> 等激活函数，网络的输出层也不是<code>sigmoid</code> 等单元，则可以使用较大的随机数初始化。</p></li></ul></li><li><p>通常使用 来校准权重初始化标准差。随着输入数据的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。</p><p>假设权重 和输入 之间的内积为 （不考虑非线性激活函数）。我们检查 的方差</p></li></ol><p>   其中假设输入和权重的平均值都是0，因此有 。同时假设所有的 服从同样的分布，假设所有的 服从同样的分布 </p><ul><li>要使得输出  和输入   具有同样的方差，则必须保证每个权重  的方差是  。则如果权重通过一个标准高斯分布初始化，则需要将标准差除以 </li><li>在<code>Glorot</code>的论文中，他推荐初始化公式为： 。其中  为输入和输出的数量。</li></ul><h3 id="8-2-偏置初始化"><a href="#8-2-偏置初始化" class="headerlink" title="8.2 偏置初始化"></a>8.2 偏置初始化</h3><ol><li>通常将偏置初始化为0。这是因为随机小数值权重已经打破了对称性</li></ol><h2 id="九、结构设计"><a href="#九、结构设计" class="headerlink" title="九、结构设计"></a>九、结构设计</h2><ol><li><p>对于任何给定的问题，很难提前去预测到底需要多深的神经网络。</p><p>一个常规的做法是：首先尝试使用逻辑回归（可以视为没有隐层的神经网络）。然后尝试单隐层的神经网络、双隐层的神经网络….</p><p>这种做法将隐层的数量看做是一个超参数。通过交叉验证来确定该超参数的值。</p></li></ol><h1 id="产品级微服务的八大原则"><a href="#产品级微服务的八大原则" class="headerlink" title="产品级微服务的八大原则"></a>产品级微服务的八大原则</h1><p>原文链接： <a href="https://mp.weixin.qq.com/s?__biz=MzA5OTAyNzQ2OA==&amp;mid=2649692258&amp;idx=1&amp;sn=f3050ff5910e2b1bc0aa91bd31b71a4a&amp;chksm=88932701bfe4ae1764defc15b7e706bf5efda626e8698aa9af6f53226f49d2067e757d0cc770&amp;mpshare=1&amp;scene=1&amp;srcid=1023My921kdaEUqWyTzwGNnd&amp;pass_ticket=0eH55AoGxZ4UW8ms36dlxd1aWA3lcLKJfSfVd%2B5emqc%3D#rd">mp.weixin.qq.com</a></p><p>虽然微服务架构给开发者带来很大的自由，但是确保服务的可用性却要求对微服务进行很好的架构，运维以及组织标准。</p><p>O’Reilly这本免费的电子书《Microservices in Production》介绍了微服务标准化的挑战，以可用性作为微服务标准化的目标，提出了八个标准化微服务的原则，包括在整个工程组织中实现production-readiness标准的策略。</p><h1 id="深度学习推理部署优化实践"><a href="#深度学习推理部署优化实践" class="headerlink" title="深度学习推理部署优化实践"></a>深度学习推理部署优化实践</h1><p>在提供服务时，需要处理接收外部输入、返回预测结果，并做一些业务逻辑相关的处理，需要引入一个处理中心，这个处理中心通常是web框架 如 flask、tornado、Django等，其作用是搭建http服务，将外部输入传给模型，将模型预测结果返回。</p><p><strong>深度学习推理服务的性能指标有哪些？</strong></p><ul><li>响应延时（latency）</li><li>吞吐量（throughput），及</li><li>模型精度（accuracy）</li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/c3ffe664-c506-45cd-b16f-4f8cf04d85f6/1545646984398.png" alt="img" style="zoom:50%;" /></p><p><img src="https://res.ailemon.net/blog/2020/20201109-2.png?x-oss-process=style/ailemon-blog-watermark-pic" alt="img"></p><p><strong>方案总结</strong></p><p><a href="https://zhuanlan.zhihu.com/p/103693973">深度学习模型部署的那些事儿</a></p><p>现在来总结一下上边的实验，先看一下以上三种方案的流程图（每个黑色框代表一台机器）：</p><p><img src="https://pic2.zhimg.com/80/v2-3a1f997aaaaf93e8fad74b66259988f9_1440w.jpg" alt="img"></p><p>然后来看一下方案分别的耗时：</p><p><img src="https://pic3.zhimg.com/80/v2-4d98027c7ae9d0afdd181607c015b7ce_1440w.jpg" alt="img" style="zoom:50%;" /></p><p>首先要说的是，能上GPU一定要上GPU！<strong>钞能力</strong>节省非常多的时间和精力。然后我们挨个方案来说说优缺点。</p><p>第一套方案就是简单地将测试模式改造成连续预测模式，通过web框架进行转发和包装。</p><p>好处：修改简单易于上手。</p><p>坏处：</p><ol><li>推理性能为三种方案之内最慢的。</li><li>推理服务器需要有python环境。</li></ol><p>第二套方案利用TF-Serving进行部署，TF-Serving自动会进行部署和端口监听。</p><p>好处：</p><ol><li>速度较方案一快。</li><li>不需要python环境。</li></ol><p>坏处：</p><ol><li>安装较麻烦，最好使用docker。</li><li>输入输出都需要转换和处理。</li></ol><p>第三套方案是在方案二上的优化，将耗费资源较多的部分放到性能较好的机器上，可以作为公共资源给多个小网络共同使用。</p><p>好处（其他同方案二）：</p><ol><li>速度最快。</li></ol><p>坏处（其他同方案二）：</p><ol><li>需要有性能好的机器存放耗资源的网络，这块需要协调。</li><li>多一次网络通信，需要承受网络波动的影响，如果TF-Serving和GPU机器属于不同网络环境则更为麻烦，需要借助VPN等手段。</li><li>每个对外服务调用模型需要配置专门的逻辑：从GPU服务器取Embedding，作为输入给TF-Serving，才能得到TF-Serving的输出。</li></ol><p>纵观所有方案，发现其实我们可以做一些小融合，以方案三（或方案二）为基础，结合方案一的包装，来去除TF-Serving输入输出需要转换，以及方案三中每个服务需要配置专门逻辑的缺点，流程如下图：</p><p><img src="https://pic4.zhimg.com/80/v2-b133ab83cb6535948f23ca41ab1a45ab_1440w.jpg" alt="img"></p><p>以方案三为例，在TF-Serving的服务器上再增加一层中介，主要做输入输出的转换，再承担中转的作用。</p><p>当外部输入到来时，对外服务接收请求，处理后传给GPU机器，得到embedding，而后将embedding传给TF-Serving的模型，得到预测结果后转换成外部需要的格式，最后打包结果返回给其他服务。</p><p>这样，方案四既拥有了方案三速度最快的优点，也避免了TF-Serving需要做输出输出转换，以及在TF-Serving与GPU Embedding服务器中来回跑的缺点。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://www.huaxiaozhuan.com/深度学习/chapters/13_practical.html">一个优秀机器学习实践者的素养</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>项目管理</tag>
      
      <tag>工程实践</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HuggingFace-Transformers手册</title>
    <link href="/2020/10/13/2020-10-13-HuggingFace-Transformers%E6%89%8B%E5%86%8C/"/>
    <url>/2020/10/13/2020-10-13-HuggingFace-Transformers%E6%89%8B%E5%86%8C/</url>
    
    <content type="html"><![CDATA[<h3 id="HuggingFace-Transformers手册"><a href="#HuggingFace-Transformers手册" class="headerlink" title="HuggingFace-Transformers手册"></a>HuggingFace-Transformers手册</h3><p>Transformers（以前称为pytorch Transformers和pytorch pretrained bert）为自然语言理解（NLU）和自然语言生成（NLG）提供了最先进的通用架构（bert、GPT-2、RoBERTa、XLM、DistilBert、XLNet、CTRL…），其中有超过32个100多种语言的预训练模型并同时支持TensorFlow 2.0和Pythorch两大深度学习框架。</p> <span id="more"></span><h2 id="设计结构"><a href="#设计结构" class="headerlink" title="设计结构"></a>设计结构</h2><p>预训练模型（TFPreTrainedModel）、模型配置（PretrainedConfig）、分词器（PreTrainedTokenizer）就是整个HuggingFace-Transformers的核心，它们分别负责模型结构和权重、模型的超参数、模型的输入预处理。<br><img src="https://yuanxiaosc.github.io/2019/12/30/HuggingFace-Transformers%E6%89%8B%E5%86%8C/HuggingFace-Transformers.svg" alt="img"></p><h2 id="使用教程"><a href="#使用教程" class="headerlink" title="使用教程"></a>使用教程</h2><h3 id="1、如何安装"><a href="#1、如何安装" class="headerlink" title="1、如何安装"></a>1、如何安装</h3><p>transformers的安装十分简单，通过pip命令即可</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> transformers<br></code></pre></td></tr></table></figure><h3 id="2、如何使用"><a href="#2、如何使用" class="headerlink" title="2、如何使用"></a>2、如何使用</h3><p>使用transformers前需要下载好pytorch(版本&gt;=1.0)或者tensorflow2.0。下面以pytorch为例，来演示使用方法</p><p>1、若要导入所有包可以输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> *<br></code></pre></td></tr></table></figure><p>2、若要导入指定的包可以输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel<br></code></pre></td></tr></table></figure><p>3、加载预训练权重和词表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">UNCASED = <span class="hljs-string">&#x27;./bert-base-uncased&#x27;</span><br>bert = BertModel.from_pretrained(UNCASED)<br></code></pre></td></tr></table></figure><p>注意：<strong>加载预训练权重时需要下载好预训练的权重文件（pytorch版本），一般来说，当缓存文件中没有所需文件时(第一次使用)，只要网络没有问题，就会自动下载。当网络出现问题的时候，就需要手动下载预训练权重了。</strong></p><p>当缓存中不存在所需文件时，一般会出现提示：<br>bert-base-uncased-pytorch_model.bin not found in cache</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://yuanxiaosc.github.io/2019/12/30/HuggingFace-Transformers手册/">https://yuanxiaosc.github.io/2019/12/30/HuggingFace-Transformers%E6%89%8B%E5%86%8C/</a></p><p><a href="https://www.cnblogs.com/lian1995/p/11947522.html">https://www.cnblogs.com/lian1995/p/11947522.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CQA调研——学术界-转载</title>
    <link href="/2020/09/16/2020-09-16-CQA%E8%B0%83%E7%A0%94-%E5%AD%A6%E6%9C%AF%E7%95%8C/"/>
    <url>/2020/09/16/2020-09-16-CQA%E8%B0%83%E7%A0%94-%E5%AD%A6%E6%9C%AF%E7%95%8C/</url>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#1-任务">1. 任务</a><ul><li><a href="#11-背景">1.1. 背景</a></li><li><a href="#12-任务定义">1.2. 任务定义</a></li><li><a href="#13-评测标准">1.3. 评测标准</a></li><li><a href="#14-数据集">1.4. 数据集</a></li></ul></li><li><a href="#2-方法总结">2. 方法总结</a><ul><li><a href="#21-基于词频的方法">2.1. 基于词频的方法</a></li><li><a href="#22-基于语义的方法">2.2. 基于语义的方法</a><ul><li><a href="#221-基于表示的方法">2.2.1. 基于表示的方法</a></li><li><a href="#222-基于比较的方法">2.2.2. 基于比较的方法</a></li></ul></li><li><a href="#23-训练方法">2.3. 训练方法</a><ul><li><a href="#231-Pointwise方法">2.3.1 Pointwise方法</a></li><li><a href="#232-Pairwise方法">2.3.2 Pairwise方法</a></li><li><a href="#233-Listtwise方法">2.3.3 Listwise方法</a></li></ul></li></ul></li><li><p><a href="#3-paper-list">3. Paper List</a></p><ul><li><a href="#31-论文列表">3.1. 论文列表</a></li><li><a href="#32-论文解读">3.2. 论文解读</a></li></ul><span id="more"></span></li></ul><h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1. 任务"></a>1. 任务</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1. 背景"></a>1.1. 背景</h3><h4 id="1-1-1-智能问答系统"><a href="#1-1-1-智能问答系统" class="headerlink" title="1.1.1 智能问答系统"></a>1.1.1 智能问答系统</h4><ul><li>智能问答系统已经有70年的发展历史。早期的智能问答系统通常只接受特定形式的自然语言问句，而且可以供智能问答系统进行训练的数据也很少，所以无法进行基于大数据的开放领域的问答从而未被广泛使用。</li><li>进入九十年代之后，由于互联网的发展，大量可供训练的问答对在网上可以被搜集和找到。尤其是 TREC-QA评测的推出，极大推动促进了智能问答系统的发展。</li><li>目前，已经有很多智能问答系统产品问世。<ul><li>国外<ul><li>IBM研发的智能问答机器人Watson在美国智力竞赛节目《Jeopardy!》中战胜人了选手，其所拥有的DeepQA 系统集成了统计机器学习、信息抽取、知识库集成和知识推理等深层技术。</li><li>苹果公司的 Siri 系统和微软公司的cortana 分别在 iPhone 手机中和 Windows10 操作系统中都取得了很好的效果。</li></ul></li><li>国内，众多企业和研究团体也推出了很多以智能问答技术为核心的机器人<ul><li>百度公司的“度秘”</li><li>中科汇联公司的“爱客服”</li><li>阿里巴巴的“小蜜”客服</li></ul></li></ul></li><li>这些机器人不仅提供情感聊天的闲聊功能，而且还能提供私人秘书和智能客服这样的专业功能。这些智能系统的出现标志着智能问答技术正在走向成熟，预计未来还会有更多功能的机器人问世和解决用户的各种需求。  <div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/智能问答历史.png" width = 60% ></div>  </li></ul><h4 id="1-1-2-社区问答"><a href="#1-1-2-社区问答" class="headerlink" title="1.1.2 社区问答"></a>1.1.2 社区问答</h4><ul><li>根据系统处理的数据格式，问答系统又可以分为：基于结构化数据的问答系统、基于自由文本数据的问答系统、基于问题答案对数据的问答系统。这里我们主要考虑基于问题答案对的问答系统。基于问答对的问答系统通常可以分为两种：  <ul><li>一种是利用各个公司或者组织在网站上提供的常用问题列表(FAQ)FAQ具有限定领域、质量高、组织好等优点，使得系统回答问题的水平大大提高，但FAQ的获取成本高，这个缺点又制约了基于FAQ的问答系统的应用范围。</li><li>另一种便是利用问答社区中用户自己产生的问答对的社区问答系统（Community Question Answering,CQA）。随着问答社区的兴起，如 Yahoo!Answers， Quora，StackOverflow，百度知道和知乎等。</li></ul></li><li>通过使用社区问答系统，人们不但可以发布问题进行提问以满足自己的信息需求，而且还可以回答其他用户提问的问题来分享自己的知识，让用户所拥有的隐性知识转化成显性知识。</li></ul><h3 id="1-2-任务定义"><a href="#1-2-任务定义" class="headerlink" title="1.2. 任务定义"></a>1.2. 任务定义</h3><h4 id="1-2-1-问题-问题匹配"><a href="#1-2-1-问题-问题匹配" class="headerlink" title="1.2.1 问题-问题匹配:"></a>1.2.1 问题-问题匹配:</h4><ul><li><p><strong>提出背景</strong>  ：社区问答网站中的问题，通常越来越多是重复问题。检测这些问题有以下几个原因：</p><ol><li>它会减少冗余; 即如果一个人回答了这个问题一次，他不需要再回答。  </li><li>如果第一个问题有很多答案，并且询问其相似问题，那么答案可以返回给提问者，节省了时间，提升了用户体验。  </li></ol></li><li><p><strong>形式化的定义</strong>：给定一个问题（后文我们称之为查询）和一个候选问题（后文称为文档）集合,返回根据与查询问题相似性排序的序列  </p></li></ul><h4 id="1-2-2-问题-答案匹配："><a href="#1-2-2-问题-答案匹配：" class="headerlink" title="1.2.2 问题-答案匹配："></a>1.2.2 问题-答案匹配：</h4><ul><li><strong>提出背景</strong>  ：考虑到社区问答网站接收的流量，在发布的众多答案中找到一个好答案的任务本身就是重要的。  </li><li><strong>形式化定义</strong>  ：给定问题q和候选答案集合，然后试着找到最好的候选答案或者每个答案根据与问题相关性排序的列表。候选答案池可能包含也可能不包含多个gold标签。  </li></ul><p>由此可见，社区问答的重点问题是计算文本和文本之间的相似性和相关性的问题。</p><h3 id="1-3-评测标准"><a href="#1-3-评测标准" class="headerlink" title="1.3. 评测标准"></a>1.3. 评测标准</h3><ul><li>ACC:判断两个文档是否相似的准确率</li><li>P@1:判断排序第一的答案是否正确</li><li>MAP(Mean Average Precision): MAP是用来评测整个排序的质量的评测指标。计算方法如下：<br><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/MAP1.svg" alt="image"><br>其中R表示相关文档的总个数，position(r)表示，结果列表从前往后看，第r个相关文档在列表中的位置。比如，有三个相关文档，位置分别为1、3、6，那么AveP=1/3 * (1/1+2/3+3/6)。<br>MAP的最终得分为所有查询的AveP的平均值  </li><li>MRR(Mean Reciprocal Rank)：仅仅考虑排名最高的正确答案在排序中的位置。将排名最高的标准答案在被评价系统给出结果中的排序取倒数作为一个查询的准确度，再对所有的查询取平均。计算公式如下，其中ranki为第i个查询排序最高的正确答案的位置<br><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/MRR.svg" alt="image">  </li></ul><h3 id="1-4-数据集"><a href="#1-4-数据集" class="headerlink" title="1.4. 数据集"></a>1.4. 数据集</h3><h4 id="QQ匹配"><a href="#QQ匹配" class="headerlink" title="QQ匹配:"></a>QQ匹配:</h4><ul><li><a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs"><strong>Quora Question Pairs</strong></a>:<br>Quora Question Pairs数据集是社区问答网站Quora中问题对的集合。任务是确定一对问题在语义上是否等效，每个问题对都用1/0标签标注是否为重复问题。QQP中的数据分布并不不平衡（负63％），因此使用F1和ACC作为评测指标。其训练集包括364K条数据，训练集包括391k条数据。</li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>ACC</th><th>论文题目</th><th>年份</th><th>论文链接</th><th>code</th></tr></thead><tbody><tr><td>ALBERT</td><td>90.5%</td><td>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</td><td>2020</td><td><a href="https://arxiv.org/pdf/1909.11942v6.pdf">https://arxiv.org/pdf/1909.11942v6.pdf</a></td><td><a href="https://github.com/google-research/ALBERT">https://github.com/google-research/ALBERT</a></td></tr><tr><td>T5-11B</td><td>90.4%</td><td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</td><td>2019</td><td><a href="https://arxiv.org/pdf/1910.10683v2.pdf">https://arxiv.org/pdf/1910.10683v2.pdf</a></td><td><a href="https://github.com/google-research/text-to-text-transfer-transformer">https://github.com/google-research/text-to-text-transfer-transformer</a></td></tr><tr><td>XLNet</td><td>90.3%</td><td>XLNet: Generalized Autoregressive Pretraining for Language Understanding</td><td>2019</td><td><a href="https://arxiv.org/pdf/1906.08237v2.pdf">https://arxiv.org/pdf/1906.08237v2.pdf</a></td><td><a href="https://github.com/zihangdai/xlnet">https://github.com/zihangdai/xlnet</a></td></tr></tbody></table></div><ul><li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52398&amp;from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F607d14d9-20cd-47e3-85bc-a2f65cd28042%2Fdefault.aspx"><strong>MRPC</strong></a>：<br>Microsoft Research Paraphrase语料库微软构建的从在线新闻源中自动提取的句子对的语料库，并带有人工注释，说明句子中的句子在语义上是否等效，和Quora Question Pairs数据集相同，每条数据用1/0标注语义是否等效。由于类别不平衡（68％积极），因此使用ACC和F1作为评测指标。该数据集共有3.7k条正例数据，1.7k条负例数据 </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>ACC</th><th>论文题目</th><th>年份</th><th>论文链接</th><th>code</th></tr></thead><tbody><tr><td>ALBERT</td><td>94.0%</td><td>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</td><td>2020</td><td><a href="https://arxiv.org/pdf/1909.11942v6.pdf">https://arxiv.org/pdf/1909.11942v6.pdf</a></td><td><a href="https://github.com/google-research/ALBERT">https://github.com/google-research/ALBERT</a></td></tr><tr><td>StructBERT</td><td>93.9%</td><td>StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</td><td>2019</td><td><a href="https://arxiv.org/abs/1908.04577">https://arxiv.org/abs/1908.04577</a></td><td>-</td></tr><tr><td>ERNIE2.0</td><td>93.5%</td><td>ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</td><td>2019</td><td><a href="https://arxiv.org/abs/1907.12412v1">https://arxiv.org/abs/1907.12412v1</a></td><td><a href="https://github.com/PaddlePaddle/ERNIE">https://github.com/PaddlePaddle/ERNIE</a></td></tr></tbody></table></div><ul><li><a href="http://icrc.hitsz.edu.cn/info/1037/1146.htm"><strong>LCQMC</strong></a>:<br>百度发布的一个大型中文问题匹配数据集，数据来自百度知道。每条数据为两个问题和它们的相似性标签（用1/0代表相似/不相似)。 数据集包含260,068个具有手动注释的问题对，作者将其分为三个部分，即包含238,766个问题对的训练集，包含8,802个问题对的验证集和包含12,500个问题对的测试集。</li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>ACC</th><th>论文题目</th><th>年份</th><th>论文链接</th><th>code</th></tr></thead><tbody><tr><td>ERNIE2.0</td><td>87.9%</td><td>ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</td><td>2019</td><td><a href="https://arxiv.org/abs/1907.12412v1">https://arxiv.org/abs/1907.12412v1</a></td><td><a href="https://github.com/PaddlePaddle/ERNIE">https://github.com/PaddlePaddle/ERNIE</a></td></tr><tr><td>ERNIE1.0</td><td>87.4%</td><td>ERNIE: Enhanced Representation through Knowledge Integration</td><td><a href="https://arxiv.org/abs/1904.09223">https://arxiv.org/abs/1904.09223</a></td><td><a href="https://github.com/PaddlePaddle/ERNIE">https://github.com/PaddlePaddle/ERNIE</a></td></tr><tr><td>BERT</td><td>87.0%</td><td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td><td><a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></td><td><a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></td></tr></tbody></table></div><h4 id="QA匹配"><a href="#QA匹配" class="headerlink" title="QA匹配:"></a>QA匹配:</h4><ul><li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52419"><strong>WikiQA</strong></a> :<br>Wikiqa是一个答案选择数据集，由Bing查询日志中的问题和从Wikipedia中提取的候选答案句子构成，然后对其进行手动标记。有些问题没有正确的答案句子或只有正确的答案句子。训练集合共有2118条问题，20360条答案。验证集合共有296条问题，2733条答案，测试集合共有633条问题，6165条答案。  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>MAP</th><th>MRR</th><th>论文题目</th><th>年份</th><th>论文链接</th><th>code</th></tr></thead><tbody><tr><td>TANDA-ROberta</td><td>0.920</td><td>0.933</td><td>TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection</td><td>2019</td><td><a href="https://arxiv.org/pdf/1911.04118.pdf">https://arxiv.org/pdf/1911.04118.pdf</a></td><td><a href="https://github.com/alexa/wqa_tanda">https://github.com/alexa/wqa_tanda</a></td></tr><tr><td>Comp-Clip + LM + LC</td><td>0.764</td><td>0.784</td><td>A Compare-Aggregate Model with Latent Clustering for Answer Selection</td><td>2019</td><td><a href="https://paperswithcode.com/paper/a-compare-aggregate-model-with-latent">https://paperswithcode.com/paper/a-compare-aggregate-model-with-latent</a></td><td>-</td></tr><tr><td>RE2</td><td>0.7452</td><td>0.7618</td><td>Simple and Effective Text Matching with Richer Alignment Features</td><td>2019</td><td><a href="https://www.aclweb.org/anthology/P19-1465/">https://www.aclweb.org/anthology/P19-1465/</a></td><td><a href="https://github.com/alibaba-edu/simple-effective-text-matching">https://github.com/alibaba-edu/simple-effective-text-matching</a></td></tr></tbody></table></div><ul><li><a href="https://trec.nist.gov/data/qa.html"><strong>TRECQA</strong></a>：<br>该数据集是答案句子选择使用最广泛的基准之一。从TRECQA8-13的数据中搜集整理，从每个问题的文档库中自动选择候选答案。包含用于训练的5300个QA对和用于验证和测试的1100/1500个QA对。  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>MAP</th><th>MRR</th><th>论文题目</th><th>年份</th><th>论文链接</th><th>code</th></tr></thead><tbody><tr><td>TANDA-ROberta</td><td>0.943</td><td>0.974</td><td>TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection</td><td>2019</td><td><a href="https://arxiv.org/pdf/1911.04118.pdf">https://arxiv.org/pdf/1911.04118.pdf</a></td><td><a href="https://github.com/alexa/wqa_tanda">https://github.com/alexa/wqa_tanda</a></td></tr><tr><td>BERT-RNN</td><td>0.872</td><td>0.899</td><td>BAS: An Answer Selection Method Using BERT Language Model</td><td>2019</td><td><a href="https://arxiv.org/ftp/arxiv/papers/1911/1911.01528.pdf">https://arxiv.org/ftp/arxiv/papers/1911/1911.01528.pdf</a></td><td>-</td></tr><tr><td>Comp-Clip + LM + LC</td><td>0.868</td><td>0.928</td><td>A Compare-Aggregate Model with Latent Clustering for Answer Selection</td><td>2019</td><td><a href="https://paperswithcode.com/paper/a-compare-aggregate-model-with-latent">https://paperswithcode.com/paper/a-compare-aggregate-model-with-latent</a></td><td>-</td></tr></tbody></table></div><ul><li><a href="https://gluebenchmark.com/tasks"><strong>QNLI</strong></a>：<br>SQuAD数据集的修改版本，允许进行答案选择任务。SQuAD中的上下文段落被分成句子，每个句子都与问题配对。当句子包含答案时，将为问题句子对提供真正的标签。有86,308 / 10,385个问题和428,998 / 169,435个问题/答案对。  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>ACC</th><th>论文题目</th><th>年份</th><th>论文链接</th><th>code</th></tr></thead><tbody><tr><td>ALBERT</td><td>99.2%</td><td>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</td><td>2020</td><td><a href="https://arxiv.org/pdf/1909.11942v6.pdf">https://arxiv.org/pdf/1909.11942v6.pdf</a></td><td><a href="https://github.com/google-research/ALBERT">https://github.com/google-research/ALBERT</a></td></tr><tr><td>Roberta</td><td>98.9%</td><td>RoBERTa: A Robustly Optimized BERT Pretraining Approach</td><td>2019</td><td><a href="https://arxiv.org/pdf/1907.11692v1.pdf">https://arxiv.org/pdf/1907.11692v1.pdf</a></td><td><a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></td></tr><tr><td>XLNet</td><td>98.6%</td><td>XLNet: Generalized Autoregressive Pretraining for Language Understanding</td><td>2019</td><td><a href="https://arxiv.org/pdf/1906.08237v2.pdf">https://arxiv.org/pdf/1906.08237v2.pdf</a></td><td><a href="https://github.com/zihangdai/xlnet">https://github.com/zihangdai/xlnet</a></td></tr></tbody></table></div><!--### 1.5 Learnboard| 数据集  | stoa |论文题目|年份|论文链接|code|| ------------- | ------------- |------------- |------------- |------------- |------------- ||Quora pairs|90.5(ACC)   |ALBERT: A Lite BERT for Self-supervised Learning of Language Representations   | 2020|https://arxiv.org/pdf/1909.11942v6.pdf |https://github.com/google-research/ALBERT ||MRPC|91.9(ACC) |StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding|2019|https://arxiv.org/abs/1908.04577 | - ||LCQMRC|87.9(ACC)|ERNIE 2.0: A Continual Pre-training Framework for Language Understanding|2019|https://arxiv.org/abs/1907.12412v1   |https://github.com/PaddlePaddle/ERNIE ||Wikiqa|92.0(MAP)|TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection  |2019|https://arxiv.org/pdf/1911.04118.pdf |https://github.com/alexa/wqa_tanda ||Trecqa|94.3(MAP)|TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection|2019|https://arxiv.org/pdf/1911.04118.pdf |https://github.com/alexa/wqa_tanda ||QNLI|99.2（ACC)|ALBERT: A Lite BERT for Self-supervised Learning of Language Representations|2020|https://arxiv.org/pdf/1909.11942v6.pdf |https://github.com/google-research/ALBERT |--><h2 id="2-方法总结"><a href="#2-方法总结" class="headerlink" title="2. 方法总结"></a>2. 方法总结</h2><p>可以初步划分为两类，基于词频的方法，通常是一些较为传统的方法，以及基于语义的方法，通常是基于机器学习的方法。</p><h3 id="2-1-基于词频的方法"><a href="#2-1-基于词频的方法" class="headerlink" title="2.1. 基于词频的方法"></a>2.1. 基于词频的方法</h3><p>在机器学习出现之前，传统文本匹配方法通常是根据句子中的词频信息进行检索的，如信息检索中的TF-IDF,BM25，语言模型等方法，主要解决字面相似度问题。这些方法由于计算简单，适用范围广，到现在依旧是很多场景下的优秀基准模型。</p><h4 id="2-1-1-TF-IDF介绍"><a href="#2-1-1-TF-IDF介绍" class="headerlink" title="2.1.1 TF-IDF介绍"></a>2.1.1 TF-IDF介绍</h4><p><strong>TF-IDF</strong>（term frequency–inverse document frequency 是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文档集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。<br><strong>TF</strong>：在一份给定的文件里，词频（term frequency，TF）指的是某一个给定的词语在该文件中出现的次数。对于在某一特定文件里的词语ti来说，它的TF可表示为：</p><div align="center">TF = 某个词在文档中的出现次数/文档中的总词数</div>  <p><strong>IDF</strong> ：逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到：  </p><div align="center">IDF = log(语料库中的总文档数/语料库中出现该词的文档数)</div>  最终，TF-IDF=TF * IDF  #### 2.1.2 BM25介绍BM25算法是一种应用广泛的对TF-IDF的改进算法，解决了TF-IDF偏向于长文档的问题。传统的TF值理论上是可以无限大的。而BM25与之不同，它在TF计算方法中增加了一个常量k，用来限制TF值的增长极限。  BM25还引入了平均文档长度的概念，单个文档长度对相关性的影响力与它和平均长度的比值有关系引入另外两个参数：L和b。L是文档长度与平均长度的比值。如果文档长度是平均长度的2倍，则L＝2。b是一个常数，它的作用是规定L对评分的影响有多大。加了L和b的TF计算公式变为:  <div align="center">TF = ((k + 1) * tf) / (k * (1.0 - b + b * L) + tf)</div>  IDF部分计算方法与TF-IDF中相同。  最终，BM25=TF * IDF  #### 2.1.3 统计语言模型介绍统计语言模型用于计算给定一个问题，另一个问题由其生成的概率。通过引入马尔可夫假设，我们可以认为一句话中每个单词出现的概率只与它前面n个词有关，整句的概率就是各个词出现概率的乘积。该模型被称为ngram语言模型。  统计语言模型通常对语料库的大小有着较强的要求，通常来说，随着n-gram模型中n的增加，模型对于概率的估计会更加准确，但是需要的数据量也会成大大增加，所以，常用的统计语言模型通常为2-gram模型或者one-gram模型。### 2.2 基于语义的方法目前，深度学习模型已经在社区问答领域得到了广泛的应用，由于深度模型考虑了问题与问题之间的语义信息，通常比传统的基于词频的模型能取得更好的效果。#### 2.2.1 基于表示的方法基于表示的方法已被用于文本匹配任务,包括语义相似性,重复问题检测,自然语言推理。下图显示了基于表示的方法的一般架构。- 输入句子的向量表示由编码器分别构建。两个输入句子对彼此表示的计算没有影响。  - 计算出句子的向量表示后，使用余弦相似度，逐元素运算或基于神经网络的组合等方法对编码的向量进行比较。  这种体系结构的优势在于，将相同的编码器应用于每个输入语句会使模型更小。另外，句子向量可以用于可视化，句子聚类和许多其他目的。  下面介绍来自《LSTM-BASED DEEP LEARNING MODELS FOR NONFACTOID ANSWER SELECTION》文章中一种基于表示的方法QA-LSTM。QA-LSTM模型采用双向长期短期记忆（biLSTM）网络和池化层来独立构建输入句子的分布式矢量表示。然后，该模型利用余弦相似度来衡量句子表示的距离。 主要可以分为单词表示层，句子表示层，相似度计算层三部分：  1. 单词表示层：该层的目标是将原始每个词的one-hot编码转换为d维的词向量编码，通常使用word2vec或者glove词向量  2. 句子表示层：模型采用双向长期短期记忆（biLSTM）网络和池化层来独立构建输入句子的向量表示。之后文章尝试了三种不同的方式来得到最终的句子向量表示：（1）最大池化（2）平均池化（3）两个方向上最后一个词的向量表示的拼接。通过试验，文章最终采用了最大池化的方法得到句子的向量表示  3. 相似度计算层利用两个句子向量的cosine相似度来得到最终的相似度得分  训练方法：loss的计算公式如下  L = max{0, M − cosine(q, a+) + cosine(q, a−)}其中a+为正确答案，a-为错误答案<div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/基于表示的方法.png" ></div>  <h4 id="2-2-2-基于比较的方法"><a href="#2-2-2-基于比较的方法" class="headerlink" title="2.2.2 基于比较的方法"></a>2.2.2 基于比较的方法</h4><p>基于比较的方法通常比较输入句子的较小单位（例如单词），然后将比较结果汇总（例如，通过CNN或RNN），以做出最终决定。与基于表示的方法相比，基于比较的方法可以捕获输入句子之间的更多交互功能，因此在对TrecQA等公共数据集进行评估时，通常具有更好的性能。下图显示了来自《Bilateral Multi-Perspective Matching for Natural Language Sentences》一个典型的基于比较的方法的模型。该模型包括以下五层。  </p><ol><li>单词表示层（Word Representation Layer）<br>该层的目标是用d维向量表示输入句子中的每个单词。BiMPM构造具有两个分量的d维向量：一个字符组成的嵌入和一个预先用GloVe或word2vec训练的词嵌入。  </li><li>上下文表示层（Contex Representation Layer）<br>该层的目标是为输入句子中的每个位置获取一个新的表示形式，该表示形式除了捕获该位置的单词以外，还捕获一些上下文信息。 BiMPM使用biLSTM生成上下文表示。  </li><li>匹配层（Matching Layer）<br>该层的目标是将一个句子的每个上下文表示与另一句子的所有上下文表示进行比较。该层的输出是两个匹配向量序列，其中每个匹配向量对应于一个句子的一个位置与另一个句子的所有位置的比较结果。  </li><li>聚合层（Aggregation Layer）<br>该层的目标是汇总来自上一层的比较结果。 BiMPM使用另一个BiLSTM将匹配向量的两个序列聚合为固定长度向量。  </li><li>预测层（Prediction Layer）<br>该层的目标是做出最终预测。 BiMPM使用两层前馈神经网络来消耗前一层的固定长度矢量，并应用softmax函数获得最终分数。  <div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/基于比较的方法.png" ></div>  </li></ol><h4 id="2-2-3-基于预训练的方法"><a href="#2-2-3-基于预训练的方法" class="headerlink" title="2.2.3 基于预训练的方法"></a>2.2.3 基于预训练的方法</h4><p>近年来，随着Bert等预训练模型的出现，由于其在大规模的语料库上进行过训练，所以能捕捉到更多的语义信息。近期社区问答领域效果最好的模型通常都采用了基于预训练的方法。这种方法通常将社区问答任务作为一个二分类任务（相似/不相似）来解决，通过[cls]标记将两个句子拼接作为模型的输入，输出为两者为相似的概率。<br>下面介绍《BERT: Pre-training of Deep Bidirectional Transformers for》中解决社区问答任务的方法:  </p><ol><li>问题的拼接：<br>首先将查询和每一个候选文档一起作为Bert模型的输入，开始加入[CLS]标记。查询和文档之间加入[SEP]标记。利用BPE算法等进行分词，得到Bert模型的输入特征向量。  </li><li>相似度计算：<br>将特征向量输入Bert后，经计算将得到BERT的输出（句子中每个词的向量表示），取[CLS]  标记的向量表示，通过一个单层或多层的线性神经网络，得到两个文档的相似度得分（相似的概率)。  <div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/基于预训练的方法.png" ></div>  </li></ol><h3 id="2-3-训练方法"><a href="#2-3-训练方法" class="headerlink" title="2.3 训练方法"></a>2.3 训练方法</h3><p>基于语义的方法的训练方法通常可以分为pointwise，pairwise，listwise三种  </p><h4 id="2-3-1-Pointwise方法"><a href="#2-3-1-Pointwise方法" class="headerlink" title="2.3.1 Pointwise方法"></a>2.3.1 Pointwise方法</h4><p>Pointwise方法是通过近似为回归问题解决排序问题，输入的单条样本为得分-文档，将每个查询-文档对的相关性得分作为标签(Pointwise的由来)，训练模型。预测时候对于指定输入，给出查询-文档对的相关性得分。  </p><h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><p>其框架具有以下特征：</p><ul><li>输入中样本是单个文档（和对应查询）构成的特征向量；</li><li>输出中样本是单个文档（和对应查询）的相关度；</li><li>假设空间中样本是打分函数；</li><li>损失函数评估单个文档的预测得分和真实得分之间差异。<br>该类方法可以进一步分成三类：基于回归的算法、基于分类的算法，基于有序回归的算法。下面详细介绍。</li></ul><ol><li>基于回归的算法<br>此时，输出空间包含的是实值相关度得分。采用传统的回归方法即可。  </li><li>基于分类的算法<br>此时，输出空间包含的是无序类别。<br>对于二分类，SVM、LR 等均可；对于多分类，提升树等均可。  </li><li>基于有序回归的算法<br>此时，输出空间包含的是有序类别。通常是找到一个打分函数，然后用一系列阈值对得分进行分割，得到有序类别。采用 PRanking、基于 margin的方法都可以。</li></ol><h5 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h5><ul><li>排序追求的是排序结果，并不要求精确打分，只要有相对打分即可。pointwise类方法并没有考虑同一个查询对应的候选文档间的内部依赖性。一方面，导致输入空间内的样本不是独立同分布的，违反了机器学习的基本假设。</li><li>另一方面，没有充分利用这种样本间的结构性。其次，当不同查询对应不同数量的docs时，整体loss将会被对应候选文档集合数量大的查询组所支配，前面说过应该每组查询都是等价的。损失函数也没有建模到预测排序中的位置信息。因此，损失函数可能无意的过多强调那些不重要的文档，即那些排序在后面对用户体验影响小的文档。  </li></ul><h5 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h5><p>Pointwise类算法也可以再改进，比如在 loss 中引入基于查询的正则化因子的RankCosine方法。</p><h4 id="2-3-2-Pairwise方法"><a href="#2-3-2-Pairwise方法" class="headerlink" title="2.3.2 Pairwise方法"></a>2.3.2 Pairwise方法</h4><p>Pairwise方法相较于Pointwise的方法，考虑了文档之间的相对位置关系。输入的单条样本为标签-文档对。对于一次查询的多个结果文档，组合任意两个文档形成文档对作为输入样本。对输入的一对文档对AB（Pairwise的由来），根据A相关性是否比B好，给出结果。对所有文档对进行计算，就可以得到一组偏序关系，从而构造文档全集的排序关系。该类方法的原理是对给定的文档全集S，降低排序中的逆序文档对的个数来降低排序错误，从而达到优化排序结果的目的。</p><h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><p>Pairwise类方法，其框架具有以下特征：</p><ul><li>输入空间中样本是（同一查询对应的）两个文档和对应查询构成的两个特征向量</li><li>输出空间中样本是样本的相对关系；</li><li>损失函数评估 doc pair 的预测 preference 和真实 preference 之间差异。<br>通常来说，Pairwise方法采用margin loss作为优化目标：<br><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/marginloss.svg" alt="image"></li></ul><h5 id="缺陷-1"><a href="#缺陷-1" class="headerlink" title="缺陷"></a>缺陷</h5><p>虽然Pairwise方法相较Pointwise方法多考虑了文档对间的相对顺序信息，但还是存在不少问题。</p><ul><li>许多评测指标考虑到整个排序结果的质量。那么转化成Pairwise时必定会损失掉一些更细粒度的相关度标注信息。  </li><li>文档对的数量将是候选文档数量的平方，从而pointwise类方法就存在的查询间文档数量的不平衡性将在Pairwise类方法中进一步放大。  </li><li>Pairwise类方法相对Pointwise方法对噪声标注更敏感，即一个错误标注会引起多个文档对标注错误。  </li><li>与Pointwise类方法相同，Pairwise类方法也没有考虑同一个查询对应的文档间的内部依赖性，即输入空间内的样本并不是独立同分布的，并且也没有充分利用这种样本间的结构性。  </li></ul><h5 id="改进-1"><a href="#改进-1" class="headerlink" title="改进"></a>改进</h5><p>Pairwise类方法也有一些尝试，去一定程度解决上述缺陷，比如：</p><ul><li>Multiple hyperplane ranker，主要针对前述第一个缺陷</li><li>magnitude-preserving ranking，主要针对前述第一个缺陷</li><li>IRSVM，主要针对前述第二个缺陷</li><li>采用 Sigmoid 进行改进的 pairwise 方法，主要针对前述第三个缺陷</li><li>P-norm push，主要针对前述第四个缺陷</li><li>Ordered weighted average ranking，主要针对前述第四个缺陷</li><li>LambdaRank，主要针对前述第四个缺陷</li><li>Sparse ranker，主要针对前述第四个缺陷</li></ul><h4 id="2-3-3-Listwise方法"><a href="#2-3-3-Listwise方法" class="headerlink" title="2.3.3 Listwise方法"></a>2.3.3 Listwise方法</h4><p>Pointwise类方法将训练集里每一个文档当做一个训练实例，Pairwise类方法将同一个査询的搜索结果里任意两个文档对作为一个训练实例，文档列表方法与上述两种方法都不同，ListWise类方法直接考虑整体序列，针对Ranking评价指标进行优化。比如常用的MAP, NDCG等。</p><h5 id="特点-2"><a href="#特点-2" class="headerlink" title="特点"></a>特点</h5><p>Listwise类方法，其框架具有以下特征：</p><ul><li>输入空间中样本是，同一查询对应的所有文档（与对应的查询构成的多个特征向量（列表）；</li><li>输出空间中样本是这些文档（和对应 query）的相关度排序列表或者排列；</li><li>假设空间中样本是多变量函数，对于文档集合得到其排列，实践中，通常是一个打分函数，根据打分函数对所有候选文档的打分进行排序得到文档集合相关度的排列；</li><li>损失函数分成两类，一类是直接和评价指标相关的，还有一类不是直接相关的。</li></ul><ol><li><p>直接基于评价指标的算法<br>直接取优化排序的评价指标，也算是Listwise类方法中最直观的方法。但这并不简单，因为很多评价指标都是离散不可微的，具体处理方式有这么几种：<br>1) 优化基于评价指标的ranking error的连续可微的近似，这种方法就可以直接应用已有的优化方法，如SoftRank，ApproximateRank，SmoothRank<br>2) 优化基于评价指标的 ranking error的连续可微的上界，如 SVM-MAP，SVM-NDCG，PermuRank<br>3) 使用可以优化非平滑目标函数的优化技术，如 AdaRank，RankGP</p></li><li><p>非直接基于评价指标的算法<br>这里，不再使用和评价指标相关的loss来优化模型，而是设计能衡量模型输出与真实排列之间差异的 loss，如此获得的模型在评价指标上也能获得不错的性能。如ListNet，ListMLE，StructRank，BoltzRank等。</p></li></ol><h5 id="缺陷-2"><a href="#缺陷-2" class="headerlink" title="缺陷"></a>缺陷</h5><p>Listwise 类相较 Pointwise、Pairwise类方法，解决了应该考虑整个排序质量的问题。<br>listwise 类存在的主要缺陷是：一些排序算法需要基于排列来计算 loss，从而使得训练复杂度较高，如 ListNet和 BoltzRank。此外，位置信息并没有在部分方法的loss中得到充分利用。</p><h3 id="3-1-论文列表"><a href="#3-1-论文列表" class="headerlink" title="3.1. 论文列表"></a>3.1. 论文列表</h3><div class="table-container"><table><thead><tr><th>会议/年份</th><th>论文</th><th>链接</th></tr></thead><tbody><tr><td>CIKM2013</td><td>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</td><td><a href="https://dl.acm.org/doi/10.1145/2505515.2505665">https://dl.acm.org/doi/10.1145/2505515.2505665</a></td></tr><tr><td>CIKM2016</td><td>A Deep Relevance Matching Model for Ad-hoc Retrieval</td><td><a href="https://arxiv.org/abs/1711.08611">https://arxiv.org/abs/1711.08611</a></td></tr><tr><td>ACL2017</td><td>Enhanced LSTM for Natural Language Inference</td><td><a href="https://arxiv.org/pdf/1609.06038.pdf">https://arxiv.org/pdf/1609.06038.pdf</a></td></tr><tr><td>KDD2018</td><td>Multi-Cast Attention Networks for Retrieval-based Question Answering and Response Prediction</td><td><a href="https://arxiv.org/pdf/1806.00778.pdf">https://arxiv.org/pdf/1806.00778.pdf</a></td></tr><tr><td>SIGIR2018</td><td>Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering</td><td><a href="https://arxiv.org/pdf/1807.01836">https://arxiv.org/pdf/1807.01836</a></td></tr><tr><td>SIGIR2018</td><td>Multihop Attention Networks for Question Answer Matching.</td><td><a href="https://dl.acm.org/doi/10.1145/3209978.3210009">https://dl.acm.org/doi/10.1145/3209978.3210009</a></td></tr><tr><td>SIGIR2018</td><td>Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs.</td><td><a href="https://dl.acm.org/doi/10.1145/3209978.3210081">https://dl.acm.org/doi/10.1145/3209978.3210081</a></td></tr><tr><td>WWW2018</td><td>Query Expansion with Neural Question-to-Answer Translation for FAQ-based Question Answering</td><td><a href="https://dl.acm.org/citation.cfm?id=3191537">https://dl.acm.org/citation.cfm?id=3191537</a></td></tr><tr><td>NAACL2018</td><td>Learning to Rank Question-Answer Pairs Using Hierarchical Recurrent Encoder with Latent Topic Clustering</td><td><a href="https://www.aclweb.org/anthology/N18-1142">https://www.aclweb.org/anthology/N18-1142</a></td></tr><tr><td>EMNLP2018</td><td>Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings</td><td><a href="https://arxiv.org/abs/1809.08928">https://arxiv.org/abs/1809.08928</a></td></tr><tr><td>CIKM2019</td><td>A Compare-Aggregate Model with Latent Clustering for Answer Selection</td><td><a href="https://arxiv.org/abs/1905.12897">https://arxiv.org/abs/1905.12897</a></td></tr><tr><td>WWW2019</td><td>A Hierarchical Attention Retrieval Model for Healthcare Question Answering</td><td><a href="http://dmkd.cs.vt.edu/papers/WWW19.pdf">http://dmkd.cs.vt.edu/papers/WWW19.pdf</a></td></tr><tr><td>IJCAI2019</td><td>Multiway Attention Networks for Modeling Sentences Pairs</td><td><a href="https://www.ijcai.org/Proceedings/2018/0613.pdf">https://www.ijcai.org/Proceedings/2018/0613.pdf</a></td></tr><tr><td>ICLR2019</td><td>GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING</td><td><a href="https://openreview.net/pdf?id=rJ4km2R5t7">https://openreview.net/pdf?id=rJ4km2R5t7</a></td></tr><tr><td>AAAI2019</td><td>DRr-Net: Dynamic Re-Read Network for Sentence Semantic Matching</td><td><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4734/4612">https://www.aaai.org/ojs/index.php/AAAI/article/view/4734/4612</a></td></tr><tr><td>SIGIR2019</td><td>FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance</td><td><a href="https://arxiv.org/pdf/1905.02851">https://arxiv.org/pdf/1905.02851</a></td></tr><tr><td>SIGIR2019</td><td>Adaptive Multi-Attention Network Incorporating Answer Information for Duplicate Question Detection</td><td><a href="http://qizhang.info/paper/sigir2019.duplicatequestiondetection.pdf">http://qizhang.info/paper/sigir2019.duplicatequestiondetection.pdf</a></td></tr><tr><td>ACL2019</td><td>Question Condensing Networks for Answer Selection in Community Question Answering</td><td><a href="https://www.aclweb.org/anthology/P18-1162.pdf">https://www.aclweb.org/anthology/P18-1162.pdf</a></td></tr><tr><td>ACL2019</td><td>Simple and Effective Text Matching with Richer Alignment Features</td><td><a href="https://www.aclweb.org/anthology/P19-1465/">https://www.aclweb.org/anthology/P19-1465/</a></td></tr><tr><td>AAAI2019</td><td>Adversarial Training for Community Question Answer Selection Based on Multi-Scale Matching</td><td><a href="https://arxiv.org/abs/1804.08058">https://arxiv.org/abs/1804.08058</a></td></tr><tr><td>WWW2019</td><td>Improved Cross-Lingual Question Retrieval for Community Question Answering</td><td><a href="https://dl.acm.org/citation.cfm?id=3313502">https://dl.acm.org/citation.cfm?id=3313502</a></td></tr><tr><td>NAACL2019</td><td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td><td><a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></td></tr><tr><td>NAACL2019</td><td>Alignment over Heterogeneous Embeddings for Question Answering</td><td><a href="https://www.aclweb.org/anthology/N19-1274/">https://www.aclweb.org/anthology/N19-1274/</a></td></tr><tr><td>ICLR2020</td><td>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</td><td><a href="https://arxiv.org/abs/1909.11942">https://arxiv.org/abs/1909.11942</a></td></tr><tr><td>AAAI2020</td><td>TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection</td><td><a href="https://arxiv.org/abs/1911.04118">https://arxiv.org/abs/1911.04118</a></td></tr><tr><td>AAAI2020</td><td>Attentive User-Engaged Adversarial Neural Network for Community Question Answering</td><td><a href="https://aaai.org/Papers/AAAI/2020GB/AAAI-XieY.3142.pdf">https://aaai.org/Papers/AAAI/2020GB/AAAI-XieY.3142.pdf</a></td></tr><tr><td>AAAI2020</td><td>Joint Learning of Answer Selection and Answer Summary Generation in Community Question Answering</td><td><a href="https://arxiv.org/abs/1911.09801">https://arxiv.org/abs/1911.09801</a></td></tr><tr><td>SIGIR2020</td><td>Reranking for Efficient Transformer-based Answer Selection</td><td><a href="https://www.amazon.science/publications/reranking-for-efficient-transformer-based-answer-selection">https://www.amazon.science/publications/reranking-for-efficient-transformer-based-answer-selection</a></td></tr><tr><td>SIGIR2020</td><td>Read, Attend, and Exclude: Multi-Choice Reading Comprehension by Mimicking Human Reasoning Process</td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401326">https://dl.acm.org/doi/pdf/10.1145/3397271.3401326</a></td></tr><tr><td>SIGIR2020</td><td>Match^2: A Matching over Matching Model for Similar Question Identification</td><td><a href="https://arxiv.org/abs/2006.11719">https://arxiv.org/abs/2006.11719</a></td></tr><tr><td>SIGIR2020</td><td>ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</td><td><a href="https://arxiv.org/abs/2004.12832">https://arxiv.org/abs/2004.12832</a></td></tr><tr><td>SIGIR2020</td><td>Finding the Best of Both Worlds: Faster and More Robust Top-k Document Retrieval</td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401076">https://dl.acm.org/doi/pdf/10.1145/3397271.3401076</a></td></tr><tr><td>SIGIR2020</td><td>Efficient Document Re-Ranking for Transformers by Precomputing Term Representations</td><td><a href="https://arxiv.org/abs/2004.14255">https://arxiv.org/abs/2004.14255</a></td></tr><tr><td>SIGIR2020</td><td>Improving Document-Level Text Retrieval using Local Attention in the Transformer-Kernel Pooling Model</td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401224">https://dl.acm.org/doi/pdf/10.1145/3397271.3401224</a></td></tr><tr><td>ACL2020</td><td>DoQA - Accessing Domain-Specific FAQs via Conversational</td><td><a href="https://arxiv.org/abs/2005.01328">https://arxiv.org/abs/2005.01328</a></td></tr><tr><td>ACL2020</td><td>Harvesting and Refining Question-Answer Pairs for Unsupervised QA</td><td><a href="https://arxiv.org/abs/2005.02925">https://arxiv.org/abs/2005.02925</a></td></tr><tr><td>ACL2020</td><td>Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport</td><td><a href="https://arxiv.org/abs/2005.13111">https://arxiv.org/abs/2005.13111</a></td></tr></tbody></table></div><h3 id="3-2-论文解读"><a href="#3-2-论文解读" class="headerlink" title="3.2. 论文解读"></a>3.2. 论文解读</h3><blockquote><p>《Convolutional Neural Network Architectures for Matching Natural Language Sentences》</p></blockquote><p><strong>介绍</strong><br>匹配模型需要对文本的表示以及它们之间的交互进行建模。之前的模型通常只考虑直接对查询和文档序列进行编码，并且没有考虑查询和文档间的交互作用。本论文针对这两个缺点，提取ARC-I和ARC-II两个模型。前者是基于表示的模型，利用CNN去提取文档特征再计算相似度。后者是基于匹配的模型，首先得到匹配矩阵再用CNN提取特征。<br><strong>模型</strong>  </p><ol><li>ARC-I<br>模型结构如下图所示<br><div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/ARC-I.jpg" ></div><br>比较经典的基于表示的匹配模型结构，对于查询和文档分别进行特征提取得到固定维度的向量，而后用MLP进行聚合和分类。因此重点是CNN的用法：</li></ol><ul><li>运用了多层卷积+pooling的方法</li><li>卷积操作采用窗口宽度为k1的卷积核，采用宽度为2的max-pooling提取特征，max-pooling可以提取最重要的特征，进而得到查询和文档的表示。</li><li>单层 CNN 可以捕捉相邻 Term 间得多种组合关系，即local的n-gram 特征。</li><li>虽然多层 CNN 的堆叠通过感受野的扩张可以得一定的全局信息，但对于序列信息还是不敏感。对语义依赖强的任务效果一般。</li></ul><ol><li>ARC-II<br><div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/ARC-II.jpg" ></div><br>ARC-II首先计算查询和文档的单词级别的相似度矩阵，先用1维卷积提取特征，而后用多层二维卷积 + 池化进行计算，最终输入 MLP进行分类。下面介绍卷积层的具体做法：</li></ol><ul><li>先构建矩阵，假设查询的长度为m，嵌入维度为H，文档长度为n，嵌入维度为H。则矩阵中每个元素是查询中的第i个词向量与 文档中第j个词向量进行拼接得到的向量。因此矩阵的维度是 [m, n, 2H] 。</li><li>用1维卷积进行扫描。通过这种方式即可以得到查询和文档间的匹配关系，还保留了语义和位置信息。</li><li>对得到的结果用2维卷积进行处理，池化。池化层的宽度也为2，之后得到最终的表示。</li></ul><p><strong>总结</strong><br>文章分别在三个任务上进行了实验，分别是(1)句子自动填充任务，(2)推文与评论的匹配，以及(3)同义句识别；总的来说，ARC-I和ARC-II相较于之前的模型均有较大的提升，而ARC-II的效果更为优秀。</p><blockquote><p>《DRr-Net: Dynamic Re-read Network for Sentence Semantic Matching》</p></blockquote><div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/dr-net.png" ></div>  **介绍**  语义匹配一直是一项十分重要的任务，目前，注意力机制大大提升了语义匹配的效果。不过过去的注意力机制通常是一次性关注所有关键词，而人类阅读过程中对关键词的注意往往是变化的。为此，本文提出了一种动态关注关键词的模型  **模型**  整个模型可以分为三个部分：输入，动态选择关注词，分类1. 输入（encode）初始输入为词向量拼接字符级别的词向量以及手工特征（pos，exact match），用一个简单的线性层作一个变换。之后将输入送入一个stack-gru中，即下一层的输入为上一层的输入拼接原始输入（类似残差网络）。  最终，通过一个self-attention将输出的加权和作为句子的表示，文中成为original represtion。2. 动态重读（ Dynamic Re-read Mechanism）利用一个注意力机制根据句子的表示，上一次选择的关键词，选择此次的关键词，送入一个gru学习。  3. 分类对原始表示，重读后的表示，分别拼接表示向量，element-wise的乘积与差，用一个线性层训练。并且动态加权。  **总结**  本文主要提出了一种动态注意力的方法，同时在SNLI，SICK，QUORA三个数据集上进行了实验，证明了动态注意力机制的有效性。>《Simple and Effective Text Matching with Richer Alignment Features》  <div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/re2.png" ></div>  <p><strong>介绍</strong><br>提出了一种简单的，不存在复杂特殊结构的文本匹配模型，主要通过point wise信息，上下文相关信息，和前一层提取的相关性信息的结合来表示文档的相关性。  </p><p><strong>模型</strong><br>模型为多层类似结构的组合，对两个输入句子采用完全对称的处理。每层的输入为上一层输出与原始embedding的拼接。每一层的输出部分要与上一层的输出相加，每层结构由embedding，encoder和fusion三部分族中。最终将最后一层的输出经过池化后利用predicter部分（一个多层的神经网络）计算最终结果。  </p><ol><li>embedding部分<br>采用glove作为初始embeeding，每层的输入为上一层输出与原始embedding的拼接。</li><li>encoder部分<br>文中采用了一个cnn作为encoder结构，将encoder输出与encoder的输入拼接，作为每个单词的表示，通过计算内积，得到两个句子attention相关的表示。</li><li>fusion部分<br>通过两个句子encoder输出的差，点积和拼接等，通过线性变换得到新的表示。  </li></ol><p><strong>总结</strong><br>本文充分结合了基于表示的模型和基于比较的模型的特点，没有选择去尝试复杂的神经网络结构，仅仅通过多层的简单结构，就在自然语言推理，意图识别，答案选择三种任务上取得了最好的效果。</p><blockquote><p>《TANDA: Transfer and Adapt Pre-Trained Transformer Models》</p></blockquote><div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/bert.png" ></div>  <p><strong>介绍</strong><br>这篇文章主要是通过利用预训练模型来解决答案选择任务。本文提出了一种用于自然语言任务的预训练变换模型精调的有效技术-TANDA( Transfer AND Adapt)。为解决答案选择问题的数据稀缺性问题和精调步骤的不稳定性提供了有效的解决方案。  </p><p><strong>模型</strong><br>本文的基础模型为Bert模型结构，在经典的任务中，一般只针对目标任务和域进行一次模型精调。对于答案选择任务，训练数据是由问题和答案组成的包含正负标签（答案是否正确回答了问题）的句子对。当训练样本数据较少时，完成答案选择任务的模型稳定性较差，此时在新任务中推广需要大量样本来精调大量的变压器参数。本文提出，将精调过程分为两个步骤：转移到任务，然后适应目标域。  </p><p><strong>总结</strong><br>首先，使用答案选择的大型通用数据集完成标准的精调处理。这个步骤应该将语言模型迁移到具体的答案选择任务。由于目标域的特殊性，所得到的模型在目标域的数据上无法达到最佳性能，此时采用第二个精调步骤使分类器适应目标域。<br>本文的工作实际上是将经典的精调（fine-tuning）过程拆成了两次，其中一次针对通用数据集，另一次针对目标数据集，此外，还专门构建了适用于答案选择任务的通用数据集ASNQ。本文在两个著名的实验基准库：WikiQA 和 TRECQA上进行实验，分别达到了92%和94.3%的MAP分数。本文还讨论了TANDA在受不同类型噪声影响的Alexa特定数据集中的实验，确认了TANDA在工业环境中的有效性。</p><blockquote><p>《ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS》</p></blockquote><p><strong>介绍</strong>  ：预训练模型通常通过增加模型大小来提升性能。但随着模型的规模越来越大，进一步增加模型大小将带来以下困难：(1)GPU/TPU内存不足(2)训练时间会更长(3)模型退化。<br>所以，为了解决上述这些问题，本文提出通过两种参数精简技术来降低内存消耗，并加快BERT的训练速度。此外，本文还引入一个自监督损失(self-supervised loss)，用于对句子连贯性(inter-sentence coherence)建模，并证明该损失函数能够提升多句子作为输入的下游任务的性能。本文所提出的模型ALBERT在 GLUE、RACE 和 SQuAD 这3个基准上都取得了新的SOTA结果，且参数量还少于 BERT-large。  </p><p><strong>模型</strong><br>本文主要提出了两种方法来减少bert模型的参数：</p><ol><li>嵌入参数因式分解   ALBERT采用了因式分解的方法来降低参数量，先将词汇表映射到低维参数空间E，再映射到高维参数空间H，使得参数量从O（V<em> H） 减少到了 O（V </em> E + E * H），当E远小于H是，能明显减少参数量。</li><li>跨层参数共享<br>共享encoder中每一层Transformer中的所有参数，之前一般采用只共享全连接层或只共享attention层，ALBERT则更直接全部共享，从实验结果看，全部共享的代价是可以接受的，同时共享权值带来了一定的训练难度，使得模型更鲁棒。<br>同时，为了进一步提升 ALBERT 的性能，本文提出一种基于语言连贯性的损失函数SOP（句子次序预测），正例为一篇文档中连续的两个句子，负例为将正例中的两个句子交换顺序。该任务比原始BERT中的NSP（下一句预测）任务更具挑战性。  </li></ol><p><strong>总结</strong><br>基于上述的这3个设计，ALBERT能够扩展为更大的版本，在参数量仍然小于BERT-large的同时，性能可以显著提升。本文在GLUE、SQuAD 和 RACE 这3个自然语言理解基准测试上都刷新了记录：在 RACE 上的准确率为 89.4%，在 GLUE 上的得分为 89.4，在 SQuAD2.0上的 F1 得分为 92.2。</p><div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/ALBERT.png" ></div>  <blockquote><p>《ERNIE 2.0: A Continual Pre-training Framework for Language Understanding》  </p></blockquote><div align="center"><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/ERNIE2.0.png" ></div>  <p><strong>介绍</strong><br>在ERNIE1.0中，通过将BERT中的随机masking改为实体或短语级别（entity or phrase）的masking，使得模型能够从中学习到更多句法语义知识，在许多中文任务上取得了SOTA。<br>ERNIE2.0是对ERNIE1.0的一种改进模型，它提出了一种基于持续学习的语义理解预训练框架，使用多任务学习增量式构建预训练任务。ERNIE2.0中，新构建的预训练任务类型可以无缝的加入训练框架，持续的进行语义理解学习。 通过新增的实体预测、句子因果关系判断、文章句子结构重建等语义任务，ERNIE2.0语义理解预训练模型从训练数据中获取了词法、句法、语义等多个维度的自然语言信息，极大地增强了通用语义表示能力。  </p><p><strong>模型</strong><br>ERNIE2.0构建了多个预训练任务，试图从 3 个层面去更好的理解训练语料中蕴含的信息：</p><ul><li>Word-aware Tasks: 词汇 (lexical) 级别信息的学习</li><li>Structure-aware Tasks: 语法 (syntactic) 级别信息的学习</li><li>Semantic-aware Tasks: 语义 (semantic) 级别信息的学习<br>同时，针对不同的 pre-training 任务，ERNIE2.0引入了Task Embedding来精细化地建模不同类型的任务。不同的任务用从0 到N的ID表示，每个ID代表了不同的预训练任务。  </li></ul><div class="table-container"><table><thead><tr><th>任务名称</th><th>任务详情</th></tr></thead><tbody><tr><td>Knowledge Masking</td><td>ERNIE 1.0 中已经引入的 phrase &amp; named entity 知识增强 masking 策略。相较于 sub-word masking, 该策略可以更好的捕捉输入样本局部和全局的语义信息。</td></tr><tr><td>Capitalization Prediction</td><td>针对英文首字母大写词汇（如 Apple）所包含的特殊语义信息,在英文 Pre-training 训练中构造了一个分类任务去学习该词汇是否为大写。</td></tr><tr><td>Token-Document Relation Prediction</td><td>针对一个 segment 中出现的词汇，去预测该词汇是否也在原文档的其他 segments 中出现。</td></tr><tr><td>Sentence Reordering</td><td>针对一个paragraph（包含M个segments），随机打乱segments的顺序，通过一个分类任务去预测打乱的顺序类别</td></tr><tr><td>Sentence Distance</td><td>通过一个 3 分类任务，去判断句对 (sentence pairs) 位置关系 (包含邻近句子、文档内非邻近句子、非同文档内句子 3 种类别)，更好的建模语义相关性。</td></tr><tr><td>Discourse Relation</td><td>通过判断句对 (sentence pairs) 间的修辞关系 (semantic &amp; rhetorical relation)，更好的学习句间语义。</td></tr><tr><td>IR Relevance</td><td>学习 IR 相关性弱监督信息，更好的建模句对相关性。</td></tr></tbody></table></div><p><strong>总结</strong><br>通过多任务训练，ERNIE2.0在中文训练集上取得了较大的提升，目前是许多中文任务效果最好的模型。</p><h2 id="4-相关资料"><a href="#4-相关资料" class="headerlink" title="4. 相关资料"></a>4. 相关资料</h2><p><a href="http://skyhigh233.com/blog/2018/04/26/cqa-intro/">社区问答之QA匹配问题探索</a><br><a href="https://www.jiqizhixin.com/articles/2019-02-19-10">社区问答系统精准匹配信息和人，满足你对获取知识的迫切需求</a><br><a href="https://zhuanlan.zhihu.com/p/37216096">Community Question Answer分享</a><br><a href="https://www.csail.mit.edu/research/community-question-answering">Community Question Answering | MIT CSAIL</a><br><a href="https://paperswithcode.com/task/question-answering">Question Answering-Paper with code</a><br><a href="https://www.cnblogs.com/szxspark/p/8424884.html">自动问答之《社区问答技术调查》</a><br><a href="https://gb-oversea-cnki-net.e2.buaa.edu.cn/KCMS/detail/detail.aspx?filename=1018813345.nh&amp;dbcode=CMFD&amp;dbname=CMFDREF">《基于社区问答的对话式问答系统研究与实现》</a><br><a href="https://zhuanlan.zhihu.com/p/98688910">2020问答系统（QA）最新论文、书籍、数据集、竞赛、课程资源分析</a><br><a href="https://github.com/NTMC-Community/awesome-neural-models-for-semantic-match">Awesome Neural Models for Semantic Match</a><br><a href="https://www.aclweb.org/anthology/C18-1181">《A Review on Deep Learning Techniques Applied to Answer Selection》</a><br><a href="https://www.cnblogs.com/shona/p/12021304.html">用BERT做语义相似度匹配任务：计算相似度的方式</a><br><a href="https://zhuanlan.zhihu.com/p/32829048">自然语言处理中N-Gram模型介绍</a><br><a href="https://my.oschina.net/stanleysun/blog/1617727">搜索中的权重度量利器: TF-IDF和BM25</a><br><a href="https://zhuanlan.zhihu.com/p/26539920">Learning to rank基本算法小结</a><br><a href="https://blog.csdn.net/ljp1919/article/details/101680220">文献阅读笔记-ALBERT ： A lite BERT for self-supervised learning of language representations</a><br><a href="https://www.ramlinbird.com/2019/08/06/ernie%E5%8F%8Aernie-2-0%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">ERNIE及ERNIE 2.0论文笔记</a><br><a href="https://panxiaoxie.cn/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/">论文笔记-Multi-cast Attention Networks</a><br><a href="http://pelhans.com/2019/10/30/text_matching/">文本匹配论文笔记</a><br><a href="https://zhuanlan.zhihu.com/p/88938220">常见文本相似度计算方法简介</a>  </p>]]></content>
    
    
    
    <tags>
      
      <tag>QA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CQA调研——工业界-转载</title>
    <link href="/2020/09/16/2020-09-16-CQA%E8%B0%83%E7%A0%94-%E5%B7%A5%E4%B8%9A%E7%95%8C/"/>
    <url>/2020/09/16/2020-09-16-CQA%E8%B0%83%E7%A0%94-%E5%B7%A5%E4%B8%9A%E7%95%8C/</url>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul><li><a href="#1-任务">1 任务</a><ul><li><a href="#11-任务定义">1.1 任务定义</a></li><li><a href="#12-任务分类">1.2 任务分类</a></li><li><a href="#13-评测标准">1.3 评测标准</a></li><li><a href="#14--数据集">1.4  数据集</a>  <ul><li><a href="#技术需求与技术成果项目之间关联度计算模型需求与成果匹配">“技术需求”与“技术成果”项目之间关联度计算模型（需求与成果匹配)</a>  </li><li><a href="#智能客服问题相似度算法设计第三届魔镜杯大赛">智能客服问题相似度算法设计——第三届魔镜杯大赛</a>  </li><li><a href="#ccks-2018-微众银行智能客服问句匹配大赛">CCKS 2018 微众银行智能客服问句匹配大赛</a>  </li><li><a href="#afqmc-蚂蚁金融语义相似度">AFQMC 蚂蚁金融语义相似度</a>  </li><li><a href="#oppo手机搜索排序query-title语义匹配数据集">OPPO手机搜索排序query-title语义匹配数据集</a>  </li><li><a href="#医疗问题相似度衡量竞赛数据集">医疗问题相似度衡量竞赛数据集</a>  </li></ul></li></ul></li><li><a href="#2-方法及模型">2 方法及模型</a><ul><li><a href="#21-无监督方法">2.1 无监督方法</a><ul><li><a href="#211-规则匹配">2.1.1 规则匹配</a></li><li><a href="#212-无监督文本表示">2.1.2 无监督文本表示</a></li><li><a href="#213-用于跨领域迁移学习方法">2.1.3 用于跨领域迁移学习方法</a></li></ul></li></ul></li><li><a href="#22-有监督匹配算法">2.2 有监督匹配算法</a><ul><li><a href="#221-基于意图识别的算法">2.2.1 基于意图识别的算法</a></li><li><a href="#222深度文本匹配模型">2.2.2深度文本匹配模型</a><ul><li><a href="#表示型模型">表示型模型</a><ul><li><a href="#siamese-networks模型">Siamese networks模型</a></li><li><a href="#dssｍ-模型">DSSＭ 模型</a></li><li><a href="#sentence-bert">Sentence Bert</a></li></ul></li><li><a href="#交互型模型">交互型模型</a><ul><li><a href="#matchpyramid模型">MatchPyramid模型</a></li><li><a href="#esim-enhanced-lstm">ESIM （Enhanced LSTM）</a></li></ul></li></ul></li><li><a href="#23-faq发现与优化">2.3 FAQ发现与优化</a><ul><li><a href="#faq发现">FAQ发现</a></li><li><a href="#faq答案优化">FAQ答案优化</a></li></ul></li></ul></li><li><a href="#3-产品案例">3 产品案例</a><ul><li><a href="#产品1-百度anyq--answer-your-questions">产品1 百度AnyQ—ANswer Your Questions</a></li><li><a href="#产品2-腾讯知文--结构化faq问答引擎">产品2:腾讯知文—结构化FAQ问答 引擎</a></li><li><a href="#产品3-阿里小蜜">产品3: 阿里小蜜</a></li></ul></li><li><a href="#4-总结">4 总结</a></li><li><p><a href="#5-相关资料">5 相关资料</a></p><span id="more"></span></li></ul><h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1 任务"></a>1 任务</h2><h3 id="1-1-任务定义"><a href="#1-1-任务定义" class="headerlink" title="1.1 任务定义"></a>1.1 任务定义</h3><p><strong>C</strong>ommunity <strong>Q</strong>uestion <strong>A</strong>nswer，中文名称是社区问答。是利用半结构化的数据（问答对形式）来回答用户的提问，其流程通常可以分为三部分。</p><ol><li><p>问题解析，对用户输入的问题进行分词，纠错等预处理步骤。</p></li><li><p>召回部分，利用信息检索引擎如Lucence等根据处理后的问题提取可能的候选问题。</p></li><li><p>排序部分，利用信息检索模型对召回的候选问题进行相似度排序，寻找到最相似的问题并返回给用户。</p></li></ol><h3 id="1-2-任务分类"><a href="#1-2-任务分类" class="headerlink" title="1.2 任务分类"></a>1.2 任务分类</h3><p>通常，根据应用场景的不同，可以将CQA任务分为两类：</p><ul><li>FAQ问答: 在智能客服的业务场景中，对于用户频繁会问到的业务知识类问题的自动解答（以下简称为FAQ）是一个非常关键的需求，可以说是智能客服最为核心的用户场景，可以最为显著地降低人工客服的数量与成本。这个场景中，知识通常是封闭的，而且变化较为缓慢，通常可以利用已有的客服回复记录提取出高质量的问答对作为知识库。</li><li>社区问答: 问答对来自于社区论坛中用户的提问和回答，较为容易获取，但是相对质量较低。而且通常是面向开放域的，知识变化与更新速度较快。</li></ul><h3 id="1-3-评测标准"><a href="#1-3-评测标准" class="headerlink" title="1.3 评测标准"></a>1.3 评测标准</h3><ul><li>查全率：用以评价系统对于潜在答案寻找的全面程度。例如：在回答的前30%中保证一定出现正确答案。</li><li>查准率：即准确率，top n个答案包含正确答案的概率。这一项与学术界一致。</li><li>问题解决率：与具体业务和应用场景紧密相关</li><li>用户满意度/答案满意度：一般对答案满意度的评价方式是在每一次交互后都设置一个评价，客户可以对每一次回答进行评价，评价该答案是否满意。但是这样的评价方式容易让客户厌烦，因为客户是来解决问题的，不是来评价知识库里面的答案是否该优化。</li><li>问题识别率/应答准确率：指智能客服机器人正确识别出客户的问题数量在所有问题数中的占比。目前业内评价智能机器人比较常用的指标之一。</li><li>问题预判准确率：指用户进入咨询后，智能客服机器人会对客户可能咨询的问题进行预判。如京东的问题预判，是通过其长期数据积累和模型给每个用户添加各种标签，可以提供更个性化和人性化的服务。例如，京东JIMI了解用户的性别、情绪类型、近期购买历史等。当用户开始交流时，就会猜到他可能要询问一个关于母婴商品的使用方法或是一个售后单的退款情况，这就是问题预判。如果预判准确的话，只需在几次甚至一次的交互中获得智能客服机器人专业的问题解答，从而缩短客户咨询时长。</li><li>意图识别准确率：要想解答用户的问题，机器人首先需要结合上下文环境，从用户提问中准确识别用户咨询的意图是什么，然后返回对应的答案。</li><li>拦截率：机器人代替人工解决的用户咨询比例</li><li>24H未转人工率：指客户咨询了智能机器人后的24H内是否有咨询人工客服</li></ul><h3 id="1-4-数据集"><a href="#1-4-数据集" class="headerlink" title="1.4  数据集"></a>1.4  数据集</h3><p>由于工业界的数据集通常来自其自身业务的记录，并不对外公开，故以下只举例介绍相关比赛中出现的数据集：</p><h5 id="“技术需求”与“技术成果”项目之间关联度计算模型（需求与成果匹配）"><a href="#“技术需求”与“技术成果”项目之间关联度计算模型（需求与成果匹配）" class="headerlink" title="“技术需求”与“技术成果”项目之间关联度计算模型（需求与成果匹配）"></a><a href="https://www.datafountain.cn/competitions/359">“技术需求”与“技术成果”项目之间关联度计算模型</a>（需求与成果匹配）</h5><ul><li><p><strong>任务目标</strong></p><ul><li>根据项目信息的文本含义，为供需双方提供关联度较高的对应信息（需求——成果智能匹配</li></ul></li><li><p><strong>数据来源</strong></p><ul><li>数据来自中国·河南开放创新暨跨国技术转移大会云服务平台（www.nttzzc.com）</li><li><strong>人工标注关联度的方法</strong>：从事技术转移工作的专职工作人员，阅读技术需求文本和技术成果文本，根据个人经验予以标注。关联度分为四个层级：强相关、较强相关、弱相关、无相关。</li></ul></li><li><p><strong>数据具体说明</strong>：<a href="https://www.datafountain.cn/competitions/359/datasets">https://www.datafountain.cn/competitions/359/datasets</a></p></li><li><p><strong>评价指标</strong>：使用MAE系数</p><ul><li>平均绝对差值是用来衡量模型预测结果对标准结果的接近程度一种衡量方法.MAE的值越小，说明预测数据与真实数据越接近。</li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/CQA-industry-MAE.png  width=200 alt=MAE公式></div><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/CQA-industry-score.png  width=180 alt=最终结果></div><ul><li>最终结果越接近1分数越高。</li></ul></li><li><p><strong>top1方案及结果</strong></p><ul><li>解决方案：<a href="https://www.sohu.com/a/363245873_787107">https://www.sohu.com/a/363245873_787107</a> </li><li>主要利用数据清洗、数据增广、孪生BERT模型</li></ul></li></ul><h5 id="平安医疗科技疾病问答迁移学习比赛（疾病问句匹配）"><a href="#平安医疗科技疾病问答迁移学习比赛（疾病问句匹配）" class="headerlink" title="平安医疗科技疾病问答迁移学习比赛（疾病问句匹配）"></a><a href="https://www.biendata.com/competition/chip2019/">平安医疗科技疾病问答迁移学习比赛</a>（疾病问句匹配）</h5><ul><li><strong>任务目标</strong><ul><li>针对中文的疾病问答数据，进行病种间的迁移学习。具体是给定来自5个不同病种的问句对，要求判定两个句子语义是否相同或者相近。简单描述是语义匹配问题</li></ul></li><li><strong>数据来源</strong><ul><li>所有语料来自互联网上患者真实的问题，并经过了筛选和人工的意图匹配标注。</li></ul></li><li><p><strong>数据分布及说明</strong></p><ul><li>具体说明：<a href="https://www.biendata.com/competition/chip2019/data/">https://www.biendata.com/competition/chip2019/data/</a></li><li>给参赛选手的文件由train.csv、dev.csv、test.csv三个文件构成<ul><li>训练集，包含2万对人工标注好的疾病问答数据，由5个病种构成，其中diabetes10000对，hypertension、hepatitis、aids、breast_cancer各2500对</li><li>验证集，包含10000对无label的疾病问答数据，由5个病种构成，各2000对</li><li>测试集，包含5万对人工标注好的疾病问答数据，其中只有部分数据供验证。</li></ul></li></ul></li><li><p><strong>评价指标</strong></p><ul><li>Precision、Recall、F1值</li></ul></li><li><strong>top1方案及结果</strong><ul><li>解决方案：<a href="https://zhuanlan.zhihu.com/p/97227793">https://zhuanlan.zhihu.com/p/97227793</a> </li><li>主要利用基于BERT与提升树模型的语义匹配方法</li><li>最高得分：0.88312</li></ul></li></ul><p>​    </p><h5 id="CAIL2019相似案例匹配大赛（法律文书匹配）"><a href="#CAIL2019相似案例匹配大赛（法律文书匹配）" class="headerlink" title="CAIL2019相似案例匹配大赛（法律文书匹配）"></a><a href="https://github.com/china-ai-law-challenge/CAIL2019/tree/master/scm">CAIL2019相似案例匹配大赛</a>（法律文书匹配）</h5><ul><li><strong>任务目标</strong><ul><li>“中国裁判文书网”公开的民间借贷相关法律文书，每组数据由三篇法律文书组成。文书主要为案件的事实描述部分，选手需要从两篇候选集文书中找到与询问文书案件性质更为相似的一篇文书。</li></ul></li><li><strong>数据具体说明</strong><ul><li>链接：同上述比赛链接</li><li>内容：对于每份数据，用三元组(A,B,C)来代表该组数据，其中A,B,C均对应某一篇文书。文书数据A与B的相似度总是大于A与C的相似度的，即sim(A,B)&gt;sim(A,C)</li><li>数据量：比赛第一阶段训练数据有500组三元文书，第二阶段有5102组训练数据，第三阶段为封闭式评测</li></ul></li><li><strong>评价指标</strong>：acc</li><li><strong>top1方案及结果</strong><ul><li>解决方案：<a href="https://www.leiphone.com/news/201910/Yf2J8ktyPE7lh4iR.html">https://www.leiphone.com/news/201910/Yf2J8ktyPE7lh4iR.html</a> </li><li>主要利用损失函数为 Triplet Loss 的 Rank 模型来解决三元组的相对相似的问题、只提取并采用三个文书文本特征、基于Bert的多模型离线的多模型融合、解决Triple Loss 过拟合</li><li>最高得分：71.88</li><li>代码：<a href="https://github.com/GuidoPaul/CAIL2019">https://github.com/GuidoPaul/CAIL2019</a></li></ul></li></ul><h5 id="智能客服问题相似度算法设计——第三届魔镜杯大赛"><a href="#智能客服问题相似度算法设计——第三届魔镜杯大赛" class="headerlink" title="智能客服问题相似度算法设计——第三届魔镜杯大赛"></a><a href="https://ai.ppdai.com/mirror/goToMirrorDetail?mirrorId=1">智能客服问题相似度算法设计——第三届魔镜杯大赛</a></h5><ul><li><p><strong>任务目标</strong></p><ul><li>计算客户提出问题与知识库问题的相似度</li></ul></li><li><p><strong>数据来源</strong></p><ul><li>智能客服聊天机器人真实数据</li></ul></li><li><p><strong>数据分布及描述</strong></p><ul><li><a href="https://ai.ppdai.com/mirror/goToMirrorDetail?mirrorId=1">https://ai.ppdai.com/mirror/goToMirrorDetail?mirrorId=1</a></li></ul></li><li><p><strong>评价指标</strong>：logloss，logloss分数越低越好</p></li><li><p><strong>方案及结果</strong></p><ul><li><p><a href="https://qrfaction.github.io/2018/07/25/%E9%AD%94%E9%95%9C%E6%9D%AF%E6%AF%94%E8%B5%9B%E7%AD%94%E8%BE%A9PPT/">rank6方法</a>(rank6结果0.145129，top1结果0.142658)</p><ul><li>主要利用传统特征(如最长公共子序列、编辑距离等)，结构特征(构造图结构。将q_id作为node，(qi,qj)作为edge，得到一个单种边的同构图，然后计算qi,qj的公共边权重和等结构)，还有半监督、相似传递性、早停优化等</li></ul></li></ul></li></ul><h5 id="CCKS-2018-微众银行智能客服问句匹配大赛"><a href="#CCKS-2018-微众银行智能客服问句匹配大赛" class="headerlink" title="CCKS 2018 微众银行智能客服问句匹配大赛"></a><a href="https://biendata.com/competition/CCKS2018_3/">CCKS 2018 微众银行智能客服问句匹配大赛</a></h5><ul><li><strong>任务目标</strong><ul><li>针对中文的真实客服语料，进行问句意图匹配</li></ul></li><li><strong>数据来源</strong><ul><li>所有语料来自原始的银行领域智能客服日志，并经过了筛选和人工的意图匹配标注。</li></ul></li><li><strong>数据具体说明</strong>：<a href="https://biendata.com/competition/CCKS2018_3/data/">https://biendata.com/competition/CCKS2018_3/data/</a></li><li><strong>评价指标</strong>：Precision、Recall、F1值、ACC</li><li><strong>top1评测论文</strong>：<a href="http://ceur-ws.org/Vol-2242/paper09.pdf?crazycache=1">An Enhanced ESIM Model for Sentence Pair Matching with Self-Attention</a></li></ul><h5 id="AFQMC-蚂蚁金融语义相似度"><a href="#AFQMC-蚂蚁金融语义相似度" class="headerlink" title="AFQMC 蚂蚁金融语义相似度"></a><a href="https://dc.cloud.alipay.com/index?click_from=MAIL&amp;_bdType=acafbbbiahdahhadhiih#/topic/intro?id=3">AFQMC 蚂蚁金融语义相似度</a></h5><ul><li><strong>任务目标</strong><ul><li>给定客服里用户描述的两句话，用算法来判断是否表示了相同的语义</li></ul></li><li><strong>数据来源</strong><ul><li>所有数据均来自蚂蚁金服金融大脑的实际应用场景。</li></ul></li><li><strong>数据分布</strong><ul><li>初赛阶段提供10万对的标注数据作为训练数据，包括同义对和不同义对，可下载；复赛阶段不提供下载</li><li>具体说明：<a href="https://dc.cloud.alipay.com/index?click_from=MAIL&amp;_bdType=acafbbbiahdahhadhiih#/topic/data?id=3">链接</a></li></ul></li><li><strong>评测指标</strong>：F1-score为准（得分相同时，参照accuracy排序）</li><li><strong>top1解决方案</strong>：<a href="https://www.jiqizhixin.com/articles/2018-10-15-14">链接</a>。<ul><li>主要利用char-level feature、ESIM 模型、ensemble</li></ul></li></ul><h5 id="OPPO手机搜索排序query-title语义匹配数据集"><a href="#OPPO手机搜索排序query-title语义匹配数据集" class="headerlink" title="OPPO手机搜索排序query-title语义匹配数据集"></a><a href="https://tianchi.aliyun.com/competition/entrance/231688/introduction">OPPO手机搜索排序query-title语义匹配数据集</a></h5><ul><li><p><strong>数据集链接</strong>：<a href="https://pan.baidu.com/s/1Hg2Hubsn3GEuu4gubbHCzw">https://pan.baidu.com/s/1Hg2Hubsn3GEuu4gubbHCzw</a> (密码7p3n)</p></li><li><p><strong>数据来源</strong></p><ul><li>该数据集来自于OPPO手机搜索排序优化实时搜索场景, 该场景就是在用户不断输入过程中，实时返回查询结果。 该数据集在此基础上做了相应的简化， 提供了一个query-title语义匹配。</li></ul></li><li><p><strong>数据分布</strong></p><ul><li>初赛数据约235万 训练集200万，验证集5万，A榜测试集5万，B榜测试集25万</li><li>具体说明：<a href="https://tianchi.aliyun.com/competition/entrance/231688/information">https://tianchi.aliyun.com/competition/entrance/231688/information</a></li></ul></li><li><p><strong>评测指标</strong>：F1 score 指标，正样本为1</p></li><li><p><strong>top1 解决方案</strong></p><ul><li><p>答辩链接：<a href="https://tianchi.aliyun.com/course/video?spm=5176.12586971.1001.83.1770262auKlrTZ&amp;liveId=41001">链接</a>(00:42开始)</p></li><li><p>主要应用：数据预处理、CTR问题的特征挖掘、TextCNN&amp;TF-IDF、attention net、数据增强、回归CTR模型融合lightGBM、阈值选择。（rank 1,rank2两只队伍都是使用了lightGBM模型和模型融合）</p></li><li><p>最后得分：0.7502</p></li></ul></li></ul><h5 id="医疗问题相似度衡量竞赛数据集"><a href="#医疗问题相似度衡量竞赛数据集" class="headerlink" title=" 医疗问题相似度衡量竞赛数据集"></a><a href="https://biendata.com/competition/chip2018/"> 医疗问题相似度衡量竞赛数据集</a></h5><ul><li><p><strong>比赛链接</strong>：</p></li><li><p><strong>任务目标</strong>：针对中文的真实患者健康咨询语料，进行问句意图匹配。给定两个语句，要求判定两者意图是否相同或者相近</p></li><li><p><strong>数据来源</strong></p><ul><li>来源于真实问答语料库，该任务更加接近于智能医疗助手等自然语言处理任务的实际需求</li><li>所有语料来自互联网上患者真实的问题，并经过了筛选和人工的意图匹配标注。</li></ul></li><li><p><strong>数据分布</strong></p><ul><li>训练集包含20000条左右标注好的数据（经过脱敏处理，包含标点符号），供参赛人员进行训练和测试。</li><li>测试集包含10000条左右无label的数据（经过脱敏处理，包含标点符号）</li><li>具体描述：<a href="https://biendata.com/competition/chip2018/data/">链接</a></li></ul></li><li><p><strong>评测指标</strong>：Precision，Recall和F1值。最终排名以F1值为基准</p></li></ul><h2 id="2-方法及模型"><a href="#2-方法及模型" class="headerlink" title="2 方法及模型"></a>2 方法及模型</h2><h3 id="2-1-无监督方法"><a href="#2-1-无监督方法" class="headerlink" title="2.1 无监督方法"></a>2.1 无监督方法</h3><h4 id="2-1-1-规则匹配"><a href="#2-1-1-规则匹配" class="headerlink" title="2.1.1 规则匹配"></a>2.1.1 规则匹配</h4><p>目前，流行的问答系统中依旧大量应用着规则匹配的方法。基于规则的方法拥有可解释性强，易于控制，效率高，易于实现，不需要标注数据等优势。针对FAQ库中的标问和相似问进行分词、应用正则表达式等方法提炼出大量的概念，并将这些概念进行组合，构成大量的句式，句式再进行组合形成标问。</p><ul><li>例如，标问“华为mate30现在的价格是多少？”，拆出来“华为mate30”是cellphone概念，“价格是多少”是askMoney概念，“现在”是time概念，那么“华为mate30现在的价格是多少？”就是cellphone+askMoney+time。用户输入”华为mate30现在卖多少钱？”进行分词，可以得到相同的句式和概念组合，就能够命中“华为mate30现在的价格是多少？”这个相似问了。</li></ul><p>在基于规则的匹配中, 如何进行规则的自动发现与更新、检验与评估是最关键的问题。究其原因, 由人工维护的产生式规则需要高水平的、具备丰富的领域知识的专家.在问答系统所应用的领域较为狭窄时， 这有可能得到满足。然而, 随着问答系统涉及知识的广度和深度不断提高, 依赖于专家知识对管理规则的难度也大为提高。</p><h4 id="2-1-2-无监督文本表示"><a href="#2-1-2-无监督文本表示" class="headerlink" title="2.1.2 无监督文本表示"></a>2.1.2 无监督文本表示</h4><p>在缺少标记数据的场景，我们可以利用算法对文本本身进行表示，再利用常用的向量距离计算方法（如余弦距离，欧式距离等）进行相似性度量。常见的无监督文本表示方法主要可以分为两种，一种是基于词频信息的方法，一种是基于词向量的方法。</p><ul><li><p>基于词频信息的方法：传统的文本表示方法通常是基于词频特征的，例如TF-IDF，语言模型等。</p><ul><li><p><strong>TF-IDF</strong>：将文档表示为其每个单词的TF-IDF值向量形式，并通过计算两个文本向量表示的余弦相似度来衡量其相似性。</p></li><li><p><strong>语言模型</strong>：根据现有的文本对每个单词由一篇文档生成的概率根据词频进行建模，将一段文本由另一段文本生成的概率作为其相似度得分。</p></li></ul></li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/languagemodel.png  width=650 alt=语言模型></div><ul><li><p>基于浅层语义的方法，这些方法对文档的浅层语义分布进行建模，用来估计文档的生成概率，如PLSA，LDA等。</p><ul><li><p><strong>PLSA</strong> </p><p>PLSA假设整个词频矩阵服从多项式分布，并引入了主题（z）的概念。假设每篇文章都由若干主题构成，每个主题的概率是p(z|d)，在给定主题的条件下，每个词都以一定的概率p(w|z)产生。这样就能解决多义词的分布问题。这种分析的基础仍然是文档和词的共现频率，分析的目标是建立词/文档与这些潜在主题的关系，而这种潜在主题进而成为语义关联的一种桥梁。其概率图模型如下：</p></li></ul></li></ul><pre><code class="hljs">&lt;div align=center&gt;&lt;img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/PLSA.jpg  width=650 alt=PLSA&gt;&lt;/div&gt;其中p(z|d)和P（w|z）是需要学习的参数。P(z|d)参数数目是主题数和文档数乘的关系，p(w|z)是词表数乘主题数的关系，参数空间很大，容易过拟合。</code></pre><ul><li><p><strong>LDA</strong></p><p>如果说pLSA是频度学派代表，那LDA就是<strong>贝叶斯学派</strong>代表。LDA通过引入Dirichlet分布作为多项式共轭先验，在数学上完整解释了一个文档生成过程，其概率图模型如图所示。</p><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/LDA.jpg  width=650 alt=LDA></div><p>我们可以看出LDA中每篇文章的<strong>生成过程</strong>如下：</p><ol><li>选择单词数N服从泊松分布，N~Possion(β)。</li><li>文档θ服从狄利克雷分布，θ~Dir(α)。</li><li>对于文档内N个单词中的每个单词<br>a. 选择一个主题z，服从多项分布Mult(θ)<br>b. 以概率p（w|z，β）生成单词w，其中p（w|z，β）表示在主题z上的条件多项式概率。</li></ol><p>和pLSA不太一样，LDA概率图模型引入了两个随机变量α和β，它们就是控制参数分布的分布，即文档-主题符合多项式分布。这个多项式分布的产生受Dirichlet先验分布控制，这样就解决了PLSA参数量过大的问题。</p></li></ul><ul><li><p>基于词向量的方法： word embedding技术如word2vec，glove等已经广泛应用于NLP，极大地推动了NLP的发展。既然词可以embedding，句子也可以。该类算法通常是基于词袋模型的算法，如TF-IDF加权平均，SIF等。</p><ul><li><p><strong>SIF</strong></p><p>发表于2016年的论文<a href="https://openreview.net/pdf?id=SyK00v5xx">A simple but tough-to-beat baseline for sentence embeddings</a>提出了一种非常简单但很有一定竞争力的句子向量表示算法。算法包括两步，第一步是对句子中所有的词向量进行加权平均，得到平均向量；第二步是移出（减去）在所有句子向量组成的矩阵的第一个<strong>主成分</strong>上的投影。</p><p>第一步主要是对TFIDF加权平均词向量表示句子的方法进行改进。论文提出了一种<strong>平滑倒词频</strong> (smooth inverse frequency, SIF)方法用于计算每个词的加权系数，具体地，单词的权重为a/(a+p(w))，其中a为平滑参数，p(w)为（估计的）词频。直观理解SIF，就是说<strong>频率越低的词在当前句子出现了，说明它在句子中的重要性更大，也就是加权系数更大</strong>。对于第二步，通过移出所有句子的共有信息，因此保留下来的句子向量更能够表示本身并与其它句子向量产生差距。</p><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/SIF.png  width=650 alt=SIF算法流程></div></li><li><p><strong>WMD</strong></p><p>WMD是一种基于word embeddings 计算两个文本间的距离，即测量一个文本转化为另一个文本的最小距离。其将文本距离度量问题转化为一个最优传输（translation）问题。</p><p>Word2Vec得到的词向量可以反映词与词之间的语义差别，WMD距离即对两个文档中的任意两个词所对应的词向量求欧氏距离然后再加权求和，大概是这样的形式：</p><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/WMD_eq.png   alt=WMD_eq></div><p>其中c(i,j)为i，j两个词所对应的词向量的欧氏距离。矩阵T代表了文档1中的一个词转移到文档2中一个词的权重。即解决如下线性规划问题（|d1|代表文档1的长度，|d2|代表文档2长度）：</p><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/wmd_eq2.png  alt=wmd_eq2></div><p>通过最小化矩阵Ｔ，我们就可以得到两个文档的ＷＭＤ距离。具体例子如下：</p><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/ＷＭＤ.png  width=650 alt=WMD></div></li></ul></li></ul><h4 id="2-1-3-用于跨领域迁移学习方法"><a href="#2-1-3-用于跨领域迁移学习方法" class="headerlink" title="2.1.3 用于跨领域迁移学习方法"></a>2.1.3 用于跨领域迁移学习方法</h4><ul><li><p><strong>背景</strong></p><ul><li><strong>迁移学习</strong><ul><li>一种机器学习的方法。指的是一个预训练的模型被重新用在另一个任务中，一般两种任务之间需要有一定的相似性和关联性</li></ul></li><li><strong>为什么要迁移学习</strong><ul><li>随着近年来NLP的发展，研究发现，有监督的方法虽然准确率高，但是有标数据的获取成本太高，因此迁移学习的效果越来越凸显出来，并在各种NLP（包括短文本相似度）场景出现了革命性进展</li></ul></li></ul></li><li><p><strong>模型有两种</strong></p><ul><li><strong>unsupervised</strong>：假设完全没有目标领域的标注数据</li><li><strong>supervised</strong>：假设仅有少部分目标领域的标注数据。</li></ul><p><strong>在实际的商业应用中主要以supervised的迁移学习技术为主，同时结合深度神经网络（DNN）</strong>。</p><p>在这个设定下主要有两种框架：</p><ul><li><strong>Fully</strong>-Shared Model：用于<strong>比较相似的两个领域</strong>。</li><li><strong>Specific</strong>-Shared Model：用于<strong>相差较大的两个领域</strong>。</li></ul></li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/CQA-Survey/blob/master/images/Screen%20Shot%202020-04-20%20at%207.36.26%20PM.png width=650 alt=迁移学习模型></div><h2 id="2-2-有监督匹配算法"><a href="#2-2-有监督匹配算法" class="headerlink" title="2.2 有监督匹配算法"></a>2.2 有监督匹配算法</h2><h4 id="2-2-2-问题意图分类—深度学习多分类模型（CNN-DNN-LSTM-…）"><a href="#2-2-2-问题意图分类—深度学习多分类模型（CNN-DNN-LSTM-…）" class="headerlink" title="2.2.2 问题意图分类—深度学习多分类模型（CNN\DNN\LSTM\…）"></a>2.2.2 问题意图分类—深度学习多分类模型（CNN\DNN\LSTM\…）</h4><ul><li>问答匹配任务在大多数情况下可以用意图分类解决，如先匹配用户问题意图，然后给出对应意图的答案。进而问答匹配任转化为二分类或多分类任务。</li><li>工业真正的场景中，用户问题的问题个数是不固定的，所以会把最后一层Softmax更改为多个二分类模型。模型图如下：</li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/多个二分类模型.jpeg  width=650 alt=多个二分类模型模型图></div><h3 id="2-2-2深度文本匹配模型"><a href="#2-2-2深度文本匹配模型" class="headerlink" title="2.2.2深度文本匹配模型"></a>2.2.2深度文本匹配模型</h3><p>一般来说，深度文本匹配模型分为两种类型，表示型和交互型。</p><h4 id="表示型模型"><a href="#表示型模型" class="headerlink" title="表示型模型"></a>表示型模型</h4><p>表示型模型更侧重对表示层的构建，它首先将两个文本表示成固定长度的向量，之后计算两个文本向量的距离来衡量其相似度。这种模型的问题是没有考虑到两个句子词级别的关联性。容易失去语义焦点。</p><h5 id="Siamese-networks模型"><a href="#Siamese-networks模型" class="headerlink" title="Siamese networks模型"></a>Siamese networks模型</h5><ul><li>Siamese networks(孪生神经网络)是一种相似性度量方法，内部采用深度语义匹配模型（DSSM，Deep Structured Semantic Model），该方法在检索场景下使用点击数据来训练语义层次的匹配。</li><li>Siamese networks有两个输入(Input1 and Input2),将两个输入feed进入两个神经网络(Network1 and Network2)，这两个神经网络分别将输入映射到新的空间，形成输入在新的空间中的表示。通过Loss的计算，评价两个输入的相似度。</li><li>基于Siamese networks神经网络架构，比如有Siamese结构的LSTM、CNN和ESIM等。</li></ul><h5 id="DSSM-模型"><a href="#DSSM-模型" class="headerlink" title="DSSＭ 模型"></a>DSSＭ 模型</h5><ul><li><strong>论文地址</strong>：<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf">Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</a></li><li><strong>模型简介</strong><ul><li>先把 query 和 document 转换成 BOW 向量形式，然后通过 word hashing 变换做降维得到相对低维的向量，feed给 MLP 网络，输出层对应的低维向量就是 query 和 document 的语义向量（假定为 Q 和 D）。计算(D, Q)的余弦相似度后，用 softmax 做归一化得到的概率值是整个模型的最终输出，该值作为监督信号进行有监督训练。</li></ul></li><li><strong>模型结构</strong>：</li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/CQA-industry-DSSM.png  width=650 alt=DSSM></div><h5 id="Sentence-Bert"><a href="#Sentence-Bert" class="headerlink" title="Sentence Bert"></a>Sentence Bert</h5><ul><li><p><strong>论文地址</strong>：<a href="https://arxiv.org/pdf/1908.10084.pdf">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a></p></li><li><p><strong>模型简介</strong></p><p>Sentence BERT(Sbert) 网络是通过 SNLI 数据集（标注了一对句子之间的关系，可能是蕴含、矛盾或者中立）进行预训练。模型使用孪生网络，即两个一模一样共享参数的Bert网络进行推理。首先将第一个句子输入到BERT，通过不同的Pooling方法获得句子的Embedding表示，第二个句子同样如此，然后将这两个Embedding变换后通过Softmax输出这对句子之间关系的概率进行训练（类似分类问题）。在训练完毕后，就可以将下面的BERT和pooling层拿出来，将句子输入得到其Embedding，再进行其他操作（比如计算相似度可以直接使用余弦）。</p><p>原始的Bert模型如果要为一个句子寻找最相似的句子，需要两两计算其相似度，这样的时间消耗是ｎ的平方级别的。Sentence Bert可以首先计算出每个句子的向量表示，然后直接计算句子间的相似度，这样可以将时间消耗减少到Ｏ（ｎ）的级别，同时论文中的实验证明这样的方法并没有降低模型的效果。</p></li><li><p><strong>模型结构</strong> :</p><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/使用孪生BERT网络生成句子的嵌入表示.png  width=400 alt=Ssentence Bert></div></li></ul><h4 id="交互型模型"><a href="#交互型模型" class="headerlink" title="交互型模型"></a>交互型模型</h4><p>交互型模型认为全局的匹配度依赖于局部的匹配度，在输入层就进行词语间的先匹配，之后利用单词级别的匹配结果进行全局的匹配。它的优势是可以很好的把握语义焦点，对上下文重要性合理建模。由于模型效果显著，业界都在逐渐尝试交互型的方法。</p><h5 id="MatchPyramid模型"><a href="#MatchPyramid模型" class="headerlink" title="MatchPyramid模型"></a>MatchPyramid模型</h5><ul><li><strong>论文地址</strong>：<a href="https://arxiv.org/pdf/1602.06359.pdf">Text Matching as Image Recognition</a></li><li><strong>模型简介</strong><ul><li>先将文本使用相似度计算构造相似度矩阵，然后CNN网络来提取特征。</li><li>模型可以学习到Down the ages（n-gram特征），noodles and dumplings与dumplings and noodles（打乱顺序的n-term特征）、were famous Chinese food和were popular in China（相似语义的n-term特征）<ul><li><strong>层次化卷积步骤</strong><pre><code class="hljs">- 1.Ai和Bj距离度量方式：完全一样 (Indicator），余弦相似度 (Cosine)，点乘 (Dot Product)。        - 2.卷积，RELU激活，动态pooling（pooling size等于内容大小除以kernel大小）                 - 3.卷积核第一层分别算，第二层求和算。可以见下图3*3的kernel分别算，2*4*4求和算。                             - 4.MLP拟合相似度，两层，使用sigmoid激活，最后使用softmax，交叉熵损失函数。</code></pre></li></ul></li></ul></li></ul><div align=center><img src=https://img-blog.csdn.net/20171219172641689  width=400 alt=MatchPyramid-Hierarchical-Convolution></div><ul><li><strong>结构</strong></li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/CQA-industry-MatchPyramid.png  width=400 alt=MatchPyramid-overview></div><h5 id="ESIM-（Enhanced-LSTM）"><a href="#ESIM-（Enhanced-LSTM）" class="headerlink" title="ESIM （Enhanced LSTM）"></a>ESIM （Enhanced LSTM）</h5><ul><li><p><strong>论文地址</strong>：Enhanced LSTM for Natural Language Inference</p></li><li><p><strong>源码</strong>：<a href="https://github.com/coetaur0/ESIM">链接</a></p></li><li><p><strong>模型简介</strong></p><p>Enhanced LSTM for Natural Language Inference(ESIM)是2017年提出的一个文本相似度计算模型，是一种转为自然语言推断而生的加强版LSTM，由原文中知这种精心设计的链式LSTM顺序推理模型可以胜过以前很多复杂的模型。ESIM的模型主要包括３个部分：编码层，推理层和预测层。</p><ul><li>编码层：采用BiLSTM（双向LSTM）对输入的两个句子分别编码。</li><li>推理层：模型的核心部分，首先计算两个句子和另外句子相关的表示向量，然后计算该向量和原始向量的点积，差值等。之后利用各种不同的池化方式得到最后的句子表示，将两个句子的表示拼接，得到预测层的输出ｖ。</li><li>预测层：在这一层中，本模型将上述得到的固定长度向量 ｖ，连接两层全连接层，第一层采用tanh激活函数，第二层采用softmax激活函数，最后得到文本蕴含的结果。</li></ul></li><li><p><strong>模型结构</strong></p></li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/CQA-industry-ESIM.png  width=400 alt=ESIM></div><h3 id="2-3-FAQ发现与优化"><a href="#2-3-FAQ发现与优化" class="headerlink" title="2.3 FAQ发现与优化"></a>2.3 FAQ发现与优化</h3><h4 id="FAQ发现"><a href="#FAQ发现" class="headerlink" title="FAQ发现"></a>FAQ发现</h4><p>将用户问句进行聚类，对比已有的FAQ，发现并补足未覆盖的知识点。将FAQ与知识点一一对应。</p><ul><li><strong>FAQ的拆分与合并</strong> </li></ul><p>FAQ拆分是当一个FAQ里包含多个意图或者说多种情况的时候，YiBot后台会自动分析触达率较高的FAQ，聚类FAQ对应的问句，按照意图将其拆分开来。</p><ul><li><strong>FAQ合并</strong></li></ul><p>最终希望希望用户的每一个意图能对应到唯一的FAQ，这样用户每次提问的时候，系统就可以根据这个意图对应的FAQ直接给出答案。而如果两个FAQ意思过于相近，那么当用户问到相关问题时，就不会出现一个直接的回答，而是两个意图相关的推荐问题，这样用户就要再进行一步选择操作。这时候YiBot就会在后台同样是分析触达率较高的FAQ，分析哪一些问句总是被推荐相同的答案，将问句对应的意图合并。</p><ul><li><strong>淘汰机制</strong></li></ul><p>分析历史日志，采用淘汰机制淘汰废弃知识点，如已下线业务知识点等。</p><h4 id="FAQ答案优化"><a href="#FAQ答案优化" class="headerlink" title="FAQ答案优化"></a>FAQ答案优化</h4><ul><li><strong>挖掘对话，进行答案优化</strong></li></ul><p>如果机器人已经正确识别意图但最后仍然转人工，说明知识库的答案不对，需要进一步修正这一类知识点相对应的答案。</p><ul><li><strong>分析头部场景，回答应用文本、图片、自动化解决方案等多元化方式</strong></li></ul><p>比如在电商场景中，经常会有查询发货到货时间、订单状态等的场景。利用图示指引、具体订单处理等方式让用户操作更便捷。</p><h2 id="3-产品案例"><a href="#3-产品案例" class="headerlink" title="3 产品案例"></a>3 产品案例</h2><h3 id="产品1-百度AnyQ—ANswer-Your-Questions"><a href="#产品1-百度AnyQ—ANswer-Your-Questions" class="headerlink" title="产品1 百度AnyQ—ANswer Your Questions"></a>产品1 <a href="https://github.com/baidu/AnyQ">百度AnyQ—ANswer Your Questions</a></h3><ul><li><strong>简介</strong><ul><li>AnyQ开源项目主要包含面向FAQ集合的问答系统框架、文本语义匹配工具SimNet。</li></ul></li><li><strong>FAQ问答系统框架</strong><ul><li>AnyQ系统框架主要由Question Analysis、Retrieval、Matching、Re-Rank等部分组成。</li><li>框架中包含的功能均通过插件形式加入，如Analysis中的中文切词，Retrieval中的倒排索引、语义索引，Matching中的Jaccard特征、SimNet语义匹配特征，当前共开放了20+种插件。</li></ul></li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/CQA-industry-AnyQFramework.png  width=400 alt=AnyQ-FAQ问答系统框></div><ul><li><p><strong>特色</strong></p><ul><li><p><strong>框架设计灵活，插件功能丰富</strong></p><ul><li>AnyQ 系统AnyQ 系统集成了检索和匹配的丰富插件，通过配置的方式生效；以相似度计算为例，包括字面匹配相似度 Cosine、Jaccard、BM25 等，同时包含了语义匹配相似度。</li><li>用户自定义插件只需实现对应的接口即可，如 Question 分析方法、检索方式、匹配相似度、排序方式等。</li></ul></li><li><p><strong>极速语义检索</strong></p><ul><li>语义检索技术将用户问题和 FAQ 集合的相似问题通过深度神经网络映射到语义表示空间的临近位置，检索时，通过高速向量索引技术对相似问题进行检索。</li></ul></li><li><strong>SimNet 语义匹配模型</strong>：<ul><li>AnyQ 使用 SimNet 语义匹配模型构建文本语义相似度，克服了传统基于字面匹配方法的局限，增强 AnyQ 系统的语义检索和语义匹配能力。</li></ul></li><li>其他：针对无任何训练数据的开发者，AnyQ 还包含了基于百度海量数据训练的语义匹配模型，开发者可零成本直接使用。</li></ul></li></ul><h3 id="产品2-腾讯知文—结构化FAQ问答引擎"><a href="#产品2-腾讯知文—结构化FAQ问答引擎" class="headerlink" title="产品2: 腾讯知文—结构化FAQ问答引擎"></a>产品2: <a href="https://cloud.tencent.com/developer/article/1172017">腾讯知文—结构化FAQ问答引擎</a></h3><p>基于结构化的FAQ的问答引擎流程由两条技术路线来解决</p><ul><li>无监督学习，基于快速检索</li><li>有监督的学习，基于深度匹配</li></ul><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/CQA-%E7%9F%A5%E6%96%87-%E5%9F%BA%E4%BA%8EAttention%E6%9C%BA%E5%88%B6%E7%9A%84Interaction-based%20networks.jpeg  width=500 alt=知文-基于Attention机制的Interaction-based_networks></div><p>采用了三个层次的方法来实现快速检索的方法</p><ul><li><strong>层次1：基础的TFIDF提取query的关键词，用BM25来计算query和FAQ库中问题的相似度</strong>。这是典型的词汇统计的方法，该方法可以对rare word比较鲁棒，但同时也存在词汇匹配缺失的问题。</li><li><strong>层次2：采用了language model（简写LM）的方法</strong>。主要使用的是Jelinek-Mercer平滑法和Dirichlet平滑法，对于上面的词汇匹配问题表现良好，但是也存在平滑敏感的问题。</li><li><strong>层次3：最后一层使用Embedding，采用了LSA/word2vec和腾讯知文自己提出的Weighted Sum/WMD方法</strong>，以此来表示语义层面的近似，但是也同样引发了歧义问题。</li></ul><h3 id="产品3-阿里小蜜"><a href="#产品3-阿里小蜜" class="headerlink" title="产品3: 阿里小蜜"></a>产品3: <a href="https://www.alixiaomi.com/#/">阿里小蜜</a></h3><p><a href="https://www.alixiaomi.com/#/">产品链接</a></p><p><strong>意图与匹配分层的技术架构体系</strong></p><p>在阿里小蜜这样在电子商务领域的场景中，对接的有客服、助理、聊天几大类的机器人。这些机器人，由于本身的目标不同，就导致不能用同一套技术框架来解决。因此，我们先采用分领域分层分场景的方式进行架构抽象，然后再根据不同的分层和分场景采用不同的机器学习方法进行技术设计。首先我们将对话系统从分成两层：</p><p>1、意图识别层：识别语言的真实意图，将意图进行分类并进行意图属性抽取。意图决定了后续的领域识别流程，因此意图层是一个结合上下文数据模型与领域数据模型不断对意图进行明确和推理的过程；</p><p>2、问答匹配层：对问题进行匹配识别及生成答案的过程。在阿里小蜜的对话体系中我们按照业务场景进行了3种典型问题类型的划分，并且依据3种类型会采用不同的匹配流程和方法：</p><ul><li>问答型：例如“密码忘记怎么办？”→ 采用基于知识图谱构建+检索模型匹配方式</li><li>任务型：例如“我想订一张明天从杭州到北京的机票”→ 意图决策+slots filling的匹配以及基于深度强化学习的方式</li><li>语聊型：例如“我心情不好”→ 检索模型与Deep Learning相结合的方式</li></ul><p>下图表示了阿里小蜜的意图和匹配分层的技术架构。</p><p><img src="https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/640.png" alt="image"></p><p><strong>意图识别介绍：结合用户行为deep-learning模型的实践</strong></p><p>通常将意图识别抽象成机器学习中的分类问题，在阿里小蜜的技术方案中除了传统的文本特征之外，考虑到本身在对话领域中存在语义意图不完整的情况，我们也加入了用实时、离线用户本身的行为及用户本身相关的特征，通过深度学习方案构建模型，对用户意图进行预测, 具体如下图：</p><div align=center><img src=https://github.com/BDBC-KG-NLP/QA-Survey/blob/master/image/640-2.jpeg  width=400 alt=意图识别></div><p>在基于深度学习的分类预测模型上，我们有两种具体的选型方案：一种是多分类模型，一种是二分类模型。多分类模型的优点是性能快，但是对于需要扩展分类领域是整个模型需要重新训练；而二分类模型的优点就是扩展领域场景时原来的模型都可以复用，可以平台进行扩展，缺点也很明显需要不断的进行二分，整体的性能上不如多分类好，因此在具体的场景和数据量上可以做不同的选型。</p><p><strong>行业三大匹配模型</strong></p><p>目前主流的智能匹配技术分为如下3种方法：<br>1、基于模板匹配(Rule-Based)<br>2、基于检索模型(Retrieval Model)<br>3、基于深度学习模型(Deep Learning)</p><p>在阿里小蜜的技术场景下，我们采用了基于模板匹配，检索模型以及深度学习模型为基础的方法原型来进行分场景(问答型、任务型、语聊型)的会话系统构建。</p><p><strong>阿里小蜜的核心算法之一：自然语言理解(NLU)方法</strong></p><ul><li><strong>无样本冷启动方法</strong><ul><li>写一套简单易懂的规则表示语法</li></ul></li><li><strong>小样本方法</strong><ul><li>先整理出一个大数量级的数据，每一个类目几十条数据，为它建立 meta-learning 任务。对于一个具体任务来说：构建支撑集和预测集，通过 few-shot learning 的方法训练出 model，同时与预测集的 query 进行比较，计算 loss 并更新参数，然后不断迭代让其收敛。</li><li>这只是一个 meta-learning 任务，可以反复抽样获得一系列这样的任务，不断优化同一个模型。在线预测阶段，用户标注的少量样本就是支撑集，将 query 输入模型获得分类结果。</li><li>模型的神经网络结构分为3部分，首先是 Encoder 将句子变成句子向量，然后再通过 Induction Network 变成类向量，最后通过 Relation Network 计算向量距离，输出最终的结果。</li><li>具体地，Induction Network中把样本向量抽象到类向量的部分，采用 matrix transformation 的方法，转换后类边界更清晰，更利于下游 relation 的计算。在 Induction Network 的基础上，又可以引入了 memory 机制，形成Memory-based Induction Network ，目的是模仿人类的记忆和类比能力，在效果上又有进一步提升。</li></ul></li><li><strong>多样本方法</strong><ul><li>构建一个三层的模型，最底层是具有较强迁移能力的通用模型 BERT，在此基础上构建不同行业的模型，最后用相对较少的企业数据来训练模型。这样构建出来的企业的 NLU 分类模型，F1 基本都在90%+。性能方面，因为模型的结构比较复杂，在线预测的延时比较长，因此通过知识蒸馏的方法来进行模型压缩，在效果相当的同时预测效率更快了。</li></ul></li></ul><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><ul><li>整个CQA问答，可能经过的模块共两个：召回模块和检索模块。<ul><li><strong>召回模块</strong><br>  -主要采用传统信息检索方法实现     </li><li><strong>IR检索模块</strong><ul><li><strong>无监督的匹配</strong>方式<pre><code class="hljs">  - 规则，LDA，Sentence bert等</code></pre></li><li><strong>有监督的深度模型匹配</strong>方式<pre><code class="hljs">- 文本语义表达的Siamese networks深度模型。应用广泛的模型只要有DSSM、ESIM，MAtchPyramid等    - **DSSM(采用了词袋模型，损失了上下文信息，可选用CNN-DSSM等优化模型)**               - **ESIM(适用于短文本)**</code></pre><ul><li><strong>MatchPyramid() 基于交互的深度模型)</strong></li></ul></li></ul></li></ul></li><li>问答对较少等情况下可以将IR模块改为分类任务（意图识别）进行。</li><li>如果在数据不充足，或数据效果质量不高的情况下，可以使用迁移学习，以训练好的模型为基础。</li><li>在系统设计初期，根据数据的不同情况，可参考阿里小蜜自然语言理解(NLU)方法中的无样本冷启动方法、小样本方法、多样本方法的思路。</li></ul><h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><ul><li>有标记的相似文本训练数据标注难以自动获取</li><li>高质量的问答对数据获取与维护成本较高</li><li>用户可能的输入类型较多，匹配模型的鲁棒性无法保证</li></ul><h3 id="未来研究方向"><a href="#未来研究方向" class="headerlink" title="未来研究方向"></a>未来研究方向</h3><ul><li>利用预训练模型解决文本匹配问题</li><li>FAQ的发现与优化的自动化</li></ul><h2 id="5-相关资料"><a href="#5-相关资料" class="headerlink" title="5 相关资料"></a>5 相关资料</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/50799128">智能客服FAQ问答任务的技术选型探讨</a></li><li><a href="https://mp.weixin.qq.com/s/n-uicubtTFyOH00HAvRgMQ">不是所有的智能机器人都能做好客服——浅谈智能客服机器人评价指标新趋势</a></li><li><a href="https://www.jiqizhixin.com/articles/2018-08-24-17">百度开源 FAQ 问答系统—AnyQ</a></li><li><a href="https://cloud.tencent.com/developer/article/1172017">腾讯知文，从0到1打造下一代智能问答引擎【CCF-GAIR】</a></li><li><a href="https://mp.weixin.qq.com/s/9-HUoePmGvv40JVWcPtHew">你问我答之「YiBot知识体系运营知多少」</a></li><li><a href="https://www.weiyangx.com/338587.html">短文本相似度在金融智能客服中的应用 - 专注金融科技与创新</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247494321&amp;idx=1&amp;sn=7f58bafd7f1962e17f3162ef0917c431&amp;chksm=fbd758ddcca0d1cb19c452c40697c816f788d29b90af4f703a0fc776897f80b087d0a3bc885a&amp;scene=27#wechat_redirect">阿里小蜜新一代智能对话开发平台技术解析</a></li><li><a href="https://mp.weixin.qq.com/s/ksVbQq42ay5lxcfqNwBgxA">阿里云小蜜对话机器人背后的核心算法</a></li><li><a href="https://mp.weixin.qq.com/s/uzmcISuDbf7EkralufAKhA">阿里小蜜：智能服务技术实践及场景探索</a></li><li><a href="https://mp.weixin.qq.com/s/eFm89Q_AMeYFTrJl4uLOgA">干货 | 阿里小蜜-电商领域的智能助理技术实践</a></li><li><a href="https://myslide.cn/slides/6148#">阿里小蜜机器阅读理解技术揭秘</a></li><li><a href="https://zhuanlan.zhihu.com/p/62217668">从学术前沿到工业领先：解密阿里小蜜机器阅读的实践之路</a></li><li><a href="https://www.jiqizhixin.com/articles/2018-10-23-15">云知声：深度文本匹配在智能客服中的应用</a></li><li>[<a href="https://www.cnblogs.com/xlturing/p/6136690.html#simhash">NLP点滴——文本相似度</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>QA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络通信通俗解释-转载</title>
    <link href="/2020/09/11/2020-09-11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A/"/>
    <url>/2020/09/11/2020-09-11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>天各一方的两台计算机是如何通信的呢？在成千上万的计算机中，为什么一台计算机能够准确着寻找到另外一台计算机，并且把数据发送给它呢？</p><p>可能很多人都听说过网络通信的 5 层模型，但是可能并不是很清楚为什么需要五层模型，五层模型负责的任务也有可能经常混淆。下面是网络通信的五层模型</p><p><img src="https://picb.zhimg.com/80/v2-52cda6aef4bd666ec1055b7f74241e5a_1440w.jpg" alt="img"></p><p>说实话，五层模型的具体内容还是极其复杂的，不过今天这篇文章，我将用最简洁的模式，通过网络通信的五层模型来讲解<strong>一台计算机是如何找到另外一台计算机并且把数据发送给另一台计算机的</strong>，就算你没学过计算机网络，也能够听的懂。</p> <span id="more"></span><h2 id="1-物理层"><a href="#1-物理层" class="headerlink" title="1. 物理层"></a>1. 物理层</h2><p>一台计算机与另一台计算机要进行通信，第一件要做的事是什么？当然是要把这台计算机与另外的其他计算机连起来啊，这样，我们才能把数据传输过去。例如可以通过光纤啊，电缆啊，双绞线啊等介质把他们连接起来，然后才能进行通信。</p><p><img src="https://pic1.zhimg.com/80/v2-d096642484c91fb9358c82db2203115f_1440w.jpg" alt="img"></p><p>也就是说，物理层负责把两台计算机连起来，然后在计算机之间通过高低电频来传送0,1这样的电信号。</p><h2 id="2-数据链路层"><a href="#2-数据链路层" class="headerlink" title="2. 数据链路层"></a>2. 数据链路层</h2><p>前面说了，物理层它只是单纯着负责把计算机连接起来，并且在计算机之间传输0，1这样的电信号。如果这些0，1组合的传送毫无规则的话，计算机是解读不了的。一大堆0，1谁知道是什么鬼啊。</p><p><img src="https://pic3.zhimg.com/80/v2-ba5442a8a02379c04fe625053b1e4103_1440w.jpg" alt="img"></p><p>因此，我们需要制定一套规则来进行0，1的传送。例如多少个电信号为一组啊，每一组信号应该如何标识才能让计算机读懂啊等等。</p><p>于是，有了以太网协议。</p><p><strong>1. 以太网协议</strong></p><p>以太网协议规定，一组电信号构成一个数据包，我们把这个数据包称之为<strong>帧</strong>。每一个桢由标头(Head)和数据(Data)两部分组成。</p><p><img src="https://pic4.zhimg.com/80/v2-d8e4c0461b8841e9d9fcefce9089a6c6_1440w.jpg" alt="img"></p><p>帧的大小一般为 64 - 1518 个字节。假如需要传送的数据很大的话，就分成多个桢来进行传送。</p><p>对于表头和数据这两个部分，他们存放的都是一些什么数据呢？我猜你眯着眼睛都能想到他们应该放什么数据。 毫无疑问，我们至少得知道这个桢是谁发送，发送给谁的等这些信息吧？所以标头部分主要是一些说明数据，例如发送者，接收者等信息。而数据部分则是这个数据包具体的，想给接守者的内容。</p><p>大家想一个问题，一个桢的长度是 64~1518 个字节，也就是说桢的长度不是固定的，那你觉得标头部分的字节长度是固定的吗？它当然是固定的啊，假如不是固定的，每个桢都是单独发的，那计算机怎么知道标头是几个字节，数据是几个字节呢。所以标头部分的字节是固定的，并且固定为18个字节。</p><p>把一台计算的的数据通过物理层和链路层发送给另一台计算机，究竟是谁发给谁的，计算机与计算机之间如何区分，，你总得给他们一个唯一的标识吧？</p><p>于是，MAC 地址出现了。</p><p><strong>2. MAC 地址</strong></p><p>连入网络的每一个计算机都会有网卡接口，每一个网卡都会有一个唯一的地址，这个地址就叫做 MAC 地址。计算机之间的数据传送，就是通过 MAC 地址来唯一寻找、传送的。</p><p><img src="https://pic1.zhimg.com/80/v2-448a6c7b75c415502dbec7df950b2566_1440w.jpg" alt="img"></p><p>MAC地址 由 48 个字节所构成，在网卡生产时就被唯一标识了。</p><p><strong>3. 广播与ARP协议</strong></p><p><strong>(1). 广播</strong></p><p><img src="https://pic2.zhimg.com/80/v2-38cb7a2c4245841d9f3ddf5d6c578cb8_1440w.jpg" alt="img"></p><p>如图，假如计算机 A 知道了计算机 B 的 MAC 地址，然后计算机 A 想要给计算机 B 传送数据，虽然计算机 A 知道了计算机 B 的 MAC 地址，可是它要怎么给它传送数据呢？计算机 A 不仅连着计算机 B，而且计算机 A 也还连着其他的计算机。 虽然计算机 A 知道计算机 B 的 MAC 地址，可是计算机 A 却不知道知道计算机 B 是分布在哪边路线上，为了解决这个问题，于是，有了<strong>广播</strong>的出现。</p><p>在同一个<strong>子网</strong>中，计算机 A 要向计算机 B 发送一个数据包，这个数据包会包含接收者的 MAC 地址。当发送时，计算机 A 是通过<strong>广播</strong>的方式发送的，这时同一个子网中的计算机 C, D 也会收到这个数据包的，然后收到这个数据包的计算机，会把数据包的 MAC 地址取出来，与自身的 MAC 地址对比，如果两者相同，则接受这个数据包，否则就丢弃这个数据包。这种发送方式我们称之为广播,就像我们平时在广场上通过广播的形式呼叫某个人一样，如果这个名字是你，你就理会一下，如果不是你，你就当作听不见。</p><p><strong>(2). ARP 协议</strong>。</p><p>那么问题来了，计算机 A 是如何知道计算机 B 的 MAC 地址的呢？这个时候就得由 ARP 协议这个家伙来解决了，不过 ARP 协议会涉及到IP地址，我们下面才会扯到IP地址。因此我们先放着，就当作是有这么一个 ARP 协议，通过它我们可以知道子网中其他计算机的 MAC 地址。</p><h2 id="3-网络层"><a href="#3-网络层" class="headerlink" title="3. 网络层"></a>3. 网络层</h2><p>上面我们有说到子网这个关键词，实际上我们所处的网络，是由无数个子网络构成的。广播的时候，也只有同一个子网里面的计算机能够收到。</p><p>假如没有子网这种划分的话，计算机 A 通过广播的方式发一个数据包给计算机 B , 其他所有计算机也都能收到这个数据包，然后进行对比再舍弃。世界上有那么多它计算机，每一台计算机都能收到其他所有计算机的数据包，那就不得了了。那还不得奔溃。 因此产生了<strong>子网</strong>这么一个东西。</p><p>那么问题来了，我们如何区分哪些 MAC 地址是属于同一个子网的呢？假如是同一个子网，那我们就用广播的形式把数据传送给对方，如果不是同一个子网的，我们就会把数据发给网关，让网关进行转发。</p><p>为了解决这个问题，于是，有了 IP 协议。</p><p><strong>1. IP协议</strong></p><p>IP协议，它所定义的地址，我们称之为<strong>IP地址</strong>。IP协议有两种版本，一种是 IPv4,另一种是 IPv6。不过我们目前大多数用的还是 IPv4，我们现在也只讨论 IPv4 这个版本的协议。</p><p>这个 IP 地址由 32 位的二进制数组成，我们一般把它分成4段的十进制表示，地址范围为0.0.0.0~255.255.255.255。</p><p>每一台想要联网的计算机都会有一个IP地址。这个IP地址被分为两部分，前面一部分代表<strong>网络部分</strong>，后面一部分代表<strong>主机部分</strong>。并且网络部分和主机部分所占用的二进制位数是不固定的。</p><p>假如两台计算机的网络部分是一模一样的，我们就说这两台计算机是处于同一个子网中。例如 192.168.43.1 和 192.168.43.2, 假如这两个 IP 地址的网络部分为 24 位，主机部分为 8 位。那么他们的网络部分都为 192.168.43，所以他们处于同一个子网中。</p><p>可是问题来了，你怎么知道网络部分是占几位，主机部分又是占几位呢？也就是说，单单从两台计算机的IP地址，我们是无法判断他们的是否处于同一个子网中的。</p><p>这就引申出了另一个关键词————<strong>子网掩码</strong>。子网掩码和IP地址一样也是 32 位二进制数，不过它的网络部分规定全部为 1，主机部分规定全部为 0.也就是说，假如上面那两个IP地址的网络部分为 24 位，主机部分为 8 位的话，那他们的子网掩码都为 11111111.11111111.11111111.00000000，即255.255.255.0。</p><p><img src="https://pic2.zhimg.com/80/v2-3722ea56f9e5b6959134811207357e9b_1440w.jpg" alt="img"></p><p>那有了子网掩码，如何来判端IP地址是否处于同一个子网中呢。显然，知道了子网掩码，相当于我们知道了网络部分是几位，主机部分是几位。我们只需要把 IP 地址与它的子网掩码做与(and)运算，然后把各自的结果进行比较就行了，如果比较的结果相同，则代表是同一个子网，否则不是同一个子网。</p><p>例如，192.168.43.1和192.168.43.2的子码掩码都为255.255.255.0，把IP与子码掩码相与，可以得到他们都为192.168.43.0，进而他们处于同一个子网中。</p><p><strong>2. ARP协议</strong></p><p>有了上面IP协议的知识，我们回来讲一下ARP协议。</p><p>有了两台计算机的IP地址与子网掩码，我们就可以判断出它们是否处于同一个子网之中了。</p><p>假如他们处于同一个子网之中，计算机A要给计算机B发送数据时。我们可以通过ARP协议来得到计算机B的MAC地址。</p><p>ARP协议也是通过广播的形式给同一个子网中的每台电脑发送一个数据包(当然，这个数据包会包含接收方的IP地址)。对方收到这个数据包之后，会取出IP地址与自身的对比，如果相同，则把自己的MAC地址回复给对方，否则就丢弃这个数据包。这样，计算机A就能知道计算机B的MAC地址了。</p><p><img src="https://pic3.zhimg.com/80/v2-199b82b43ce0332561662f535c3e1e62_1440w.jpg" alt="img"></p><p>可能有人会问，知道了MAC地址之后，发送数据是通过广播的形式发送，询问对方的MAC地址也是通过广播的形式来发送，那其他计算机怎么知道你是要传送数据还是要询问MAC地址呢？其实在询问MAC地址的数据包中，在对方的MAC地址这一栏中，填的是一个特殊的MAC地址，其他计算机看到这个特殊的MAC地址之后，就能知道广播想干嘛了。</p><p>假如两台计算机的IP不是处于同一个子网之中，这个时候，我们就会把数据包发送给网关，然后让网关让我们进行转发传送</p><p><strong>3. DNS服务器</strong></p><p>这里再说一个问题，我们是如何知道对方计算机的IP地址的呢？这个问题可能有人会觉得很白痴，心想，当然是计算机的操作者来进行输入了。这没错，当我们想要访问某个网站的时候，我们可以输入IP来进行访问，但是我相信绝大多数人是输入一个网址域名的，例如访问百度是输入 <a href="https://link.zhihu.com/?target=http%3A//www.baidu.com">http://www.baidu.com</a> 这个域名。其实当我们输入这个域名时，会有一个叫做DNS服务器的家伙来帮我们解析这个域名，然后返回这个域名对应的IP给我们的。</p><p><strong>因此，网络层的功能就是让我们在茫茫人海中，能够找到另一台计算机在哪里，是否属于同一个子网等。</strong></p><h2 id="4-传输层"><a href="#4-传输层" class="headerlink" title="4. 传输层"></a>4. 传输层</h2><p>通过物理层、数据链路层以及网络层的互相帮助，我们已经把数据成功从计算机A传送到计算机B了，可是，计算机B里面有各种各样的应用程序，计算机该如何知道这些数据是给谁的呢？</p><p>这个时候，<strong>端口(Port)</strong>这个家伙就上场了，也就是说，我们在从计算机A传数据给计算表B的时候，还得指定一个端口，以供特定的应用程序来接受处理。</p><p>也就是说，传输层的功能就是建立端口到端口的通信。相比网络层的功能是建立主机到主机的通信。</p><p>也就是说，只有有了IP和端口，我们才能进行准确着通信。这个时候可能有人会说，我输入IP地址的时候并没有指定一个端口啊。其实呢，对于有些传输协议，已经有设定了一些默认端口了。例如http的传输默认端口是80，这些端口信息也会包含在数据包里的。</p><p>传输层最常见的两大协议是 TCP 协议和 UDP 协议，其中 TCP 协议与 UDP 最大的不同就是 TCP 提供可靠的传输，而 UDP 提供的是不可靠传输。</p><h2 id="5-应用层"><a href="#5-应用层" class="headerlink" title="5. 应用层"></a>5. 应用层</h2><p>终于说到应用层了，应用层这一层最接近我们用户了。</p><p>虽然我们收到了传输层传来的数据，可是这些传过来的数据五花八门，有html格式的，有mp4格式的，各种各样。你确定你能看的懂？</p><p>因此我们需要指定这些数据的格式规则，收到后才好解读渲染。例如我们最常见的 Http 数据包中，就会指定该数据包是 什么格式的文件了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>五层模型至此讲到这里。对于有些层讲的比较简洁，就随便概况了一下。因为如果我说的详细一点的话，篇幅肯定会特别特别长，我着已经是尽最大的努力以最简洁的方式来讲的了。如果你想详细去了解，可以去买计算机网络相应的资料，强烈推荐《计算机网络：自顶向下》这本书。希望我的讲解能让你对计算机之间数据的传输有个大概的了解。</p><p>最后推广下我的公众号：<strong>苦逼的码农</strong>，文章都会首发于我的公众号，期待各路英雄的关注交流。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/58327059">https://zhuanlan.zhihu.com/p/58327059</a></p><p><a href="https://github.com/iamshuaidi/algo-basic/blob/master/学计算机网络/关于三次握手与四次挥手面试官想考我们什么？.md">关于三次握手与四次挥手面试官想考我们什么？</a></p><p><a href="https://juejin.im/post/6844904079974465544">前端需要了解的计算机网络知识， 这一篇就够了！(图文并茂，很详细)</a></p><p><a href="https://github.com/iamshuaidi/algo-basic/tree/master/学计算机网络">https://github.com/iamshuaidi/algo-basic/tree/master/%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一位浙大CS大佬的校招面试心得</title>
    <link href="/2020/09/07/2019-01-03-%E4%B8%80%E4%BD%8D%E6%B5%99%E5%A4%A7CS%E5%A4%A7%E4%BD%AC%E7%9A%84%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95%E5%BF%83%E5%BE%97/"/>
    <url>/2020/09/07/2019-01-03-%E4%B8%80%E4%BD%8D%E6%B5%99%E5%A4%A7CS%E5%A4%A7%E4%BD%AC%E7%9A%84%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95%E5%BF%83%E5%BE%97/</url>
    
    <content type="html"><![CDATA[<h1 id="写在20年初的校招面试心得与自学CS经验及找工作分享"><a href="#写在20年初的校招面试心得与自学CS经验及找工作分享" class="headerlink" title="写在20年初的校招面试心得与自学CS经验及找工作分享"></a>写在20年初的校招面试心得与自学CS经验及找工作分享</h1><blockquote><p>转载自 <a href="https://github.com/conanhujinming/tips_for_interview/blob/master/README-zh_CN.md">https://github.com/conanhujinming/tips_for_interview/blob/master/README-zh_CN.md</a></p></blockquote><p>我于大三（15年下旬）开始自学CS，并在去年（19年）参加了校招的实习与春招，很幸运地拿到了10来家公司的offer。在这里分享一下自己总结的面试心得与技巧、自学CS的方法与资料、自学CS的历程以及找工作的历程。当然，本人水平有限，且观点局限于个人经历，故有些说法难免会有不妥甚至不正确，欢迎指正。</p><p>全文接近4万字，可以根据目录各取所需。</p><ul><li><p><a href="#面试心得">面试心得</a></p><ul><li><a href="#声明">声明</a></li><li><a href="#编程部分心得">编程部分心得</a><ul><li><a href="#一个非常简单的例子">一个非常简单的例子</a></li><li><a href="#练习白板编程">练习白板编程</a></li><li><a href="#问清题目">问清题目</a></li><li><a href="#与面试官确认函数签名">与面试官确认函数签名</a></li><li><a href="#设计简单的测试样例">设计简单的测试样例</a></li><li><a href="#与面试官确认思路">与面试官确认思路</a></li><li><a href="#抓住面试官给的提示">抓住面试官给的提示</a></li><li><a href="#确认边界处理">确认边界处理</a></li><li><a href="#代码中使用可读性高的变量名和函数名">代码中使用可读性高的变量名和函数名</a></li><li><a href="#写代码过程中保持与面试官交流">写代码过程中保持与面试官交流</a></li><li><a href="#写完代码后主动测试">写完代码后主动测试</a></li><li><a href="#主动给出算法的复杂度">主动给出算法的复杂度</a></li><li><a href="#讨论算法的trade-off">讨论算法的trade-off</a></li></ul></li><li><a href="#计算机基础部分心得">计算机基础部分心得</a><ul><li><a href="#面经的使用">面经的使用</a></li><li><a href="#抓住面试官想问的点">抓住面试官想问的点</a></li><li><a href="#说出自己的insight">说出自己的insight</a></li><li><a href="#结合自己的使用经验阐述">结合自己的使用经验阐述</a></li></ul></li><li><a href="#项目部分心得">项目部分心得</a><ul><li><a href="#简要介绍项目背景">简要介绍项目背景</a></li><li><a href="#介绍项目的approach">介绍项目的approach</a></li><li><a href="#指出项目中的困难点和解决方案">指出项目中的困难点和解决方案</a></li></ul></li><li><a href="#论文部分心得">论文部分心得</a><ul><li><a href="#简要介绍自己research的背景">简要介绍自己research的背景</a></li><li><a href="#像做talk一样介绍一遍自己的论文">像做talk一样介绍一遍自己的论文</a></li></ul></li><li><a href="#其他部分">其他部分</a><ul><li><a href="#把握提问的机会">把握提问的机会</a></li></ul></li><li><a href="#模拟面试">模拟面试</a></li><li><a href="#面试大忌">面试大忌</a><ul><li><a href="#不懂装懂">不懂装懂</a></li><li><a href="#狂傲不羁">狂傲不羁</a></li><li><a href="#远远达不到面试官对自己的期望">远远达不到面试官对自己的期望</a></li></ul></li><li><a href="#心态">心态</a></li><li><a href="#最后">最后</a></li></ul></li><li><p><a href="#番外篇找工作的流水账与心路历程">番外篇：找工作的流水账与心路历程</a></p><span id="more"></span><ul><li><a href="#背景介绍与cs学习历程">背景介绍与CS学习历程</a><ul><li><a href="#我总结的学习方式">我总结的学习方式</a></li><li><a href="#cs学习历程">CS学习历程</a></li></ul></li><li><a href="#找工作之前的准备">找工作之前的准备</a><ul><li><a href="#刷题">刷题</a></li><li><a href="#面经与面试技巧">面经与面试技巧</a></li><li><a href="#模拟面试-1">模拟面试</a></li><li><a href="#日常实习">日常实习</a></li><li><a href="#做research">做research</a></li></ul></li><li><a href="#找实习">找实习</a><ul><li><a href="#google">Google</a></li><li><a href="#拼多多">拼多多</a></li><li><a href="#摩根士丹利">摩根士丹利</a></li><li><a href="#头条">头条</a></li><li><a href="#阿里">阿里</a></li><li><a href="#腾讯">腾讯</a></li><li><a href="#微软">微软</a></li><li><a href="#optiver">Optiver</a></li><li><a href="#百度">百度</a></li><li><a href="#airbnb">Airbnb</a></li><li><a href="#hulu">Hulu</a></li></ul></li><li><a href="#实习经历">实习经历</a><ul><li><a href="#入职">入职</a></li><li><a href="#项目初期进展">项目初期进展</a></li><li><a href="#进抢救室">进抢救室</a></li><li><a href="#恢复实习">恢复实习</a></li><li><a href="#总结">总结</a></li></ul></li><li><a href="#秋招">秋招</a><ul><li><a href="#百度-1">百度</a></li><li><a href="#腾讯wxg">腾讯WXG</a></li><li><a href="#阿里-1">阿里</a></li><li><a href="#optiver-1">Optiver</a></li><li><a href="#腾讯数据库内核">腾讯数据库内核</a></li><li><a href="#google-1">Google</a></li><li><a href="#offer选择">Offer选择</a></li></ul></li><li><a href="#一些学习资料推荐">一些学习资料推荐</a><ul><li><a href="#数学">数学</a></li><li><a href="#cs导论">CS导论</a></li><li><a href="#cs实用课程">CS实用课程</a></li><li><a href="#数据结构与算法">数据结构与算法</a></li><li><a href="#操作系统">操作系统</a></li><li><a href="#组成原理体系结构">组成原理/体系结构</a></li><li><a href="#计算机网络">计算机网络</a></li><li><a href="#编程语言">编程语言</a></li><li><a href="#软件工程">软件工程</a></li><li><a href="#机器学习">机器学习</a></li><li><a href="#深度学习">深度学习</a></li></ul></li><li><a href="#尾声">尾声</a></li></ul></li></ul><h1 id="面试心得"><a href="#面试心得" class="headerlink" title="面试心得"></a>面试心得</h1><p>2019年春招和秋招，我在中国进行了多场面试，其目的是找一个暑期实习职位和找秋招的正式工作。这是我的个人心得总结。</p><p>我实习和秋招都已经面了数家国内的一线大厂（包括腾讯/阿里/头条/百度/拼多多等）和数家外企（包括Optiver/Google/Microsoft/Hulu/Airbnb/Morgan Stanley等），收到过一次拒信。经过一段时间的面试准备与几次面试经历，总结出了一些个人心得，仅供参考。本文的前半段给出了一些我认为比较通用的技巧与心得，以供参考。本文的后半段介绍了自己跨专业学CS的一些经历以及自己找工作过程的流水账，并简单讲述了与找工作期间的心路历程，也可作为自学CS与择业的一个简单参考。</p><p>在进行了比较与思考后，我最终选择的公司是Optiver，职位是Low-Latency System Developer，工作地点在上海。文末也会阐述选择的理由。</p><h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><p>所有的面试技巧都是建立在一个基础之上：面试者已经具备了相对合格的实力。2018年下半年我在一家创业公司实习，秋招时也面试过一些候选人。在我看来，面试者如果自身基础不扎实、实力不够合格，那看所谓的面经、学习所谓的技巧也意义不大：合格的面试官可以非常轻易地通过一些follow-up问题问出面试者的真实实力。面试技巧和面经固然有意义，但学习技巧和了解面经，<strong>只能帮助有实力的面试者更大程度地发挥出自己的实力</strong> 。<strong>学习没有捷径可走，nothing replaces hard work.</strong> 希望每一位面试者都能尽早明白这个道理。</p><p>另一方面，我身边确实有一些这样的同学：他们相当有实力，但是却因为种种原因无法在面试中展现出自己的全部实力。事实上，不同的企业有不同的面试文化，比如Google的面试官希望面试者能成为一个他愿意一起工作的同事，字节跳动的面试官也许希望面试者是一个数学、算法、coding、工程都不错的全面人才，这样的人才更可能成为一个“能解决问题的人”。但是，作为面试候选人，我们其实没必要去针对各家公司的文化对症下药：应对面试应当有一些共通的要点。在我看来，面试最关键的一点在于面试者要意识到这不仅是一场测试，<strong>更是一次需要充满着沟通与交流的谈话，让面试官认为他/她愿意成为你的同事</strong>，希望每一位面试者都能尽早明白这个道理。</p><p>除了上面提到的我认为至关重要的两点以外，面试还有一些其他相对通用的面试技巧和要点。我这篇文章旨在总结一些这方面的东西，希望能够帮助到这样的同学。</p><h2 id="编程部分心得"><a href="#编程部分心得" class="headerlink" title="编程部分心得"></a>编程部分心得</h2><p>在面试过程中，面试官常常会给出几道算法问题，需要面试者提供思路或写下代码。在大多数公司的面试中，这一部分的表现都非常重要，而对一些外企来说，这部分的表现是具有决定性的（甚至是唯一重要的表现）。对于这部分的准备，首推<a href="https://leetcode.com/problemset/algorithms/">LeetCode</a>等网站，这里不再赘述。再提几句话，对于一些重视算法问题的公司如Google, hulu, airbnb, 微软, 头条等，不要抱着可能撞到原题的心态去准备，很难撞到原题的，对于这些公司，你需要做的就是反复练习提升自己的能力，而且由于题目较难，需要有较多的训练量。而另一些不是很重视这类问题的公司像阿里、腾讯什么的，则刷一些常见的题目就很可能撞到原题了，而且难度一般不大。因此，根据target公司的不同，可以有不同的准备方式。下面将列举一些其他在面试中我认为比较关键的点。</p><h3 id="一个非常简单的例子"><a href="#一个非常简单的例子" class="headerlink" title="一个非常简单的例子"></a>一个非常简单的例子</h3><p>这里先给出一个非常简单的问题，下面的关键点将结合这个问题来阐述。该问题为，<em>计算一棵二叉树的高度</em>。 简单的实现如下：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-built_in">int</span> get<span class="hljs-constructor">HeightOfBinaryTree(TreeNode<span class="hljs-operator">*</span> <span class="hljs-params">root</span>)</span> &#123;<br>    <span class="hljs-keyword">if</span> (!root) return <span class="hljs-number">0</span>;<br>    <span class="hljs-built_in">int</span> left_height = get<span class="hljs-constructor">HeightOfBinaryTree(<span class="hljs-params">root</span>-&gt;<span class="hljs-params">left</span>)</span>;<br>    <span class="hljs-built_in">int</span> right_height = get<span class="hljs-constructor">HeightOfBinaryTree(<span class="hljs-params">root</span>-&gt;<span class="hljs-params">right</span>)</span>;<br>    return max(left_height, right_height) + <span class="hljs-number">1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="练习白板编程"><a href="#练习白板编程" class="headerlink" title="练习白板编程"></a>练习白板编程</h3><p>面试的编程部分往往是白板编程：面试官要么要求在一个类似于Google Doc的地方写代码，要么就是干脆在白纸上写代码。这种情况下coding的体验与平时使用IDE的体验是完全不同的。以Google Doc为例，许多人（比如我）一开始甚至很难写出能编译的代码，更别说一遍写出bug-free的代码了。同时，没了IDE，debug的难度也会大大增加。而在白纸上写代码的难度则还要更进一步。适应白板编程的方法也很简单，只需要足量的练习即可。</p><h3 id="问清题目"><a href="#问清题目" class="headerlink" title="问清题目"></a>问清题目</h3><p>问清题目至关重要。如果你对面试官的编程问题理解得不清晰，那你应该立刻问一些能帮助你理解的问题。例如：数据范围是多少？这个数组的大小范围是多少？能不能给个样例？如果输入是这个，那输出应该是什么等等。在上面这个简单的问题中，可以问的一个问题是，二叉树的高度是什么（据我所知，高度的定义并非所有教材都一致）？</p><p>许多面试官在面试的时候，会故意先抛出一个模糊的问题。实际上，他们希望面试者能够经过一些询问理解问题。在这个过程中，面试者能够展现出自己对<strong>问题的分析能力</strong>以及<strong>沟通的能力</strong>。前者的重要性参见编程珠玑第一章：明确问题，战役就成功了90%。后者的重要性在于，问清题目的这个交流过程与面试者入职之后与同事讨论问题的形式非常类似。显而易见，一个能够很难沟通的面试者也很难成为一个很好沟通的同事。</p><p>如果没有问清题目，那会发生什么事情呢？在最坏情况下，面试者可能会花大量时间去解决一个完全错误的问题，面试结果也可想而知。或者运气好些，碰到了一个比较nice的面试官，给一些提示告诉面试者已经进入误区了，但这样不仅会浪费不少珍贵的面试时间，更会降低面试官对面试者的评价。我在面一家公司的时候，面试官给我出了一个题，这个题听上去比较困难，需要用到动态规划才能实现。我当时想，在面试开始阶段就给出一道比较困难的题，这对我来说也太不友好了！于是我询问了一句”数据的范围是什么呢？“面试官告诉我，数组的范围都是0-10的整数。这样的话，这个问题就变成了一个只需要6行代码就可以解决的贪心问题。如果我没有问清这个问题的话，面试的难度显然大大增加。</p><h3 id="与面试官确认函数签名"><a href="#与面试官确认函数签名" class="headerlink" title="与面试官确认函数签名"></a>与面试官确认函数签名</h3><p>确认了题目之后，我认为合理的做法是先和面试官确认函数签名，也即输入是什么参数，输出是什么参数等等。这一步的代价很低，而且相当重要。第一，这可以告诉面试官，你对函数签名的设计相当重视，而这一点在实际应用中很有价值。第二，这可以进一步帮你确认自己理解了题意。一个合理的函数签名可能就类似于LeetCode题目里的函数签名。上面代码中的签名就是一个比较合理的签名。</p><h3 id="设计简单的测试样例"><a href="#设计简单的测试样例" class="headerlink" title="设计简单的测试样例"></a>设计简单的测试样例</h3><p>写完了函数签名之后，可以针对函数签名简单地设计一组测试样例（如果面试官之前给了样例的话，也可以直接用面试官给的样例作为测试样例）。设计测试样例地主要有三个目的：一是进一步帮助确认自己对题意的理解没有出偏差；二是告诉面试官自己对测试十分重视；三是提醒自己编码完成的时候测试自己的程序。</p><h3 id="与面试官确认思路"><a href="#与面试官确认思路" class="headerlink" title="与面试官确认思路"></a>与面试官确认思路</h3><p>在自己有了一个思路之后，一定要和面试官确认这个思路是否合理。你可以给面试官解释你的思路为什么合理，面试官可能会和你讨论其中的一些要点。这样做有几点好处。第一，在解释的过程中，你的思路也会变得更加清晰（面试官充当小黄鸭）。第二，这也展现出你对沟通的重视性。第三，可能也是最重要的一点是，如果你的思路不正确，nice的面试官会提示你甚至直接指出错误所在，这样你至少不会在一个错误的思路上耽误太多时间。<strong>切忌有了思路之后，不与面试官交流直接写代码</strong>。尤其需要指出的是，如果你的思路对数据有什么假设，或者需要<strong>修改输入数据</strong>，那一定要和面试官确认这样的做法是合理的。</p><p>如果你认为这个问题与某个经典的问题思路一致，或者可以用到某个经典的算法，那么就直接点出来。例如计算二叉树的高度，实际上是一个后序遍历，那么可以直接点出来。</p><h3 id="抓住面试官给的提示"><a href="#抓住面试官给的提示" class="headerlink" title="抓住面试官给的提示"></a>抓住面试官给的提示</h3><p>有的时候一道题难度比较大，候选人一时想不到最优的思路，或者目前提出的思路是错误的，那合格的面试官可能会给一些提示帮助候选人思考，这时候候选人一定要抓住面试官给的提示。以上文给的例子为例，如果候选人想不到思路，那面试官可能会提示：“你觉得一棵树的高度与它的左右子树的高度可能有什么样的关系？”这就提示候选人可以用递归的方式来解决问题。抓住提示是很重要的：一方面，面试官给的提示很可能可以帮助你想到正确/最优的思路；另一方面，这其实也是双方能够进行不错的沟通的体现。</p><h3 id="确认边界处理"><a href="#确认边界处理" class="headerlink" title="确认边界处理"></a>确认边界处理</h3><p>在开始写代码以前或者是写代码的过程中，一定要思考代码的边界条件。最典型的边界条件有：数据是否会溢出？指针是否可能为空？链表是不是可能存在环？数组的长度是不是零？输入的数据会不会完全不符合题意的要求？在示例中，边界条件就是当结点指针为空时，高度应该是0。当你察觉到边界条件存在时，就可以询问面试官处理方式，或者直接告诉面试官你认为什么样的处理方式是合理的。对边界条件的处理在开发软件时也异常重要。忽视了一个边界条件，就会对程序鲁棒性造成极大的影响，可能直接造成巨大经济损失甚至是人员伤亡。</p><h3 id="代码中使用可读性高的变量名和函数名"><a href="#代码中使用可读性高的变量名和函数名" class="headerlink" title="代码中使用可读性高的变量名和函数名"></a>代码中使用可读性高的变量名和函数名</h3><p>在写代码的时候，尽量使用可读性较高的函数名和变量名。例如，要计算二叉树的深度，函数签名可以为<code>int getHeightOfBinaryTree(TreeNode* root)</code>入参就叫<code>root</code>（而非<code>node</code>）。递归时，左子树的高度的变量名可以叫<code>left_height</code> 。诸如此类。这样操作的主要目的也是让面试官看到你良好的编码习惯。</p><h3 id="写代码过程中保持与面试官交流"><a href="#写代码过程中保持与面试官交流" class="headerlink" title="写代码过程中保持与面试官交流"></a>写代码过程中保持与面试官交流</h3><p>实现算法的过程中，切忌闷头狂写而不与面试官交流。实际上，在写一些关键代码的时候，你完全可以告诉面试官你在实现什么功能。同样如前例计算二叉树深度，那你就可以告诉面试官，<code>int left_height = getHeightOfBinaryTree(root-&gt;left)</code> 是在计算左子树的高度（良好的函数名和变量名其实也让这行代码不言自明），而<code>int root_height= max(left_height, right_height) + 1</code> 则是根据左子树和右子树的高度计算当前根节点的高度。</p><p>当然了，在这个简单的示例中，交流或许显得不是那么重要，但是在一些复杂的问题中交流可能会非常重要。例如，示例的follow-up是请不用递归实现同样的功能，或者更进一步，请用常数空间实现同样的功能。在这样的问题中（代码可能长达数十行），交流就至关重要了。面试官需要和你交流来理解你的思路与状态，你同样需要交流来理清思路。这种写代码过程中的交流也是正式工作时非常重要的能力。当然了，这样的交流也不必过于频繁，否则也可能影响自己的编码状态。</p><h3 id="写完代码后主动测试"><a href="#写完代码后主动测试" class="headerlink" title="写完代码后主动测试"></a>写完代码后主动测试</h3><p>在你写完代码之后，不要急着告诉面试官你已经写完了。最好先手动跑一个/数个简单的样例。注意跑这个样例的过程要让面试官可以看见并轻易地理解，这常常是需要一些练习的。例如，我在Google Doc上跑样例的做法是，在屏幕上写出中间变量的当前取值，然后用鼠标光标告诉面试官现在程序跑到了哪一行代码，当前各个变量的取值是多少等等。主动测试的好处有很多。第一，这告诉面试官你很重视测试，而测试在实际生产中是非常非常重要的。第二，一个简单的样例常常可以找出不少类似于typo这样的小错误。第三，如果你的样例给得不错，那你甚至能够借助这个样例找到程序中的bug并纠正它，这总是要好过面试官发现并告诉你程序中存在着bug。主动测试时，你也可以确认你的程序可以很好地处理边界数据。</p><p>我自己在面一家外企的时候，主动测试的习惯就给我带了很大的回报。当时我写了一段不算复杂的程序（约20行左右），可是因为情绪紧张，程序中包含了一个相对隐蔽的bug。写完之后，我习惯性地跑了一个简单的样例，这花了我大约3分钟的时间，但却让我注意到了那个bug。我赶紧修复了这个bug。到了面试的提问环节，我问面试官本场面试中我表现最好的一点是什么。他告诉我：”是你通过一个样例发现了你的bug。实际上，在你写出了那段代码的时候我就注意到了这个bug，当时我在犹豫要不要提醒你。而你随即开始了测试并找到了这个bug。“这场面试的结果是，在面试结束半小时左右我就收到了通过面试的消息。</p><h3 id="主动给出算法的复杂度"><a href="#主动给出算法的复杂度" class="headerlink" title="主动给出算法的复杂度"></a>主动给出算法的复杂度</h3><p>在写完代码之后，应当主动分析自己算法的时间与空间复杂度。一方面，这样可以展示自己扎实的算法基础。另一方面，这也可以告诉面试官自己有这方面的意识。当然了，如果复杂度分析的有误，那这个分析也可能会成为一个减分项。</p><h3 id="讨论算法的trade-off"><a href="#讨论算法的trade-off" class="headerlink" title="讨论算法的trade-off"></a>讨论算法的trade-off</h3><p>有些时候，题目的解法可能存在一些trade-off。最常见的就是时间-空间的trade-off，当然有时也会有一些其他的trade-off。如果意识到了这道题目存在trade-off，那么可以主动地与面试官聊trade-off，让他/她知道你的思考过程与选择。</p><h2 id="计算机基础部分心得"><a href="#计算机基础部分心得" class="headerlink" title="计算机基础部分心得"></a>计算机基础部分心得</h2><h3 id="面经的使用"><a href="#面经的使用" class="headerlink" title="面经的使用"></a>面经的使用</h3><p>计算机基础部分的内容包括数据结构、操作系统、编程语言、计算机网络等等。这部分的准备很大程度上是需要一些扎实的基础的，再配合一些面试公司的面经。有些同学想仅仅靠看面经就应付过去，我可以说大多数情况下是不太可能的。有经验和水平的面试官可以轻易地通过几个follow-up问题来判断出来这名候选者是不是靠面经回答出来前面的问题的。当然了，面经对于这块内容仍然是非常有价值的，但阅读面经的时候要注意，并不能仅仅看一下某道题目的答案就够了，而是要看这个题目考察的是哪一块的知识，这一块知识自己有没有遗忘的、生疏的、不扎实的，如果有的话要去做相应的准备。<strong>面经是告诉你这家公司面试的时候喜欢问哪些知识，而不是告诉你他们喜欢问哪些特定的问题</strong>，虽然有的时候有些高频问题确实可能在你的面试中出现。</p><h3 id="抓住面试官想问的点"><a href="#抓住面试官想问的点" class="headerlink" title="抓住面试官想问的点"></a>抓住面试官想问的点</h3><p>有些同学被问到一些自己会的基础知识的时候会特别激动，想抓住这个机会表现自己，就会事无巨细地回答一波。我个人认为，如果是基础知识的话，其实不用回答得特别详细，说出一些面试官想问的关键要点就可以了。有时候不一定能判断出来面试官想问的要点，这也不要紧，就说一些自己认为是关键的要点，然后等着面试管继续问follow-up就可以了。这里举一个简单的例子，如果面试官问进程与线程的区别，那么简单地说线程是调度的最小单位，同一个进程的线程共享地址空间，容易有线程安全问题；进程是多数资源分配的最小单位，所以进程的地址空间都是独立的，资源安全问题相对较少。回答到这个份上就差不多够了，然后等面试官继续问follow-up，而不需要去解释为什么线程会有安全问题等。之所以建议这么做，是因为对于有些公司，面试时间是有限制的（例如Google, hulu等），所以面试时间是很宝贵的，你应该用这珍贵的时间去展示自己的优势，而不是说一些绝大部分人都懂的trivial的知识。当然了，有经验/不nice的面试官可能会打断你，问他自己感兴趣想问的东西，但如果你运气不好恰好面试官没啥经验或者不喜欢打断人，那这样浪费宝贵的时间是很可惜的。</p><h3 id="说出自己的insight"><a href="#说出自己的insight" class="headerlink" title="说出自己的insight"></a>说出自己的insight</h3><p>如果针对某个问题有自己一些独到的见解，或者是这个知识在很多教科书上可能看不到，很多同学也不一定知道，那么在回答问题的时候说出自己的这个insight，当然前提是自己的说法是有道理的。这里举一个简单的例子，比如一个面试问题是，可以用什么数据结构来实现队列。回答可以说是链表，接着可以补一句但是链表实现队列的性能不一定很好，因为链表节点的地址空间不是连续的，对cache不友好（小问题：那么如何改进这一点呢？）。这种知识其实是有一些经验的人或者基础扎实的人都知道的，不算是什么难点，但作为应届生，能直接说出这一点还是可能会让面试官觉得这个候选人基础不错。</p><h3 id="结合自己的使用经验阐述"><a href="#结合自己的使用经验阐述" class="headerlink" title="结合自己的使用经验阐述"></a>结合自己的使用经验阐述</h3><p>如果在某些基础问题上自己有一些实际经验，那么可以结合自己的经验来回答，这样会让面试官觉得这个候选人不仅基础扎实、经验丰富，而且学以致用、分析问题的能力也挺强的。</p><p>这里举一个简单的例子，比如面试官问hash table处理冲突有哪些常用的方式，各有什么优缺点。那么可以回答常用的有线性探测和拉链法两种。如果自己有相应的经验，那么就可以结合经验谈谈优缺点，例如线性探测在实际使用的时候常常需要空间开得比较大，hash table的装载因子需要维持一个一直比较小的状态（比如25%-50%这样），否则的话性能就会很差，因为查询和插入都会频繁地进行长距离的线性探测。而拉链法对空间的利用效率就会比较高。在提供足够的空间的时候，按经验线性探测会比拉链法快很多，比如之前做了个项目，在满足空间条件的时候线性探测会快7倍左右（这是在结合经验谈），原因是线性探测比拉链法对cache更友好（这是基础知识）。</p><p>类似于这样的回答方式，可以让面试官留下一个很好的印象，认为这位候选人的整体素质也非常出色。</p><h2 id="项目部分心得"><a href="#项目部分心得" class="headerlink" title="项目部分心得"></a>项目部分心得</h2><h3 id="简要介绍项目背景"><a href="#简要介绍项目背景" class="headerlink" title="简要介绍项目背景"></a>简要介绍项目背景</h3><p>如果面试官是很熟悉这个领域、这类项目的人，那么你可以make some assumptions，即不需要做多少背景介绍。否则的话，还是建议简单谈一下自己项目的背景是什么。这是因为在不同的背景下，同一种功能的实现常常会有不同的选择。这样的背景介绍能帮助面试官更好地理解这个项目，以及大概理解一些实现的选择。背景主要包括场景、问题定义、需求、自己负责的部分扮演的角色等等。</p><h3 id="介绍项目的approach"><a href="#介绍项目的approach" class="headerlink" title="介绍项目的approach"></a>介绍项目的approach</h3><p>介绍完项目背景后，需要简单介绍一下自己这个项目的解决方案。解决方案主要是使用了什么技术、什么工具、怎么样的实现等等。需要注意的是，介绍解决方案的时候最好要结合场景一起说，否则会缺乏一些说服力。</p><p>这里仍然举个简单的例子。例如做深度学习的落地，深度学习框架选用的是腾讯的ncnn，那么最好说一下因为场景是嵌入式arm设备，且没有显卡，在这种场景下，ncnn做了很多指令级的优化，速度会更快。</p><h3 id="指出项目中的困难点和解决方案"><a href="#指出项目中的困难点和解决方案" class="headerlink" title="指出项目中的困难点和解决方案"></a>指出项目中的困难点和解决方案</h3><p>针对项目中的困难点要特别认真地谈论一下，需要介绍为什么这个点是个困难点，解决方案大致是什么样的思路，为什么要这样去设计解决方案，最终达成了一个什么样的效果。如果一个候选人能展示出准确的痛点、瓶颈分析能力，并且能提出合理的解决方案的能力，那我相信面试官对他的评价会大大提升。</p><p>这里同样举一个简单的例子。例如做数据库实现，项目中有一个问题是数据库太大，不可能放到内存里，但如果都放硬盘的话又太慢，这是项目中的一个困难点。解决困难点的关键是同时利用内存的速度优势与硬盘的容量优势，设计一个存储分层模型。做实验观察到90%的针对数据库的查询仅集中在10%的数据上。那么解决方案可以是设计一个冷热分离的模型，仅仅在内存中存储一些热（即查询频繁）的数据，而将冷（即查询频率很低）的数据存在硬盘上，同时设定一定的策略定期做冷热数据替换。经过这样的设计之后，数据库的查询速度提升了30倍。</p><h2 id="论文部分心得"><a href="#论文部分心得" class="headerlink" title="论文部分心得"></a>论文部分心得</h2><h3 id="简要介绍自己research的背景"><a href="#简要介绍自己research的背景" class="headerlink" title="简要介绍自己research的背景"></a>简要介绍自己research的背景</h3><p>与项目不同，很多冷门的research的背景面试官往往是不了解的，所以常常需要做相对详细一些的背景介绍。</p><h3 id="像做talk一样介绍一遍自己的论文"><a href="#像做talk一样介绍一遍自己的论文" class="headerlink" title="像做talk一样介绍一遍自己的论文"></a>像做talk一样介绍一遍自己的论文</h3><p>在面试之前，可以先自己精细地准备一下论文的介绍。假设这个面试官对这个领域不熟悉，如何才能让他在较短时间理解这个研究领域，大概明白领域的痛点，并理解你的论文的思路、解决方案与重要性呢？</p><h2 id="其他部分"><a href="#其他部分" class="headerlink" title="其他部分"></a>其他部分</h2><h3 id="把握提问的机会"><a href="#把握提问的机会" class="headerlink" title="把握提问的机会"></a>把握提问的机会</h3><p>大多数面试的最后一个环节，面试官都会问：“你有什么想问我的吗？”要抓住这个机会问一些问题：一些好的问题是可以让面试官印象非常深刻的；并且面试官很多是一家公司中比较厉害的员工，也是同行业同方向的资深从业者，问他们问题是有可能切实地学到一些东西的。而且由于这是面试的最后一个环节，会对印象分有比较大的加成。举一些算法岗方向的简单例子：</p><ol><li>您觉得算法岗的从业者的核心竞争力是什么？（对行业现状与自己发展方向的关心）</li><li>您对我有什么建议吗？ （对自己发展方向的关心）</li><li>请问您的部门目前主要是在做什么方向的？ （对技术的好奇，对部门的关心）</li><li>那您在做xx项目的时候会不会遇到xx问题呢？您觉得解决这个问题的核心在哪里？ （这个问题对能力的要求就比较高了，如果能问出有价值的的话是非常强的加分项）</li><li>我在做算法的时候，有的时候效果不work，会有点不知道怎么去debug。您能分享一些您debug算法的经验吗？（展现出自己确实有一些这方面的经验，会思考自己做事的时候存在的问题并寻求改进措施，并且展现对知识、技术的热情）</li><li>请问您的部门在平时做算法方面的研究的时候，会用哪些方法来保证程序/算法的正确性？（对测试的重视，并且对实践中重要的东西有一定的认识）</li></ol><h2 id="模拟面试"><a href="#模拟面试" class="headerlink" title="模拟面试"></a>模拟面试</h2><p>强烈建议在面试之前找人模拟一下，并让对方给你一些反馈。这样能够大大降低紧张感，熟悉面试流程并提高面试表现。当然了，还有一个重要的方式就是多多投递，先拿一些自己不target的公司练练手，磨练自己的心态与面试技巧。</p><h2 id="面试大忌"><a href="#面试大忌" class="headerlink" title="面试大忌"></a>面试大忌</h2><p>我也曾当过几次面试官，也参加过一些面试并了解过其他人的面试情况，这里简单说几条面试大忌，一定要避免犯的错误。</p><h3 id="不懂装懂"><a href="#不懂装懂" class="headerlink" title="不懂装懂"></a>不懂装懂</h3><p>对自己不懂的东西（甚至是没有十成把握的东西），一定要诚实地说出来，千万不要不懂装懂。我把这一点放在最前面，是因为我作为面试官以及平时与人讨论技术的时候，就非常讨厌别人不懂装懂。面试官的水平往往比你高很多，一下子就能判断出来你是真懂还是装懂。所以，碰到自己不懂或者没把握的问题，我建议直接告诉面试官说这个问题我没把握，不是很懂。但如果你有一些思路的话，可以接着说“虽然我不太懂，但是可以试着说一下”，这就可以变成一个展示你解决问题分析问题能力的机会了。而如果你的分析思路很合理，得出的结论也大差不差，那甚至可以很大程度地提升面试官对你的评价。</p><h3 id="狂傲不羁"><a href="#狂傲不羁" class="headerlink" title="狂傲不羁"></a>狂傲不羁</h3><p>面试的时候，人要有自信，但是态度一定要平和并且尊重面试官，切不可恃才傲物、狂傲不羁。有一些公司会非常看重这一点，如果你给面试官留下了不好沟通的印象，那往往是一票否决。但面试的时候，偶尔也会碰到面试官不是很懂犯错误的情况（比如国内的一些大厂），这个时候你最好是平和地去与面试官讨论，如果他坚持不肯认错，那你也不要去较真，否则的话可能你面试就挂了。有一种情况是可以去与面试官较真的，那就是你完全不在乎这家公司的offer，这时候你可以放开了较真哈哈哈。另一方面，当你面试一家公司或者一个组，碰到面试官不懂装懂又不肯认错的时候，你也得考虑一下这个组是不是值得你去。</p><h3 id="远远达不到面试官对自己的期望"><a href="#远远达不到面试官对自己的期望" class="headerlink" title="远远达不到面试官对自己的期望"></a>远远达不到面试官对自己的期望</h3><p>在面试之前，面试官往往会根据候选人的Profile而对候选人心里有一个大致的期望值。在面试的时候，面试官会根据候选人的表现评分，而这个评分实际上有很大一部分是与对候选人的期望值有关。如果一个候选人的Profile很好、简历里吹得天花乱坠，那面试官对这位候选人的期望值就会很高；结果面试的时候一问细节三不知，远远达不到面试官对候选人的期望。在这种情况下，面试官往往会给一个非常差的评价。这种case或许比较极端，但候选人面试展示的水平（以阿里的的评级为例，假设是A）与自己的Profile（假设是A+甚至阿里星）不match是相对常见的现象。这时候，面试官甚至有可能给候选人一个比候选人真实水平还差的评价（譬如B+）。这也是候选人为什么要对自己简历上的东西了如指掌，能够做到即使被狂轰滥炸也能谈笑间应对的一个原因。其实上文所提到的，当面试的时候被问到自己不会或者不确定的问题的时候要先诚实地告诉面试官，之后再靠自己的common sense、逻辑思维以及其他的一些知识来推理出一个相对合理的答案。这也是一种对面试官的期望值的管理。先诚实地告诉面试官来降低他对自己的期望值（譬如降低为B+），然后再展现自己其他的能力超出这个期望值（譬如展现出A的水平），面试官说不定反而更容易appreciate你的表现。</p><h2 id="心态"><a href="#心态" class="headerlink" title="心态"></a>心态</h2><p>面试总会有运气成分与偶然性，放平心态，不要因为害怕被拒就不敢投递，也不要因为患得患失而在面试的时候十分紧张。在面试中尽量让自己自然、轻松。当然，一些轻微地紧张有时是可以让自己发挥更好的，但是要适度，切不可紧张过头。面试中即使有些内容答得不好，也不要当场就心态崩盘，要沉着应付。当自己没有什么思路的时候也不要太慌，可以试着从基本的地方开始分析。例如做算法题，可以分析一些toy example，有时候能获得一些思路。回答CS基础题、system design等题目也可以从基础的地方开始分析，甚至是与面试官一起一步一步得出结果。我自己在参加一次面试的时候，一道算法题问清楚题目就花了10多分钟，然后10多分钟没有思路，同时面试官还在给我施加一定的压力。要知道面试总共就45分钟，这样的表现属于非常糟糕的了。所幸我当时稳住了心态，利用一个toy example得到了正确的思路，写出了bug-free的代码，最后还是让面试官相当满意。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>需要说明的是，每个人都有自己的面试风格，很多面试官也会有自己的喜好，所以没有一套universal的面试方案。本文提到的一些技巧什么的，主要是我自己总结出来适用于自己的风格与方案，读者完全可以根据自己的实际情况与面试时候的感受来调整。举个例子，本文提到的编程部分心得，主要是针对Google这样的公司的算法题部分。我自己也有过一些面试经历，面试官非常不喜欢候选人在写代码的时候与他交流，甚至会在你写代码的时候自己去做别的事情 :( 这时候你最好就乖乖闭嘴，把代码写出来即可:) 因此，也希望各位因时制宜，因地制宜，结合实际情况来进行面试。最后祝大家都能有满意的offer~</p><h1 id="番外篇：找工作的流水账与心路历程"><a href="#番外篇：找工作的流水账与心路历程" class="headerlink" title="番外篇：找工作的流水账与心路历程"></a>番外篇：找工作的流水账与心路历程</h1><p>本文的这一部分将以流水账的形式简单讲讲学CS这几年来的一些经历，以及找工作的流水账与心路历程。</p><h2 id="背景介绍与CS学习历程"><a href="#背景介绍与CS学习历程" class="headerlink" title="背景介绍与CS学习历程"></a>背景介绍与CS学习历程</h2><h3 id="我总结的学习方式"><a href="#我总结的学习方式" class="headerlink" title="我总结的学习方式"></a>我总结的学习方式</h3><p>在学习CS初期我走了很多弯路，相信了一些不合理（或至少是不适合我）的所谓“编程入门指南”。之后经过自己的摸索，到现在总结出了一套适合自己的学习方案。这里首先给出我的学习方案以供参考。使用这个学习方案还需要不错的英语水平，以后我有时间的话也许也会写一下自己学习英语的经验与心得。</p><p>一个CS领域的学习过程大致可以分为以下三个阶段。当然，有的时候不同阶段是可以迭代地进行，例如开始科研之后发现自己还缺少了某些基础知识，那可以再进行基础知识的学习。同时，在学习中，很重要的一个指导思想是要获得<strong>监督信息与正反馈</strong>。三个阶段如下：</p><ol><li><p>如果想学某门课程知识，那就找国外名校（主要是MIT/Stanford/CMU/Berkeley）有课程录像的对应课程，假装自己真的在上课一般地按照课程安排上课、看阅读材料并<strong>完成作业</strong>。<strong>完成作业是极其重要的</strong>，因为在这个过程中你会获得大量的监督信息，来指导你发现自己哪些地方学得不扎实；而如果仅仅看视频与阅读材料的话，常常会产生自己已经学懂了的错觉。另一方面，许多编程的作业完成之后也可以带来<strong>成就感与正反馈，支持着自己的学习动力</strong>。</p></li><li><p>之后，找自己感兴趣的领域。根据是做开发还是做算法，做研究还是做工程，第二阶段可以分成两种不同的方式。</p><p>a) 做工程：找一些适合练手的项目，自己实现与重构，并对照他人的实现方案。实际上如果第一阶段认真完成了好课程的作业的话，那么已经算是完成了很多练手的项目了。</p><p>b) 做科研：了解这个领域的经典方法与最新方法，并<strong>复现这些方法</strong>，做实验比较结果，同时以论文的要求写报告分析结果，这个其实基本上就是国外很多课的project。<strong>复现好的论文是极其重要的，因为复现可以获得监督信息</strong>。在真正入门一个领域之前，人们常常会产生自己已经读懂了论文细节的错觉，实际上如果你没有能力复现一篇论文，你就不能说自己已经完全读懂了这篇论文。另外，论文中往往不会给出全部的细节，但这些细节并不trivial，需要你在复现论文的过程中自己发现体会。尤其是在系统相关的领域，<strong>论文中一句带过的设计细节往往也蕴含着insight</strong>，而这在你复现的时候会有更深的体会。另外，复现论文也能带来一些正反馈。复现完论文做实验的报告也是很重要的，这个整理的过程会逼迫你去进行一些更深层次的思考，整理的实验结果也可以供以后随时查阅。</p></li><li><p>同样，第三阶段也可以分成两种不同的方式。</p><p>a) 做工程：可以考虑在GitHub上找一些好的开源的项目，读源代码，并且帮助社区进行开发。有条件的话，可以去实习。</p><p>b) 做科研：之后就是阅读论文、跟踪最新的成果、提出想法并撰写提交论文了。</p></li></ol><p>需要指出的是，这套方案不一定适用于所有人，仅仅是我摸索出来的适合自己的方法。这里再多说一句，在整个学习的过程中，一定要<strong>多提出问题</strong>。例如刚开始阅读一些经典论文的时候，可以逼自己去提出5个与论文相关的问题，而且一定是那种自己提出来之前不知道答案的问题，再试着自己解答。中国人造词说学问学问，只有学没有问的话是一定不够的。</p><p>这里再简单说一下如何找到合适的课程。我一般直接找MIT/Stanford/CMU/Berkeley（四大）的有视频的课程。MIT的课程很多在<a href="https://ocw.mit.edu/index.htm">OCW</a>有收录，可以在OCW里直接搜索。更常用的方式大概有两种，其一是在Google上搜学校名字+课程的名字找到课程主页，然后就可以跟这门课程了。例如想学操作系统，就搜MIT Operating System即可。第二种是在Youtube上搜学校名字+课程，很多有录像的课程可以这样搜到。除了这些名校的课程以外，很多MOOC的课程也是值得一看的。这里再简单给出我对各大院校/MOOC课程质量的评价：</p><p>主要的MOOC平台有Coursera, Edx和Udacity。Coursera算是三大mooc平台做得比较成功的了，课很多，有好课也有一般的课，鱼龙混杂。课程类型偏学院风。Edx我以前看的时候是平均质量最高的，但那时候完全不商业化，课很少，学院风。Udacity课也不错，挺多的质量挺高，课程是工业风。MOOC的好处是针对自学的人有优化，但坏处是课程难度普遍较低，适合零基础入门的时候用。四大院校的真实课程的难度与质量往往会高好几个档次。其中MIT的课程质量在我看来是最高的，公开的课程也多。Berkeley的也不错，但是公开的课程相对少。CMU的课程质量也还可以。Stanford的课程的讲课质量感觉可能会低一些&gt;_&lt;，因为很多是PhD TA上课，感觉比很多经验丰富的Professor还是有一些差距。如果实在找不到有视频的课程资源，那也只好不看视频，直接看课件、reading材料和写作业了。这样往往吃力一些，效果差一些，但也能学到不少东西。</p><h3 id="CS学习历程"><a href="#CS学习历程" class="headerlink" title="CS学习历程"></a>CS学习历程</h3><p>这段非常冗长，可以直接跳过。</p><p>我是根不正苗不红的浙大CS小硕。高考填志愿的时候，突然有一些家国情怀，选择去西北工业大学航天学院学习航天，立志航天报国，也正是因此自己的计算机基础异常不扎实:( 本科的时候与CS相关的课程只有一门C++程序设计，自己学得也算是班上最认真的之一了，还刷了100来道POJ来巩固。当时任课老师劝我参加学校的ACM集训，我却中二地认为coding这种事情只不过是实现航天的工具而已，就没去参加，错过了一个亿。后来到了大三开始接触一些系里一些项目以及七七八八的事情的时候，我突然意识到他们做的航天项目和自己想的不太一样，航天系统里的人也与自己想得不太一样，遂萌生退意。</p><p>于是开始自学CS。因为周围没有认识学CS的人，就去看知乎上大家的推荐，大家都推荐从SICP和CLRS入门，于是我就去看SICP和CLRS并努力做习题，而且当时太年轻，看的还是翻译版的。看到后面怀疑自己完全就是弱智，别人的入门书我怎么就学不懂呢…其实有几个原因，一方面是这两门书挺难的，不见得适合入门；另一方面是翻译版翻译得也不是很好。可以说自己刚开始学CS的时候走了不少弯路。后来看到Coursera上有开Princeton的Algorithm课，就去把那个课跟了一遍，完成了习题。这时候才终于感觉原来自己还是能学会一些东西的:) 又把Edx上MIT的6.001x跟完了，还看了一些Harvard的CS50课程的内容，这时候终于感觉自己算是开始学CS了。</p><p>那门课跟完之后，AlphaGO的事情开始刷屏。因为自己之前对围棋有些兴趣，也自学了一些，知道李世石是什么水准的棋手，于是就关注了AlphaGO与小李的比赛。本以为小李会轻松击败AlphaGO，没想到AlphaGO把小李吊锤了一通，很是震撼又让人感到excited，有一种看到世界的大幕缓缓拉开的感觉。也是在那时听到了人工智能(AI)这个词，觉得哇好高大上又有趣，就萌生了做AI方向的想法。</p><p>然后就去找了《Artificial Intelligence: a Modern Approach》看。结果又是一本垃圾翻译书，国防科大的一些老师翻译的，我怀疑我用Google Translate都能比他们翻译得好…后来我终于开窍了，买了本影印的英文版看。这本书写得挺有意思的，容易读懂又不失深度，就是太厚了有1000多页，读了我很久，读到后面晕乎乎的。当时看了最有印象的几段有：启发式搜索A*，书上我记得还将围棋归在这一类，说AI围棋距离人类职业水平仍有较大距离，短时间内很难得到人类顶尖水平:) ；用一阶逻辑从数据与规则中学习新的规则（也就是规则学习），当看到这样的程序证明了许多数学定理，有的证明甚至比原始证明更优雅的时候尤其激动与兴奋，不过后来真的开始学AI/ML之后发现这已经完全是个dead area；机器人使用强化学习来学习走迷宫和闯关游戏；用MM做NLP，HMM来做POS Tagging等等。不过要说明的是，这本书以现在的眼光来看已经有些过时了，而且里面大部分的内容与现在的研究、应用领域关系不大，不是特别建议阅读了。</p><p>看完那本书后，调研发现大家都在做机器学习(ML)，就开始自学一些ML的知识。大三的暑假先从Coursera上Andrew NG课程入门，这门课是一门挺不错的课，我跟完并完成了习题，学习的过程中觉得ML真有趣又有用。然后开始读一些相关的书，先看了Tom Mitchell的那本《机器学习》，这也是一本挺不错的教材（虽然我又看的翻译版，晕乎乎的），不过也有些过时了。看完之后注意到周志华老师有本新出的《机器学习》，还有本出名的书是李航的《统计学习方法》，我就去买了这两本看，不过也只是很快地扫了一遍 。周志华老师的书写得还是挺认真的；李航的书的话我觉得太干了，不适合用来学习，适合用来复习。草草过了一遍两本书之后，又看了张学工老师的《模式识别》，不过后来发现这本书的内容与《Pattern Classification》非常接近，还是推荐直接去看PC。之后开始看《Elements of Statistical Learning》 （ESL）。但是ESL对当时的我来说有点太难了，很多都看不懂。而且我发现自己线性代数的基础似乎不够扎实，就先学了MIT的18.06，再继续去看ESL。值得一提的是，MIT的18.06是一门非常好的课，很适合作为对自己线代知识的拨乱反正，消除一些国内垃圾教材带来的坏毛病。学完之后继续看ESL，胡乱看到unsupervised learning之后，开始写research proposal申请日本的大学。</p><p>这里要说一下，我之前一直比较向往能出国学习，但是美国的master实在太贵了，家里没钱上不起，CS PhD我又不可能申得上，我就转而准备申请日本的学校。日本的学校学费很便宜，一年也就2w人民币左右，东京的话生活费一年在8w左右，但是日本的master很多能申请到奖学金，至不济也能打工赚钱。我本来想走他们的G30计划，这是一个不需要日语成绩只需要英语成绩，并提供全奖的项目，结果那个项目那年因为日本扩充军费而没有资金了……为此我还自学了一段时间的日语去考了N1，也考了托福(110)。日本的教授在申请中往往具有绝对的权力，说要你就要你，所以陶瓷在申请日本的大学中极为重要。陶瓷的方式往往是写一个研究计划书，发给教授，教授会判断你的基础、能力、vision等等。总之，我在大三的暑假学了一些机器学习知识之后就开始写research proposal了，当时还不懂深度学习，写的是半监督学习，主要是对以前的半监督学习（self-training, co-training那套）的工作的简要介绍，并写了一些自己浅陋的看法。现在看来，那时候的水平自然是非常差劲，对机器学习并没有形成一个整体的认识，也没有自己的insight；research上完全没有上路，对领域未来发展的看法自然也是错漏百出，不值一提。暑假结束之后，9月初的时候调研了很久教授的信息，看了一些他们最近的工作，并找了三个不同大学的教授发了套磁信。不过非常幸运的是三个教授都给了比较积极的回复，进行了一些交流后，其中两个教授直接表示愿意提供funding :) 我想我当时吸引到他们的可能是本科还可以的GPA，以及出色的语言成绩？</p><p>这时候家里出了一些变故，然后我的姐姐也告诉我说自己想出国的想法给父母造成了很大的压力和负担。经过了一晚上的仔细思考，我决定放弃出国准备考研浙大，主要原因是浙大离家比较近，照顾家里的事情也会方便一些。但是我不确定自己能不能考得上研，毕竟准备时间很短只有3个月左右，我也没有什么CS基础。第二天起床之后，我借了同学的考研真题做了一下前一年（2016）的数学卷和英语卷，数学的话卷面大概120分左右吧，英语的话除掉作文翻译大概是扣了5分左右。当时就感觉3个月左右就努力突击专业课，应该有机会考上。遂写了邮件告诉日本的教授们自己因为一些原因不打算去了，并买了一些考研的书开始准备考研。</p><p>准备考研的过程是漫长且枯燥乏味的。除了中间有5天回老家参加姐姐的婚礼以外，我每天都能保证9-10小时左右的有效学习时间。我的时间分配策略很简单：大部分时间（也许70%-80%）都投入到专业课的学习上，花一些时间在政治上，数学则定期做一套真题，而英语则一点时间都不花。总得来说这个策略是合理的，当时的失误之处在于学习专业课的资料选择问题。我觉得需要针对考试准备，就使用了国内的教材进行学习，事实证明这个选择是非常错误的，我不仅学得很吃力，最后专业课考得也很低，而且计算机基础也不是很扎实，后面还是花了大量的时间用四大的课程去补。总之呢，就这样日复一日地学习与上课。值得一提的是，我们专业大四上仍然有很多会点名的专业课:) 我在课上就一直看政治的内容，也因此有些课分数考得很低，让我本科的GPA最终变得很难看。终于，考研的日子到来了，我并不很紧张地上了考场，发现我考位左右两个考计算机的同学都弃考了。考场上觉得专业课是真的难，其他的科目都普普通通。按部就班地考完之后，就开始了漫长的等待成绩的生活。在等待成绩期间，我开始刷浙大的PAT，即考研的上机考试，主要是按照胡凡学长的《算法笔记》刷。一开始我还不是特别认真，一天大概只做两三个小时的机试题吧，剩下的时间去学一些其他的东西，例如Stanford的CS229。后来考研成绩出来了，我发现自己的初试成绩排名不靠前，这样如果机试考得很低的话就有可能考不上研了。为了让自己的机试成绩尽量高，我把每天刷题的时间提高到了8-10小时左右，这个状态维持了一个月左右，把历年PAT题目（大概100多道？）来来回回刷了两三遍，这才有了一定的信心。机试的考试来临了，我在西安交通大学的考场参加考试，考场里总共也就10来个人。题目非常简单，应该是历年最简单的一次。我顺利地秒完所有的题目之后，就交卷出场了。</p><p>之后就是准备考研面试与找导师了。我并没有认识在浙大计算机学院的同学，只能在网上搜一些相关资料并咨询一些前几年考研的学长学姐，按照他们的推荐联系了几名老师。当时联系的老师们基本都给了回复，约我前去实验室面试。我有一些机器学习的基础，除了本科不是CS这一个致命伤以外其他的条件也不算很差，所以也收获了一些offer。同一个时间，考研的复试面试也来了，我并不紧张地参加完了学院的面试，并取得了大概是前三的面试成绩吧。最后经过一些时间的考虑，参考学长学姐的意见和网上的一些帖子选择了一个实验室的老师。</p><p>选导师结束之后我回到了本科学校，开始做毕设，毕设课题是在一个叫NAO的机器人上做一些与机器学习相关的事情。因为选择的实验室看上去似乎与数据库相关，我就找了Stanford的CS145课程（没有课程录像）的课件看了一下。这门课程的学习经历让我意识到课程的录像是非常重要的，在此之后我找课程学习的时候一定优先挑选有课程录像的课。学了点数据库之后，我补了点概率论的知识（Harvard Stat110），然后开始学习Stanford的CS231n课程。这门课让我首次接触到deep learning，感受到计算机视觉CV（学习）的有趣之处以及deep learning的强大与玄学。我比较认真地看完了视频并做完了课程作业之后，决定毕设做一个机器人上简单的目标检测（object detection)。内容设定为机器人通过手眼相机检测到目标之后，根据目标位置抓取目标。由于当时根本没有显卡，只有一台i3的小破笔记本，我挑选了YOLO v2 tiny作为检测的骨干网络，并且使用了Github上的一个基于TensorFlow的YOLO项目。显然我的计算资源不支持我从头训练网络，我只能找了一个pretrain的网络finetune。当时那个YOLO项目还不支持finetune，我就自己看TensorFlow的文档瞎改代码，改出了个支持finetune的版本。然后自己收集了一些图片，标注了一些数据之后开始finetune。由于机器太差，区区百来张图片的finetune就让我费劲了力气：单组参数的finetune大约需要48h才能完成训练，而且训练过程中电脑会非常卡。我大概花了两周的时间才终于训出了一个勉强可用的网络。在此期间由于电脑不能用，我买了本《Information Theory, Inference, and Learning》开始看，也做一些里面能用笔算的习题。训完了网络之后，我在机器人做了一些实验，并录了一些demo，然后写完了毕业论文。暑假学车，利用空余的时间学了几门课，记得有Stanford的CS41 Python Language，还有MIT的6.006 Intro to Algorithms。</p><p>需要说明的是 ，直到这个阶段，我对学习/CS/机器学习/做研究都还没真正的入门。现在回忆起来已经有点模糊了，但我大致记得当时的学习并不“主动”，纯粹是“被动”地理解书上写的内容、公式推导和代码等等。现在我认为真正的学习应该是<strong>非常“主动”地去进行</strong>，看书、论文的时候要对框架和motivation有一个清晰的把握，知道/理解算法这样设计的目的是什么，并要相对频繁地去ask some questions，批判地进行阅读与消化。但可惜当时的自己还差得很远。另一方面，自己习题、coding做得太少，没有获得足够的<strong>监督信息</strong>，自以为自己学懂了，但其实只学了点皮毛。好在那时候还算是通过一些课程的作业获得了很多<strong>正反馈</strong>，支持着我的学习热情。</p><p>我一开始对能够进入浙大计算机学院是很有一些激动的心情的，也破天荒地去积极认识了很多同学，还认真制定了课程计划等等。但开学之后，我发现研究生的课程质量比我想象中差了很多。好在<a href="http://dengcai.zjulearning.org.cn/">蔡登老师</a>的<a href="http://dengcai.zjulearning.org.cn/Courses/ml/">机器学习课程</a>让我感觉非常好，我认为蔡老师的授课水平和这门课的作业都是world-class的。可惜我运气不好，没能选上这门课，而且蔡老师怕TA压力太大，不给增加课程容量。我一开始就只能蹭课听，坐在最前面，学得很认真，课上蔡老师的问题也总是积极地第一个回答，最后竟然让蔡老师破例地为我签了条子，选上了这门课:) 托这门课的福，我的机器学习基础扎实了很多，这门课也取得了不错的成绩。值得一提的是，这件事情也成了后来我成为蔡老师学生的契机。我还担任了这门课接下来的三任TA，在此期间我与另一位TA一起将之前的MATLAB作业改写成了Python的作业。后来这门课要扩充一倍的新内容，我还与蔡老师一起设计了新的课程内容，并狗尾续貂地再出了四份对应新内容的作业。只做了这么一点微小的工作，非常惭愧。</p><p>其他还有两门课给自己留下了一些印象。一门是潘纲老师与章国锋老师合上的《计算机视觉》。课程project我做的是preserve information in style transfer，大概就是说image style transfer中会丢失很多信息，于是就引入了一些正则项来保留部分原始图片中一些可能有用的信息。这个project是与室友一起做的，一开始做得很挣扎，主要原因是自己的Laptop上transfer一张图片就要一个晚上，所以做实验非常缓慢。后来得知实验室有一张1080Ti，就与实验室同学轮流使用。这也是我第一次亲眼见识到显卡的强大：竟然能在几分钟内就完成一张图片的style transfer，太强了！后来课程做pre的时候，两位老师建议我们可以把工作整理一下投一篇paper，但我俩觉得肯定中不了好的会议，就没去写。第二门课是钱徽老师的《凸优化引论》，这门课印象深刻的主要是教材：Nesterov 的《Introductory Lectures on Convex Optimization》。这本“Introductory”的教材让我久违地感受到了被支配的恐惧，里面各种反（我的）直觉的推导看得我头大，也没有任何motivation说明，而是那种苏联式教材的典型风格：就是硬推。钱徽老师说这本学懂之后，看paper会比较轻松。我抱着对他的信任一顿硬啃，后来竟然渐渐学懂了一点。课程作业是挑几篇paper看，我挑了几篇Zeyuan Zhu的paper看，发现竟然还真能比较轻松地看懂&gt;_&lt; paper大概是说SGD对数据梯度的估计会有一些variance，这些variance在训练后期会导致训练的不稳定，并推导了一些方法降低variance，感觉也挺有意思的。不过后来自己还是因为数理基础不扎实，没敢继续做理论方向。</p><p>后来开始接触了一些自己实验室的项目之后，感觉不是很感兴趣，就与实验室下另一个组的一位同学cc一起合作搞一些超分辨率(SR)相关的工作，经常跑到他那边去合作。后来我和那边的老师商量，想换到那边的组去。我想大家都是一个实验室名下的，应该可以成功。那边的导师去找我原来的导师聊，结果失败了。我原来的导师也很不高兴我去自己实验室的频率太低，把我训斥了一顿。后来我只能放弃与那个组的合作，在自己组里认真干活。但最终因为方向不感兴趣、缺少显卡等原因，我选择了转出原来的实验室，这里也非常感谢原导师对我意愿的理解与尊重。后来很幸运地，蔡登老师愿意接收我作为他的学生，也许是认为我在他的课上表现还可以吧哈哈。</p><p>我在实习之前的学习经历就到此为止。前期因为没人指导，自己也没有很好地判断力，很愚蠢地相信了很多知乎上的答案，后来才慢慢有点上路了，总结出了一些适合自己的学习方案。这个学习方案在前问已经给出来了，此处不再赘述。</p><p>最后在这里小小地宣传一下我们组。组里的蔡登老师与何晓飞老师都是学术顶尖、人品很好的老师。何晓飞老师现在创业做无人驾驶了，我与他接触不多；但我与蔡老师交流很多，有一起讨论review过大概几十篇paper吧。蔡老师学术水平极高，读博期间就发表过非常多很有影响力的论文（虽然现在那些topic已经不是很热门了），也经常能一眼看出组里同学许久也看不出来的问题要点；有很好的学术准则，绝对不会抢夺学生的成果，也坚决不参与学术圈一些拉帮结派的事情；人品非常好，很为学生考虑，对我的请求从来都是有求必应&gt;_&lt;；还是一位很照顾家庭的好男人和好父亲。总之，蔡老师不仅是我的学术导师，更是我的人生榜样；在我眼中他是一位真正的计算机科学家，也是一位脱离了低级趣味的高尚老师。能成为他的学生是我的幸运。组里的同学们也都基础扎实，且很努力，产出也不少。组里2019年大概有发了15篇左右的CCF A类的paper，学术实力有目共睹，且蔡老师要求组里每篇发表的paper原则上都要公开代码在组里的Github账号<a href="https://github.com/ZJULearning">zjulearning</a>下，并开放issue欢迎随时challenge，这也说明组里的工作至少都是扎实可复现的工作。</p><h2 id="找工作之前的准备"><a href="#找工作之前的准备" class="headerlink" title="找工作之前的准备"></a>找工作之前的准备</h2><h3 id="刷题"><a href="#刷题" class="headerlink" title="刷题"></a>刷题</h3><p>我在找实习和找工作之前刷了一些LeetCode题目以做准备。找实习之前大约是刷了400道左右，到找工作之前大约是刷了700多道，具体可以移步<a href="https://www.zhihu.com/question/32019460/answer/887877092">我的知乎回答</a>。不过由于我不是竞赛背景出身，不够有天赋，刷题也不是全都认真地独立完成而常常参考discussion，我直到最后也没能达到可以轻松做出绝大多数hard题目的水平（周围的很多朋友都可以轻松秒杀），这也给我面试一些对算法题要求很高的公司（例如hulu等）带来了一些不确定性。</p><h3 id="面经与面试技巧"><a href="#面经与面试技巧" class="headerlink" title="面经与面试技巧"></a>面经与面试技巧</h3><p>我面经倒是看得不多，基本上只有看过Google和Optiver的面经。原因是我运气比较好，在找实习初期（2月底3月初）就拿到了实习dream offer Google，而在秋招初期（9月初）就拿到了秋招dream offer Optiver，游戏开局就爆了神器；所以之后的面试都比较无所谓，主要是抱着聊天的心态参加面试的。至于面试技巧，主要是根据自己的经历总结出来的。当时并没有找到这样的文章供我参考&gt;_&lt;</p><h3 id="模拟面试-1"><a href="#模拟面试-1" class="headerlink" title="模拟面试"></a>模拟面试</h3><p>找实习之前幸运地获得了Google官方提供的模拟面试(mock)机会，通过mock直观地感受到了Google面试的形式与风格等，对后面拿到Google的实习offer有很大的帮助。后来在Google的实习的时候，公司很贴心地给每个实习生都安排了四次mock，这四次mock也让我学到了很多东西。除此以外就没有进行过mock了 &gt;_&lt;</p><h3 id="日常实习"><a href="#日常实习" class="headerlink" title="日常实习"></a>日常实习</h3><p>加入蔡老师的组之后，我一开始是在何老师的无人驾驶公司飞步实习。实习期间受宠若惊地担任了不少重任，例如一开始独自一人地做一个产品的某个算法模块（包括数据采集（与标注）、算法选型、模型训练调整、开发代码、测试、部署等）。值得一提的是，为了降低自己标注数据的负担（大概标了几万张分类图片），也自己想了一些合适的采集数据的方式，并设计了一些半自动标注的算法，大概能够降低95%的工作量吧；又开发了一些简单的标注工具，才终于让我survive the labeling task。后来还担任了某个产品的开发主力（甚至很多时候是唯一开发人员&gt;_&lt;），同样担任了数据收集、算法选型、模型训练调整、开发代码、测试、部署等等。在这段为期5个月的实习中我学/锻炼了非常多的能力，为我后面找工作也奠定了一些基础。</p><p>不过实习期间，我发现自己应用机器学习的水平并不很好：我虽然了解大多数常用的算法的原理，但当deep learning模型不work的时候我往往会不知从何下手debug。尤其是做第一个项目的时候，模型在测试集上能够达到很好的精度，但算法上线之后效果很差，会有很高的虚警率。我当然知道这是因为自己收集的训练集与测试集过于单一，均无法很好地代表线上场景，但当时确实不具备收集更完善的数据集的条件。无奈之下，只能选择利用类似迁移学习的方式从一些有一定相似性的数据集中学习一些信息，效果虽有提升但仍然很差。后来尝试调参数、模型，但都没什么效果（我也知道不可能有效果&gt;_&lt;，你永远不可能解决数据上的问题，调参只是为了给leader一个交待）。最后为了用户体验，只能从阈值与告警策略着手，调整了很久的阈值与告警策略，并做了尝试自适应的阈值、bad case的判断与特殊优化等等。做第二个项目的时候，模型的训练倒是相对顺利，因为是已经相对成熟的应用领域了，倒是大部分的工作是写文档开发测试部署等等。总之，经过这段时间的实习，我发现自己其实不太喜欢收集数据、标注数据、调参调阈值调模型这样的工作，更重要的是我很惶恐于那种我不知道这个deep model为什么不work，不知道从何入手debug的感觉；也惶恐于不知道这个deep model怎么就work了，到底是因为什么而work的感觉。与之形成对比的是，我发现自己做开发的工作感觉还挺有意思的。这也是我第一次萌生转行做开发的念头。</p><p>印象比较深的还有将算法deploy到塞林斯的板子上的时候碰到的两个bug。我是组里最晚往板子上deploy算法的人，所以被leader催得比较厉害。deploy完，我测了一下算法的结果是否正确，结果发现结果是完全错的，就开始了debug。leader催我在两天内搞定，我能力不足，只能加班来弥补。第一天debug的晚上发现应该是塞林斯官方的交叉编译器的浮点运算有bug，就汇报给了leader，leader不相信。后来另一位围观的小伙伴帮忙弄了一个minimal复现的程序(记得是sin(pi/2)=0.08之类的)，才终于说服了leader。最后我们给塞林斯官方写了邮件，官方回复承认了bug的存在，并说一时半会儿解决不了。然后小伙伴Z找了个开源的交叉编译器解决了问题。这个bug告诉了我一个道理：要勇于challenge，有的时候编译器真的会有bug。但是浮点运算正确之后，程序的结果还是不对。第二天又debug到晚上1点多，发现bug是OpenCV版本带来的问题。具体地说，服务器上当时的OpenCV是2.4.9，而deploy环境的OpenCV是2.4.8。我在2.4.9版本里使用了一个操作似乎是scalar times matrix，这个操作在2.4.9里能给出正确结果，然而2.4.8的OpenCV只支持matrix times scalar（不确定，有可能把这两个记反了，总之是2.4.8只支持一个顺序，而我的代码在2.4.9里开发的，写的是另一个顺序）。更过分的是，OpenCV的2.4.8还重载了scalar times matrix这个操作，然后返回了一个错误的结果（似乎是直接return 零矩阵）。最后还是小伙伴Z过来帮我用gdb一步一步看汇编代码发现的。Z把我们一顿喷：“为什么不用最新的版本？小版本的更新说明之前的版本一定是有bug的！”然而我早就提议用更新的版本了&gt;_&lt; 这个bug告诉我一个道理：广泛使用的库中也可能一些很弱智的bug，一定要用最新版的。（实际上我碰到了OpenCV 2.4.x版本的很多bug）</p><p>在飞步实习期间要特别感谢我的实验室同学Z。Z是我身边技术最强的朋友之一，无论是技术深度还是广度都是我生平罕见得强大。Z虽然并没有在公司实习，但在我实习期间他偶尔会来公司carry我一把。Z给我的帮助非常非常大，例如帮我一起解决了那个编译器的bug，带我一起完成了第二个项目的系统方案设计等等。后来Z也选择吃了我的安利，与我一起加入Optiver，成为浙大第三个应届拿到Optiver offer的人&gt;_&lt;</p><p>这里再说一句题外话。我在实习期间因为要用C++，但自己以前用的基本上都是C++中的C，所以又找了些资料学了一些C++。特别推荐一个C++的学习资料，<a href="http://web.stanford.edu/class/cs106l/">Stanford的C++课程CS106L</a>的Course Reader。这本书讲的C++虽然有些过时，主要是C++11之前的一些东西，但仍然能够从里面学到非常多重要的C++思想（如封装、继承、抽象、多态、const的重要性等等）。对我来说，C++是一门不容易学好的语言，许多人推荐的C++ Primer和C++ Primer Plus会将我淹没在语法细节的海洋中。但是这本Course Reader不一样，作者会从很多motivation/design的角度来讲述C++，并给出了很多motivation example，况且作者的写作水平也很好，写得很有趣&gt;_&lt; 如果你和我一样没什么学C++的天赋，发现其他的书学起来很困难，那么不妨试试这本。稍稍遗憾的是，这本书自10年之后就没有更新了。</p><p>最后总结一下，在这段实习让我学到了挺多东西的，对我后面找实习和工作都起到了很大的帮助。我之前从来没有在公司待过，更别说做工程产品什么的了，这次实习让我体会了一把工业界的感觉。刚开始写产品开发代码的时候是诚惶诚恐的，就我这垃圾代码也能当产品？做个demo还差不多。但后来也算是适应了这种惶恐的心理。另一方面，我一开始是独立负责产品线上一个大的算法模块的所有内容，后来有一段时间更是几乎独立开发了一个产品，因此对于算法工程师所必须的一些能力有了比较好的锻炼。第二个项目的产品最后还成功交付出去了，成为了公司的（第？）一笔营收，甚至听说现在那块业务还成为了公司的一项重要业务。</p><h3 id="做research"><a href="#做research" class="headerlink" title="做research"></a>做research</h3><p>交付了第二个项目之后，我开始想做一些research，就选择回了实验室专心做research。这里主要有两个原因：其一是我想体验一下做research的生活，来帮助决定以后是否读博走学术道路；其二是听说算法岗的job market现在已经是神仙打架，甚至知乎上有人说“没有顶会paper可能就直接表刷了”。当然，根据我后来自己找工作的经验，我觉得这种话纯粹是危言耸听的。就我的经历而言，大家会更看重coding的能力、基础是否扎实、是否具有解决问题的能力以及是否具有不错的沟通能力等等。相比较而言，在job market上，顶会paper很多时候只是能justify个人能力而已，况且以现在ML会议的现状而言，能不能（短时间内）中也是挺看运气的。如果真的有某家公司招硕士算法岗要用顶会paper来表刷人，那我觉得可以考虑一下这家公司是不是真的值得投递了。</p><p>这里再简单讲讲我做research的一些经历。需要说明的是，我做research的能力并不强，也没什么拿得出手的成果，所以仅供参考。最早想做research的时候是本科，那时候非常中二，希望自己能花多年时间在一个领域上，以达到专家的水平，甚至能推动领域的进步。在学了CS231n的课程之后就开始思考做research的idea，主要的想法是找一些目前还没应用deep learning但可能可以应用的领域。硕士入学期间（2017年5月-9月）总共想了三四个idea，现在看来回顾起来还是有一些有趣的。第一个idea是想用deep RL来做启发式搜索的评估函数，不过这个没有设备支持只能放弃。后来看paper感觉AlphaGo Zero的思路与这个有些类似。第二个idea是在学6.006的课的hash部分时候想到的。Prof说hash function往往是一个比较complex的function，我就想那能不能用deep learning来做hash呢？但自己闭门造车，一直想不到合适的监督信息，就放弃了。过了一年才知道有个领域叫deep hash，是用deep learning来代替LSH中的hash function，来做近似最近邻检索的，而早在16年就已经有deep hash的paper了。第三个idea可能是我最接近能做出来的一个idea了。当时的idea是想用神经网络来近似B-tree。我做了一些实验，感觉效果不错，但是一直困扰在一个点上：神经网络的输出的上下限是不确定的，万一query的数据的输出超出了训练时候的上下限的话就不知道怎么处理了。后来2017年12月的时候看到Jeff Dean发了一篇paper《The Case for Learned Index Structures》，我看了之后，发现他们的assumption是query的数据都在train中出现过，这样就不会出现上下限的不确定性问题了。而如果要去update这个神经网络模拟的B-tree，他们就选择重新训练整个模型。应该说他们的想法是更加合理的，因为直到现在似乎也没有人做出不需要retrain的learned B-tree。当时看到这篇paper的时候我非常激动，觉得有人和自己想到一块去了，而且还是一位大牛，这说明了自己的idea还是有一定靠谱性的。于是我还很兴奋地在数据库的课上分享了这篇paper&gt;_&lt;</p><p>应该说，我在硕士之前做research的路子是很有一些问题的，这个问题主要在于我的野心太大，常常想做一些以自己的能力/资源不大可能做出来的research，简直就像很多民科想证1+1=2或者P != NP的人一样可笑。当时也没有人来指导自己的research，就拿一台i3小破笔记本自己一个人瞎想瞎折腾。后来有一位朋友和我说，他认为PhD需要培养两个重要的能力：其一是能够<strong>大致判断自己能否做出一项工作</strong>；另一点则是能够<strong>大致判断工作的impact</strong>。PhD应该<strong>在自己能够做出来的工作中，挑选impact最大的去做</strong>。而我当时显然就不具备判断自己能否做出某项工作的能力。</p><p>研二的时候回到实验室开始做research，一开始和导师商量，打算做deep learning与manifold learning的结合，算是semi-supervised learning的一种吧。当时觉得这个工作最大的困难点在于如何用mini-batch去做manifold learning中graph的更新。一些已有的相关工作（如cvpr16的《Joint Unsupervised Learning of Deep Representations and Image Clusters》）基本上是都用一些iterative的方式来训练和迭代的。后来想了一段时间，觉得还是先从相关领域的paper复现开始做起吧。然后就开始复现semi-supervised learning的paper。大概有尝试复现了7、8篇吧，发现都无法复现出他们的效果，反而复现的baseline能比他们的baseline高5-10个点左右。终于在复现一篇领域大牛（无人不知的那种）的paper的时候复现出了paper claim的效果，结果复现他们的baseline比他们paper claim的自己的效果还要好&gt;_&lt;</p><p>经历了一些失败的复现之后，向一位刚中了AAAI的小伙伴C请教了一些经验。C安利我做他们的领域，我听了他们的工作之后，当天晚上突然想了一个比较偏data mining的idea。第二天就写了代码实现一下，发现效果特别好。当时非常激动，觉得随便一弄效果就这么好，这要是认真调调岂不是要上天，结果那天的结果就基本上是最后调出来最好的结果了&gt;_&lt;然后匆匆写了一篇paper，submit到了IJCAI上。结果被reject掉了。AC评价说觉得是一个简单但是有效的工作，但是漏引了两篇10多年前idea相似的paper，我一看才知道原来10多年前就有人做过了相似的工作了，被拒得挺没有脾气的。后来这篇paper又submit到了AAAI上，有一位reviewer给了非常低的分，理由是认为data mining这类approach早就out-dated了。我后来也觉得这篇paper确实很难投中，就扔掉不管了。总得来说算是一段有些失败的research经验吧。</p><p>除了自己做的research工作以外，我还与导师一起讨论review了很多paper，注意到有些会议里的review真是招呼满天飞，不禁感到有些失望。就开始思考自己究竟是否是真正想做research的人。最后得出结论，我想做的是真正实用的顶级的research，但我显然远没有那个能力，只得作罢。硕士期间的research经历基本就到此为止了。</p><p>最后在这里安利一个沈向洋老师在华中科技大学给的talk: <a href="https://youtu.be/U6r3R87AKHI">“How to do research”</a>，里面分享了很多有用的方法。不过里面有些点我也不是特别同意（虽然我完全无法与沈老师相比），譬如他说做研究，应该有3年做不出来的觉悟。我觉得对大多数普通的PhD来说，这么长时间拿不到正反馈可能是会压力很大且非常痛苦的&gt;_&lt;</p><h2 id="找实习"><a href="#找实习" class="headerlink" title="找实习"></a>找实习</h2><p>先写一些简单的总结体会吧：</p><ol><li>能找内推尽量找内推。一方面是有些公司可以免掉笔试避免自己翻车，另一方面是有内推人的话在很多公司可以帮忙查询进度什么的，比较方便。</li><li>多看公司的面经了解风格。不同公司考察的重点、风格可以有很大差异，一定要提前了解公司的面试。</li><li>简历上的内容一定要非常熟悉。这个没啥可说的了。</li><li>面试官真的是一家公司的门面，极大地影响我对公司的印象。</li><li>剩下的要点都在前面tips部分提过了，这里不再赘述。</li></ol><p>实际上，我实习的第一个offer就是自己的dream offer Google，本来打算就此结束找实习了，把时间用来补CS基础上。陪妹子参加了一场拼多多的面试，与面试官聊了聊他们在做的事情以及难点与痛点的时候，竟然受到了一些启发，有了个idea（虽然这个fancy的idea最后没有work）。于是决定多投递多面试，与各家公司的面试官多聊聊。每家公司的面试官往往是公司里水平比较不错的人，尤其是最后一面技术面的面试官更是技术leader甚至是技术Boss，平时想要与他们聊天的机会可不容易获得。但是面试这样的事情能够让他们自动送上门来聊天，何乐而不为呢？</p><p>在这样的想法下，我投递了阿里，腾讯，头条，百度，微软，hulu，airbnb，摩根士丹利。需要特别提一下的是Optiver。对Optiver的投递其实是比较巧合的。三年前ZJUCS有一位硕士学姐应届去了Optiver，我认识那位学姐的几个学弟，学弟们对学姐是极其推崇，各种膜拜。我想，这么厉害的学姐会选择的公司，那肯定也非常厉害，于是就这样投递了Optiver。当然要说明的是我并没有对Google以外的公司有做什么面试准备，也没继续刷题，因为我早就确定了要去Google实习了。而其他公司给我发了offer之后我也是立刻拒掉，避免耽误其他同学的机会。下面按时间顺序简单谈谈各家公司的面试。</p><h3 id="Google"><a href="#Google" class="headerlink" title="Google"></a>Google</h3><p>流程：两轮电面，主要是问算法题。</p><p>过程：找实习时候的dream company就是Google了。平时有空的时候会做一下Google的kickstart比赛，并因此拿到了去Google参观的机会，参观完更想去Google了。于是投递了Google的实习。当时自己想去Google上海做开发SWE，同时Google的算法岗ML SWE也只在冬令营中产生，也没办法投递。运气不好的是当时Google的面试与IJCAI会议的ddl基本上重合，所以当时是做research赶paper与刷题准备面试同时进行。Google的面试安排相对较早，大约是在2月底吧。有趣的是，由于我在填Google的表的时候写了中英文面试均可，结果HR小姐姐就给我安排了两场英文面试Orz 而且都是晚上11点半或者早上7点这样的事件Orz 第一面似乎是位印度的Googler，结果因为一些原因鸽了我。第二次一面是一位新加坡的Googler，结果又因为被浙大的邮箱坑了进不去Google的视频会议。第三次一面试终于顺利完成，面试官是一位纽约的小哥哥，题目并不难，我很轻松地写完之后我们闲聊了一会儿。二面是一位上海的Googler，题目挺难的，而且给的压力不小，差点就崩了，好在我还是稳住了心态顺利地做了出来。面试完半小时左右就收到HR小姐姐的消息通过了面试，此时我还没从面试的地方走回宿舍……后来过了Google的Hiring Committee，进入Team Match后竟然也很快（不到半天）地就被捞了起来，还有点受宠若惊。捞我的人（也就是我后来的Host）希望我能去北京做ML SWE，我想应该是看中了我的ML背景吧。我问HR小姐姐我的项目内容，小姐姐给我发了俩，说不确定是哪个，其中一个内容大概是自动化地针对手机设备压缩网络，我比较感兴趣；另一个项目看上去是与TensorFlow相关的，感觉也不错。同时注意到自己的Host是Berkeley的PhD和美国一所大学的professor，还发现那个组的创建人是李飞飞。在与妹子商量后，就接了这个offer。可以说第一个offer就是自己的dream offer，非常幸运和激动。</p><h3 id="拼多多"><a href="#拼多多" class="headerlink" title="拼多多"></a>拼多多</h3><p>流程：笔试+HR面 + 两轮onsite技术面</p><p>过程：投递的是算法岗。拼多多在3月中旬的时候在浙大附近租了个酒店，在那里进行了现场的面试。HR通知我们早早地过去，然后在那边等着排队，记得自己等了挺久的。先进行了HR面，主要是问我对公司的工作时间什么看法，我当然是回答996不在话下，不然我可能就要当场被赶出去了？之后一面技术面问了一道数据结构的coding题。一面的面试官有些傲慢，一开始还弄错了一个地方怼了我一会儿，后来我只能耐心纠正他的错误。之后问了些机器学习基础与项目的问题。这位面试官让我对拼多多的印象非常糟糕，后来也就直接拒了offer，秋招也没有投递。二面技术面倒是感觉不错，问了一道很难写的链表题目，我比较轻松地写出来之后同样问了机器学习基础与项目的问题。面试完过了一两周吧，晚上10点半接到offer call，直接拒掉了。</p><h3 id="摩根士丹利"><a href="#摩根士丹利" class="headerlink" title="摩根士丹利"></a>摩根士丹利</h3><p>流程：全英文，笔试+电面+两轮onsite技术面</p><p>过程：投递的是C++开发岗。3月底大摩突袭打电话电面，全程英文面试，问了很多数据结构、操作系统与计算机网络的基础问题，最后问了一道system design的题。之后约了4月下旬的onsite面试，我与妹子一块参加。结果因为一些原因（起晚了）到达现场迟到了，非常不好意思。到达现场之后尴尬地发现除了我和妹子之外的所有候选人都穿着正装Orz 现场先做了一点笔试题，交了之后hr小姐姐就带人去面试。面试也是全程英文面。两轮面试问的都是数据结构、操作系统、计算机网络与system design的题。两位面试官给我留下的印象都不错，谦逊和气，水准也不错。一位面试官当时问我有了哪些offer，我就诚实地回答了一下，面试官非常惊讶地问我”Then why do you come to Morgan for an interview?”我说自己想稍微了解一下金融的情况。大摩也是在面试完大约一两周后来了offer call，我拒绝之后并说明了自己的去向，HR说那我们继续保持联系。</p><h3 id="头条"><a href="#头条" class="headerlink" title="头条"></a>头条</h3><p>流程：笔试（内推可免)+3轮视频面+HR面</p><p>过程：内推的是头条产品的推荐算法岗。4月初进行了前两轮技术面。头条很喜欢考算法题，两位面试官都问了很难的算法题，当时比较困状态不好，艰难地写出来之后又问了一些数学基础、机器学习基础和项目的问题。二面结束5分钟后接到电话约三面。过了两天进行了三面，这次的题倒不是很难，写了两道之后面试官问了一些操作系统、编程语言的问题，最后问了我一个场景设计题。面完5分钟后收到offer call。最后还是拒绝了。</p><h3 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a>阿里</h3><p>流程：笔试（内推可免)+3轮电面+1轮交叉面+HR面</p><p>过程：内推的是阿里搜索推荐部门的算法岗。4月初的一个晚上一面，面试官是内推我的师兄，我觉得可能怕被骂防水有点矫枉过正，问了足足70分钟的问题，主要是算法题（很简单而且不用写代码）、机器学习基础、深度学习基础与项目问题。过了两天进行了二面，也是算法题（很简单而且不用写代码）、机器学习基础、深度学习基础与项目问题，面试官有问一些开放性的问题，还蛮有意思的。过了一周左右进行了三面，基本上只问了项目问题和开放性问题，后来知道那个开放性问题竟然是他们投kdd的paper……然后过了挺久的，在安徽参加valse的时候突然接到电话，原来是阿里的交叉面（即另一个部门的人来面试我），问了机器学习基础和项目的问题，还问了深度学习框架的一些实现细节（不知道为什么问我这个，但还是回答了出来）。总得来说技术面试体验还可以。又过了一周，晚上9点半，接到阿里HR的电话，阿里HR还真是如传闻一般……后来拒了offer了。</p><h3 id="腾讯"><a href="#腾讯" class="headerlink" title="腾讯"></a>腾讯</h3><p>流程：笔试（内推可免）+3轮电面+HR面</p><p>过程：投的是数据挖掘岗位。比较尴尬的是，当时接到了优图Lab的电话，说他们要求实习4个月以上，问我能不能满足要求，我没经过思考就说了那肯定不行，和导师说好了3个月。然后就面试终止了。我真傻，我本来面试的目的就不是去实习啊，直接答应下来说可以4个月就好了…</p><h3 id="微软"><a href="#微软" class="headerlink" title="微软"></a>微软</h3><p>流程：笔试（内推可免）+3轮onsite面</p><p>过程：找学长内推了微软苏州的SDE，与妹子一起去苏州onsite。一面问了项目和一个算法题，又问了两个设计模式的题。面试官给我的印象非常差，看不懂Range For就challenge我，没用过priority_queue也challenge我。二面的面试官比较nice，先问了项目。之后问了一道比较简单的算法题，然后是难一些follow-up，都相对轻松地解决了。最后还剩下10分钟，面试官就开始和我闲聊，问了我已经拿的offer，并问我有没有肉翻的打算，我说至少要等妹子也有肉翻能力了再一起吧，暂时不考虑。三面面试官是一个leader，先问了一道system design，并讨论了一些面向对象的设计思想，然后问了我开放性的题目，比较难。总得来说，除了一面面试官给我印象非常差外，另外两个面试官都很nice，尤其三面的面试官水平感觉也很好。</p><h3 id="Optiver"><a href="#Optiver" class="headerlink" title="Optiver"></a>Optiver</h3><p>流程：全英文，笔试+电面+两轮onsite面</p><p>过程：</p><p>终于要讲到自己最终选择的雇主Optiver了。我投递的是C++ developer。之前提到，投递Optiver的主要原因是相信大神S学姐的眼光。在校期间听说了不少S学姐的传说，据说她读书期间写System代码就从来是bug-free的，秋招的时候腾讯问她“你开个价吧”，但照样被她拒绝。还听说她当时是拒了一家叫Jane Street的公司的offer去了Optiver。不过S学姐的那些传说我也没有向她考证过。后来我才知道S学姐是浙大第一个应届进Optiver的人，而我则是第二个。</p><p>总之我就这样投递了。过了几天收到笔试通知，笔试题有两套，一套是在一个平台上做两道C++偏数据处理的题目。另外一套是一些数学智力题，很多题目难度非常大，做得我头晕眼花。做完之后第二天接到HR小姐姐的电话，开始了HR面。HR的口语非常好，我听上去与native speaker没有什么差别，问了许多比较有趣的问题，我也一一回答。面试完过了两小时，HR给我打电话问我愿不愿意做一下公司另一个岗位Application Engineer (AE)的笔试题。我一向是相当flexible的，就答应了下来。这个岗位的笔试题主要是一些与Linux操作相关的问题。第二天HR给我打电话，说我AE的题做的分非常高，建议我先和这个Team的人交流下，看看感不感兴趣，我反问难道我C++ Developer的分不高吗。HR说了两点。1. HR觉得我的沟通能力很不错，而AE非常需要沟通能力，她觉得AE说不定会挺适合我的。2. 我投递得比较晚，Developer他们已经有一些不错的候选人了，不一定能排得到我。HR问我为什么想做C++ Developer，我诚实地说因为周围有一些大神朋友是做System Developer的，我和他们交流觉得挺有意思的，而且我觉得System Developer的技术很硬核，具有很好的技术护城河，能够让我在多年后仍然保持很强的竞争力。HR于是建议我与两个team的人都聊聊，看看自己对哪一块更感兴趣。我接受了HR的建议，毕竟我早就决定去Google实习了:) 与越多越的人交流我越开心（请Optiver的同事们看到这段不要打我&gt;_&lt;）。</p><p>去大摩参加onsite面试的高铁上我接到了Optiver面试电话，稍微想了一下，我决定就接受了面试，因为我其实不在意拿不拿得到offer，面试环境差一些也不要紧。于是就这样在嘈杂的高铁上开始了AE的面试（听说我开始电面之后周围的人都瞬间不说话了，感谢有爱的乘客们）。面试的内容很多，考察了数据库、操作系统、Linux常用操作、运维场景问题、计算机网络、版本管理与system design等内容。这场电面是给我印象最深刻的电面之一，内容很多，且很多问题是很有意思的一些design问题，电面的面试官甚至还有两位&gt;_&lt; 无奈我自己其实不太懂运维的内容，很多问题都答不上来，只能靠猜测（如前面面试心得里所述，我是先claim了自己要猜测的），在一些design方面与面试官进行了比较多的讨论。面试官的脾气也非常好，并没因高铁上时常响起的播报声而恼怒，而是一直说never mind, that’s ok，让我感到非常非常不好意思。如果早知道高铁上环境如此嘈杂，我就推迟面试时间了……</p><p>然而，这次面试让我意识到自己和公司的AE也即运维岗的技术栈很不match，发现自己对运维也不感兴趣，后来HR联系我的时候，我就请她还是继续安排我走system developer的流程，HR尊重了我的意愿。然而大约在4月22日的时候，HR打电话告诉我说公司的员工们大都去休年假了，这样可以与五一假期连一块，休一个比较长的假期（我怀疑这是公司的PR，告诉我公司年假很多&gt;_&lt;），因此面试可能得安排到5月之后，我自然是欣然同意。可惜到了5月多的时候，HR联系我说他们已经有一些合适的候选人了，就不继续我的System Developer的面试了，并说如果我对AE有兴趣的话可以给我AE的offer。这一次的Optiver实习面试旅程就到此为止了。虽然没有去Optiver实习，但这次实习面试之旅为我后来秋招选择Optiver埋下了伏笔。</p><h3 id="百度"><a href="#百度" class="headerlink" title="百度"></a>百度</h3><p>流程：笔试+三次技术面</p><p>过程：过了百度笔试之后，大约在4月中旬接到百度的面试通知。三次技术面试都有考算法题、机器学习基础和项目等。百度的算法题不算难，大都是LeetCode medium难度吧。百度虽然这几年发展不好，名声口碑也一般，但是说实话面试官水平还是挺不错的，给我印象挺好。4月底的时候收到offer然后拒掉了，但是不知道为什么后面有一位百度的员工加了我微信，说我去实习的时候他负责带我Orz</p><h3 id="Airbnb"><a href="#Airbnb" class="headerlink" title="Airbnb"></a>Airbnb</h3><p>流程：笔试+笔试确认面+两次技术面+文化面</p><p>过程：投递的是全栈工程师。投递的原因主要是听说Airbnb的package挺不错的，不加班，而且前景也还可以。大概是4月中旬接到笔试通知，笔试题同样是在一个平台上完成，算是一道稍微有点麻烦的模拟题吧，但也不算很难。做完笔试题的第二天有一位面试打电话来和我约笔试确认面。笔试确认面时间很短，大概是15分钟，面试官非常客气地问了我design的motivation是什么，并让我分析了一下复杂度，个人感觉只要代码是自己写的，这一面不会有任何问题。过了一两周之后安排了两次技术面试。技术面试的内容主要就是写算法题，不过Airbnb的算法题风格与其他家还不太一样，他们的题目思维难度不大，但常常较难实现，会有一些corner case什么的。一面的时候我在实验室的学生休息室面，结果附近在修路，非常吵，十分艰难地才把代码写出来，面试官不太满意。二面的时候换了个安静的地方，相对轻松地搞定了题目，面试官还比较满意。但是应该是由于我一面表现不好，Airbnb让我在备胎池里待了很久……总得来说Airbnb的面试难度也挺高的，尤其是这种只做很难的算法/模拟题的风格，受运气/状态影响不小，一不小心就会挂。</p><h3 id="Hulu"><a href="#Hulu" class="headerlink" title="Hulu"></a>Hulu</h3><p>流程：笔试（内推可免）+三次技术面</p><p>过程：投递的是软件开发工程师（SDE）。在5月的时候Hulu来浙大办了宣讲会，并于接下来几天在浙大安排了现场面试。这种送上门来的面试机会我当然不会错过。没有投递Research SDE（即算法岗）的原因似乎是Hulu不打算从浙大招算法实习生？总之是明确说了只招SDE。猜测可能是只收清北的吗hhh 总之就这样上场面了Hulu。一面的面试官看了我写满了机器学习项目的履历之后，决定问我分布式系统的问题，因为他在公司是做分布式系统的。尽管我一再强调自己完全没接触也没学过分布式系统，但还是被迫回答了两个问题。之后就开始写算法题了，第一个算法题是道LeetCode medium难度的题，大概花了5分钟秒杀。然后面试官给我出了一道hard难度的题，想了10多分钟，在面试官的提示下我终于给出了最优解，然后面试官没让我写代码，又问了我一道hard难度的题，然后又是想了10多分钟，这次终于没能给出最优解。当时我的做题能力就是medium难度基本秒杀，hard难度的要么不会，要么就得做很久才能做出来:) 总之面完之后面试官和我说了一句你辛苦了，然后就把我挂掉了，终于吃到了第一个也是唯一的一个拒信&gt;_&lt; 总得来说还是实力不济，Hulu的面试题大都是hard难度的，自己当时确实做不出来，而且3个月没刷题了手也生。</p><p>至此，实习面试就全部结束了。总得来说各家公司的面试各有一定的特点，这可以通过看面经来了解。另一方面，这些面试也多多少少有一些共同点，考察的能力与一些相应的技巧我在本文前半部分也已经介绍过了，此处不再赘述。</p><h2 id="实习经历"><a href="#实习经历" class="headerlink" title="实习经历"></a>实习经历</h2><p>Google的实习经历对我的影响、帮助都挺大，再加上Google可能是很多同学心目中的dream company，这里就简单介绍一下我在Google的实习经历，以供参考。当然，其中也夹杂了许多我的个人经历，那些绝对是个例现象，与Google无关。</p><h3 id="入职"><a href="#入职" class="headerlink" title="入职"></a>入职</h3><p>Google的入职活动（onboarding day）还是挺有意思的，上午是一位技术同事志愿服务地带我们逛了北京Office的各个地方，并给了一些介绍，然后给我们分了Laptop。然后HR小姐姐Y带我们参加了一个活动，是整个大中华区当天的入职的Googler（Google员工）一起开一个会，介绍Google的文化、基本信息等等，有一位非常Senior的领导会一起聊一聊。我记得当时应该是台北那边的一位VP参加了会议。印象最深的是提问环节，我看到没有人有问题，我就问了一个问题：“你对新入职的Googler有什么建议吗？”VP回答：“Try to confident. Your products and efforts change the world.”</p><p>之后与我的host见面聊了聊。他是一位非常nice的host，水平也很高，也很会带人，在实习期间教了我很多东西。host与我聊项目，是一个OCR的项目，我发现这个项目似乎和之前我得到的信息不太一样，就问他为什么变了。host告诉我说后来招了两个清华的本科EP实习生，打算让她们做之前安排我的项目。我还是比较flexible的，就愉快地接受了新项目。</p><h3 id="项目初期进展"><a href="#项目初期进展" class="headerlink" title="项目初期进展"></a>项目初期进展</h3><p>新项目一开始做得很顺利，主要是需要我对Google一个很有影响力的开源项目(30k+ star)做一些改动，增加一些新的功能。确切地说，原始的项目的自动生成训练数据的训练脚本只支持字体文件作为输入，而我们期望它能够支持image-label这样的输入。这需要我阅读项目的代码结构（大概几十万或者几百万行代码），理解系统的设计、数据结构以及内部使用的算法，并找到自己需要改动的地方，再做修改。困难点在于这个项目并没有什么完善的文档，也没有（在我看来）非常完善的注释，需要直接阅读源代码。我大概花了一周大致看懂了实现原理（略去了一些技术细节），又花了一周左右理清思路并实现了需要的功能。但是我的噩梦才刚刚开始&gt;_&lt; 我开发的功能结合原始的训练代码训练出来的模型效果不合理得差。进行了一周多的debug，以及与host非常多的讨论，终于找到了bug在哪里，原来是原始的训练代码对于输入的数据是有一定的assumption的，但是在文档和注释中并没有说明。项目的训练数据均是由他们的代码自动生成的，这些数据会是符合这个assumption的，但是我新增的功能生成的数据则不符合这个assumption，这样会导致训练过程中调用的聚类算法中出现矩阵不可逆的问题，使得生成的训练模型效果很差。然而即使在修复了这个bug之后，模型的性能还是不能完全令人满意，又做了一周的实验慢慢提升性能。最终的效果虽然还不完全令人满意，但已经勉强可以使用。至此，我已在Google实习了一个月。</p><p>这里顺带一提，在北京office的实习生们大多数都是清北的，而且很多同学都有本科甚至是初高中的竞赛背景。考虑到我的CS背景与他们的差距，这多多少少给了我一些peer pressure。又因为妹子在杭州实习，我周末就没什么安排，而我的房子租得又离公司非常近（步行5分钟），故我整个实习期间基本都是一周去7天公司，且在第一个月每天都是早上9点到晚上10点以后走的。顺带一提，Google确实相当注重work-life balance，我晚上的时候时常会闲逛一圈，极少看到有除了实习生之外的同事加班，周末则只能看到少数一些实习生。</p><p>虽然第一个月的前两周我每周都会100个小时左右的时间在公司，但实际上我大概只有一半左右的时间花在自己的实习项目上，还有一半的时间则在如饥似渴地阅读Google内部的一些课程、资料。进Google之前我就听说公司有非常好的学习资源，而我可能在Google的时间只有这么3个月，我希望能够尽量地多学一些东西。不过遗憾的是我觉得我所看的学习资源比四大的课程基本都有明显差距，但想来也是，四大毕竟是最顶级的学校，且专门focus在教育上，要是还比不过Google的课程的话那也太没天理了。这两周我虽然学习/工作的时间很长，但项目进展顺利，也学到了不少东西，自己感觉非常得充实与满意。第一个月的后两周因为模型的效果一直很差，我给自己的压力很大，原因主要是觉得自己项目做得这么挣扎，一定是自己能力不足，要多花时间赶上，不能拖host的后腿。实际上我host没给我多少压力，对我也一直非常nice，但正因为如此，我更不想拖他的后腿（但我后来发现，项目做得挣扎是一个实习生尤其是ML实习生的常态:)  ）。我一直希望能够尽快地让模型work，每天都会debug到11点多才离开公司，而且晚上睡觉睡得也不好，基本上每天都会梦到自己在debug模型，而且debug不出来。这段时间的经历也让我意识到自己的ML水平其实也不怎么高，并不能以一种很systematic的方式去逐步debug模型的问题，而且也对debug的进度没有很好的把握，并不能合理地预估让模型work的工期。算法岗与开发岗不太一样，后者的工作我往往可以相对合理地预估工期。例如我前两周的工作就更偏向于开发，我每天都能有一些合理的进度，并且我能够相对清晰地预估自己需要多久阅读理解代码/实现功能/debug等等。另一方面，我做的算法岗的工作很多时候的工作内容就是调参数/调模型/调特征，会相对无聊，且我往往没有很好的insight（尤其是deep learning相关的），这让我感觉不到很好的技术沉淀与积累。总之，这段时间的经历让我又开始思考算法岗/开发岗的选择问题，并且对我最后做出选择起到了关键的作用。</p><p>顺带一提的是关于实习生转正的事情。由于妹子表示在阿里的组非常好，师兄们带她都很尽心，人也很好，晚上下班也挺早的，故我此时的秋招打算就是去阿里做推荐。当然Google的转正还是要争取一下，可以用来compete嘛。</p><h3 id="进抢救室"><a href="#进抢救室" class="headerlink" title="进抢救室"></a>进抢救室</h3><p>（下面将描述完全与CS无关的事情了，完全可以跳过&gt;_&lt;）</p><p>然而，更严重的问题出现了。我多年来一直肠胃不好，也去医院检查医治多次（但没做过胃肠镜），但都没什么效果。也许是因为实习的第三四周给自己的压力太大、工作时间太长，也许是因为还没习惯北京的饮食与生活，也许只是凑巧，总之我大概在第四周末尾开始出现胃溃疡引起的胃出血。但是我一个人生活，对生活也向来不太注意，竟然没留意到这个现象。大约胃出血了3、4天之后，某天晚上我上厕所的时候竟然感觉到一阵眩晕，眩晕得我无法站稳而摔了一跤。我意识到了不对，大概查询了一下，猜测自己是得了胃溃疡并且导致了胃出血。于是我决定第二天去附近的北医三院检查一下。我试图在网上挂号，但发现号子已经没了。</p><p>但是第二天有一场会议，我不想耽误他人的进度，故第二天起来挣扎地去了公司。路上我已经察觉到不对：我甚至已经没法正常走路了，我保持站立或者走路姿势大概两分钟左右就会脑供血不足，而不得不蹲一会儿让大脑供上血再继续前行。开完会后，我和host说明了情况，host表示非常担心，让我赶紧去医院检查一下，我就打了个的独自前往医院。实际上我对当时自己的危险情况并没有一个清楚的认识，正确的做法应该是找一位朋友一起，并拨打120去医院。</p><p>车子快要开到医院的时候开始堵了起来，原来北医三院门口的道路非常狭窄，但前来看病的病人非常多。堵了10多分钟，我开始有点焦急，出租车师傅建议我下车步行过去。思考了下，我下了车开始往医院前行。实际上这条道路只有区区几百米，但却绝对是我人生走过的最漫长的几百米。我每走大概10多米就要蹲一下，让脑子供上血，再继续前行。我运气还算不错，如果中间不幸晕了过去，各位可能就看不到这篇文章了。在路上我给妹子发了消息说了一下情况，并不敢告诉自己的家里人，怕他们担心。我终于还是这样磨到了医院，先去自助挂号机上挂号，同样显示肠胃科没号了，我想可能人工窗口有额外的号呢？于是我去人工窗口排队，排队的时候直接坐在了地上，前面排队的人还发出了轻蔑的嘲笑。终于排到我的时候，我直接说挂肠胃科，工作人员告诉我没号了。她问我现在什么情况，我解释了下自己的症状，说我觉得应该是胃溃疡导致的胃出血，现在脑子供血不足，工作人员说那要不给你挂个脑科吧:( 我苦笑了一下，问急诊科在哪里，她告诉我在地下一层，我于是继续挣扎着向急诊科磨去。</p><p>总算磨到了急诊科，科室要先分诊，我直接上去描述了症状，分诊的医生让我先去边上测血压。我于是只好先去边上测血压，但是可能是因为出血过多血压太低，血压计一直测不出来。大概测了10来次吧，总算是拿到了一个结果，我赶紧拿着结果找分诊医生。此时我的状态已经非常非常糟糕，几乎走不了路，说话也非常费劲。医生看到我的状态说你怎么样啦，我感觉你是出血过多了，来我带你测一下血压，然后把我扶到血压计边上继续测，但怎么也测不出来。我非常虚弱地说了一句这大概是测不出来了，突然感觉想吐，就说您让开一下，然后吐了一地的血&gt;_&lt;，吐完说了句不好意思。医生赶紧让人去叫抢救室的人过来，随后我就躺上了抢救室的那种移动病床。躺下之后，我感觉自己的状态明显好转了，就和旁边的医生开玩笑说要不要这么夸张，这就送我进抢救室了？医生大概是怕我害怕，以一种相对轻松的态度告诉我说你刚刚吐了些血，我们要检查一下。把我送到抢救室的时候，医生问我有同伴一起来吗，我说没有。他们让我赶紧喊家人，我说自己一个人在外地实习，他们就说那你赶紧喊个朋友啊老师啊过来吧，没有一个人不行的。我于是拨打了同样在Google实习的另一个同学H，但他在睡午觉，没接到电话。然后不得已打了在北京实习另一家实习的女性好朋友L的电话。我与L关系很好，但因为怕妹子吃醋，所以万不得已不想让L过来。L接了电话之后，我简单描述了下情况，然后说你别慌我现在没啥事，很正常，你方便的话就赶紧过来吧。L二话不说直接答应，随即打车过来。</p><p>之后医生给我做了很多检查，所幸检查结果都是正常，只有血压和血色素等指标很低。我说我现在感觉差不多好啦，一定要在抢救室待着吗，这里除了我以外的病人最年轻应该也有70岁吧。医生安慰我说应该没啥事，大概率是胃溃疡导致的出血，但是要做一下胃镜检查。我说那今天就能做吗。医生告诉我北医三院的肠胃科非常有名，预约胃镜通常都是要等一个月甚至3个月以上的，“但是我们急诊科去排应该可以快一点，今天或者明天就可以了。”我说那确认是胃溃疡的就能出院吗，医生笑了下应该很快的。这时候我给妹子打了个电话说了一下情况，她想要赶紧请假从杭州过来，我说再观望一下吧。</p><p>L到了之后，医生让她去帮我买了很多生活用品，我意识到似乎就算是胃溃疡也会是一场相对持久的战役了。不过我自己感觉状态还不错，看她面如死灰的样子，就和她说点玩笑话缓和一下气氛。后来医生让L到急诊室外面去等。H这时候回了我电话，问我怎么了，我描述了一下情况，请他在下班之后来接替L的班，因为抢救室的病人是需要24小时有人陪护的，L一个人肯定是扛不住的。H也二话不说答应了。考虑到这样的24小时陪护不知道还要多久，我并不想太耽误这两位朋友，终于还是让妹子过来了。另一位老家在北京的浙大同学C知道了这个事情，也表示要一起过来。于是C和妹子一起买了高铁票出发过来。</p><p>我又给host说明了一下情况，host非常担心，和Google的hr小姐姐们说了一下这个事情。hr小姐姐们也非常担心，赶紧问了一下我的情况，还有两位比较熟的小姐姐直接出发来公司看望了我，让我再次感受到了公司的温暖&gt;_&lt; 我和她们笑着聊了会儿天，她们发现我还挺能说的，状态也不错。我说不用担心，没啥事的，就是个小意外，等我出院吧。</p><p>到了晚上C和妹子到了。后面的事情就省略说了。第二天做了胃镜，检查出来确实有两处严重的溃疡，高水准的医生用两个钛夹夹了一下。后来就转到肠胃科住院区去，并告诉了家里，我父亲之后来了北京。我住了一周的院，住院期间主要是妹子，我父亲和C陪护我，妹子主要负责照顾我的生活。期间还有一些朋友同学过来看我。巧合的是，我住院的主治医生是Google一位HR的闺蜜，定期向HR们汇报我的情况。经过了一周的治疗后我出院了。带妹子玩了两天之后送她回了杭州，然后休息了一天，就去公司学习去了。</p><h3 id="恢复实习"><a href="#恢复实习" class="headerlink" title="恢复实习"></a>恢复实习</h3><p>第一天去的晚上我host突然出现，和我说要不quit实习回家休养吧，还给我说了他Berkeley的导师给他说的一句话：“学业与事业是个橡胶球，挤一挤，变形了也总会恢复；身体与爱情是个玻璃球，挤一挤碎了就回不去了。”我解释说医生表示我现在其实状态已经相对正常了，只要不过分劳累、注意饮食、不要运动即可，是可以工作的。当时我心里有个很中二的想法，认为这一切的阻碍都是台阶，越是这样，我越是要迈过这些台阶，完成我的实习。另一个重要原因是我希望能用Google的offer来compete阿里的A+，因为我的profile直接面阿里应该是拿不到A+的。</p><p>由于项目的算法部分已经勉强能用，后面的任务重心就转向了产品的前端与后台开发。我之前没有任何相关经验，只能从头学起。在一位朋友的指导下，我一周就连学带做搞了个简陋的雏形。后来就与host他们讨论界面的优化、功能的加强等等，就这么迭代地进行项目的改进。</p><p>印象比较深的是我花了大量的时间在试图把我的服务部署到Google Cloud上，使得项目的用户可以接公司的WIFI即可访问到。然而Google的一些隐私相关的policy让这件看上去简单的事情变得异常麻烦，我试了很多办法都没能成功。最后按照host的建议，我向Google Cloud申请了一段时间的WiFi IP访问特权，结果帮忙查出了个Google防火墙的bug Orz 美国负责防火墙的同事表示这个bug他要上报领导解决。这使得我不得不放弃了部署到云上的打算，转而在我的台式机上部署并让手机端可以访问，但这件事情也花了很大的力气才搞定:) 这些时间花得是让我非常frustrated的，因为这些事情枯燥乏味，没有技术含量，如果不是policy的问题我只需要10分钟就可以搞定Orz</p><p>在写前端、改进用户交互页面的时候，我意识到自己对这种工作内容是不感兴趣的；另外一方面，我的产品的impact也相对很小，潜在的用户大概是个位数的。这两者都让我对写项目的前端不是很提得起兴趣。不过我还是希望自己能把这个实习项目，还是很认真地改进项目，最后也算是做出了一个简陋的产品出来，勉强完成了项目的初期目标。在这段时间，我也有去接触一些其他在Google的同事，了解他们在做的事情，这些都引起了我对自己未来职业生涯的思考。</p><p>后来公司的HR小姐姐问我要不要extend几周来补上住院的两周。我也直接拒绝了：我已迫不及待想回杭州了。答辩完，整理完所有项目的文档与代码，并与我的host以及两位co-host还有众多小伙伴告别后，我checkout了，并于第二天回到了杭州。Google的实习到此结束。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>首先感谢我的诸位家人、朋友、同事、领导、师长等对我在这个阶段的支持与照顾，非常非常感谢。</p><p>总得来说，由于种种原因，我的Google实习经历对自己来说不打算是特别成功的一次经历。当然，这与Google的关系不大，Google是一家非常好的公司：work-life balance，卓越的同事和leader，世界级的vision，认可度很高的履历，人性化的制度、管理与福利等等。我的同事们都很优秀又nice，给了我很多帮助；我的host和co-host们更是水平极高又认真负责，教了我很多东西。只是在Google中国工作似乎不完全是我现在所期望的生活。然而，往抢救室走了一趟之后，我又有些害怕阿里的工作时间与压力。这时候的我有了挺多迷茫的地方：似乎自己期望的工作内容与work-life balance不可兼得？我又对算法岗的工作有一些concern，有心转开发，但阿里的实习offer已经转成了秋招直通终面了，换转组或者岗在阿里又都是极其忌讳的事情。我究竟该何去何从？</p><h2 id="秋招"><a href="#秋招" class="headerlink" title="秋招"></a>秋招</h2><p>在实习部分已经总结过各家公司面试的特点了，这里就不再赘述了，只以简单的流水账形式说一下自己的秋招经历与心路历程。</p><h3 id="百度-1"><a href="#百度-1" class="headerlink" title="百度"></a>百度</h3><p>我的秋招从百度的广告部门的算法岗开始，是实习过了面试之后自动给我转了秋招提前批。当时还在北京实习，被要求去公司参加onsite。总共有3轮技术面试，面试官问了一些coding、机器学习基础、项目和系统设计的问题。百度的面试官感觉实力还挺不错的。印象比较深的是百度面试官真的很喜欢考核各种设计问题，尤其是二面面试官，一定要我独立地设计一个系统，还得事无巨细地给出系统每个组成部分的算法、实现细节等等，即使我已经多次声称对那块知识不熟悉。</p><h3 id="腾讯WXG"><a href="#腾讯WXG" class="headerlink" title="腾讯WXG"></a>腾讯WXG</h3><p>第二家的面试是腾讯的WXG看一看团队的推荐算法岗，同样也是要求去公司参加onsite。可能是运气不好，这次面试给我留下了非常糟糕的印象。首先，我到了office之后，安保人员不让我进入office，需要在门口站着等面试官出来接，出示任何信息都没用。其次，面试官迟迟没有出现，而面试官联系我的电话又是总机电话Orz，我没法联系他。无奈之下，我只能按照邮件里说的拨打了腾讯深圳总部的相关电话，请他们帮忙催面试官，结果他们告诉我只能给面试官发消息，让我等面试官回复。在门口大约站了半小时后，腾讯深圳总部的校招热线告诉我说面试官回消息了，说马上就来。结果这一个马上又让我等了半个小时还不见人影。因为实习最后一周还有很多东西要整理，我没有时间继续浪费下去了，就选择离开，结果快回到Google的时候接到面试官的电话问我走了吗。Orz你们是认为我会在门口站两个小时等你们吗。面试官问我还回去参加面试吗，我说那行吧，就回去继续参加面试。总得来说面试不算很难。第三面是一位漂亮的小姐姐，面试完已经6点多了。我问还有第四面吗，小姐姐一笑，问我饿了吗。此时腾讯的员工们已经在领盒饭准备吃饭了，饭菜的香味飘来，我也确实感到了饥饿，就说是的。本以为她要拿一盒盒饭给我，没想到她再次温柔一笑，说我也饿了，我先去吃饭了，你在这里等下一位面试官吧&gt;_&lt;。四面面试官看上去挺年轻的，技术也很强，一直问我愿不愿意秋招去北京工作，我说我申请的是上海only啊，他说我们在上海没有组Orz 后来我强调说我只接受上海，我们只能不欢而散，他们把我放回了池子里。</p><h3 id="阿里-1"><a href="#阿里-1" class="headerlink" title="阿里"></a>阿里</h3><p>第三家的面试是阿里，同样是实习offer转秋招直通车，只需要一次面试。当时我内心的第一选择是秋招加入阿里，就很认真地准备了一下。面试官春招的时候已经面过我一次了，也没问什么特别的问题，只是特别问了我的意愿，我就照实说第一选择是阿里。面试官告诉我说等HR的面试吧。后来接了Optiver的offer之后我就拒掉了阿里了&gt;_&lt;</p><h3 id="Optiver-1"><a href="#Optiver-1" class="headerlink" title="Optiver"></a>Optiver</h3><p>在即将结束实习的时候，Optiver的HR小姐姐给我打了电话问我秋招愿意考虑Optiver的机会吗，因为我有实习offer，就可以直通onsite面试了。我说当然愿意啦。HR与我聊了挺久的，问了我一些对公司的concern，并一一为我解答。最后HR说那我们约你下周travel来上海面试吧，我说不用不用，下周我就在杭州了。于是HR给我订了两张杭州去上海的商务座，约了实习结束的周一去上海onsite面试。HR告诉我说到时候会有两个技术团队的team lead一起面我，一个是公司的data team，工作内容会与machine learning相关，另一个是公司的system team，工作内容主要是system programming。我可以根据自己的喜好以及给我offer的team，选择一个加入，我欣然同意了。</p><p>说实话，此时我还没有把Optiver放到内心的第一位置，当时的想法还是与妹子一起去阿里。不过我还是很担心自己能否承受得住阿里的工作强度，就与阿里的师兄交流了一下，他当时表示说应该还是可以的。</p><p>实习checkout之后，我迫不及待地回到了杭州。妹子向我吐槽了很久她暑假的一些没有告诉我的遭遇，并且说了一些她家里比较棘手的事情。说完她就和我提了分手，转身回去，我只能一脸懵逼地回宿舍，收拾心情准备第二天的面试。我想那些棘手的事情可能会需要比较多的钱，而之前就听说Optiver的待遇丰厚，那么这个offer一定要拿下来。于是第二天就晕乎乎地坐上了高铁，坐上高铁之后给妹子发了个消息，说等我回来我们再好好聊聊。</p><p>我带着少有的必胜的决心进了Optiver的上海office。面试安排在下午两点左右，我到得比较早，大约是下午一点，所以HR小姐姐先和我聊了会儿天。她先和我聊了一下自己的方向选择问题，她有些疑惑我本科毕业就大跨度地从航天转了CS，现在竟然又要从算法岗转开发岗。我大概解释了一下。我们又聊了一些公司culture什么的。HR大概告诉了我几点。其中有几点特别吸引了我的关注：</p><ol><li>公司注重work-life balance。例如公司不提供晚饭，不鼓励大家晚上还继续加班。</li><li>公司的福利也是相当不错。有很多年假带薪病假等，也有一些免费的餐饮、咖啡厅、大额商业保险、健身房等等。</li><li>高频交易是一个竞争激烈的行业，公司希望大家都能够<strong>self-motivated地，独当一面地去做出有impact的工作</strong>。</li><li>公司招聘宁缺毋滥，highly selective。例如我实习面的AE岗位，最后公司也没有勉强招不合适的人。</li></ol><p>技术面试终于开始了，两位team lead进入面试房间后先很客气地与我握手并做了自我介绍。随后他们给了我一道相当复杂的系统设计的问题。在提问以确认自己理解了题目之后，我进行了一些思考，并给出了第一个设计方案。两位面试官非常细致地问了我这个方案的各个细节之处、性能分析、trade-off等等，这些问题涉及到数据结构、算法、操作系统、体系结构与计算机网络的各种知识。好在我在说出自己的解决方案之前就已经对这些细节进行了思考，所以回答得还算流利。面试官接下来让我在白板上实现自己的代码，在这个过程中同时也考察了我的抽象能力、模块设计能力等等。写完代码并证明它是bug-free之后，面试官突然狡黠一笑，问我能不能再改进这个方案。与面试官进行了大量的讨论之后，我终于成功地给出了第二版的设计方案，当我给出来的时候一位面试官非常激动地说了一句“Exactly！”然而两位面试官并不打算就这么简单地放过我，又继续问了大量的follow-up问题，直到确认我每个细节都考虑到了之后才终于流露出了满意的笑容。随后，他们让我问了一些问题。我最好奇的一个问题是，明明有一位面试官是data组的team lead，为什么并没有问我任何机器学习相关的问题，而是问了这么一个在我看来更偏system的问题。面试官告诉我说，这个问题在他们看来是problem solving ability的问题，而Optiver认为基础扎实、沟通能力出色并且有很好的problem solving ability的人可以胜任公司的任何一个IT岗位。我还问了一个问题是，他们选择optiver的最重要的原因是什么。他们告诉我是<strong>exciting work and impact</strong>。</p><p>终于这轮面试结束了，时间大概过了两个小时左右。我休息了几分钟去上了个卫生间。这里有个小插曲，我突然收到一条来自腾讯的短信约我15分钟后面试，问我是否能参加，我回复了否。后面在Optiver head of IT面我的时候，腾讯的面试官突然打电话过来问我什么时候有空。好在Optiver面试官没有生气，还让我接了电话商量了和腾讯的面试时间。</p><p>回到office里的时候注意到公司在为一位同事庆生，记得是一位毕业于MIT的小哥，我也过去分了点蛋糕吃。随后开始第二轮面试，第二轮的面试是Head of IT面我，也就是Optiver IT部门的老大。面试官问了一些项目相关的问题、我对一些技术的深入理解与思考等。之后就轮到我问了面试官一些问题。大约到了5点出头，面试官看了一眼时间，我估计面试官可能在思考下班的事情了，就只再问了一个问题，结束了面试。随后HR小姐姐送我离开了公司。</p><p>毫无疑问，Optiver的面试在我参加的所有面试中最具有特点。这并不完全是说面试的难度很高或者压力很大（当然难度也确实很高），还包括面试的形式、内容、考察的能力等等。通过面试，需要具有扎实的基础、严谨的思维、优秀的沟通能力、出色的抽象问题并解决问题的能力等等。更重要的是，在与面试官讨论设计方案时，我可以说是首次在面试中感受到了exciting的感觉。</p><p>面试的第二天下午，我接到了HR小姐姐的电话，通知我通过了两个部门的面试，并问我要选择哪一个部门。根据我的Profile，公司似乎更希望我去做ML相关的项目，但是我坚持要了System部门的offer。HR小姐姐告诉我如果是接System的offer的话，那第二天上午还需要再接受一场电话面试，考察一些CS基础，我欣然同意。</p><p>电面如期而至，这次是更加细致地考察了CS的基础知识。又过了一天，HR的offer call到了，薪酬比其他公司开的要高不少。我后来才知道，这是Optiver秋招唯一的一个System Developer的offer。HR给了我一些时间考虑Offer的选择，并建议我与尽量多的朋友交流。</p><h3 id="腾讯数据库内核"><a href="#腾讯数据库内核" class="headerlink" title="腾讯数据库内核"></a>腾讯数据库内核</h3><p>从Optiver onsite面试回来后，我发现了妹子的事情似乎比自己想象中更加复杂一些。在了解到了更多的真相之后，我的第一选择就不再是阿里了。我去喝了点酒&gt;_&lt; 也是过去几年内第一次喝酒，然后就引发了酒精过敏。过敏当天腾讯的电面过来了。为了避免上次被Location不合适的部门打捞的问题，这次特地找人内推了上海的数据库内核团队，并且投递的是开发岗。面试官主要问了一些操作系统与数据结构的知识，还问了一点算法题和计算机网络。由于我简历上都是机器学习的项目，面试官就没有问，面试也很快就结束了。第二天大团队在深圳的leader面我，考核范围与之前相同，但问了很多Linux系统相关的东西。面完问我愿意去深圳吗，我说我肯定不考虑上海/杭州以外的office，面试官说那行，给你上海的offer吧。10分钟后是腾讯的HR面，面完告诉我说等着接收预录取的邮件。总得来说这次面试的体验挺不错的，面试官们水平都不错，也很和气，HR也非常和气。</p><p>过了两周左右，腾讯给我开了价格，让我稍微有些惊讶地开了个很高的价格。据我所知，国内的公司向来非常看重项目经历与岗位匹配度，因此我有点惊讶像我这样背景与岗位完全不匹配的候选人竟然能拿到这样的offer。看来腾讯的面试官们对我的潜力还挺看好的&gt;_&lt;不过我当时已经接了Optiver的offer了，因此也就直接拒绝了。不过让我有些没想到的是之后腾讯又联系了我好几次（4次以上？），非常有诚意，也感谢各位腾讯面试官的欣赏。其实这个offer也是一个挺不错的选择，工作内容有意思又核心，待遇也挺高，部门加班也不算多，如果没有Optiver的offer，我可能也会考虑这个offer的。</p><h3 id="Google-1"><a href="#Google-1" class="headerlink" title="Google"></a>Google</h3><p>面完腾讯之后我又作死地去喝了一次酒&gt;_&lt;这次终于引起了相当严重的酒精过敏问题，让我一直发烧，连续几天都睡不了觉。喝完酒的第二天刚好是中秋节，回了趟家，并且因为不敢让父母知道自己喝酒的事情，没有去医院解决酒精过敏问题，只好终日在床上躺着。</p><p>Google的转正面试在中秋节结束后的第一天。很遗憾我在去上海前并没能成功地睡着。因此大概是72小时没睡着地去了上海。在上海的时候碰到了一起来面试的NLP大佬工友F，与F一块去吃饭的时候进行了一些关于text style transfer的学术讨论，感觉挺有意思。也许是因为与人讨论学术问题带来的愉悦感，也许只是因为酒精过敏终于好转了些，当天晚上终于成功地睡着了一会儿&gt;_&lt;</p><p>第二天到了Google的office进行面试。第一位面试官是毕业于Stanford的资深Googler，问了比较多的算法题。第二面是一位相对年轻一些的Googler，也考了两道算法题。虽然因为连续多天的缺乏睡眠而有些疲惫，但我还是顺利地把算法都写了出来。面完之后HR小姐姐问我想不想去国外的Office，我说我只考虑国内的Office&gt;_&lt;</p><h3 id="Offer选择"><a href="#Offer选择" class="headerlink" title="Offer选择"></a>Offer选择</h3><p>在与许多老师、朋友沟通后，我最终做出了选择Optiver上海的决定。这里还要特别感谢在Optiver的H学长，学长在收到我咨询的第二天竟然直接来了浙大与我面谈Orz 至于我最终做出这个选择的理由此处就不说了。这里引用一位浙大学长求职经历帖的话：”如果你跟我足够熟的话，私下问我我会告诉你的。或者你就当我是选择了给钱给得多的offer就好了。“</p><p>这里简单谈谈我从算法岗转到开发岗的理由。这些理由在前文中也多多少少地提到了，此处也就不再赘述。其中最大的一个concern就是我对算法岗的工作内容有些不感兴趣。我担心自己对Optiver的data scientist岗位也不感兴趣，这才坚持选择了System Developer。不过公司在知道我的concern之后表示，我可以到公司之后看一看data scientist在做的事情，如果感兴趣的话也可以随时转岗，甚至两个岗位的项目一起做&gt;_&lt; 另一方面，我也觉得自己的工程能力还是太弱了一些，也想借着从事System Developer的机会把自己的工程能力培养起来。</p><p>最后再简单谈谈为什么我会选择金融行业。这个问题的答案其实非常简单，因为Optiver是金融行业的公司&gt;_&lt;我选择了Optiver，所以也就这样选择了金融行业。</p><p>确认选择Optiver之后，我的秋招到此算是结束了。虽然我后面还面了几家公司，但主要也是奔着与面试官交流去的。</p><h2 id="一些学习资料推荐"><a href="#一些学习资料推荐" class="headerlink" title="一些学习资料推荐"></a>一些学习资料推荐</h2><p>这里简单推荐一些我看过的（有些看完了，有些看了部分）适合自学的学习资料，以供参考。因为我太懒了，所以覆盖的内容不全面，自学CS的同学也可以参考<a href="http://catalog.mit.edu/degree-charts/computer-science-engineering-course-6-3/">MIT的课表</a>看看要学哪些内容。我个人推荐学习从一门导论课程开始，然后是6.null，然后是Berkeley的CS61系列。之后学习算法课程、操作系统课程、组成原理/体系结构课程与计算机网络课程。再之后，我相信各位已有足够的判断力选择自己感兴趣或者是需要学习的课程了。再次强调，<strong>上课不做题，等于白上课。</strong></p><h3 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h3><p><a href="https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/">MIT 18.01</a>，<a href="https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/">MIT 18.02</a>：微积分。</p><p><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">MIT 18.06</a>：线性代数</p><p><a href="https://projects.iq.harvard.edu/stat110/home">Harvard Stat110</a>：概率论。印象深刻的有一句话，”Random variable is a function”。</p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/">MIT 6.042J</a>：Mathematics for Computer Science. 在我看来最有趣的数学课，如果TA能少上几节课就好了:(</p><h3 id="CS导论"><a href="#CS导论" class="headerlink" title="CS导论"></a>CS导论</h3><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/">MIT 6.001</a>：Introduction to Computer Science and Programming in Python</p><p><a href="https://cs50.harvard.edu/x/2020/">Harvard CS50</a>: This is CS50!一门很好的导论课程，覆盖的范围非常之广，且内容与MIT的导论课程颇为互补。</p><h3 id="CS实用课程"><a href="#CS实用课程" class="headerlink" title="CS实用课程"></a>CS实用课程</h3><p><a href="https://missing.csail.mit.edu/">MIT 6.null</a>: CS实用工具集合介绍。强烈推荐早点看这门课，非常实用。<a href="https://zhuanlan.zhihu.com/p/139361685">这里</a>有我之前写的一篇简介文章。</p><h3 id="数据结构与算法"><a href="#数据结构与算法" class="headerlink" title="数据结构与算法"></a>数据结构与算法</h3><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/">MIT 6.006</a>: Introduction to Algorithms，教材是CLRS。</p><p><a href="https://cs61a.org/">Berkeley CS61系列</a>: 其实我没上过这门课，因为当年入门CS的时候这门课似乎还没录像？现在有公开的录像了，而且口碑也非常好。</p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/">MIT 6.046</a>: Design and Analysis of Algorithms. 进阶版，但其实前面6.006就已经内容不少了。</p><p>Stanford也有几门有点老但是也非常好的相关课程。不过我觉得上面这些课程内容更新，质量也不逊色，各位就看自己喜好来挑选吧。</p><p>算法笔记：胡凡著。一位浙大学长写的使用c++实现基础数据结构与算法的书，写得挺清晰的，里面的代码也很实用。我现在面试之前还会翻一下这本书上一些算法的实现。</p><h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><p><a href="https://www.cs.cmu.edu/~213/">CMU 15-213</a>: 教材是CSAPP。</p><p><a href="https://cs162.eecs.berkeley.edu/">Berkeley CS162</a>：youtube和B站上都有2019Spring的视频。professor上课上得很好，hw和lab也都很给力。美中不足的是hw的autograder没有开放出来，但好在lab有提供test case。</p><p><a href="https://pdos.csail.mit.edu/6.824/">MIT 6.824</a>: 分布式系统。不久前终于感人地放出视频了。</p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-828-operating-system-engineering-fall-2012/">MIT 6.828</a>：其实还没看，小伙伴都说好。</p><p>Modern Operating System</p><h3 id="组成原理-体系结构"><a href="#组成原理-体系结构" class="headerlink" title="组成原理/体系结构"></a>组成原理/体系结构</h3><p><a href="https://www.cs.cmu.edu/~213/">CMU 15-213</a>：教材是CSAPP。</p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-004-computation-structures-spring-2017/">MIT 6.004</a>：Computer Structure. 偏硬件，老师讲得很好。</p><p>MHRD: From NAND to CPU. Steam上一款有趣的小游戏，从NAND开始搭建各种逻辑电路，最后制作出一款简易的CPU。<a href="https://zhuanlan.zhihu.com/p/148077075">这里</a>有我之前写的一篇简介文章。</p><p><a href="https://safari.ethz.ch/architecture/fall2019/doku.php">CMU Computer Architecture</a>: 无论是在CMU还是ETH，Onur每年都会将这门课的录像和课程资源都公开，这是怎样的一种情怀Orz 19年的录像中Slide不是很清晰，可以自己使用双屏配合一下；或者观看18年的录像。</p><p>Computer Architecture: a Quantitative Approach</p><h3 id="计算机网络"><a href="#计算机网络" class="headerlink" title="计算机网络"></a>计算机网络</h3><p><a href="https://cs144.github.io/">Stanford CS144</a>: 这门课多年前有一个mooc版本，后来不知道为啥被撤了，但是授课视频还是流传了出来。我个人不是很适应这门课的授课视频，看的有点犯困，但是课程的Lab还是出得很好的。</p><p>Computer Network: a Top Down Approach</p><h3 id="编程语言"><a href="#编程语言" class="headerlink" title="编程语言"></a>编程语言</h3><p><a href="https://cs61a.org/">Berkeley CS61系列</a>: 系列的几门课使用的是不同的语言，也都有很不错的配套作业，又基本都有视频公开，因此蛮推荐使用这个系列的课程来学编程语言的。</p><p><a href="https://stanfordpython.com/">Stanford CS41</a>: Python Language Programming. 可惜没有视频，作业挺有意思的。其实我觉得学语言比较好的一种方式就是学了语法之后做一些练手的项目、作业，这样会掌握得比较快。</p><p><a href="http://web.stanford.edu/class/cs106l/">Stanford CS106L</a>: C++ Language Programming. 前文也介绍过了，我觉得那本Full Course Reader写得真的很好，推荐一下~</p><p>还有一些领域内很知名的书，这里就不再专门推荐啦，这些课程里也会有介绍/推荐。这里再啰嗦一句，对大多数人来说，往往只需要学会某种编程语言中20%不到的常用特性，而这20%的常用特性往往占了实际使用这门编程语言中的90%。所以我不是很建议一开始弄本厚厚大大的书看，那样又吃力又缺少反馈，而且可能会花大量精力在自己根本用不到的地方。我其他的编程语言似乎都是用官方tutorial入门的，这里也就不推荐啦。</p><h3 id="软件工程"><a href="#软件工程" class="headerlink" title="软件工程"></a>软件工程</h3><p><a href="http://web.mit.edu/6.031/www/sp20/">MIT 6.031</a>: Software Construction. 算是介绍软件工程的一些知识吧，包括如何写出更鲁棒的代码等等。有很好的在线阅读资料，但可惜没有公开的视频。</p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2018/index.htm">MIT 6.172</a>: Performance Engineering of Software Systems. 讲的是如何优化程序的性能，非常有趣的一门课。</p><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><p>浙大的同学来蹭蔡登老师的课&gt;_&lt;</p><p><a href="https://www.coursera.org/learn/machine-learning">Coursera Andrew NG的Machine Learning</a>：机器学习之路从这里开始。</p><p><a href="http://cs229.stanford.edu/">Stanford CS229</a>：Machine Learning. Youtube上有视频。</p><p><a href="http://www.cs.cmu.edu/~mgormley/courses/10701-f16/schedule.html">CMU 15-701</a>：Intro to Machine Learning. 就找到这么一年有视频的。</p><p><a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Coursera PGM</a>：知名课程。</p><p><a href="http://www.cs.cmu.edu/~epxing/Class/10708-19/lectures/">CMU 15-708</a>：Probabilistic Graphical Model 我只看过两个Lecture。</p><p>下面有几本书，不过还是推荐和课程一起看。</p><p>Pattern Classification (PC)：比较老的书了，但是内容还是很有意义。</p><p>Elements of Statistical Learning：insight很多。</p><p>Pattern Recognition and Machine Learning (PRML)</p><p>Machine Learning: a Probabilistic Approach (MLAPP)</p><p>Information Theory, Inference, and Learning：可惜David J. C. MacKay英年早逝。</p><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><p><a href="http://cs231n.github.io/">Stanford CS231n</a>：Deep learning for CV.  推荐深度学习从这里开始</p><p><a href="http://web.stanford.edu/class/cs224n/">Stanford CS224n</a>：Deep learning for NLP.</p><p><a href="http://rll.berkeley.edu/deeprlcourse/">Berkeley CS285</a>：Deep RL. 我只挑着看了几个Lecture，小伙伴说好。</p><p><a href="https://www.deeplearningbook.org/">Deep Learning Book</a>：花书。</p><h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p>本来这篇的主要目的是总结一些自己的面试心得供大家参考，后来又加入了自己自学CS历程的介绍、心得等，又介绍了自己找工作的历程和思考，反而使得本文的后半部分显得有些喧宾夺主了&gt;_&lt; 此时突然想到一位师兄博客里讨论的话题：“如果能回到十年前，你会对过去的自己说什么？”如果我能回到开始CS的旅程之前，我也许会对过去的自己说：“你会经历一段难以置信、跌宕起伏的旅程。你会遇到很多志同道合的好友，以及你十分尊敬的师长。虽然你也会走很多弯路，碰到很多困难与挫折，但是不要担心，你一直兢兢业业、勤勉刻苦，你的努力最终都会得到回报。”</p><p>总之，我希望这篇文章里所写的自己的一些经历、思考、心得、总结等能对后来者起到一些帮助。如果你在阅读了这篇文章之后觉得有一些收获，那本文的目的也就达到了。</p><p>各位同学，我们江湖上见。</p>]]></content>
    
    
    
    <tags>
      
      <tag>面经心得</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Lightgbm使用指南</title>
    <link href="/2020/08/24/2020-08-24-Lightgbm%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/08/24/2020-08-24-Lightgbm%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h1 id="Lightgbm介绍"><a href="#Lightgbm介绍" class="headerlink" title="Lightgbm介绍"></a>Lightgbm介绍</h1><p><a href="https://github.com/Microsoft/LightGBM">LightGBM</a> 是Light Gradient Boosted Machine的缩写，是Microsoft开发的用于机器学习的免费开源分布式梯度提升框架。它基于决策树算法，用于排名，分类和其他机器学习任务。开发重点是性能和可伸缩性。该框架支持不同的算法，包括GBT，GBDT，GBRT，GBM和MART。</p><p>本文介绍Lightgbm的常用参数以及调参方法。最后给出一个实用超参数优化库optuna来帮助实现参数的随机搜索！</p><ul><li>LightGBM的介绍及优势</li><li>LightGBM的使用（代码）</li><li>LightGBM的调参指导</li><li>LightGBM的API接口方法</li><li><p>附录：optuna超参数优化</p><span id="more"></span></li></ul><h1 id="LightGBM在哪些地方进行了优化-区别XGBoost-？"><a href="#LightGBM在哪些地方进行了优化-区别XGBoost-？" class="headerlink" title="LightGBM在哪些地方进行了优化    (区别XGBoost)？"></a>LightGBM在哪些地方进行了优化    (区别XGBoost)？</h1><p>参考《<a href="https://blog.csdn.net/huacha__/article/details/81057150">LightGBM——提升机器算法（图解+理论）</a>》</p><ul><li>基于Histogram的决策树算法（直方图优化）</li><li>LightGBM的生长策略（基于最大深度的Leaf-wise）</li><li>直方图做差加速</li><li>支持类别特征</li><li>Cache命中率优化</li><li>支持并行学习</li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/13876065-d31ee3257cba9977.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/600/format/webp" alt="img"></p><p>image</p><h2 id="1-1-直方图优化"><a href="#1-1-直方图优化" class="headerlink" title="1.1 直方图优化"></a>1.1 直方图优化</h2><p>直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数（其实又是分桶的思想，而这些桶称为bin，比如[0,0.1)→0, [0.1,0.3)→1），同时构造一个宽度为k的直方图。</p><p>在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p><p><img src="https:////upload-images.jianshu.io/upload_images/13876065-6ef4ba7cff5dca51.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/557/format/webp" alt="img"></p><p>使用直方图算法有很多优点。首先，最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。然后在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data<em>#feature)优化到O(k</em>#features)。</p><h2 id="1-2-带深度限制的Leaf-wise的叶子生长策略"><a href="#1-2-带深度限制的Leaf-wise的叶子生长策略" class="headerlink" title="1.2 带深度限制的Leaf-wise的叶子生长策略"></a>1.2 带深度限制的Leaf-wise的叶子生长策略</h2><p>在XGBoost中，树是按层生长的，称为Level-wise tree growth，同一层的所有节点都做分裂，最后剪枝，如下图所示：</p><p><img src="https:////upload-images.jianshu.io/upload_images/13876065-1eed3540256cb9d3.png?imageMogr2/auto-orient/strip|imageView2/2/w/596/format/webp" alt="img"></p><p>Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p><p>在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise)<br> 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise)算法。</p><p><img src="https:////upload-images.jianshu.io/upload_images/13876065-4e4e2cd1bff18db7.png?imageMogr2/auto-orient/strip|imageView2/2/w/769/format/webp" alt="img"></p><p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p><h2 id="1-3-直方图做差优化"><a href="#1-3-直方图做差优化" class="headerlink" title="1.3 直方图做差优化"></a>1.3 直方图做差优化</h2><p>LightGBM另一个优化是Histogram（直方图）做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。</p><p>利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p><p><img src="https://img-blog.csdn.net/20180719163927276?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="1-4-直接支持类别特征"><a href="#1-4-直接支持类别特征" class="headerlink" title="1.4 直接支持类别特征"></a>1.4 直接支持类别特征</h2><p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的0/1特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。</p><p>基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。在Expo数据集上的实验，相比0/1展开的方法，训练速度可以加速8倍，并且精度一致。<strong>LightGBM是第一个直接支持类别特征的GBDT工具</strong>。</p><p><img src="https://img-blog.csdn.net/20180719172833390?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="1-5-Cache命中率优化"><a href="#1-5-Cache命中率优化" class="headerlink" title="1.5 Cache命中率优化"></a>1.5 Cache命中率优化</h2><p>当我们用数据的bin描述数据特征的时候带来的变化：首先是不需要像预排序算法那样去存储每一个排序后数据的序列，也就是下图灰色的表，在LightGBM中，这部分的计算代价是0；第二个，一般bin会控制在一个比较小的范围，所以我们可以用更小的内存来存储</p><p><img src="https://img-blog.csdn.net/2018071916292255?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="1-6-支持并行学习"><a href="#1-6-支持并行学习" class="headerlink" title="1.6 支持并行学习"></a>1.6 支持并行学习</h2><p>LightGBM原生支持并行学习，目前支持<strong>特征并行(Featrue Parallelization)</strong>和<strong>数据并行(Data Parallelization)</strong>两种，还有一种是<strong>基于投票的数据并行(Voting Parallelization)</strong></p><ul><li><strong>特征并行</strong>的主要思想是在不同机器、在<strong>不同的特征集合</strong>上分别寻找最优的分割点，然后在机器间同步最优的分割点。</li><li><strong>数据并行</strong>则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后<strong>在合并的直方图上面</strong>寻找最优分割点。</li></ul><p>LightGBM针对这两种并行方法都做了优化。</p><ul><li><strong>特征并行</strong>算法中，通过在本地保存全部数据避免对数据切分结果的通信。</li><li><strong>数据并行</strong>中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。</li><li><strong>基于投票的数据并行(Voting Parallelization)</strong>则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。</li></ul><p>下图更好的说明了以上这三种并行学习的整体流程：</p><p><img src="https://img-blog.csdn.net/20180804213102717?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180804213248677?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>在直方图合并的时候，通信代价比较大，基于投票的数据并行能够很好的解决这一点。</p><p><img src="https://img-blog.csdn.net/20180804213500786?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="1-7-LightGBM-vs-XGBoost"><a href="#1-7-LightGBM-vs-XGBoost" class="headerlink" title="1.7 LightGBM vs. XGBoost"></a>1.7 LightGBM vs. XGBoost</h2><p>下面这个表格给出了XGBoost和<a href="https://github.com/Microsoft/LightGBM">LightGBM</a> （Light Gradient Boosting Machine）之间更加细致的性能对比，包括了树的生长方式，LightGBM是直接去选择获得最大收益的结点来展开，而XGBoost是通过按层增长的方式来做，这样呢LightGBM能够在更小的计算代价上建立我们需要的决策树。当然在这样的算法中我们也需要控制树的深度和每个叶子结点的最小数据量，从而减少过拟合。</p><div class="table-container"><table><thead><tr><th></th><th><strong>XGBoost</strong></th><th><strong>LightGBM</strong></th></tr></thead><tbody><tr><td>树木生长算法</td><td><strong>按层生长的方式</strong>有利于工程优化，但对学习模型效率不高</td><td>直接<strong>选择最大收益的节点</strong>来展开，在更小的计算代价上去选择我们需要的决策树控制树的深度和每个叶子节点的数据量，能减少过拟合</td></tr><tr><td>划分点搜索算 法</td><td>对特征预排序的方法</td><td>直方图算法：将特征值分成许多小筒，进而在筒上搜索分裂点，减少了计算代价和存储代价，得到更好的性能。另外数据结构的变化使得在细节处的变化理上效率会不同</td></tr><tr><td>内存开销</td><td>8个字节</td><td>1个字节</td></tr><tr><td>划分的计算增益</td><td>数据特征</td><td>容器特征</td></tr><tr><td>高速缓存优化</td><td>无</td><td>在Higgs数据集上加速40%</td></tr><tr><td>类别特征处理</td><td>无</td><td>在Expo数据集上速度快了8倍</td></tr></tbody></table></div><h1 id="Lightgbm的实战应用（代码）"><a href="#Lightgbm的实战应用（代码）" class="headerlink" title="Lightgbm的实战应用（代码）"></a>Lightgbm的实战应用（代码）</h1><blockquote><p>参考: </p><p>1、用lightgbm算法实现鸢尾花种类的分类任务，GitHub：<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FNLP-LOVE%2FML-NLP%2Fblob%2Fmaster%2FMachine%20Learning%2F3.4%20LightGBM%2F3.4%20LightGBM.ipynb">点击进入</a></p><p>2、<a href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247539023&amp;idx=1&amp;sn=b105f6b08d3b886f62e16b367eabfd02&amp;chksm=e8738802df0401146dfdd1a9052f7848d20c6439e39299d065d89f77e6604a97609430f86923&amp;scene=126&amp;sessionid=1604042119&amp;key=c3402f98b9ff36463480cd8ac280815c46524dc60b794c1a66aef363ca2df3e0af7ad5b74aa091fdf8bd3b1142bc96d10c3b6be0d9be874a1554e7300b8368731a5610c2dc0ccf10d3b5492c95c08b1ca2b216e355bedb99cf9f36ab5892b229c0e9875addd229e8354b3fedba81cf192d0c5b3fcfc3b9f7c64124742c31c959&amp;ascene=1&amp;uin=MjM2MDA1NjcyMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=AyhYM9JTEQwg%2Fzwv1Q%2F9PJQ%3D&amp;pass_ticket=r5dszQk2lRNNteX%2BZmX6%2B1wF2g3D57o1FKmEWEXG6IdPb9qLFiUO6rf4TKQjjajJ&amp;wx_header=0">比赛杀器LightGBM常用操作总结！</a></p><p><a href="LightGBM小课.ipynb">3、《机器学习⼩课堂之初探 LightGBM》，强烈建议逐行阅读学习！</a></p></blockquote><p>lightgbm的使用起来也很简单。大致步骤可以分为下面几个</p><ul><li>首先用lgb包的DataSet类包装一下需要测试的数据；</li><li>将lightgbm的参数构成一个dict字典格式的变量</li><li>将参数字典，训练样本，测试样本，评价指标一股脑的塞进lgb.train()方法的参数中去</li><li>上一步的方法会自觉地得到最佳参数和最佳的模型，保存模型</li><li>使用模型进行测试集的预测</li></ul><p>其中比较重要的是第二步也就是设置参数。有很多很重要的参数，在下面的第二部分（参数字典）中，我大概介绍一下使用的比较多的比较有意义的参数。</p><h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install lightgbm<br>pip install --no-binary :<span class="hljs-built_in">all</span>: lightgbm <span class="hljs-comment">#从源码编译安装</span><br>pip install lightgbm --install-option=--mpi <span class="hljs-comment">#从源码编译安装 MPI 版本</span><br>pip install lightgbm --install-option=--gpu <span class="hljs-comment">#从源码编译安装 GPU 版本</span><br></code></pre></td></tr></table></figure><h2 id="2-定义数据集"><a href="#2-定义数据集" class="headerlink" title="2. 定义数据集"></a>2. 定义数据集</h2><p>lightgbm的一些特点：</p><ul><li>LightGBM 支持 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FComma-separated_values">CSV</a>, <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTab-separated_values">TSV</a> 和 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.csie.ntu.edu.tw%2F~cjlin%2Flibsvm%2F">LibSVM</a> 格式的输入数据文件。</li><li><strong>LightGBM 可以直接使用 categorical feature（类别特征）（不需要单独编码）</strong>。 <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fstat-computing.org%2Fdataexpo%2F2009%2F">Expo data</a> 实验显示，与 one-hot 编码相比，其速度提高了 8 倍。可以在包装数据的时候指定哪些属性是类别特征（也可以使用pd.DataFrame存放特征X, 每一列表示1个特征, 将类别特征设置为X[cat_cols].astype(‘category’). 这样模型在fit时会自动识别类别特征 <a href="https://blog.csdn.net/u013385018/article/details/104167969">参考</a>）</li><li>LightGBM 也支持加权训练，可以在包装数据的时候指定每条记录的权重</li></ul><p>LightGBM 中的 Dataset 对象由于只需要保存 discrete bins（离散的数据块）, 因此它具有很好的内存效率. 然而, Numpy/Array/Pandas 对象的内存开销较大. 如果你关心你的内存消耗. 您可以根据以下方式来节省内存:</p><ul><li>在构造 Dataset 时设置 free_raw_data=True （默认为 True）</li><li>在 Dataset 被构造完之后手动设置 raw_data=None</li><li>调用 gc</li></ul><p>LightGBM Python 模块能够使用以下几种方式来加载数据:</p><ul><li>libsvm/tsv/csv txt format file（libsvm/tsv/csv 文本文件格式）</li><li>Numpy 2D array, pandas object（Numpy 2维数组, pandas 对象）</li><li>LightGBM binary file（LightGBM 二进制文件）<br> 加载后的数据存在 Dataset 对象中.</li></ul><p>要加载 numpy 数组到 Dataset 中:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># data = np.arange(0, 5000).reshape((500, 10))</span><br>data = np.random.rand(500, 10)  <span class="hljs-comment"># 500 个样本, 每一个包含 10 个特征</span><br>label = np.random.randint(2, size=500)  <span class="hljs-comment"># 二元目标变量,  0 和 1</span><br>train_data = lgb.Dataset(data, label=label)<br></code></pre></td></tr></table></figure><p>在现实情况下，我们可能之前使用的是pandas的dataFrame格式在训练数据，那也没有关系，可以先使用sklearn包对训练集和测试集进行划分，然后再使用DataSet类包装。DataSet第一个参数是训练特征，第二个参数是标签</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jsx"><span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">model_selection</span> <span class="hljs-keyword">import</span> train_test_split<br>X_train,X_val,y_train,y_val = <span class="hljs-title function_">train_test_split</span>(X,Y,test_size=<span class="hljs-number">0.2</span>)<br>xgtrain = lgb.<span class="hljs-title class_">Dataset</span>(X_train, y_train)<br>xgvalid = lgb.<span class="hljs-title class_">Dataset</span>(X_val, y_val, reference=xgtrain)<br></code></pre></td></tr></table></figure><p>在 LightGBM 中, 验证数据应该与训练数据一致（格式一致）.<br> 保存 Dataset 到 LightGBM 二进制文件将会使得加载更快速:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">train_data = lgb.Dataset(<span class="hljs-string">&#x27;train.svm.txt&#x27;</span>)<br>train_data.save_binary(<span class="hljs-string">&#x27;train.bin&#x27;</span>)<br></code></pre></td></tr></table></figure><p><strong>指定 feature names（特征名称）和 categorical features（分类特征）,注意在你构造 Dataset 之前, 你应该将分类特征转换为 int 类型的非负整数。还可以指定每条数据的权重（比如在样本规模不均衡的时候希望少样本的标签对应的记录可以拥有较大的权重）</strong></p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs kotlin">w = np.random.rand(<span class="hljs-number">500</span>, )<br>train_data = lgb.Dataset(<span class="hljs-keyword">data</span>, label=label, feature_name=[<span class="hljs-string">&#x27;c1&#x27;</span>, <span class="hljs-string">&#x27;c2&#x27;</span>, <span class="hljs-string">&#x27;c3&#x27;</span>], categorical_feature=[<span class="hljs-string">&#x27;c3&#x27;</span>],weight=w)<br></code></pre></td></tr></table></figure><p>或者</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs kotlin">train_data = lgb.Dataset(<span class="hljs-keyword">data</span>, label=label, group=group_x)<br>w = np.random.rand(<span class="hljs-number">500</span>, )<br>train_data.set_weight(w)<br></code></pre></td></tr></table></figure><h2 id="3-设置参数"><a href="#3-设置参数" class="headerlink" title="3. 设置参数"></a>3. 设置参数</h2><ol><li><p>参数字典</p><p>每个参数的含义后面介绍（使用<code>max_position</code> 设置 <code>NDCG</code> 优化的位置）</p></li></ol><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs clean"># 将参数写成字典下形式<br>lgb_params = &#123;<br>    <span class="hljs-string">&quot;task&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>,  # task type, support train and predict<br>    <span class="hljs-string">&quot;objective&quot;</span>: <span class="hljs-string">&quot;lambdarank&quot;</span>,  # 排序任务(目标函数)<br>    <span class="hljs-string">&quot;boosting_type&quot;</span>: <span class="hljs-string">&quot;gbdt&quot;</span>,  # 基学习器 gbrt dart<br>    <span class="hljs-string">&quot;metric&quot;</span>: &#123;<span class="hljs-string">&#x27;ndcg&#x27;</span>, <span class="hljs-string">&#x27;map&#x27;</span>&#125;,  # 评估函数<br>    # <span class="hljs-string">&quot;ndcg_at&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>],<br>    # <span class="hljs-string">&quot;max_position&quot;</span>: <span class="hljs-number">5</span>,  # @NDCG 位置优化 <span class="hljs-number">5</span><br>    <span class="hljs-string">&quot;train_metric&quot;</span>: <span class="hljs-literal">True</span>,  # 训练时就输出度量结果 <span class="hljs-literal">True</span><br>    <span class="hljs-string">&quot;tree_learner&quot;</span>: <span class="hljs-string">&quot;serial&quot;</span>,  # 用于并行学习<br>    <span class="hljs-string">&quot;num_threads&quot;</span>: <span class="hljs-number">1</span>,  # 线程数，可以限制模型训练时CPU的占用率！！！<br>    <span class="hljs-string">&quot;verbose&quot;</span>: <span class="hljs-number">-1</span>,  # &lt;<span class="hljs-number">0</span> 显示致命的, =<span class="hljs-number">0</span> 显示错误 (警告), &gt;<span class="hljs-number">0</span> 显示信息<br>    <br>    <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">0.05</span>,  # 学习速率<br>    <span class="hljs-string">&#x27;max_depth&#x27;</span>: <span class="hljs-number">-1</span>,<br>    <span class="hljs-string">&#x27;num_leaves&#x27;</span>: <span class="hljs-number">31</span>,  # 叶子节点数，一般设为少于<span class="hljs-number">2</span>^(max_depth)<br>    <span class="hljs-string">&#x27;max_bin&#x27;</span>: <span class="hljs-number">256</span>,  # 设置连续特征或大量类型的离散特征的bins的数量<br>    <span class="hljs-string">&#x27;feature_fraction&#x27;</span>: <span class="hljs-number">0.8</span>,  # 特征采样<br>    <span class="hljs-string">&#x27;bagging_fraction&#x27;</span>: <span class="hljs-number">0.8</span>,  # 数据采样<br>    <span class="hljs-string">&#x27;bagging_freq&#x27;</span>: <span class="hljs-number">5</span>,  # k 意味着每 k 次迭代执行bagging<br>    <span class="hljs-string">&#x27;lambda_l1&#x27;</span>: <span class="hljs-number">0.1</span>,<br>    <span class="hljs-string">&#x27;lambda_l2&#x27;</span>: <span class="hljs-number">0.1</span>,<br>    <span class="hljs-string">&#x27;min_split_gain&#x27;</span>: <span class="hljs-number">0.8</span>,<br>    <br>    <span class="hljs-string">&#x27;is_unbalance&#x27;</span>: <span class="hljs-string">&#x27;true&#x27;</span>,  #当训练数据是不平衡的，正负样本相差悬殊的时候，可以将这个属性设为true,此时会自动给少的样本赋予更高的权重<br>&#125;<br></code></pre></td></tr></table></figure><ol><li><p>自定义评价函数</p><p>评价函数可以是自定义的，也可以是sklearn中使用的。这里是一个自定义的评价函数写法：</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">feval_spec</span>(<span class="hljs-params">preds, train_data</span>):<br>    <span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve<br>    fpr, tpr, threshold = roc_curve(train_data.get_label(), preds)<br>    tpr0001 = tpr[fpr &lt;= <span class="hljs-number">0.0005</span>].<span class="hljs-built_in">max</span>()<br>    tpr001 = tpr[fpr &lt;= <span class="hljs-number">0.001</span>].<span class="hljs-built_in">max</span>()<br>    tpr005 = tpr[fpr &lt;= <span class="hljs-number">0.005</span>].<span class="hljs-built_in">max</span>()<br>    <span class="hljs-comment">#tpr01 = tpr[fpr.values &lt;= 0.01].max()</span><br>    tprcal = <span class="hljs-number">0.4</span> * tpr0001 + <span class="hljs-number">0.3</span> * tpr001 + <span class="hljs-number">0.3</span> * tpr005<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;spec_cal&#x27;</span>,tprcal,<span class="hljs-literal">True</span><br><br>lgb.train(feval=feval_spec)<br></code></pre></td></tr></table></figure><p>如果是自定义的评价函数，那么需要函数的输入是预测值、输入数据。返回参数有三个，第一个是评价指标名称、第二个是评价值、第三个是True表示成功。</p><h2 id="4-模型训练"><a href="#4-模型训练" class="headerlink" title="4. 模型训练"></a>4. 模型训练</h2><h3 id="4-1基础版"><a href="#4-1基础版" class="headerlink" title="4.1基础版"></a>4.1基础版</h3><p>训练一个模型时, 需要一个 parameter list（参数列表、字典）和 data set（数据集）这里使用上面定义的param参数字典和上面提到的训练数据:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">num_round = 10<br>bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])<br></code></pre></td></tr></table></figure><h3 id="4-2-交叉验证"><a href="#4-2-交叉验证" class="headerlink" title="4.2 交叉验证"></a>4.2 交叉验证</h3><p>时间充足的时候，应该使用交叉验证来选择最好的训练模型，使用 5-折 方式的交叉验证来进行训练（4 个训练集, 1 个测试集）:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">num_round = 10<br>lgb.cv(param, train_data, num_round, nfold=5)<br></code></pre></td></tr></table></figure><h3 id="4-3-提前停止"><a href="#4-3-提前停止" class="headerlink" title="4.3 提前停止"></a>4.3 提前停止</h3><p>如果您有一个验证集, 你可以使用提前停止找到最佳数量的 boosting rounds（梯度次数）. 提前停止需要在 valid_sets 中至少有一个集合. 如果有多个，它们都会被使用:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">bst = lgb.train(param, train_data, num_round, valid_sets=valid_sets, <br>      early_stopping_rounds=10)<br>bst.save_model(<span class="hljs-string">&#x27;model.txt&#x27;</span>, num_iteration=bst.best_iteration)<br></code></pre></td></tr></table></figure><p>该模型将开始训练, 直到验证得分停止提高为止. 验证错误需要至少每个 early_stopping_rounds 减少以继续训练.</p><p>如果提前停止, 模型将有 1 个额外的字段: bst.best_iteration. 请注意 train() 将从最后一次迭代中返回一个模型, 而不是最好的一个.. 请注意, 如果您指定多个评估指标, 则它们都会用于提前停止.</p><p>提前停止可以节约训练的时间。</p><h3 id="4-4-查看特征重要性"><a href="#4-4-查看特征重要性" class="headerlink" title="4.4 查看特征重要性"></a>4.4 查看特征重要性</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># feature names</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Feature names:&#x27;</span>, gbm.feature_name())<br><br><span class="hljs-comment"># feature importances</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Feature importances:&#x27;</span>, list(gbm.feature_importance()))<br></code></pre></td></tr></table></figure><h3 id="4-5-动态调整模型超参数"><a href="#4-5-动态调整模型超参数" class="headerlink" title="4.5 动态调整模型超参数"></a>4.5 动态调整模型超参数</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># decay learning rates</span><br><span class="hljs-comment"># learning_rates accepts:</span><br><span class="hljs-comment"># 1. list/tuple with length = num_boost_round</span><br><span class="hljs-comment"># 2. function(curr_iter)</span><br>gbm = lgb.train(params,<br>                lgb_train,<br>                <span class="hljs-attribute">num_boost_round</span>=10,<br>                <span class="hljs-attribute">init_model</span>=gbm,<br>                <span class="hljs-attribute">learning_rates</span>=lambda iter: 0.05 * (0.99 ** iter),<br>                <span class="hljs-attribute">valid_sets</span>=lgb_eval)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished 20 - 30 rounds with decay learning rates...&#x27;</span>)<br><br><span class="hljs-comment"># change other parameters during training</span><br>gbm = lgb.train(params,<br>                lgb_train,<br>                <span class="hljs-attribute">num_boost_round</span>=10,<br>                <span class="hljs-attribute">init_model</span>=gbm,<br>                <span class="hljs-attribute">valid_sets</span>=lgb_eval,<br>                callbacks=[lgb.reset_parameter(bagging_fraction=[0.7] * 5 + [0.6] * 5)])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished 30 - 40 rounds with changing bagging_fraction...&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="5-模型保存-amp-加载"><a href="#5-模型保存-amp-加载" class="headerlink" title="5. 模型保存&amp;加载"></a>5. 模型保存&amp;加载</h2><p>在训练完成后, 可以使用如下方式来存储模型:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">bst.save_model(<span class="hljs-string">&#x27;model.txt&#x27;</span>)<br><span class="hljs-comment"># with open(model_path, &quot;wb&quot;) as f:</span><br><span class="hljs-comment">#     pkl.dump(bst, f)</span><br><span class="hljs-comment"># with open(&#x27;model.json&#x27;, &#x27;w+&#x27;) as f:</span><br><span class="hljs-comment">#     json.dump(bst, f, indent=4)</span><br></code></pre></td></tr></table></figure><p>模型的重新载入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">bst = lgb.Booster(model_file=<span class="hljs-string">&#x27;model.txt&#x27;</span>)<br><span class="hljs-comment"># with open(model_path, &quot;rb&quot;) as fin:</span><br><span class="hljs-comment">#     bst = pkl.load(fin)</span><br></code></pre></td></tr></table></figure><p>已经训练或加载的模型都可以对数据集进行预测:</p><h2 id="6-预测"><a href="#6-预测" class="headerlink" title="6. 预测"></a>6. 预测</h2><p>7 个样本, 每一个包含 10 个特征</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs kotlin"><span class="hljs-keyword">data</span> = np.random.rand(<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)<br>ypred = bst.predict(<span class="hljs-keyword">data</span>)<br></code></pre></td></tr></table></figure><p>如果在训练过程中启用了提前停止, 可以用 bst.best_iteration 从最佳迭代中获得预测结果:</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs kotlin">ypred = bst.predict(<span class="hljs-keyword">data</span>, num_iteration=bst.best_iteration)<br></code></pre></td></tr></table></figure><h2 id="7-自定义损失函数"><a href="#7-自定义损失函数" class="headerlink" title="7. 自定义损失函数"></a>7. 自定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 类似在xgboost中的形式</span><br><span class="hljs-comment"># 自定义损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loglikelood</span>(<span class="hljs-params">preds, train_data</span>):<br>    labels = train_data.get_label()<br>    preds = <span class="hljs-number">1.</span> / (<span class="hljs-number">1.</span> + np.exp(-preds))<br>    grad = preds - labels<br>    hess = preds * (<span class="hljs-number">1.</span> - preds)<br>    <span class="hljs-keyword">return</span> grad, hess<br><br>lgb.train(fobj=loglikelood)<br></code></pre></td></tr></table></figure><h1 id="Lightgbm调参指导★"><a href="#Lightgbm调参指导★" class="headerlink" title="Lightgbm调参指导★"></a>Lightgbm调参指导★</h1><p>参考《<a href="https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.2.spilt.2.8d95f7184b045e7a.md">AI算法工程师手册—lightgbm使用指南—调参</a>》</p><p>参考《<a href="https://mp.weixin.qq.com/s?__biz=MzI4ODY2NjYzMQ==&amp;mid=2247487494&amp;idx=2&amp;sn=bbe940e4748e6ac5cf6db0340fe0b014&amp;chksm=ec3bb56edb4c3c78718ab413f70e5786116acdf18923d0b17368f919c8ceb7537743ab262afb&amp;scene=27#wechat_redirect">深入理解LightGBM—Lightgbm调参</a>》√</p><ol><li><p>针对 <code>leaf-wise</code> 树的参数优化：</p><ul><li><p><code>num_leaves</code>：控制了叶节点的数目。它是控制树模型复杂度的主要参数。</p><p>如果是<code>level-wise</code>， 则该参数为 <img src="https://static.bookstack.cn/projects/huaxiaozhuan-ai/7fbff4847407186f47048e68c9f3567f.svg" alt="2.1 调参指导 - 图1">，其中 <img src="https://static.bookstack.cn/projects/huaxiaozhuan-ai/bcee7d20347ab929a0ad08ef43059880.svg" alt="2.1 调参指导 - 图2"> 为树的深度。</p><p>但是当叶子数量相同时，<code>leaf-wise</code> 的树要远远深过<code>level-wise</code> 树，非常容易导致过拟合。因此应该让 <code>num_leaves</code> 小于 <img src="https://static.bookstack.cn/projects/huaxiaozhuan-ai/7fbff4847407186f47048e68c9f3567f.svg" alt="2.1 调参指导 - 图3"></p><blockquote><p>在<code>leaf-wise</code> 树中，并不存在<code>depth</code> 的概念。因为不存在一个从<code>leaves</code> 到 <code>depth</code> 的合理映射</p></blockquote></li><li><p><code>min_data_in_leaf</code>： 每个叶节点的最少样本数量。它是处理<code>leaf-wise</code> 树的过拟合的重要参数。</p><p>将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合。</p></li><li><p><code>max_depth</code>： 控制了树的最大深度。</p><p>该参数可以显式的限制树的深度。</p></li></ul></li><li><p>针对更快的训练速度：</p><ul><li>通过设置 <code>bagging_fraction</code> 和 <code>bagging_freq</code> 参数来使用 bagging 方法</li><li>通过设置 <code>feature_fraction</code> 参数来使用特征的子抽样</li><li>使用较小的 <code>max_bin</code></li><li>使用 <code>save_binary</code> 在未来的学习过程对数据加载进行加速</li></ul></li><li><p><strong>获取更好的准确率：</strong></p><ul><li>使用较大的 <code>max_bin</code> （学习速度可能变慢）</li><li>使用较小的 <code>learning_rate</code> 和较大的 <code>num_iterations</code></li><li>使用较大的 <code>num_leaves</code> （可能导致过拟合）</li><li>使用更大的训练数据</li><li>尝试 <code>dart</code></li></ul></li><li><p>缓解过拟合：</p><ul><li>使用较小的 <code>max_bin</code></li><li>使用较小的 <code>num_leaves</code></li><li>使用 <code>min_data_in_leaf</code> 和 <code>min_sum_hessian_in_leaf</code></li><li>通过设置 <code>bagging_fraction</code> 和 <code>bagging_freq</code> 来使用 <code>bagging</code></li><li>通过设置 <code>feature_fraction</code> 来使用特征子抽样</li><li>使用更大的训练数据</li><li>使用 <code>lambda_l1</code>, <code>lambda_l2</code> 和 <code>min_gain_to_split</code> 来使用正则</li><li>尝试 <code>max_depth</code> 来避免生成过深的树</li></ul></li></ol><h2 id="核心参数"><a href="#核心参数" class="headerlink" title="核心参数"></a>核心参数</h2><div class="table-container"><table><thead><tr><th>核心参数</th><th>含义</th><th>设置</th></tr></thead><tbody><tr><td>task</td><td>要执行的任务</td><td>train/predict/convert_model</td></tr><tr><td>application 或者objective 或者 app</td><td>任务类型</td><td>default = <code>regression</code>, options: <code>regression</code>, <code>binary</code>, <code>multiclass</code>, <code>cross_entropy</code>, <code>cross_entropy_lambda</code>, <code>lambdarank</code>, <code>rank_xendcg</code>, …</td></tr><tr><td>boosting或者boost或者boosting_type</td><td>基学习器模型算法</td><td>gbdt/rf/dart/goss</td></tr><tr><td>num_iteration或者num_tree或者 num_round或者 num_boost_round</td><td>迭代次数</td><td>默认100</td></tr><tr><td>learning_rate</td><td>学习率</td><td>默认为 0.1</td></tr><tr><td>num_leaves或者num_leaf</td><td>一棵树上的叶子数</td><td>默认为 31</td></tr></tbody></table></div><h2 id="学习控制参数"><a href="#学习控制参数" class="headerlink" title="学习控制参数"></a>学习控制参数</h2><div class="table-container"><table><thead><tr><th>学习控制参数</th><th>含义</th><th>设置</th></tr></thead><tbody><tr><td>max_depth</td><td>树模型的最大深度</td><td>默认值为-1</td></tr><tr><td>min_data_in_leaf</td><td>一个叶子节点上包含的最少样本数量。</td><td>默认值为 20。将其设置的较大可以避免生成一个过深的树, 但有可能导致欠拟合</td></tr><tr><td>feature_fraction</td><td>如0.8 表示：在每棵树训练之前选择80% 的特征来训练</td><td>取值范围为[0.0,1.0]， 默认值为1.0。降低过拟合</td></tr><tr><td>bagging_fraction 或者 subsample</td><td>如0.8 表示：在每棵树训练之前选择80% 的样本（非重复采样）来训练</td><td>取值范围为[0.0,1.0]， 默认值为1.0。降低过拟合</td></tr><tr><td>early_stopping_round或者early_stopping</td><td>如果一个验证集的度量在early_stopping_round 循环中没有提升，则停止训练</td><td>-</td></tr><tr><td>lambda_l1 或者reg_alpha</td><td>表示L1正则化系数。</td><td>默认为0。降低过拟合</td></tr><tr><td>lambda_l2 或者reg_lambda</td><td>表示L2正则化系数。</td><td>默认为0。降低过拟合</td></tr><tr><td>min_split_gain 或者min_gain_to_split</td><td>一个浮点数，表示执行切分的最小增益</td><td>默认为0</td></tr><tr><td>min_data_per_group</td><td>表示每个分类组的最小数据量 用于排序任务</td><td>默认值为100</td></tr><tr><td>cat_smooth</td><td>用于category 特征的概率平滑，降低噪声在category 特征中的影响，尤其是对于数据很少的类。</td><td>默认值为 10</td></tr></tbody></table></div><h2 id="度量参数"><a href="#度量参数" class="headerlink" title="度量参数"></a>度量参数</h2><div class="table-container"><table><thead><tr><th>度量参数</th><th>含义</th><th>设置</th></tr></thead><tbody><tr><td>metric</td><td>度量的指标</td><td>对于回归问题，使用l2 ； 对于二分类问题，使用binary_logloss；对于lambdarank 问题，使用ndcg</td></tr><tr><td>metric_freq或者’output_freq</td><td>一个正式，表示每隔多少次输出一次度量结果</td><td>默认为1</td></tr><tr><td>train_metric 或者training_metric</td><td>如果为True，则在训练时就输出度量结果</td><td>默认值为 False</td></tr><tr><td>ndcg_at 或者 ndcg_eval_at 或者eval_at</td><td>指定了NDCG 评估点的位置。</td><td>默认为1,2,3,4,5</td></tr></tbody></table></div><h2 id="并行学习"><a href="#并行学习" class="headerlink" title="并行学习"></a>并行学习</h2><ol><li><p><code>lightgbm</code> 已经提供了以下并行学习算法：</p><p>| 并行算法 | 开启方式               |<br>| :———- | :——————————- |<br>| 数据并行 | tree_learner=’data’    |<br>| 特征并行 | tree_learner=’feature’ |<br>| 投票并行 | tree_learner=’voting’  |</p><blockquote><p><code>tree_learner</code> 默认为 <code>&#39;serial&#39;</code>。 表示串行学习。</p></blockquote></li></ol><h2 id="调参示例"><a href="#调参示例" class="headerlink" title="调参示例"></a>调参示例</h2><ol><li><p><strong>第一步：学习率和迭代次数</strong>我们先把学习率先定一个较高的值，这里取 learning_rate = 0.1，其次确定估计器boosting/boost/boosting_type的类型，不过默认都会选gbdt。</p><p>迭代的次数，也可以说是残差树的数目，参数名为n_estimators/num_iterations/num_round/num_boost_round。我们可以先将该参数设成一个较大的数</p></li><li><p><strong>第二步：确定max_depth和num_leaves</strong>这是提高精确度的最重要的参数。这里我们引入sklearn里的GridSearchCV()函数进行搜索</p></li><li><strong>第三步：确定min_data_in_leaf和max_bin</strong></li><li><strong>第四步：确定feature_fraction、bagging_fraction、bagging_freq</strong></li><li><strong>第五步：确定lambda_l1和lambda_l2</strong></li><li><strong>第六步：确定 min_split_gain</strong></li><li><strong>第七步：降低学习率，增加迭代次数，验证模型</strong></li></ol><h1 id="Lightgbm-API接口方法"><a href="#Lightgbm-API接口方法" class="headerlink" title="Lightgbm API接口方法"></a>Lightgbm API接口方法</h1><h2 id="数据接口-Dataset"><a href="#数据接口-Dataset" class="headerlink" title="数据接口 Dataset"></a>数据接口 Dataset</h2><ol><li><p><code>Dataset</code>： 由<code>lightgbm</code> 内部使用的数据结构，它存储了数据集。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">class lightgbm.Dataset(data, <span class="hljs-attribute">label</span>=None, <span class="hljs-attribute">max_bin</span>=None, <span class="hljs-attribute">reference</span>=None, <span class="hljs-attribute">weight</span>=None,    <span class="hljs-attribute">group</span>=None, <span class="hljs-attribute">init_score</span>=None, <span class="hljs-attribute">silent</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">feature_name</span>=<span class="hljs-string">&#x27;auto&#x27;</span>,    <span class="hljs-attribute">categorical_feature</span>=<span class="hljs-string">&#x27;auto&#x27;</span>, <span class="hljs-attribute">params</span>=None, <span class="hljs-attribute">free_raw_data</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><ul><li><p>参数：</p><ul><li><p><code>data</code>： 一个字符串、<code>numpy array</code> 或者 <code>scipy.parse</code>， 它指定了数据源。</p><p>如果是字符串，则表示数据源文件的文件名。</p></li><li><p><code>label</code>： 一个列表、1维的<code>numpy array</code> 或者<code>None</code>， 它指定了样本标记。默认为<code>None</code>。</p></li><li><p><code>max_bin</code>： 一个整数或者<code>None</code>， 指定每个特征的最大分桶数量。默认为<code>None</code>。</p><p>如果为<code>None</code>，则从配置文件中读取。</p></li><li><p><code>reference</code>： 一个<code>Dataset</code> 或者 <code>None</code>。 默认为<code>None</code>。</p><p>如果当前构建的数据集用于验证集，则<code>reference</code> 必须传入训练集。否则会报告<code>has different bin mappers</code>。</p></li><li><p><code>weight</code>： 一个列表、1维的<code>numpy array</code> 或者<code>None</code>， 它指定了样本的权重。默认为<code>None</code>。</p></li><li><p><code>group</code>： 一个列表、1维的<code>numpy array</code> 或者<code>None</code>， 它指定了数据集的<code>group/query size</code>。默认为<code>None</code>。</p></li><li><p><code>init_score</code>： 一个列表、1维的<code>numpy array</code> 或者<code>None</code>， 它指定了<code>Booster</code>的初始<code>score</code> 。默认为<code>None</code>。</p></li><li><p><code>silent</code>： 一个布尔值，指示是否在构建过程中输出信息。默认为<code>False</code></p></li><li><p><code>feature_name</code>： 一个字符串列表或者<code>&#39;auto&#39;</code>，它指定了特征的名字。默认为<code>&#39;auto&#39;</code></p><ul><li>如果数据源为<code>pandas DataFrame</code> 并且<code>feature_name=&#39;auto&#39;</code>，则使用<code>DataFrame</code> 的 <code>column names</code></li></ul></li><li><p><code>categorical_feature</code>： 一个字符串列表、整数列表、或者<code>&#39;auto&#39;</code>。它指定了<code>categorical</code> 特征。默认为<code>&#39;auto&#39;</code></p><ul><li>如果是整数列表，则给定了<code>categorical</code> 特征的下标</li><li>如果是字符串列表，在给定了<code>categorical</code> 特征的名字。此时必须设定<code>feature_name</code> 参数。</li><li>如果是<code>&#39;auto&#39;</code> 并且数据源为<code>pandas DataFrame</code>，则<code>DataFrame</code> 的 <code>categorical</code> 列将作为<code>categorical</code> 特征</li></ul></li><li><p><code>params</code>： 一个字典或者<code>None</code>，指定了其它的参数。默认为<code>None</code></p></li><li><p><code>free_raw_data</code>： 一个布尔值，指定是否在创建完<code>Dataset</code> 之后释放原始的数据。默认为<code>True</code></p><p>调用<code>Dataset()</code> 之后，并没有构建完<code>Dataset</code>。 构建完需要等到构造一个<code>Booster</code> 的时候。</p></li></ul></li></ul></li><li><p>方法：</p><ul><li><p><code>.get_group()</code>： 获取当前<code>Dataset</code> 的<code>group</code></p><blockquote><p><code>get_xxx()</code> 等方法，都是调用的 <code>get_field()</code> 方法来实现的</p></blockquote><ul><li>返回值：一个<code>numpy array</code>，表示每个分组的<code>size</code> 。</li></ul></li><li><p><code>.set_group(group)</code>： 设置当前<code>Dataset</code> 的<code>group</code></p><ul><li>参数：<code>group</code>： 一个列表、<code>numpy array</code> 或者<code>None</code>，表示每个分组的<code>size</code> 。</li></ul></li><li><p><code>.num_data()</code>： 返回<code>Dataset</code> 中的样本数量</p></li><li><code>.num_feature()</code>： 返回<code>Dataset</code> 中的特征数量</li></ul></li><li><p>示例：</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DatasetTest</span>:<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    self._matrix1 = lgb.Dataset(<span class="hljs-string">&#x27;data/train.svm.txt&#x27;</span>)<br>    self._matrix2 = lgb.Dataset(data=np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">12</span>).reshape((<span class="hljs-number">4</span>, <span class="hljs-number">3</span>)), <br>                                label=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>], weight=[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.2</span>],<br>                                silent=<span class="hljs-literal">False</span>, feature_name=[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>])<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">print</span>(<span class="hljs-params">self,matrix</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Matrix 构建尚未完成时的属性</span><br><span class="hljs-string">    :param matrix:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;data: %s&#x27;</span> % matrix.data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;label: %s&#x27;</span> % matrix.label)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;weight: %s&#x27;</span> % matrix.weight)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;init_score: %s&#x27;</span> % matrix.init_score)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;group: %s&#x27;</span> % matrix.group)<br>​<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_method</span>(<span class="hljs-params">self,matrix</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    测试一些 方法</span><br><span class="hljs-string">    :param matrix:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;get_ref_chain():&#x27;</span>, matrix.get_ref_chain(ref_limit=<span class="hljs-number">10</span>))<br>    <span class="hljs-comment"># get_ref_chain(): &#123;&lt;lightgbm.basic.Dataset object at 0x7f29cd762f28&gt;&#125;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;subset():&#x27;</span>, matrix.subset(used_indices=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]))<br>    <span class="hljs-comment"># subset(): &lt;lightgbm.basic.Dataset object at 0x7f29a4aeb518&gt;</span><br>  <br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">self</span>):<br>    self.<span class="hljs-built_in">print</span>(self._matrix1)<br>    <span class="hljs-comment"># data: data/train.svm.txt</span><br>    <span class="hljs-comment"># label: None</span><br>    <span class="hljs-comment"># weight: None</span><br>    <span class="hljs-comment"># init_score: None</span><br>    <span class="hljs-comment"># group: None</span><br>    self.<span class="hljs-built_in">print</span>(self._matrix2)<br>    <span class="hljs-comment"># data: [[ 0  1  2]</span><br>    <span class="hljs-comment">#  [ 3  4  5]</span><br>    <span class="hljs-comment">#  [ 6  7  8]</span><br>    <span class="hljs-comment">#  [ 9 10 11]]</span><br>    <span class="hljs-comment"># label: [1, 2, 3, 4]</span><br>    <span class="hljs-comment"># weight: [0.5, 0.4, 0.3, 0.2]</span><br>    <span class="hljs-comment"># init_score: No</span><br>    self.run_method(self._matrix2)<br></code></pre></td></tr></table></figure><h2 id="模型接口-lightgbm-train"><a href="#模型接口-lightgbm-train" class="headerlink" title="模型接口 lightgbm.train"></a>模型接口 lightgbm.train</h2><ol><li><p><code>lightgbm.train()</code> 函数执行直接训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">lightgbm.train(params, train_set, num_boost_round=<span class="hljs-number">100</span>, valid_sets=<span class="hljs-literal">None</span>,<br>  valid_names=<span class="hljs-literal">None</span>, fobj=<span class="hljs-literal">None</span>, feval=<span class="hljs-literal">None</span>, init_model=<span class="hljs-literal">None</span>, feature_name=<span class="hljs-string">&#x27;auto&#x27;</span>,<br>  categorical_feature=<span class="hljs-string">&#x27;auto&#x27;</span>, early_stopping_rounds=<span class="hljs-literal">None</span>, evals_result=<span class="hljs-literal">None</span>, <br>  verbose_eval=<span class="hljs-literal">True</span>, learning_rates=<span class="hljs-literal">None</span>, keep_training_booster=<span class="hljs-literal">False</span>, callbacks=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li><p><code>params</code>： 一个字典，给出了训练参数</p></li><li><p><code>train_set</code>： 一个<code>Dataset</code>对象，给出了训练集</p></li><li><p><code>num_boost_round</code>： 一个整数，给出了<code>boosting iteration</code> 的次数。默认为<code>100</code></p></li><li><p><code>valid_sets</code>：一个<code>Dataset</code> 的列表或者<code>None</code>，给出了训练期间用于<code>evaluate</code>的数据集。默认为<code>None</code></p></li><li><p><code>valid_names</code>：一个字符串列表或者<code>None</code>， 给出了<code>valid_sets</code> 中每个数据集的名字。默认为<code>None</code></p></li><li><p><code>fobj</code>：一个可调用对象或者<code>None</code>，表示自定义的目标函数。默认为<code>None</code></p></li><li><p><code>feval</code>：一个可调用对象或者<code>None</code>， 它表示自定义的<code>evaluation</code> 函数。默认为<code>None</code>。它的输入为<code>(y_true, y_pred)</code>、或者<code>( y_true, y_pred, weight)</code> 、或者<code>(y_true, y_pred, weight, group)</code>， 返回一个元组：<code>(eval_name,eval_result,is_higher_better)</code> 。或者返回该元组的列表。</p></li><li><p><code>init_model</code>：一个字符串或者<code>None</code>，它给出了<code>lightgbm model</code> 保存的文件名，或者<code>Booster</code>实例的名字。后续的训练在该<code>model</code> 或者<code>Booster</code> 实例的基础上继续训练。默认为<code>None</code></p></li><li><p><code>feature_name</code>： 一个字符串列表或者<code>&#39;auto&#39;</code>，它指定了特征的名字。默认为<code>&#39;auto&#39;</code></p><ul><li>如果数据源为<code>pandas DataFrame</code> 并且<code>feature_name=&#39;auto&#39;</code>，则使用<code>DataFrame</code> 的 <code>column names</code></li></ul></li><li><p><code>categorical_feature</code>：一个字符串列表、整数列表、或者<code>&#39;auto&#39;</code>。它指定了<code>categorical</code> 特征。默认为<code>&#39;auto&#39;</code></p><ul><li>如果是整数列表，则给定了<code>categorical</code> 特征的下标</li><li>如果是字符串列表，在给定了<code>categorical</code> 特征的名字。此时必须设定<code>feature_name</code> 参数。</li><li>如果是<code>&#39;auto&#39;</code> 并且数据源为<code>pandas DataFrame</code>，则<code>DataFrame</code> 的 <code>categorical</code> 列将作为<code>categorical</code> 特征</li></ul></li><li><p><code>early_stopping_rounds</code>：一个整数或者<code>None</code>，表示验证集的<code>score</code> 在连续多少轮未改善之后就早停。默认为<code>None</code></p><p>该参数要求至少有一个验证集以及一个<code>metric</code>。</p><p>如果由多个验证集或者多个<code>metric</code>，则对所有的验证集和所有的<code>metric</code> 执行。</p><p>如果发生了早停，则模型会添加一个<code>best_iteration</code>字段。该字段持有了最佳的迭代步。</p></li><li><p><code>evals_result</code>：一个字典或者<code>None</code>，这个字典用于存储在<code>valid_sets</code> 中指定的所有验证集的所有验证结果。默认为<code>None</code></p></li><li><p><code>verbose_eval</code>：一个布尔值或者整数。默认为<code>True</code></p><ul><li>如果是<code>True</code>，则在验证集上每个<code>boosting stage</code> 打印对验证集评估的<code>metric</code>。</li><li>如果是整数，则每隔<code>verbose_eval</code> 个 <code>boosting stage</code> 打印对验证集评估的<code>metric</code>。</li><li>否则，不打印这些</li></ul><p>该参数要求至少由一个验证集。</p></li><li><p><code>learning_rates</code>：一个列表、<code>None</code>、 可调用对象。它指定了学习率。默认为<code>None</code></p><ul><li>如果为列表，则它给出了每一个<code>boosting</code> 步的学习率</li><li>如果为一个可调用对象，则在每个<code>boosting</code> 步都调用它，从而生成一个学习率</li><li>如果为一个数值，则学习率在学习期间都固定为它。</li></ul><p>你可以使用学习率衰减从而生成一个更好的学习率序列。</p></li><li><p><code>keep_training_booster</code>：一个布尔值，指示训练得到的<code>Booster</code>对象是否还会继续训练。默认为<code>False</code></p><ul><li><p>如果为<code>False</code>，则返回的<code>booster</code> 对象在返回之前将被转换为<code>_InnerPredictor</code> 。</p><p>当然你也可以将<code>_InnerPredictor</code> 传递给<code>init_model</code> 参数从而继续训练。</p></li></ul></li><li><p><code>callbacks</code>：一个可调用对象的列表，或者<code>None</code>。 它给出了在每个迭代步之后需要执行的函数。默认为<code>None</code></p></li></ul><p>返回：一个<code>Booster</code> 实例</p></li><li><p><code>lightgbm.cv()</code> 函数执行交叉验证训练。</p></li></ol><h2 id="绘图接口"><a href="#绘图接口" class="headerlink" title="绘图接口"></a>绘图接口</h2><p><a href="https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.3.spilt.4.8d95f7184b045e7a.md">https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.3.spilt.4.8d95f7184b045e7a.md</a></p><ol><li><code>lightgbm.plot_importance()</code>： 绘制特征的重要性。</li><li><code>lightgbm.plot_metric()</code>： 在训练过程中绘制一个<code>metric</code></li><li><code>lightgbm.plot_tree()</code>：绘制指定的树模型。</li><li><code>lightgbm.create_tree_digraph()</code>： 绘制指定的树模型，但是返回一个<code>digraph</code>，而不是直接绘制。</li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bookstack.cn/read/huaxiaozhuan-ai/8d95f7184b045e7a.md">AI算法工程师手册 - lightgbm使用指南</a></p><p>【<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FNLP-LOVE%2FML-NLP">机器学习通俗易懂系列文章</a>】</p><p><a href="https://www.jianshu.com/p/ba9ab1adbfe1">https://www.jianshu.com/p/ba9ab1adbfe1</a></p><p><a href="https://www.jianshu.com/p/d07f0b0726da">关于lightgbm处理category特征的理解</a></p><h1 id="附录：自动超参数优化"><a href="#附录：自动超参数优化" class="headerlink" title="附录：自动超参数优化"></a>附录：自动超参数优化</h1><p><strong>网格搜索</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs routeros">lg = lgb.LGBMClassifier(<span class="hljs-attribute">silent</span>=<span class="hljs-literal">False</span>)<br>param_dist = &#123;<span class="hljs-string">&quot;max_depth&quot;</span>: [4,5, 7],<br>              <span class="hljs-string">&quot;learning_rate&quot;</span> : [0.01,0.05,0.1],<br>              <span class="hljs-string">&quot;num_leaves&quot;</span>: [300,900,1200],<br>              <span class="hljs-string">&quot;n_estimators&quot;</span>: [50, 100, 150]<br>             &#125;<br><br>grid_search = GridSearchCV(lg, <span class="hljs-attribute">n_jobs</span>=-1, <span class="hljs-attribute">param_grid</span>=param_dist, cv = 5, <span class="hljs-attribute">scoring</span>=<span class="hljs-string">&quot;roc_auc&quot;</span>, <span class="hljs-attribute">verbose</span>=5)<br>grid_search.fit(train,y_train)<br>grid_search.best_estimator_, grid_search.best_score_<br></code></pre></td></tr></table></figure><p><strong>贝叶斯优化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br><span class="hljs-keyword">import</span> time<br>warnings.filterwarnings(<span class="hljs-string">&quot;ignore&quot;</span>)<br><span class="hljs-keyword">from</span> bayes_opt <span class="hljs-keyword">import</span> BayesianOptimization<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lgb_eval</span>(<span class="hljs-params">max_depth, learning_rate, num_leaves, n_estimators</span>):<br>    params = &#123;<br>             <span class="hljs-string">&quot;metric&quot;</span> : <span class="hljs-string">&#x27;auc&#x27;</span><br>        &#125;<br>    params[<span class="hljs-string">&#x27;max_depth&#x27;</span>] = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(max_depth, <span class="hljs-number">1</span>))<br>    params[<span class="hljs-string">&#x27;learning_rate&#x27;</span>] = np.clip(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, learning_rate)<br>    params[<span class="hljs-string">&#x27;num_leaves&#x27;</span>] = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(num_leaves, <span class="hljs-number">1</span>))<br>    params[<span class="hljs-string">&#x27;n_estimators&#x27;</span>] = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(n_estimators, <span class="hljs-number">1</span>))<br>    cv_result = lgb.cv(params, d_train, nfold=<span class="hljs-number">5</span>, seed=<span class="hljs-number">0</span>, verbose_eval =<span class="hljs-number">200</span>,stratified=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> * np.array(cv_result[<span class="hljs-string">&#x27;auc-mean&#x27;</span>]).<span class="hljs-built_in">max</span>()<br><br>lgbBO = BayesianOptimization(lgb_eval, &#123;<span class="hljs-string">&#x27;max_depth&#x27;</span>: (<span class="hljs-number">4</span>, <span class="hljs-number">8</span>),<br>                                            <span class="hljs-string">&#x27;learning_rate&#x27;</span>: (<span class="hljs-number">0.05</span>, <span class="hljs-number">0.2</span>),<br>                                            <span class="hljs-string">&#x27;num_leaves&#x27;</span> : (<span class="hljs-number">20</span>,<span class="hljs-number">1500</span>),<br>                                            <span class="hljs-string">&#x27;n_estimators&#x27;</span>: (<span class="hljs-number">5</span>, <span class="hljs-number">200</span>)&#125;, random_state=<span class="hljs-number">0</span>)<br><br>lgbBO.maximize(init_points=<span class="hljs-number">5</span>, n_iter=<span class="hljs-number">50</span>,acq=<span class="hljs-string">&#x27;ei&#x27;</span>)<br><span class="hljs-built_in">print</span>(lgbBO.<span class="hljs-built_in">max</span>)<br></code></pre></td></tr></table></figure><p><strong>optuna超参数优化，参考<a href="https://zhuanlan.zhihu.com/p/138521995">《GIVE OPTUNA A SHOT!》</a></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> optuna<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">objective</span>(<span class="hljs-params">trial</span>):<br>    x = trial.suggest_uniform(<span class="hljs-string">&#x27;x&#x27;</span>, -<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>    <span class="hljs-keyword">return</span> (x - <span class="hljs-number">2</span>) ** <span class="hljs-number">2</span><br><br>study = optuna.create_study()<br>study.optimize(objective, n_trials=<span class="hljs-number">100</span>)<br><br>study.best_params  <span class="hljs-comment"># E.g. &#123;&#x27;x&#x27;: 2.002108042&#125;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>sklearn分类评价指标介绍</title>
    <link href="/2020/08/24/2020-08-24-sklearn%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
    <url>/2020/08/24/2020-08-24-sklearn%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p>在机器学习中，性能指标（Metrics）是衡量一个模型好坏的关键，通过衡量模型输出y_predict和y_true之间的某种“距离”得出的。</p><p>下面是常用的分类评估指标的Sklearn方法：</p><ul><li><p>准确率</p><p><strong>准确率</strong>是指我们的模型预测正确的结果所占的比例。</p><p>正式点说，准确率的定义如下：</p><p>$Accuracy=\frac{Number of correct predictions}{Total number of predictions}$</p></li><li><p>精确率</p><p><strong>精确率</strong>指标尝试回答以下问题：</p><blockquote><p>在被识别为正类别的样本中，确实为正类别的比例是多少？</p></blockquote><p>精确率的定义如下：</p><p>$Precision=\frac{TP}{TP+FP}$</p></li><li><p>召回率</p><p><strong>召回率</strong>尝试回答以下问题：</p><blockquote><p>在所有正类别样本中，被正确识别为正类别的比例是多少？</p></blockquote><p>从数学上讲，召回率的定义如下：</p><p>$recall=\frac{TP}{TP+FN}$</p></li><li><p>F1 Score</p></li><li><p>混淆矩阵</p></li><li><p>分类报告</p></li><li><p>ROC</p><p>ROC 曲线用于绘制采用不同分类阈值时的 TPR 与 FPR。</p><p>降低分类阈值会导致将更多样本归为正类别，从而增加假正例和真正例的个数。</p></li></ul> <span id="more"></span><h2 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h2><p>分类正确的样本个数占总样本的比例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br>y_pred = [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>,<span class="hljs-number">9</span>,<span class="hljs-number">9</span>,<span class="hljs-number">8</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]<br>y_true = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">6</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]<br> <br>accuracy_score(y_true, y_pred)<br>Out[<span class="hljs-number">127</span>]: <span class="hljs-number">0.33333333333333331</span><br></code></pre></td></tr></table></figure><h2 id="精确率"><a href="#精确率" class="headerlink" title="精确率"></a>精确率</h2><p>所有分正确的正样本/所有预测为正类的样本数。</p><ul><li>宏平均（Macro-averaging），是先对每一个类统计指标值，然后在对所有类求算术平均值。</li><li>微平均（Micro-averaging），是对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标。</li></ul><p>（来源：<a href="http://www.cnblogs.com/robert-dlut/p/5276927.html">谈谈评价指标中的宏平均和微平均</a>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><br>metrics.precision_score(y_true, y_pred, average=<span class="hljs-string">&#x27;micro&#x27;</span>)  <span class="hljs-comment"># 微平均，精确率</span><br>Out[<span class="hljs-number">130</span>]: <span class="hljs-number">0.33333333333333331</span><br>    <br>metrics.precision_score(y_true, y_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>)  <span class="hljs-comment"># 宏平均，精确率</span><br>Out[<span class="hljs-number">131</span>]: <span class="hljs-number">0.375</span><br> <br>metrics.precision_score(y_true, y_pred, labels=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], average=<span class="hljs-string">&#x27;macro&#x27;</span>)  <span class="hljs-comment"># 指定特定分类标签的精确率</span><br>Out[<span class="hljs-number">133</span>]: <span class="hljs-number">0.5</span><br></code></pre></td></tr></table></figure><h2 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h2><p>所有分正确的正样本/所有的正样本数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">metrics.recall_score(y_true, y_pred, average=<span class="hljs-string">&#x27;micro&#x27;</span>)<br>Out[<span class="hljs-number">134</span>]: <span class="hljs-number">0.33333333333333331</span><br> <br>metrics.recall_score(y_true, y_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>)<br>Out[<span class="hljs-number">135</span>]: <span class="hljs-number">0.3125</span><br></code></pre></td></tr></table></figure><h2 id="F值"><a href="#F值" class="headerlink" title="F值"></a>F值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">metrics.f1_score(y_true, y_pred, average=<span class="hljs-string">&#x27;weighted&#x27;</span>) <br>Out[<span class="hljs-number">136</span>]: <span class="hljs-number">0.37037037037037035</span><br></code></pre></td></tr></table></figure><p><strong>关键参数: average</strong></p><p>①None：返回每一类各自的<code>f1_score，得到一个array。</code></p><p>②’binary’ 只对二分类问题有效，返回由<code>pos_label指定的类的f1_score。</code></p><p><strong>③’micro’ : 设置average=’micro’时，Precision = Recall = F1_score = Accuracy。</strong></p><p><strong>④’macro’: 对每一类别的f1_score进行简单算术平均（unweighted mean）</strong></p><p><strong>⑤’weighted’: 对每一类别的f1_score进行加权平均，权重为各类别数在y_true中所占比例。</strong></p><p>⑥’samples’</p><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>混淆矩阵通过计算各种分类度量，指导模型的评估</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix<br><br>confusion_matrix(y_true, y_pred)<br> <br>Out[<span class="hljs-number">137</span>]:<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>       ...,<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])<br> <br></code></pre></td></tr></table></figure><h2 id="分类报告"><a href="#分类报告" class="headerlink" title="分类报告"></a>分类报告</h2><p>sklearn中的classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息。 主要参数: </p><ul><li>y_true：1维数组，或标签指示器数组/稀疏矩阵，目标值。 </li><li>y_pred：1维数组，或标签指示器数组/稀疏矩阵，分类器返回的估计值。 </li><li>labels：array，shape = [n_labels]，报表中包含的标签索引的可选列表。 </li><li>target_names：字符串列表，与标签匹配的可选显示名称（相同顺序）。 </li><li>sample_weight：类似于shape = [n_samples]的数组，可选项，样本权重。 </li><li>digits：int，输出浮点值的位数．</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report<br><br>y_true = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>]<br>y_pred = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>]<br>target_names = [<span class="hljs-string">&#x27;class 0&#x27;</span>, <span class="hljs-string">&#x27;class 1&#x27;</span>, <span class="hljs-string">&#x27;class 2&#x27;</span>]<br><br><span class="hljs-built_in">print</span>(classification_report(y_true, y_pred, target_names=target_names))<br></code></pre></td></tr></table></figure><p>在版本<strong>0.20.3</strong>的结果：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache">              <span class="hljs-attribute">precision</span>    recall  f1-score   support<br><br>     <span class="hljs-attribute">class</span> <span class="hljs-number">0</span>       <span class="hljs-number">0</span>.<span class="hljs-number">67</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">80</span>         <span class="hljs-number">2</span><br>     <span class="hljs-attribute">class</span> <span class="hljs-number">1</span>       <span class="hljs-number">0</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">00</span>         <span class="hljs-number">1</span><br>     <span class="hljs-attribute">class</span> <span class="hljs-number">2</span>       <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>         <span class="hljs-number">2</span><br><br>   <span class="hljs-attribute">micro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">80</span>      <span class="hljs-number">0</span>.<span class="hljs-number">80</span>      <span class="hljs-number">0</span>.<span class="hljs-number">80</span>         <span class="hljs-number">5</span><br>   <span class="hljs-attribute">macro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">56</span>      <span class="hljs-number">0</span>.<span class="hljs-number">67</span>      <span class="hljs-number">0</span>.<span class="hljs-number">60</span>         <span class="hljs-number">5</span><br><span class="hljs-attribute">weighted</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">67</span>      <span class="hljs-number">0</span>.<span class="hljs-number">80</span>      <span class="hljs-number">0</span>.<span class="hljs-number">72</span>         <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><p>在版本<strong>0.21.x</strong>以上的结果：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache">              <span class="hljs-attribute">precision</span>    recall  f1-score   support<br><br>     <span class="hljs-attribute">class</span> <span class="hljs-number">0</span>       <span class="hljs-number">0</span>.<span class="hljs-number">67</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">80</span>         <span class="hljs-number">2</span><br>     <span class="hljs-attribute">class</span> <span class="hljs-number">1</span>       <span class="hljs-number">0</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">00</span>         <span class="hljs-number">1</span><br>     <span class="hljs-attribute">class</span> <span class="hljs-number">2</span>       <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>         <span class="hljs-number">2</span><br><br>    <span class="hljs-attribute">accuracy</span>                           <span class="hljs-number">0</span>.<span class="hljs-number">80</span>         <span class="hljs-number">5</span><br>   <span class="hljs-attribute">macro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">56</span>      <span class="hljs-number">0</span>.<span class="hljs-number">67</span>      <span class="hljs-number">0</span>.<span class="hljs-number">60</span>         <span class="hljs-number">5</span><br><span class="hljs-attribute">weighted</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">67</span>      <span class="hljs-number">0</span>.<span class="hljs-number">80</span>      <span class="hljs-number">0</span>.<span class="hljs-number">72</span>         <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><p><strong>③’micro’ : 设置average=’micro’时，Precision = Recall = F1_score = Accuracy</strong></p><p><strong>④’macro’: 对每一类别的f1_score进行简单算术平均 (0.8+0+1)/3=0.6 </strong></p><p><strong>⑤’weighted’: 对每一类别的f1_score进行加权平均，权重为各类别数在y_true中所占比例  (0.8*0.4+0*0.2+1*0.4)/3=0.6 </strong></p><h2 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h2><p>1，计算ROC值，即Auc</p><p>Auc是ROC(Receiver Operating Characteristic)曲线下的面积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_auc_score<br><br>y_true = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>y_scores = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.35</span>, <span class="hljs-number">0.8</span>])<br>roc_auc_score(y_true, y_scores)<br></code></pre></td></tr></table></figure><p>2，ROC曲线</p><p>描述分类器的True Positive Rate与False Positive Rate之间的变化关系</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve<br><br>y = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br>scores = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.35</span>, <span class="hljs-number">0.8</span>])<br>fpr, tpr, thresholds = roc_curve(y, scores, pos_label=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://blog.csdn.net/sinat_26917383/article/details/75199996">python + sklearn ︱分类效果评估——acc、recall、F1、ROC、回归、距离</a></p><p><a href="https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall?hl=zh-cn">https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall?hl=zh-cn</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>评价指标</tag>
      
      <tag>精确率和召回率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Word embeddings in 2020</title>
    <link href="/2020/08/10/2020-08-10-Word%20embeddings%20in%202020/"/>
    <url>/2020/08/10/2020-08-10-Word%20embeddings%20in%202020/</url>
    
    <content type="html"><![CDATA[<h1 id="Word-embeddings-in-2020"><a href="#Word-embeddings-in-2020" class="headerlink" title="Word embeddings in 2020"></a>Word embeddings in 2020</h1><blockquote><p>转载自 <a href="https://colab.research.google.com/drive/1N7HELWImK9xCYheyozVP3C_McbiRo1nb">https://colab.research.google.com/drive/1N7HELWImK9xCYheyozVP3C_McbiRo1nb</a></p></blockquote><p><img src="https://ningshixian.github.io/resources/images/word-embeddings.png" alt=""></p><p>本文对每个词嵌入方法都有一个（非常）简短的描述，进一步研究的链接以及Python中的代码示例。所有代码都打包为<a href="https://colab.research.google.com/drive/1N7HELWImK9xCYheyozVP3C_McbiRo1nb">Google Colab Notebook</a>。</p><p>根据Wikipedia的说法，<strong>单词嵌入</strong>是自然语言处理（NLP）中一组语言建模和功能学习技术的总称，其中词汇表中的单词或短语被映射为实数向量。</p> <span id="more"></span><h1 id="One-hot-or-CountVectorizing"><a href="#One-hot-or-CountVectorizing" class="headerlink" title="One-hot or CountVectorizing"></a>One-hot or CountVectorizing</h1><p>将单词转换为向量的最基本方法是计算每个文档中每个单词的出现次数。这种方法称为计数向量化或独热编码。</p><p>此方法的主要原理是收集一组文档（它们可以是单词，句子，段落甚至文章），并计算每个文档中每个单词的出现次数。严格来说，结果矩阵的列是单词，行是文档。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-keyword">from</span> sklearn.feature_extraction.<span class="hljs-built_in">text</span> import CountVectorizer<br><span class="hljs-comment"># create CountVectorizer object</span><br>vectorizer = CountVectorizer()<br>corpus = [<br>          &#x27;Text <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> very <span class="hljs-keyword">first</span> new sentence <span class="hljs-keyword">with</span> <span class="hljs-keyword">the</span> <span class="hljs-keyword">first</span> <span class="hljs-built_in">words</span> <span class="hljs-keyword">in</span> sentence.&#x27;,<br>          &#x27;Text <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> <span class="hljs-keyword">second</span> sentence.&#x27;,<br>          &#x27;Number three <span class="hljs-keyword">with</span> lot <span class="hljs-keyword">of</span> <span class="hljs-built_in">words</span> <span class="hljs-built_in">words</span> <span class="hljs-built_in">words</span>.&#x27;,<br>          &#x27;Short <span class="hljs-built_in">text</span>, less <span class="hljs-built_in">words</span>.&#x27;,<br>]<br><br><span class="hljs-comment"># learn the vocabulary and store CountVectorizer sparse matrix in term_frequencies</span><br>term_frequencies = vectorizer.fit_transform(corpus) <br>vocab = vectorizer.get_feature_names()<br><br><span class="hljs-comment"># convert sparse matrix to numpy array</span><br>term_frequencies = term_frequencies.toarray()<br><br><span class="hljs-comment"># visualize term frequencies </span><br>import seaborn <span class="hljs-keyword">as</span> sns<br>sns.heatmap(term_frequencies, annot=True, cbar = False, xticklabels = vocab);<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-1.png" alt="Image for post"></p><p>计数向量化的另一种方法是，如果在文档中找到单词（无论频率如何），则放置1；如果在文档中找不到单词，则放置0。在这种情况下，我们得到了真正的“独热”编码。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">one_hot_vectorizer = CountVectorizer(<span class="hljs-attribute">binary</span>=<span class="hljs-literal">True</span>)<br>one_hot = one_hot_vectorizer.fit_transform(corpus).toarray()<br><br>sns.heatmap(one_hot, <span class="hljs-attribute">annot</span>=<span class="hljs-literal">True</span>, cbar = <span class="hljs-literal">False</span>, xticklabels = vocab)<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-2.png" alt="Image for post"></p><h1 id="TF-IDF编码"><a href="#TF-IDF编码" class="headerlink" title="TF-IDF编码"></a>TF-IDF编码</h1><p>对于大量的文档集，“ a”，“ the”，“ is”等词经常出现，但它们携带的信息不多。使用 one-hot 编码方法无法决定这些单词的重要性。解决此问题的方法之一是停用词过滤（stopwords filtering），但是此解决方案是离散的，不灵活。</p><p>TF-IDF（term frequency — inverse document frequency）可以更好地解决此问题。TF-IDF降低了常用单词的权重，增加了仅在当前文档中出现的稀有单词的权重。TF-IDF公式如下所示：</p><script type="math/tex; mode=display">tfidf(term, document)=tf(term, document) \cdot idf(term)</script><p>TF是通过单词在文档中出现的次数除以文档中单词的总数计算得到：</p><script type="math/tex; mode=display">tf(term, document)=\frac{n_{i}}{\sum_{k=1}^{W} n_{k}}</script><p>IDF（反向文档频率），其解释方式与反向文档数量相同，其中N是文档数量，$n(t)$是包含当前单词 $t$ 的文档数量。</p><script type="math/tex; mode=display">idf(term)=\log \frac{N}{n_{t}}</script><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br>corpus = [<br>          <span class="hljs-string">&#x27;Time flies like an arrow.&#x27;</span>,<br>          <span class="hljs-string">&#x27;Fruit flies like a banana.&#x27;</span><br>]<br><br>vocab = [<span class="hljs-string">&#x27;an&#x27;</span>, <span class="hljs-string">&#x27;arrow&#x27;</span>, <span class="hljs-string">&#x27;banana&#x27;</span>, <span class="hljs-string">&#x27;flies&#x27;</span>, <span class="hljs-string">&#x27;fruit&#x27;</span>, <span class="hljs-string">&#x27;like&#x27;</span>, <span class="hljs-string">&#x27;time&#x27;</span>]<br><br>tfidf_vectorizer = TfidfVectorizer()<br>tfidf = tfidf_vectorizer.fit_transform(corpus).toarray()<br><br>sns.heatmap(tfidf, annot=<span class="hljs-keyword">True</span>, cbar = <span class="hljs-keyword">False</span>, xticklabels = vocab)<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-3.png" alt="Image for post"></p><h1 id="Word2Vec-和-GloVe"><a href="#Word2Vec-和-GloVe" class="headerlink" title="Word2Vec 和 GloVe"></a>Word2Vec 和 GloVe</h1><p>词嵌入的最常用模型是<a href="https://github.com/dav/word2vec/">word2vec</a>和<a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>，它们都是基于分布假设的无监督方法（在相同上下文中出现的词往往具有相似的含义）。</p><p>Word2Vec单词嵌入是单词的矢量表示，输入大量文本作为输入（例如Wikipedia，科学，新闻，文章等）时，并由无监督模型进行学习。单词的这些表示形式捕获了单词之间的语义相似性。Word2Vec单词嵌入以如下方式学习，即意思相近的单词（例如“ king”和“ queen”）的向量之间的距离比含义完全不同的单词（例如“ king”和“ carpet”）的距离更近 。</p><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-4.png" alt="Image for post"></p><p>Word2Vec向量甚至允许对向量进行一些数学运算。例如，在此操作中，我们为每个单词使用word2vec向量：</p><script type="math/tex; mode=display">king — man + woman = queen</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Download Google Word2Vec embeddings https://code.google.com/archive/p/word2vec/</span><br><br>!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.<span class="hljs-built_in">bin</span>.gz<br>!gunzip GoogleNews-vectors-negative300.<span class="hljs-built_in">bin</span><br><br><span class="hljs-comment"># Try Word2Vec with Gensim</span><br><br><span class="hljs-keyword">import</span> gensim<br><br><span class="hljs-comment"># Load pretrained vectors from Google</span><br>model = gensim.models.KeyedVectors.load_word2vec_format(<span class="hljs-string">&#x27;GoogleNews-vectors-negative300.bin&#x27;</span>, binary=<span class="hljs-literal">True</span>)<br>king = model[<span class="hljs-string">&#x27;king&#x27;</span>]<br><br><span class="hljs-comment"># king - man + woman = queen</span><br><span class="hljs-built_in">print</span>(model.most_similar(positive=[<span class="hljs-string">&#x27;woman&#x27;</span>, <span class="hljs-string">&#x27;king&#x27;</span>], negative=[<span class="hljs-string">&#x27;man&#x27;</span>]))<br><br><span class="hljs-built_in">print</span>(model.similarity(<span class="hljs-string">&#x27;woman&#x27;</span>, <span class="hljs-string">&#x27;man&#x27;</span>))<br></code></pre></td></tr></table></figure><p>另一个词嵌入方法是<strong>Glove</strong>（“Global Vectors”）。它是一种基于单词-上下文矩阵的矩阵分解技术。它首先构造一个由（单词x上下文）共现信息组成的大型矩阵，即，对于每个“单词”（行），您需要计算在一个大型语料库中，该单词在某个“上下文”（列）中出现的频率。然后，将此矩阵分解为低维（单词x特征）矩阵，其中每行现在存储每个单词的矢量表示。通常，这是通过最小化“reconstruction loss”来完成的。这种损失试图找到可以解释高维数据中大部分变化的低维表示形式。</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-comment"># Try Glove word embeddings with Spacy</span><br><br>!python3 -m spacy download en_core_web_lg<br><span class="hljs-built_in">import</span> spacy<br><br><span class="hljs-comment"># Load the spacy model that you have installed</span><br><span class="hljs-built_in">import</span> en_core_web_lg<br><span class="hljs-attr">nlp</span> = en_core_web_lg.load()<br><span class="hljs-comment"># process a sentence using the model</span><br><span class="hljs-attr">doc</span> = nlp(<span class="hljs-string">&quot;man king stands on the carpet and sees woman queen&quot;</span>)<br></code></pre></td></tr></table></figure><p>找到King和Queen 之间的相似之处（值越高越好）。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">doc</span>[<span class="hljs-number">1</span>].similarity(doc[<span class="hljs-number">9</span>])<br><span class="hljs-comment"># 0.72526103</span><br></code></pre></td></tr></table></figure><p>Find similarity between King and carpet.</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">doc</span>[<span class="hljs-number">1</span>].similarity(doc[<span class="hljs-number">5</span>])<br><span class="hljs-comment"># 0.20431946</span><br></code></pre></td></tr></table></figure><p>Check if king — man + woman = queen. We will multiply vectors for ‘man’ and ‘woman’ by two, because subtracting one vector for ‘man’ and adding the vector for ‘woman’ will do little to the original vector for “king”, likely because those “man” and “woman” are related themselves.</p><figure class="highlight golo"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs golo">v =  doc[<span class="hljs-number">1</span>].<span class="hljs-keyword">vector</span> - (doc[<span class="hljs-number">0</span>].<span class="hljs-keyword">vector</span>*<span class="hljs-number">2</span>) + (doc[<span class="hljs-number">8</span>].<span class="hljs-keyword">vector</span>*<span class="hljs-number">2</span>)<br><br>from scipy.spatial <span class="hljs-keyword">import</span> distance<br><span class="hljs-keyword">import</span> numpy as np<br><br><span class="hljs-comment"># Format the vocabulary for use in the distance function</span><br>vectors = [token.<span class="hljs-keyword">vector</span> <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc]<br>vectors = np.<span class="hljs-keyword">array</span>(vectors)<br><br><span class="hljs-comment"># Find the closest word below </span><br>closest_index = distance.cdist(np.expand_dims(v, axis = <span class="hljs-number">0</span>), vectors, metric = &#x27;cosine&#x27;).argmin()<br>output_word = doc[closest_index].text<br><span class="hljs-keyword">print</span>(output_word)<br><br><span class="hljs-comment"># queen</span><br></code></pre></td></tr></table></figure><h1 id="快速文字"><a href="#快速文字" class="headerlink" title="快速文字"></a>快速文字</h1><p><a href="https://github.com/facebookresearch/fastText">FastText</a>是word2vec的扩展，由Tomas Mikolov团队开发（他于2013年创建了word2vec框架）</p><p>与原始word2vec向量相比，FastText的主要改进是包含了字符<a href="https://en.wikipedia.org/wiki/N-gram">n-gram</a>，它可以为未出现在训练数据中的单词（“词汇外”单词）计算单词表示形式。</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript">!pip install Cython --install-option=<span class="hljs-string">&quot;--no-cython-compile&quot;</span><br>!pip install fasttext<br><br><span class="hljs-comment"># download pre-trained language word vectors from one of 157 languges  https://fasttext.cc/docs/en/crawl-vectors.html</span><br><span class="hljs-comment"># it will take some time, about 5 minutes</span><br><span class="hljs-keyword">import</span> fasttext<br><span class="hljs-keyword">import</span> fasttext.util<br>fasttext.util.download_model(<span class="hljs-string">&#x27;en&#x27;</span>, if_exists=<span class="hljs-string">&#x27;ignore&#x27;</span>)  <span class="hljs-comment"># English</span><br>ft = fasttext.load_model(<span class="hljs-string">&#x27;cc.en.300.bin&#x27;</span>)<br></code></pre></td></tr></table></figure><p>为“king”一词创建嵌入</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-literal">ft</span>.get_word_vector(<span class="hljs-string">&#x27;king&#x27;</span>)<br></code></pre></td></tr></table></figure><p>为“king”一词获取最相似的词</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-literal">ft</span>.get_nearest_neighbors(<span class="hljs-string">&#x27;king&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-5.png" alt="Image for post"></p><p>测试模型为未知单词创建向量的能力</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">ft.get<span class="hljs-constructor">_nearest_neighbors(&#x27;<span class="hljs-params">king</span>-<span class="hljs-params">warrior</span>&#x27;)</span><br></code></pre></td></tr></table></figure><h1 id="ELMo-Embeddings-from-Language-Models"><a href="#ELMo-Embeddings-from-Language-Models" class="headerlink" title="ELMo (Embeddings from Language Models)"></a>ELMo (Embeddings from Language Models)</h1><p>与传统的单词嵌入（例如word2vec和GLoVe）不同，分配给token或单词的ELMo矢量取决于当前上下文，实际上是包含该单词的整个句子的函数。因此，同一单词在不同上下文中可以具有不同的单词向量。而且，ELMo表示完全基于字符，因此它们不限于任何预定义的词汇表。</p><p>来自官方网站的说明：</p><p><a href="https://allennlp.org/elmo"><strong>ELMo</strong></a> is a deep contextualized word representation that models both (1) complex characteristics of the word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.</p><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs monkey"><span class="hljs-meta"># use tensorflow 1.x for ELMo, because trere are still no ELMo for tensorflow 2.0</span><br><br>%tensorflow_version <span class="hljs-number">1</span>.x<br><br><span class="hljs-keyword">import</span> tensorflow_hub as hub<br><span class="hljs-keyword">import</span> tensorflow as tf<span class="hljs-meta"></span><br><span class="hljs-meta"></span><br><span class="hljs-meta"># Download pretrained ELMo model from Tensorflow Hub https://tfhub.dev/google/elmo/3</span><br><br>elmo = hub.<span class="hljs-keyword">Module</span>(<span class="hljs-string">&quot;https://tfhub.dev/google/elmo/3&quot;</span>, trainable=<span class="hljs-literal">True</span>)<br><br>sentences =  \<br>[<span class="hljs-comment">&#x27;king arthur, also called arthur or aathur pendragon, legendary british king who appears in a cycle of \</span><br>medieval romances (known as the matter of britain) as the sovereign of a knightly fellowship of the round table.<span class="hljs-comment">&#x27;, </span><br><span class="hljs-comment">&#x27;it is not certain how these legends originated or whether the figure of arthur was based on a historical person.&#x27;, </span><br><span class="hljs-comment">&#x27;the legend possibly originated either in wales or in those parts of northern britain inhabited by brythonic-speaking celts.&#x27;, </span><br><span class="hljs-comment">&#x27;for a fuller treatment of the stories about king arthur, see also arthurian legend.&#x27;]</span><br></code></pre></td></tr></table></figure><p>为了将句子输入到模型训练，我们需要将它们分成单词数组和填充数组，并保持相同的长度。另外，我们将创建“mask”数组来表示每个element是一个实词还是填充符号（在我们的示例中为“ _”）。稍后我们将使用“掩码”数组进行可视化，来显示真实存在的单词。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-keyword">words</span> = []<br>mask = []<br>masked_words = []<br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> <span class="hljs-keyword">sentences</span>:<br>  splitted = sent.<span class="hljs-built_in">split</span>()<br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">36</span>):<br>    <span class="hljs-keyword">try</span>:<br>      <span class="hljs-keyword">words</span>.append(splitted[i])<br>    except:<br>      <span class="hljs-keyword">words</span>.append(<span class="hljs-string">&#x27;_&#x27;</span>)<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">word</span> <span class="hljs-keyword">in</span> <span class="hljs-keyword">words</span>:<br>  <span class="hljs-keyword">if</span> <span class="hljs-built_in">word</span> == <span class="hljs-string">&quot;_&quot;</span>:<br>    mask.append(False)<br>  <span class="hljs-keyword">else</span>:<br>    mask.append(True)<br>    masked_words.append(<span class="hljs-built_in">word</span>)<br></code></pre></td></tr></table></figure><p>使用ELMo创建嵌入：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">embeddings = elmo(<br>    sentences,<br>    <span class="hljs-attribute">signature</span>=<span class="hljs-string">&quot;default&quot;</span>,<br>    <span class="hljs-attribute">as_dict</span>=<span class="hljs-literal">True</span>)[<span class="hljs-string">&quot;elmo&quot;</span>]<br></code></pre></td></tr></table></figure><p>将Tensorflow张量转换为numpy数组。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs css">%%<span class="hljs-selector-tag">time</span><br>with tf<span class="hljs-selector-class">.Session</span>() as sess:<br>  sess.<span class="hljs-built_in">run</span>(tf.<span class="hljs-built_in">global_variables_initializer</span>())<br>  sess.<span class="hljs-built_in">run</span>(tf.<span class="hljs-built_in">tables_initializer</span>())<br>  x = sess.<span class="hljs-built_in">run</span>(embeddings)<br><br>embs = x.<span class="hljs-built_in">reshape</span>(-<span class="hljs-number">1</span>, <span class="hljs-number">1024</span>)<br><br>masked_embs = embs[mask]<br></code></pre></td></tr></table></figure><p>使用<a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>可视化词嵌入</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> sklearn.decomposition import PCA<br><br>pca = PCA(<span class="hljs-attribute">n_components</span>=10)<br>y = pca.fit_transform(masked_embs)<br><br><span class="hljs-keyword">from</span> sklearn.manifold import TSNE<br><br>y = TSNE(<span class="hljs-attribute">n_components</span>=2).fit_transform(y)<br><br>import plotly as py<br>import plotly.graph_objs as go<br><br>data = [<br>    go.Scatter(<br>        x=[i[0] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> y],<br>        y=[i[1] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> y],<br>        <span class="hljs-attribute">mode</span>=<span class="hljs-string">&#x27;markers&#x27;</span>,<br>        text=[i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> masked_words],<br>    <span class="hljs-attribute">marker</span>=dict(<br>        <span class="hljs-attribute">size</span>=16,<br>        color = [len(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> masked_words], #<span class="hljs-built_in">set</span> color equal <span class="hljs-keyword">to</span> a variable<br>        opacity= 0.8,<br>        <span class="hljs-attribute">colorscale</span>=<span class="hljs-string">&#x27;Viridis&#x27;</span>,<br>        <span class="hljs-attribute">showscale</span>=<span class="hljs-literal">False</span><br>    )<br>    )<br>]<br>layout = go.Layout()<br>layout = dict(<br>              yaxis = dict(zeroline = <span class="hljs-literal">False</span>),<br>              xaxis = dict(zeroline = <span class="hljs-literal">False</span>)<br>             )<br>fig = go.Figure(<span class="hljs-attribute">data</span>=data, <span class="hljs-attribute">layout</span>=layout)<br>fig.show()<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-6.gif" alt="Image for post"></p><h1 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h1><p>最后，是时候使用最新技术- Transformers。著名的<a href="https://openai.com/blog/better-language-models/">GPT-2</a>，<a href="https://github.com/google-research/bert">BERT</a>，<a href="https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/">CTRL</a> 都是基于Transformers生成上下文相关的词嵌入。但是与ELMo 不同，Transformers不使用<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN</a>，它们不需要一个接一个地顺序处理句子中的单词。句子中的所有单词都是并行处理的，这种方法可以加快处理速度，并解决<a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">梯度消失的问题</a>（vanishing gradient problem）。</p><p>Transformers 使用<a href="https://arxiv.org/abs/1706.03762">注意力机制</a>来描述每个特定单词与句子中所有其他单词的联系和依存关系。Jay Alammar在<a href="http://jalammar.github.io/illustrated-transformer/">精美插图</a>中详细描述了Transformers 的这种机制和主要原理。</p><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-7.png" alt="Image for post"></p><p>示例，我们将使用 Hugging face 开源的<a href="https://huggingface.co/transformers/">Transformers</a>库，其中包含最新的基于Transformers的模型（例如<a href="https://huggingface.co/transformers/model_doc/bert.html">BERT</a>，<a href="https://huggingface.co/transformers/model_doc/xlnet.html">XLNet</a>，<a href="https://huggingface.co/transformers/model_doc/dialogpt.html">DialoGPT</a>或<a href="https://huggingface.co/transformers/model_doc/gpt2.html">GPT-2</a>）。</p><p>使用BERT获取词嵌入。首先，我们需要安装Transformers库。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">!pip <span class="hljs-keyword">install</span> transformers<br></code></pre></td></tr></table></figure><p>现在，我们导入pytorch, the pretrained BERT model, and a BERT tokenizer，它将句子转换为适合BERT的输入格式（标记自身并添加特殊标记，例如[SEP]和[CLS]）的所有必需工作。</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">import</span> torch<br>torch.manual_seed(<span class="hljs-number">0</span>)<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertModel<br><br><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>% matplotlib <span class="hljs-keyword">inline</span><br><br># Load pre-trained model tokenizer (vocabulary)<br>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, do_lower_case=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>输入一些句子并将其标记化。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs vbnet">sentences =  \<br>[<span class="hljs-comment">&#x27;king arthur, also called arthur or aathur pendragon, legendary british king who appears in a cycle of \</span><br>medieval romances (known <span class="hljs-keyword">as</span> the matter <span class="hljs-keyword">of</span> britain) <span class="hljs-keyword">as</span> the sovereign <span class="hljs-keyword">of</span> a knightly fellowship <span class="hljs-keyword">of</span> the round table.<span class="hljs-comment">&#x27;, </span><br><span class="hljs-comment">&#x27;it is not certain how these legends originated or whether the figure of arthur was based on a historical person.&#x27;, </span><br><span class="hljs-comment">&#x27;the legend possibly originated either in wales or in those parts of northern britain inhabited by brythonic-speaking celts.&#x27;, </span><br><span class="hljs-comment">&#x27;for a fuller treatment of the stories about king arthur, see also arthurian legend.&#x27;]</span><br><br># Print the original sentence.<br>print(<span class="hljs-comment">&#x27; Original: &#x27;, sentences[0][:99])</span><br><br># Print the sentence splitted <span class="hljs-keyword">into</span> tokens.<br>print(<span class="hljs-comment">&#x27;Tokenized: &#x27;, tokenizer.tokenize(sentences[0])[:15])</span><br><br># Print the sentence mapped <span class="hljs-keyword">to</span> token ids.<br>print(<span class="hljs-comment">&#x27;Token IDs: &#x27;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))[:15])</span><br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-8.png" alt="Image for post"></p><p>请注意，某些标记可能看起来像这样：[‘aa’, ‘##th’, ‘##ur’, ‘pen’, ‘##dra’, ‘##gon’]。这是因为 BERT tokenizer 是使用WordPiece模型创建的。该模型贪婪地创建一个固定大小的词汇表，其中包含最适合我们的语言数据的单个字符，子词和单词。BERT tokenizer 生成器使用的词汇表包含所有英语字符，以及在该模型所训练的英语语料库中找到的约30,000个最常见的单词和子单词。因此，如果词汇表中未提及该词，则该词将分为子词和字符。某些子词之前的两个井号（##）表明该子词是较大词的一部分，并在另一个子词之前。</p><p>我们将使用 <code>tokenizer.encode_plus</code> 函数，该函数：</p><ul><li>将句子拆分为 tokens</li><li>添加特殊的[CLS]和[SEP] tokens</li><li>将令牌映射到其ID</li><li>将所有句子填充或截断为相同长度</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># Tokenize all of the sentences and map tokens to word IDs.</span><br>input_ids = []<br>attention_masks = []<br>tokenized_texts = []<br><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences:<br>    encoded_dict = tokenizer.encode_plus(<br>                        sent,                      <br>                        add_special_tokens = <span class="hljs-literal">True</span>,<br>                        <span class="hljs-attribute">truncation</span>=<span class="hljs-literal">True</span>,<br>                        max_length = 48,          <br>                        pad_to_max_length = <span class="hljs-literal">True</span>,                        <br>                        return_tensors = <span class="hljs-string">&#x27;pt&#x27;</span>,    <br>                   )<br>    <br>    # Save tokens <span class="hljs-keyword">from</span> sentence as a separate array. <br>    marked_text = <span class="hljs-string">&quot;[CLS] &quot;</span> + sent + <span class="hljs-string">&quot; [SEP]&quot;</span><br>    tokenized_texts.append(tokenizer.tokenize(marked_text))<br>    <br>    <br>    # <span class="hljs-built_in">Add</span> the encoded sentence <span class="hljs-keyword">to</span> the list.    <br>    input_ids.append(encoded_dict[<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br><br><span class="hljs-comment"># Convert the list into tensor.</span><br>input_ids = torch.cat(input_ids, <span class="hljs-attribute">dim</span>=0)<br></code></pre></td></tr></table></figure><p><strong>Segment ID</strong>. BERT通过使用1和0来区分两个句子这种方式训练的。我们将分别对每个句子进行编码，因此我们将每个句子中的每个标记标记为1。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">segments_ids</span> = torch.<span class="hljs-literal">on</span>es_like(input_ids)<br></code></pre></td></tr></table></figure><p>现在，我们可以调用BERT模型，并最终获得模型隐层状态，然后根据这些状态创建单词嵌入。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">with torch<span class="hljs-selector-class">.no_grad</span>():<br>    outputs = <span class="hljs-built_in">model</span>(input_ids, segments_ids)<br>    hidden_states = outputs<span class="hljs-selector-attr">[2]</span><br></code></pre></td></tr></table></figure><p>Let’s examine what we’ve got</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Number of layers:&quot;</span>, <span class="hljs-built_in">len</span>(hidden_states), <span class="hljs-string">&quot;  (initial embeddings + 12 BERT layers)&quot;</span>)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Number of batches:&quot;</span>, <span class="hljs-built_in">len</span>(hidden_states[<span class="hljs-number">0</span>]))<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Number of tokens:&quot;</span>, <span class="hljs-built_in">len</span>(hidden_states[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]))<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Number of hidden units:&quot;</span>, <span class="hljs-built_in">len</span>(hidden_states[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]))<br><br><span class="hljs-comment"># 13</span><br><span class="hljs-comment"># 4</span><br><span class="hljs-comment"># 48</span><br><span class="hljs-comment"># 768</span><br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Concatenate the tensors for all layers. </span><br><span class="hljs-attribute">token_embeddings</span> = torch.stack(hidden_states, dim=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># Swap dimensions, so we get tensors in format: [sentence, tokens, hidden layes, features]</span><br><span class="hljs-attribute">token_embeddings</span> = token_embeddings.permute(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>我们将使用最后四个隐层来创建每个单词嵌入。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">processed_embeddings</span> = token_embeddings[:, :, <span class="hljs-number">9</span>:, :]<br></code></pre></td></tr></table></figure><p>将每个token的四层连接起来以创建嵌入</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">embeddings</span> = torch.reshape(processed_embeddings, (<span class="hljs-number">4</span>, <span class="hljs-number">48</span>, -<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>让我们检查一下第一句的嵌入。首先，我们获取需要比较的令牌ID</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span>, token_str <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokenized_texts<span class="hljs-selector-attr">[0]</span>):<br>  print (<span class="hljs-selector-tag">i</span>, token_str)<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/word-embeddings-9.png" alt="Image for post"></p><p>我们可以看到“king”一词位于索引1和17。我们将检查嵌入1和17之间的距离。此外，我们还将检查“arthur”一词的嵌入是否更接近“king”，然后是否接近“table”。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">from</span> scipy.spatial.distance import cosine<br> <br><span class="hljs-attribute">kings</span> = cosine(embeddings[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>], embeddings[<span class="hljs-number">0</span>][<span class="hljs-number">17</span>])<br><span class="hljs-attribute">king_table</span> = cosine(embeddings[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>], embeddings[<span class="hljs-number">0</span>][<span class="hljs-number">46</span>])<br><span class="hljs-attribute">king_archtur</span> = cosine(embeddings[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>], embeddings[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>])<br> <br><span class="hljs-attribute">print</span>(&#x27;Distance for two kings:  %.<span class="hljs-number">2</span>f&#x27; % kings)<br><span class="hljs-attribute">print</span>(&#x27;Distance from king to table:  %.<span class="hljs-number">2</span>f&#x27; % king_table)<br><span class="hljs-attribute">print</span>(&#x27;Distance from Archtur to king:  %.<span class="hljs-number">2</span>f&#x27; % king_archtur)<br><br><span class="hljs-comment"># 0.21</span><br><span class="hljs-comment"># 0.73</span><br><span class="hljs-comment"># 0.40</span><br></code></pre></td></tr></table></figure><p>因此，我们看到两个“king”的嵌入非常相似，但不完全相同，并且Archtur更像是king而不是table。</p><p>使用 <a href="https://github.com/AliOsm/simplerepresentations"><strong>simplerepresentations</strong></a> 模块可能会更简单。该模块完成了我们之前所做的所有工作-从BERT中提取所需的隐层状态，并在几行代码中创建词嵌入。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs routeros">!pip install simplerepresentations<br><br>import torch<br><span class="hljs-keyword">from</span> simplerepresentations import RepresentationModel<br>torch.manual_seed(0)<br><br>model_type = <span class="hljs-string">&#x27;bert&#x27;</span><br>model_name = <span class="hljs-string">&#x27;bert-base-uncased&#x27;</span><br><br>representation_model = RepresentationModel(<br>  <span class="hljs-attribute">model_type</span>=model_type,<br>  <span class="hljs-attribute">model_name</span>=model_name,<br>  <span class="hljs-attribute">batch_size</span>=4,<br>  <span class="hljs-attribute">max_seq_length</span>=48, <br>  <span class="hljs-attribute">combination_method</span>=<span class="hljs-string">&#x27;cat&#x27;</span>, <br>  <span class="hljs-attribute">last_hidden_to_use</span>=4 <br> )<br><br>text_a = sentences<br><br>all_sentences_representations, all_tokens_representations = representation_model(<span class="hljs-attribute">text_a</span>=text_a)<br></code></pre></td></tr></table></figure><p>Check distaces between Archtur, king and table.</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">from</span> scipy.spatial.distance import cosine<br><br><span class="hljs-attribute">kings</span> = cosine(all_tokens_representations[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>], all_tokens_representations[<span class="hljs-number">0</span>][<span class="hljs-number">17</span>])<br><span class="hljs-attribute">king_table</span> = cosine(all_tokens_representations[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>], all_tokens_representations[<span class="hljs-number">0</span>][<span class="hljs-number">46</span>])<br><span class="hljs-attribute">king_archtur</span> = cosine(all_tokens_representations[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>], all_tokens_representations[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>])<br><br><span class="hljs-attribute">print</span>(&#x27;Distance for two kings:  %.<span class="hljs-number">2</span>f&#x27; % kings)<br><span class="hljs-attribute">print</span>(&#x27;Distance from king to table:  %.<span class="hljs-number">2</span>f&#x27; % king_table)<br><span class="hljs-attribute">print</span>(&#x27;Distance from Archtur to king:  %.<span class="hljs-number">2</span>f&#x27; % king_archtur)<br><br><span class="hljs-comment"># 0.21</span><br><span class="hljs-comment"># 0.73</span><br><span class="hljs-comment"># 0.40</span><br></code></pre></td></tr></table></figure><p>结果相同，代码更少。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>希望阅读本文后，对当前的词嵌入方法有了一个概念，并开始了解如何在Python中快速实现这些方法。NLP的世界是多种多样的，并且有更多的嵌入模型和方法。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/">BERT单词嵌入教程</a></li><li><a href="http://jalammar.github.io/illustrated-transformer/">图解变压器</a></li><li><a href="http://jalammar.github.io/illustrated-gpt2/">图解GPT-2（可视化变压器语言模型）</a></li><li><a href="https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598">从预训练的单词嵌入到预训练的语言模型—专注于BERT</a></li><li><a href="https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30">使用Transformers和DialoGPT进行微调，制作自己的Rick Sanchez（机器人）</a></li><li><a href="https://medium.com/swlh/playing-with-word-vectors-308ab2faa519">玩单词向量</a></li><li><a href="https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010">理解GloVe嵌入的直观指南</a></li><li><a href="https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/">具有Spacy和Gensim的Python中的单词嵌入</a></li><li><a href="https://medium.com/analytics-vidhya/brief-review-of-word-embedding-families-2019-b2bbc601bbfe">单词嵌入家庭的简要回顾（2019）</a></li><li><a href="https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795">词嵌入：探索，解释和利用（Python中的代码）</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>用人话解释交叉熵</title>
    <link href="/2020/07/23/2020-07-23-%E7%94%A8%E4%BA%BA%E8%AF%9D%E8%A7%A3%E9%87%8A%E4%BA%A4%E5%8F%89%E7%86%B5/"/>
    <url>/2020/07/23/2020-07-23-%E7%94%A8%E4%BA%BA%E8%AF%9D%E8%A7%A3%E9%87%8A%E4%BA%A4%E5%8F%89%E7%86%B5/</url>
    
    <content type="html"><![CDATA[<p><strong>交叉熵（cross entropy）</strong>是深度学习中常用的一个概念，一般用来求目标与预测值之间的差距。以前做一些分类问题的时候，没有过多的注意，直接调用现成的库，用起来也比较方便。最近开始发现自己对交叉熵的理解有些模糊，不够深入。遂花了几天的时间从头梳理了一下相关知识点，才算透彻的理解了，特地记录下来，以便日后查阅。</p><ul><li>信息论（熵概念介绍）</li><li>为什么要用交叉熵做loss函数？</li><li>交叉熵在单分类问题中的使用</li><li>交叉熵在多分类问题中的使用</li></ul><p>参考自 <a href="https://blog.csdn.net/tsyccnh/article/details/79163834">《一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉》</a></p> <span id="more"></span><h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><p>交叉熵是信息论中的一个概念，要想了解交叉熵的本质，需要先从最基本的概念讲起。</p><h2 id="1-信息量"><a href="#1-信息量" class="headerlink" title="1 信息量"></a>1 信息量</h2><p>首先是信息量。假设我们听到了两件事，分别如下：</p><ul><li>事件A：巴西队进入了2018世界杯决赛圈。</li><li>事件B：中国队进入了2018世界杯决赛圈。</li></ul><p>仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。<strong>所以当越不可能的事件发生了，我们获取到的信息量就越大</strong>。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。</p><p>假设$X$是一个离散型随机变量，其取值集合为$χ$，概率分布函数$p(x)=Pr(X=x)，x∈χ$，则定义事件$X=x_0$的信息量为：</p><script type="math/tex; mode=display">I(X=x_0)=−log(p(x_0))</script><p>由于是概率所以$p(x0)$​的取值范围是 $[0，1]$，绘制为图形如下：</p><p><img src="https://img-blog.csdn.net/20180125164333234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHN5Y2NuaA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>可见该函数符合我们对信息量的直觉。</p><h2 id="2-熵"><a href="#2-熵" class="headerlink" title="2 熵"></a>2 熵</h2><p>信息论主要研究如何量化数据中的信息。最重要的信息度量单位是<code>熵</code>Entropy，一般用<code>H</code>表示。分布的熵的公式如下：</p><script type="math/tex; mode=display">H=−∑_{i=1}^np(x_i)·log p(x_i)</script><p>上面对数没有确定底数，可以是<code>2</code>、<code>e</code>或<code>10</code>，等等。熵的主要作用是告诉我们最优编码信息方案的理论下界（存储空间），以及度量数据的信息量的一种方式。理解了熵，我们就知道有多少信息蕴含在数据之中</p><p>考虑如下例子，对于某个事件，有$n$​​​种可能性，每一种可能性都有一个概率$p(x_i)$​​​​，这样就可以计算出某一种可能性的信息量。举一个例子，假设你拿出了你的电脑，按下开关，会有三种可能性，下表列出了每一种可能的概率及其对应的信息量：</p><div class="table-container"><table><thead><tr><th>序号</th><th>事件</th><th>概率p</th><th>信息量I</th></tr></thead><tbody><tr><td>A</td><td>电脑正常开机</td><td>0.7</td><td>-log(p(A))=0.36</td></tr><tr><td>B</td><td>电脑无法开机</td><td>0.2</td><td>-log(p(B))=1.61</td></tr><tr><td>C</td><td>电脑爆炸了</td><td>0.1</td><td>-log(p(C))=2.30</td></tr></tbody></table></div><p>那么，熵用来表示所有信息量的期望，即：</p><script type="math/tex; mode=display">\begin{align}H(X) &= −∑_{i=1}^np(xi)log p(xi) \\     &= −[p(A)log(p(A))+p(B)log(p(B))+p(C))log(p(C))] \\     &= 0.7×0.36+0.2×1.61+0.1×2.30 \\     &= 0.804\end{align}</script><p>然而有一类比较特殊的问题，比如投掷硬币只有两种可能，字朝上或花朝上。买彩票只有两种可能，中奖或不中奖。我们称之为<strong>0-1分布问题</strong>（二项分布的特例）。对于这类问题，熵的计算方法可以简化为如下算式：</p><script type="math/tex; mode=display">\begin{align}H(X) &=−∑_{i=1}^n{p(xi)log(p(xi))} \\&=−p(x)log(p(x))−(1−p(x))log(1−p(x))\end{align}</script><h2 id="3-相对熵（KL散度）"><a href="#3-相对熵（KL散度）" class="headerlink" title="3 相对熵（KL散度）"></a>3 相对熵（KL散度）</h2><p>相对熵又称<code>Kullback-Leibler Divergence</code>，即<code>K-L散度</code>，是一种<strong>量化两种概率分布P和Q之间差异</strong>的方式。在概率学和统计学上，我们经常会使用一种<code>更简单的、近似的分布</code>来替代<code>观察数据</code>或<code>太复杂的分布</code>。<strong>K-L散度能帮助我们度量使用一个分布来近似另一个分布时所损失的信息量。</strong></p><p>K-L散度定义见文末附录1。另外在附录5中解释了为什么在深度学习中，训练模型时使用的是<code>Cross Entropy</code> 而非<code>K-L Divergence</code>。</p><p>KL散度的计算公式：</p><script type="math/tex; mode=display">D_{KL}(p||q)=∑_{i=1}^n{p(xi)log(\frac{p(x_i)}{q(xi)})}</script><p>注：n为事件的所有可能性。显然，根据上面的公式，K-L散度其实是数据的原始分布p和近似分布q之间的对数差值的期望。$D_{KL}$的值越小，表示q分布和p分布越接近.</p><p>在机器学习中，$P$往往用来表示样本的真实分布，比如[1，0，0]表示当前样本属于第一类。$Q$用来表示模型所预测的分布，比如[0.7，0.2，0.1]。直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。</p><h2 id="4-交叉熵"><a href="#4-交叉熵" class="headerlink" title="4 交叉熵"></a>4 交叉熵</h2><p>对上式变形可以得到：</p><script type="math/tex; mode=display">\begin{align}D_{KL}(p||q)&=∑_{i=1}^n{p(x_i)log(p(x_i))}−∑_{i=1}^n{p(x_i)log(q(x_i))} \\&=−H(p(x))+[−∑_{i=1}^n{p(x_i)log(q(x_i))}]\end{align}</script><p>等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：</p><script type="math/tex; mode=display">H(p,q)=−∑_{i=1}^n{p(x_i)log(q(x_i))}</script><p>如果你熟悉神经网络，你肯能已经猜到我们接下来要学习的内容。除去神经网络结构的细节信息不谈，整个神经网络模型其实是在构造一个参数数量巨大的函数（百万级，甚至更多），不妨记为<code>f(x)</code>，通过设定目标函数，可以训练神经网络逼近非常复杂的真实函数<code>g(x)</code>。训练的关键是要设定目标函数，反馈给神经网络当前的表现如何。训练过程就是不断减小目标函数值的过程。</p><p>我们已经知道K-L散度用来度量在逼近一个分布时的信息损失量。K-L散度能够赋予神经网络近似表达非常复杂数据分布的能力。由于<strong>KL散度中的前一部分$−H(y)$不变</strong>，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.zhihu.com/question/65288314/answer/244557337">https://www.zhihu.com/question/65288314/answer/244557337</a></p><p><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</a></p><p><a href="https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/">Why You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training</a></p><p><a href="https://www.jianshu.com/p/43318a3dc715">如何理解K-L散度（相对熵）</a></p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><ol><li><p>K-L 散度的定义</p><p><img src="https:////upload-images.jianshu.io/upload_images/75110-5d773218b2511d9a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom: 50%;" /></p></li><li><p>计算K-L的注意事项</p><p><img src="https:////upload-images.jianshu.io/upload_images/75110-f55d663d60503fa4.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom:50%;" /></p></li><li><p>遇到<code>log 0</code>时怎么办</p><p><img src="https:////upload-images.jianshu.io/upload_images/75110-a7dc83686d7206c1.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom:50%;" /></p></li><li><p>信息熵、交叉熵、相对熵</p><ul><li>信息熵，即熵，香浓熵。编码方案完美时，最短平均编码长度。</li><li>交叉熵，cross-entropy。编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度。是神经网络常用的损失函数。</li><li>相对熵，即K-L散度，relative entropy。编码方案不一定完美时，平均编码长度相对于最小值的增加值。</li></ul><p>更详细对比，见知乎<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F41252833">如何通俗的解释交叉熵与相对熵?</a></p></li><li><p>为什么在神经网络中使用交叉熵损失函数，而不是K-L散度？</p><p>K-L散度=交叉熵-熵，即 <code>DKL( p||q )=H(p,q)−H(p)</code>。</p><p>在神经网络所涉及到的范围内，<code>H(p)</code>不变，则<code>DKL( p||q )</code>等价<code>H(p,q)</code>。</p><p>更多讨论见<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fstats.stackexchange.com%2Fquestions%2F265966%2Fwhy-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne">Why do we use Kullback-Leibler divergence rather than cross entropy in the t-SNE objective function?</a>和<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F4mebvf%2Fwhy_train_with_crossentropy_instead_of_kl%2F">Why train with cross-entropy instead of KL divergence in classification?</a></p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>交叉熵</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搜索系统调研</title>
    <link href="/2020/07/21/2020-07-21-%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%A0%94/"/>
    <url>/2020/07/21/2020-07-21-%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<ul><li>如何构建搜索系统</li><li><p>搜索系统的基本组成</p><ul><li>Item 内容理解</li><li>Query 理解</li><li>检索召回</li><li>结果集排序</li></ul><span id="more"></span></li></ul><h1 id="如何构建搜索系统"><a href="#如何构建搜索系统" class="headerlink" title="如何构建搜索系统"></a>如何构建搜索系统</h1><ol><li>搜索引擎选择</li><li>离线数据/计算/导入导出</li><li>实时数据同步<br>数据的增删改等操作都需要在搜索引擎里面实现同步，还有原始数据也需要实时同步到搜索引擎中去。</li><li>数据访问/排序<br>数据部署完成后如何让外部用户访问，在一般搜素引擎中对搜索的数据会进行个性化排序。还有在 web 端和 Elasticsearch 有一个中间件加载搜索结果，对前五页或者十页进行一个精排。</li><li>日志分析</li></ol><h1 id="搜索系统的基本组成"><a href="#搜索系统的基本组成" class="headerlink" title="搜索系统的基本组成"></a>搜索系统的基本组成</h1><p>一个基本的搜索系统大体可以分为离线挖掘和在线检索两部分，其中包含的重要模块主要有：<strong>Item 内容理解、Query 理解、检索召回、排序模块</strong>等。召回阶段根据用户的兴趣和历史行为，从知识库中挑选出一个小的候选集（e.g., 几百到几千个title）。这些候选都是用户很大概率会感兴趣的内容，排序阶段在此基础上进行更精准的计算，能够给每一个title进行精确打分，进而从成千上万的候选中选出用户最感兴趣的少量title（e.g., 十几个）。</p><p>整个检索系统的目标可以抽象为给定 query，检索出最能满足用户需求的 item，也即检索出概率： $P(I i |Q) $ 最大的 item $I i $。</p><p>根据贝叶斯公式展开：</p><script type="math/tex; mode=display">argmaxP(I i |Q)=argmaxP(Q|I i)P(I i) /P(Q)argmaxP(Q|I i)P(I i)</script><p>其 中 $P(I i ) $ 表示 item 的重要程度，对应 item 理解侧的权威度、质量度等挖掘， $P(Q| I i ) $ 表示 item $I i$ 能满足用户搜索需求 Q 的程度，对 query 分词后可以表示为：</p><script type="math/tex; mode=display">P(Q| I i ) =P( t 1 ,..., t n | I i )</script><p>这部分概率对应到基于 query 理解和 item 理解的结果上进行 query 和 item 间相关性计算，同时涉及到点击调权等排序模块.</p><p><img src="https://img1.3s78.com/codercto/d5b9080aeb6b7135676e3e8484c71b08" alt="搜索中的 Query 理解及应用"></p><h2 id="1、Item-内容理解"><a href="#1、Item-内容理解" class="headerlink" title="1、Item 内容理解"></a>1、Item 内容理解</h2><p>在离线侧，我们需要做一些基础的离线挖掘工作，包括 item 内容的获取、清洗解析、item 内容理解 ( 语义 tag、权威度计算、时间因子、质量度等 )、用户画像构建、query 离线策略挖掘、以及从搜索推荐日志中挖掘 item 之间的语义关联关系、构建排序模型样本及特征工程等。</p><h3 id="索引构建"><a href="#索引构建" class="headerlink" title="索引构建"></a><strong>索引构建</strong></h3><p>进行 item 内容理解之后，对相应的结构化内容执行建库操作，分别构建正排和倒排索引库。其中，正排索引简单理解起来就是根据 itemid 能找到 item 的各个基本属性及 term 相关 ( term 及其在 item 中出现的频次、位置等信息 ) 的详细结构化数据。相反地，倒排索引就是能根据分词 term 来找到包含该 term 的 item 列表及 term 在对应 item 中词频、位置等信息。</p><p>通常会对某个 item 的 title、keyword、anchor、content 等不同属性域分别构建倒排索引，同时一般会根据 item 资源的权威度、质量度等纵向构建分级索引库，首先从高质量库中进行检索优先保证优质资源能被检索出来，如果高质量检索结果不够再从低质量库中进行检索。为了兼顾索引更新时效性和检索效率，一般会对索引库进行横向分布式部署，且索引库的构建一般分为全量构建和增量更新。</p><h2 id="2、Query-理解"><a href="#2、Query-理解" class="headerlink" title="2、Query 理解"></a>2、Query 理解</h2><p>从前面的介绍可以知道，搜索三大模块的大致调用顺序是从 query 理解到检索召回再到排序，作为搜索系统的第一道环节，query 理解的结果除了可以用于召回也可以给排序模块提供必要的基础特征，为此 QU 很大程度影响召回和排序的质量，对 query 是进行简单字面上的理解还是可以理解其潜在的真实需求很大程度决定着一个搜索系统的智能程度。目前业界的搜索 QU 处理流程还是以 pipeline 的方式为主，也即拆分成多个模块分别负责相应的功能实现，pipeline 的处理流程可控性比较强，但存在缺点就是其中一个模块不 work 或准确率不够会对全局理解有较大影响。为此，直接进行 query-item 端到端地理解如深度语义匹配模型等也是一个值得尝试的方向。</p><h3 id="1-Pipeline-流程"><a href="#1-Pipeline-流程" class="headerlink" title="1. Pipeline 流程"></a><strong>1. Pipeline 流程</strong></h3><p>搜索 Query 理解包含的模块主要有：query 预处理、query 纠错、query 扩展、query 归一、联想词、query 分词、意图识别、term 重要性分析、敏感 query 识别、时效性识别等。以下图为例，这里简单介绍下 query 理解的 pipeline 处理流程：线上来一个请求 query，为缓解后端压力首先会判断该 query 是否命中 cache，若命中则直接返回对该 query 对应的结构化数据。若不命中 cache，则首先会对 query 进行一些简单的预处理，接着由于 query 纠错可能会用到分词 term 信息进行错误检测等先进行 query 分词并移除一些噪音符号，然后进行 query 纠错处理，一些情况下局部纠错可能会影响到上下文搭配的全局合理性，为此还可能会需要进行二次纠错处理。</p><p>对 query 纠错完后可以做 query 归一、扩展及联想词用于进行扩召回及帮助用户做搜索引导。接着再对 query 做分词及对分词后的 term 做重要性分析及紧密度分析，对无关紧要的词可以做丢词等处理，有了分词 term 及对应的权重、紧密度信息后可以用于进行精准和模糊意图的识别。除了这些基本模块，有些搜索场景还需要有对 query 进行敏感识别及时效性分析等其他处理模块。最后还需要能在 cms 端进行配置的人工干预模块，对前面各个模块处理的结果能进行干预以快速响应 badcase 处理。</p><p>当然，这个 pipeline 不是通用的，不同的搜索业务可能需要结合自身情况做 pipeline 的调整定制，像百度这些搜索引擎会有更为复杂的 query 理解 pipeline，各个模块间的依赖交互也更为复杂，所以整个 QU 服务最好能灵活支持各个模块的动态热插拔，像组装乐高积木一样进行所需模块的灵活配置，下面对各个模块做进一步详细的介绍。</p><p><img src="https://img1.3s78.com/codercto/3be45355bee3518c53cf56fd4d5174c3" alt="搜索中的 Query 理解及应用"></p><h3 id="2-Query-预处理"><a href="#2-Query-预处理" class="headerlink" title="2. Query 预处理"></a><strong>2. Query 预处理</strong></h3><h3 id="3-Query-分词"><a href="#3-Query-分词" class="headerlink" title="3. Query 分词"></a><strong>3. Query 分词</strong></h3><h3 id="4-紧密度分析"><a href="#4-紧密度分析" class="headerlink" title="4. 紧密度分析"></a><strong>4. 紧密度分析</strong></h3><h3 id="5-Term-重要性分析"><a href="#5-Term-重要性分析" class="headerlink" title="5. Term 重要性分析"></a><strong>5. Term 重要性分析</strong></h3><p>考虑到不同 term 在同一文本中会有不一样的重要性，在做 query 理解及 item 内容理解时均需要挖掘切词后各个 term 的重要性，在进行召回计算相关性时需要同时考虑 query 及 item 侧的 term 重要性。如对于 query “手机淘宝”，很明显 term “淘宝”要比”手机”更重要，为此赋予”淘宝”的权重应该比”手机”高。Term 重要性可以通过分等级或0.0~1.0的量化分值来衡量，如下图的 case 所示我们可以将 term 重要性分为4个级别，重要性由高到低分别是：核心词、限定词、可省略词、干扰词。对于重要级别最低的 term 可以考虑直接丢词，或者在索引库进行召回构造查询逻辑表达式时将对应的 term 用 “or” 逻辑放宽出现限制，至于计算出的在 query 和 item 内容中的 term 重要性分值则可以用于召回时计算基础相关性，如简单地将 BM25 公式：</p><p><img src="https://img1.3s78.com/codercto/7bbd212acc71d60b2e332a3d5c0eb1ac" alt="搜索中的 Query 理解及应用"></p><p>中 term 在 item 及 query 中的词频 tf(t)、qf(t) 乘上各自的 term 权重。</p><p><img src="https://img1.3s78.com/codercto/b4a367142c73c75e8abf4228da865dd1" alt="搜索中的 Query 理解及应用"></p><p>其中 item 内容侧的 term 重要性可以采用 LDA 主题模型、TextRank 等方法来挖掘，至于 query 侧的 term 重要性，比较容易想到的方法就是把它视为分类或回归问题来考虑，通过训练 svm、gbdt 等传统机器学习模型即可进行预测。模型样本的构造除了进行人工标注还可以通过用户的搜索点击日志来自动构造。大概做法是将某个 query 对应搜索结果的用户点击频次作为同时出现在 query 及搜索结果 title 中相应 term 的重要性体现，首先通过共同点击信息或二部图方法对 query 进行聚类，再将一个 query 对应有不同点击 title 以及同一 term 在簇内不同 query 中的点击 title 频次结果加权考虑起来，同时排除掉一些搜索 qv 比较不置信的 query 及 title 对应的结果，最后将累计频次进行归一化及分档得到样本对应 label。</p><h3 id="6-搜索引导"><a href="#6-搜索引导" class="headerlink" title="6. 搜索引导"></a><strong>6. 搜索引导</strong></h3><p>受限于用户的先验知识的参差不齐或输入设备引入的噪音，用户输入的 query 可能不足以表达用户真正的需求进而影响用户搜到想要的结果。为此，除了保证搜索结果的相关性，一个完善的搜索引擎还需要给用户提供一系列搜索引导功能，以减少用户的搜索输入成本，缩短用户找到诉求的路径。搜索引导按功能的不同主要可以分为：<strong>搜索热词、搜索历史、query 改写、搜索联想词</strong>，一些电商等垂搜可能还带有类目属性等搜索导航功能。由于搜索热词及搜索历史功能相对比较好理解，这里不做过多阐述。</p><h4 id="①-Query-改写"><a href="#①-Query-改写" class="headerlink" title="① Query 改写"></a>① Query 改写</h4><p>Query 改写这个概念比较泛，简单理解就是将源 query 改写变换到另一个 query。按照改写功能的不同，query 改写可以分为 <strong>query 纠错、query 归一、query 扩展</strong>三个方向。其中 query 纠错负责对存在错误的 query 进行识别纠错，query 归一负责将偏门的 query 归一变换到更标准且同义的 query 表达，而 query 扩展则负责扩展出和源 query 内容或行为语义相关的 query 列表推荐给用户进行潜在需求挖掘发现。</p><p><strong>Query 纠错</strong></p><p>Query 纠错，顾名思义，也即对用户输入 query 出现的错误进行检测和纠正的过程。用户在使用搜索过程中，可能由于先验知识掌握不够或输入过程引入噪音 ( 如：语音识别有误、快速输入手误等 ) 输入的搜索 query 会存在一定的错误。如果不对带有错误的 query 进行纠错，除了会影响 QU 其他模块的准确率，还会影响召回的相关性及排序的合理性，最终影响到用户的搜索体验。</p><p>除了搜索场景，query 纠错还可以应用于输入法、人机对话、语音识别、内容审核等应用场景。不同的业务场景需要解决的错误类型会不太一样，比如 ASR 语音识别主要解决同谐音、模糊音等错误，而输入法及搜索等场景则需要面临解决几乎所有错误类型，如同谐音、模糊音 ( 平舌翘舌、前后鼻音等 )、混淆音、形近字 ( 主要针对五笔和笔画手写输入法 )、多漏字等错误。根据 query 中是否有不在词典中本身就有错误的词语 ( Non-word )，可以将 query 错误类型主要分为 Non-word 和 Real-word 两类错误。</p><p>其中，Non-word 错误一般出现在带英文单词或数字的 query 中，由于通过输入法进行输入，不会存在错误中文字的情况，所以中文 query 如果以字作为最小语义单元的话一般只会存在 Real-word 错误，而带英文数字的 query 则可能存在两类错误。下图对这两大类的常见错误类型进行归类及给出了相应的例子。</p><p><img src="https://img1.3s78.com/codercto/5a6d2c11b2513222a7c424b0f7b77b5d" alt="搜索中的 Query 理解及应用"></p><p>从原理上，Query 纠错可以用噪音信道模型来理解，假设用户本意是搜索 Q real ，但是 query 经过噪音信道后引进了一定的噪音，此时纠错过程相当于构建解码器将带有噪音干扰的 query Q noise 进行最大去噪还原成 Q denoise ，使得 Q denoise ≈ Q real 。</p><p>对应的公式为：</p><p>其中 P( Q denoise ) 表示语言模型概率， P( Q noise | Q denoise ) 表示写错概率，进行纠错一般都是围绕着求解这两个概率来进行的。</p><p><img src="https://img1.3s78.com/codercto/c6345485d79a4fd6b36efe71e6d381e0" alt="搜索中的 Query 理解及应用"></p><p>纠错任务主要包含错误检测和错误纠正两个子任务，其中错误检测用于识别错误词语的位置，简单地可以通过对输入 query 进行切分后检查各个词语是否在维护的自定义词表或挖掘积累的常见纠错 pair 中，若不在则根据字型、字音或输入码相近字进行替换构造候选并结合 ngram 语言模型概率来判断其是否存在错误，这个方法未充分考虑到上下文信息，可以适用于常见中文词组搭配、英文单词错误等的检测。进一步的做法是通过训练序列标注模型的方法来识别错误的开始和结束位置。</p><p>至于错误纠正，即在检测出 query 存在错误的基础上对错误部分进行纠正的过程，其主要包括纠错候选召回、候选排序选择两个步骤。在进行候选召回时，没有一种策略方法能覆盖所有的错误类型，所以一般通过采用多种策略方法进行多路候选召回，然后在多路召回的基础上通过排序模型来进行最终的候选排序。</p><p>对于英文单词错误、多漏字、前后颠倒等错误可以通过编辑距离度量进行召回，编辑距离表示从一个字符串变换到另一个字符串需要进行插入、删除、替换操作的次数，如 “apple” 可能错误拼写成 “appel”，它们的编辑距离是1。由于用户的搜索 query 数一般是千万甚至亿级别的，如果进行两两计算编辑距离的话计算量会非常大，为此需要采用一定的方法减小计算量才行。比较容易想到的做法是采用一些启发式的策略，如要求首字 ( 符 ) 一样情况下将长度小于等于一定值的 query 划分到一个桶内再计算两两 query 间的编辑距离，此时可以利用 MapReduce 进一步加速计算。</p><p>当然这种启发式的策略可能会遗漏掉首字 ( 符 ) 不一样的 case，如在前面两个位置的多漏字、颠倒等错误。还有的办法就是利用空间换时间，如对 query 进行 ngram 等长粒度切分后构建倒排索引，然后进行索引拉链的时候保留相似度 topN 的 query 作为候选。又或者利用编辑距离度量满足三角不等式 d(x,y)+d(y,z)≥d(x,z) 的特性对多叉树进行剪枝来减少计算量。</p><p>首先随机选取一个 query 作为根结点，然后自顶向下对所有 query 构建多叉树，树的边为两个结点 query 的编辑距离。给定一个 query，需要找到与其编辑距离小于等于 n 的所有 query，此时自顶向下与相应的结点 query 计算编辑距离 d，接着只需递归考虑边值在 d-n 到 d+n 范围的子树即可。如下图所示需要查找所有与 query “十面埋弧”编辑距离小于等于1的 query，由于”十面埋弧”与”十面埋伏”的编辑距离为1，此时只需考虑边值在1-1到1+1范围的子树，为此”十面埋伏怎么样”作为根结点的子树可以不用继续考虑。</p><p><img src="https://img1.3s78.com/codercto/f8bb8ca7bf3a726b36444038e70663b6" alt="搜索中的 Query 理解及应用"></p><p>对于等长的拼音字型错误类型还可以用 HMM 模型进行召回，HMM 模型主要由初始状态概率、隐藏状态间转移概率及隐藏状态到可观测状态的发射概率三部分组成。如下图所示，将用户输入的错误 query “爱奇义”视为可观测状态，对应的正确 query “爱奇艺”作为隐藏状态，其中正确 query 字词到错误 query 字词的发射关系可以通过人工梳理的同谐音、形近字混淆词表、通过编辑距离度量召回的相近英文单词以及挖掘好的纠错片段对得到。</p><p>至于模型参数，可以将搜索日志中 query 进行采样后作为样本利用 hmmlearn、pomegranate 等工具采用 EM 算法进行无监督训练，也可以简单地对搜索行为进行统计得到，如通过 nltk、srilm、kenlm 等工具统计搜索行为日志中 ngram 语言模型转移概率，以及通过统计搜索点击日志中 query-item 及搜索 session 中 query-query 对齐后的混淆词表中字之间的错误发射概率。训练得到模型参数后，采用维特比算法对隐藏状态序列矩阵进行最大纠错概率求解得到候选纠错序列。</p><p><img src="https://img1.3s78.com/codercto/3d755c2266f6547454f38ee823e7625f" alt="搜索中的 Query 理解及应用"></p><p><img src="https://img1.3s78.com/codercto/37b5d23708ce353df7af68a5ee35a5d6" alt="搜索中的 Query 理解及应用"></p><p>进一步地，我们还可以尝试深度学习模型来充分挖掘搜索点击行为及搜索 session 进行纠错候选召回，如采用 Seq2Seq、Transformer、Pointer-Generator Networks 等模型进行端到端的生成改写，通过引入 attention、copy 等机制以及结合混淆词表进行受限翻译生成等优化，可以比较好地结合上下文进行变长的候选召回。</p><p>另外结合 BERT 等预训练语言模型来进行候选召回也是值得尝试的方向，如在 BERT 等预训练模型基础上采用场景相关的无监督语料继续预训练，然后在错误检测的基础上对错误的字词进行 mask 并预测该位置的正确字词。Google 在2019年提出的 LaserTagger 模型则是另辟蹊径将文本生成建模为序列标注任务，采用 BERT 预训练模型作为 Encoder 基础上预测各个序列位置的增删留标签，同样适用于 query 纠错这种纠错前后大部分文本重合的任务。</p><p><strong>Query 扩展：</strong></p><p>Query 扩展，即通过挖掘 query 间的语义关系扩展出和原 query 相关的 query 列表。Query 列表的结果可以用于扩召回以及进行 query 推荐帮用户挖掘潜在需求，如下图在百度搜索”自然语言处理”扩展出的相关搜索 query：</p><p><img src="https://img1.3s78.com/codercto/d549bd47c8ac93cc6de76b79646fdab7" alt="搜索中的 Query 理解及应用"></p><p>搜索场景有丰富的用户行为数据，我们可以通过挖掘搜索 session 序列和点击下载行为中 query 间的语义关系来做 query 扩展。如用户在进行搜索时，如果对当前搜索结果不满意可能会进行一次或多次 query 变换重新发起搜索，那么同一搜索 session 内变换前后的 query 一般存在一定的相关性，为此可以通过统计互信息、关联规则挖掘等方法来挖掘搜索 session 序列中的频繁共现关系。</p><p>或者把一个用户搜索 session 序列当成文章，其中的每个 query 作为文章的一个词语，作出假设：如果两个 query 有相同的 session 上下文，则它们是相似的，然后通过训练 word2vec、fasttext 等模型将 query 向量化，进而可以计算得到 query 间的 embedding 相似度。</p><p>对于长尾复杂 query，通过 word2vec 训练得到的 embedding 可能会存在 oov 的问题，而 fasttext 由于还考虑了字级别的 ngram 特征输入进行训练，所以除了可以得到 query 粒度的 embedding，还可以得到字、词粒度的 embedding，此时通过对未登录 query 切词后的字、词的 embedding 进行简单的求和、求平均也可以得到 query 的 embedding 表示。还可以参考 WR embedding 的做法进一步考虑不同 term 的权重做加权求平均，然后通过减去主成分的映射向量以加大不同 query 间的向量距离，方法简单却比较有效。</p><p><img src="https://img1.3s78.com/codercto/07632eacae811163dc70f4e40761688a" alt="搜索中的 Query 理解及应用"></p><p>至于搜索点击下载行为，可以通过构建 query-item 的点击下载矩阵，然后采用协同过滤或 SVD 矩阵分解等方法计算 query 之间的相似度，又或者构建 query 和 item 之间的二部图 ( 如下图示例 )，若某个 query 节点和 item 节点之间存在点击或下载行为则用边进行连接，以 ctr、cvr 或归一化的点击下载次数等作为连接边的权重，然后可以训练 swing、simrank/wsimrank++ 等图算法迭代计算 query 间的相似度，也可以采用 Graph Embedding 的方法来训练得到 query 结点间的 embedding 相似度。</p><p>更进一步地，我们还可以利用搜索点击下载行为构造弱监督样本训练基于 CNN/LSTM/BERT 等子网络的 query-item 语义匹配模型得到 query 和 item 的 embedding 表示，如此也可以计算 query pair 间的 embedding 相似度。对于将 query 进行 embedding 向量化的方法，可以先离线计算好已有存量 query 的 embedding 表示，然后用 faiss 等工具构建向量索引，当线上有新的 query 时通过模型 inference 得到对应的 embedding 表示即可进行高效的近邻向量检索以召回语义相似的 query。</p><p><img src="https://img1.3s78.com/codercto/3ab9c07f28025775fa24ccbbb99d8a50" alt="搜索中的 Query 理解及应用"></p><p><img src="https://img1.3s78.com/codercto/9753294223a880b2f54df3440adbcf4a" alt="搜索中的 Query 理解及应用"></p><p>在给用户做搜索 query 推荐时，除了上面提到的跟用户当前输入 query 单点相关 query 推荐之外，还可以结合用户历史搜索行为及画像信息等来预测用户当前时刻可能感兴趣的搜索 query，并在搜索起始页等场景进行推荐展示。此时，可以通过 LSTM 等网络将用户在一段 session 内的搜索行为序列建模为用户 embedding 表示，然后可以通过构建 Encoder-Decoder 模型来生成 query，或采用语义匹配的方法将用户 embedding 及 query embedding 映射到同一向量空间中，然后通过计算 embedding 相似度的方法来召回用户可能感兴趣的 query。</p><p><strong>Query 归一：</strong></p><p>。。。</p><h4 id="②-搜索联想词"><a href="#②-搜索联想词" class="headerlink" title="② 搜索联想词"></a>② 搜索联想词</h4><p>联想词，顾名思义，就是对用户输入 query 进行联想扩展，以减少用户搜索输入成本及输错可能，能比较好地提升用户搜索体验。联想结果主要以文本匹配为主，文本匹配结果不足可以辅以语义召回结果提升充盈率。考虑到用户在输入搜索 query 时意图相对明确，一般会从左到右进行 query 组织，为此基于这个启发式规则，目前联想词中文本匹配召回又以前缀匹配优先。</p><p><img src="https://img1.3s78.com/codercto/64e0ef2b931559ad5e880aa2c6a823e3" alt="搜索中的 Query 理解及应用"></p><p>虽然联想词涉及的是技术主要是简单的文本匹配，在匹配过程中还需要考虑效率和召回质量，同时中文输入可能会有拼音输入的情况 ( 如上图所示 ) 也需要考虑。由于用户在搜索框中输入每一个字时都会发起一起请求，联想词场景的请求 pv 是非常大的。为加速匹配效率，可以通过对历史搜索 query 按 qv 量这些筛选并预处理后分别构建前后缀 trie 树用于对用户线上输入的 query 进行前缀及中后缀匹配召回，然后对召回的结果进行排序，如果是仅简单按 qv 降序排序，可以在 trie 树结点中存放 qv 信息并用最小堆等结构进行 topK 召回。</p><p><img src="https://img1.3s78.com/codercto/27a875679d42db4f154407583f8e0060" alt="搜索中的 Query 理解及应用"></p><p>当然仅按 qv 排序还不够，比如可能还需要考虑用户输入上文 query 后对推荐的下文 query 的点击转化、下文 query 在结果页的点击转化以及 query 的商业化价值等因素。同时一些短 query 召回的结果会非常多，线上直接进行召回排序性能压力较大，为此可以先通过离线召回并进行粗排筛选，再将召回结果写到一些 kv 数据库如 <a href="http://www.codercto.com/category/redis.html">redis</a> 、共享内存等存储供线上直接查询使用。离线召回的话，可以采用 AC 自动机同时进行高效的前中后缀匹配召回。AC 自动机 ( Aho-Corasic ) 是基于 trie 数 + KMP 实现的多模匹配算法，可以在目标文本中查找多个模式串的出现次数以及位置。此时，离线召回大致的流程是：</p><ul><li>从历史搜索 query 中构造前缀 sub-query，如 query “酷我音乐”对应的 sub-query 有中文形式的”酷”、”酷我”、”酷我音”、”酷我音乐”及拼音字符串 “ku”、”kuwo” 等，同时可以加上一些专名实体或行业词，如应用垂搜中的”音乐”、”视频”等功能需求词；</li><li>利用所有的 sub-query 构建 AC 自动机；</li><li>利用构建的 AC 自动机对历史搜索 query 及其拼音形式分别进行多模匹配，从中匹配出所有的前中后缀 sub-query，进而得到 召回候选。</li><li>按照一定策略 ( 一般在前缀基础上 ) 进行候选粗排并写到线上存储。</li><li>线上来一个请求 sub-query，直接查询存储获取召回结果，然后再基于训练的 pctr 预估模型或 pcpm 商业化导向进行重排，此时可以通过引入用户侧、context 侧等特征实现个性化排序。</li></ul><p><img src="https://img.6aiq.com/e/04993c9aea544b6cae17cb6850e634e1.webp?imageView2/2/w/768/format/webp/interlace/1" alt="img"></p><p>召回—&gt;排序—&gt;策略调整</p><p>召回强调快，排序强调准</p><p>召回决定了推荐效果的天花板，那么排序就决定了逼近天花板的程度</p><h2 id="3、检索召回"><a href="#3、检索召回" class="headerlink" title="3、检索召回"></a>3、检索召回</h2><p>搜索词经过分词成多个term，然后通过倒排索引找到包含term的文档集合，这基本是一个求并集的过程，也就是假若文档包含任意term，那么它就会被筛选回来。</p><p>一次搜索很有可能召回一个shard内所有的文档，这可能是一个千万级别的集合。需要对其进行排序，将更相关的结果排在前面。</p><h2 id="4、结果集排序"><a href="#4、结果集排序" class="headerlink" title="4、结果集排序"></a>4、结果集排序</h2><blockquote><p><strong>对召回结果和query词做相似度排序，在最短的时间内，让尽高比例的用户找到符合他需求的内容</strong></p></blockquote><p>针对命中结果集，需要进行搜索层面的粗排，而后通过离线训练的模型进行精排。</p><ol><li><p>粗排阶段根据用户长期兴趣画像召回相关度较高的Item，同时减轻精排阶段压力；</p></li><li><p>精排阶段则根据粗排召回的ItemList，通过离线训练好的排序模型预测CTR，最终下发TopN ItemList作为推荐结果；</p></li></ol><h3 id="粗排"><a href="#粗排" class="headerlink" title="粗排"></a>粗排</h3><ul><li>说明：在召回了大量文档后，一般来说会返回与搜索相关性高的文档，因此必须计算每个文档的相关性，最终排序才知道哪个文档相关性高，所以这个过程无法避免。简单的理解，召回的千万文档集合（假设这么大的话），需要遍历每个文档，计算其包含了哪些term，并根据这些term在文档中的出现次数等信息，计算出文档的相关性得分，而这个计算方法就是tf/idf算法。</li></ul><h3 id="精排"><a href="#精排" class="headerlink" title="精排"></a>精排</h3><ul><li>说明：假设召回的千万文档粗排完成，一般的搜索业务也就可以直接取top N返回了。但是随着业务深入，一定不可能只基于默认相关性排序，而是会根据业务需求订制相关性计算的算法。</li><li>其他：ES支持function_score或者script_score，简单的说就是在粗排的结果集上再执行我们自定义的计算脚本，脚本的输入是文档内容以及粗排阶段基于tf/idf计算的相关性，我们可以自定义算法生成一个新的相关性得分，作为最终排序的依据。</li></ul><h3 id="重排"><a href="#重排" class="headerlink" title="重排"></a>重排</h3><blockquote><p>对精排的结果进行重排</p></blockquote><p>Elasticsearch 内部有几种不同的算法来评估相关度(relevance)：TF-IDF、BM25、BM25F等。但是这种模型 $f(q,d)$ 其实是简单的概率模型，评估 query 在 doc 中出现的位置、次数、频率等因素，但是不能有效的根据用户反馈（点击／停留时间）来持续优化搜索结果。比如说用户搜索了某些关键词，点击了某些结果，而这些结果并不是排在最前面的，但确实是用户最想要的。那有没有什么方法可以使它们排在前面呢？</p><p><strong>一种简单的做法：</strong>离线统计文档的点击率，然后在排序时根据这个点击率进行加权。具体来说，为了更好地将用户的点击行为体现在排序中，依据 query 下 title 的点击率，进行威尔逊平滑和归一化处理，用于对 title 加权。但是，一些无点击的商品将永远得不到展示，为了解决这个问题，引入了 Query 维度的平均点击率作为 bias，保证了点击模型分的合理性。</p><p>但这样笼统的算法不一定适合所有情况。传统的排序方法通过构造相关度函数，按照相关度对于每一个文档进行打分，得分较高的文档，排的位置就靠前。但是，随着相关度函数中特征的增多，使调参变得极其的困难。所以我们自然而然的想到使用机器学习来优化搜索排序，<strong>这就导致了排序学习 LTR （Learning to rank）的诞生</strong>。</p><p>LTR 的优势是：<strong>整合大量复杂特征并自动进行参数调整，自动学习最优参数，降低了单一考虑排序因素的风险；同时，能够通过众多有效手段规避过拟合问题。</strong></p><p><img src="https://gw.alipayobjects.com/zos/skylark/6b9a255c-ef3e-419f-9813-b2d2758cd47b/2018/png/d53cc02f-34d2-462d-b78a-d7510e4b7803.png" alt="d53cc02f-34d2-462d-b78a-d7510e4b7803.png"></p><p>如上图所示，应用交互层发起搜索请求，经过opensearch的基本检索召回500条结果，之后进入rerank阶段，主要依赖LTR算法以用户的历史行为为重要特征做重排序，最终再将排序后的结果返回给用户端。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/112719984">https://zhuanlan.zhihu.com/p/112719984</a></p><p><a href="https://elasticsearch.cn/article/6208">Day 19 - 通过点击反馈优化es搜索结果排序</a></p><p><a href="https://baike.baidu.com/item/点击模型/13677663?fr=aladdin">点击模型-百度百科](https://baike.baidu.com/item/%E7%82%B9%E5%87%BB%E6%A8%A1%E5%9E%8B/13677663?fr=aladdin</a>)</p><p><a href="https://www.infoq.cn/article/build-enterprise-search-scenarios-using-elasticsearch">如何使用 Elasticsearch 构建企业级搜索方案？</a></p><p><a href="https://www.6aiq.com/article/1537369931667?p=1&amp;m=0">洋码头搜索应用架构</a></p><p><a href="https://yuerblog.cc/2018/01/09/elasticsearch-custom/">Elasticsearch搜索订制</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP数据增强方法总结</title>
    <link href="/2020/07/03/2020-07-03-NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <url>/2020/07/03/2020-07-03-NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>在这篇文章中，我将基于我的发现概述当前用于文本数据扩充的方法。</p><ol><li>词汇替代</li><li>反向翻译</li><li>文字表面转换</li><li>随机噪声注入</li><li>实例交叉扩展</li><li>语法树操作</li><li>文字混合</li><li>生成方法</li></ol><p>中文EDA工具</p><span id="more"></span><p>2021.08.13补充《最新综述：用于文本分类的数据增强方法》</p><p><strong>论文标题：</strong></p><p>A Survey on Data Augmentation for Text Classification</p><p><strong>论文链接：</strong></p><p><a href="https://arxiv.org/abs/2107.03158">https://arxiv.org/abs/2107.03158</a></p><p><strong>数据增强方法分类</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFmusIPsxmtmk9xx34XRFQrASyNXS1T6VYkbUB3v5vLZV1L8P4nrwcj9edzWJ6ys1iaSDMnGoEFJQg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom: 67%;" /></p><p><strong>用于文本分类的的数据增强方法集合</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/51jnKjdIjAMmv8N07dZWXER0G2JD77LAmMo3dmWPJCVLSQdav5ib3VpIdojib062zX5VaATDm1NdOQsia3fTu0zow/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" /></p><h1 id="NLP数据扩充技术"><a href="#NLP数据扩充技术" class="headerlink" title="NLP数据扩充技术"></a>NLP数据扩充技术</h1><h2 id="1-词汇替代"><a href="#1-词汇替代" class="headerlink" title="1.词汇替代"></a>1.词汇替代</h2><p>此工作尝试在不更改句子含义的情况下替换文本中出现的单词。</p><ul><li><p><strong>基于同义词库的替换</strong></p><p>在此技术中，我们从句子中抽取一个随机单词，然后使用同义词库将其替换为其同义词。例如，我们可以使用<a href="https://wordnet.princeton.edu/">WordNet</a>数据库中的英语查找同义词，然后执行替换。它是一个人工策划的数据库，具有单词之间的关系。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-wordnet.png" alt=""></p><p><a href="https://arxiv.org/abs/1509.01626">张等。</a>在他们的2015年论文“用于文本分类的字符级卷积网络”中使用了该技术。<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12195/12023">Mueller等。</a>使用类似的策略为其句子相似性模型生成额外的10K训练示例。<a href="https://arxiv.org/abs/1901.11196">Wei等人</a>也使用了这种技术<a href="https://arxiv.org/abs/1901.11196">。</a>作为“轻松数据扩充”论文中四个随机扩充库中的一项技术。</p><p>为了实现，NLTK提供了对WordNet 的编程<a href="https://www.nltk.org/howto/wordnet.html">访问</a>。您也可以使用<a href="https://textblob.readthedocs.io/en/dev/quickstart.html#wordnet-integration">TextBlob API</a>。此外，还有一个名为<a href="http://paraphrase.org/#/download">PPDB</a>的数据库，<a href="http://paraphrase.org/#/download">其中</a>包含数百万个可以通过编程方式下载和使用的复述。</p></li><li><p><strong>词嵌入替换</strong></p><p>在这种方法中，我们采用了经过预训练的词嵌入，例如Word2Vec，GloVe，FastText，Sent2Vec，并使用嵌入空间中最近的邻居词代替句子中某个词。<a href="https://arxiv.org/abs/1909.10351">焦等。</a>在他们的论文“ <em>TinyBert</em> ” <em>中将</em>这种技术与GloVe嵌入一起使用，以改进其语言模型在下游任务上的通用性。<a href="https://www.aclweb.org/anthology/D15-1306.pdf">Wang等。</a>用它来增强学习主题模型所需的推文。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-embedding.png" alt="单词向量最近的邻居"></p><p>例如，您可以将单词替换为最接近的3个单词，并获得文本的三种变体。</p><p><img src="https://amitness.com/images/nlp-aug-embedding-example.png" alt="用词嵌入来增强文本"></p><p>使用Gensim之类的包来访问预先训练的单词向量并获取最近的邻居是很容易的。例如，在这里我们使用在推特上训练的单词向量找到单词“ awesome”的同义词。</p></li></ul><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs clean"># pip install gensim<br><span class="hljs-keyword">import</span> gensim.downloader <span class="hljs-keyword">as</span> api<br><br>model = api.load(<span class="hljs-string">&#x27;glove-twitter-25&#x27;</span>)  <br>model.most_similar(<span class="hljs-string">&#x27;awesome&#x27;</span>, topn=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p>您将获得5个最相似的词以及余弦相似度。</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[(<span class="hljs-symbol">&#x27;amazing</span>&#x27;, <span class="hljs-number">0.9687871932983398</span>),<br> (<span class="hljs-symbol">&#x27;best</span>&#x27;, <span class="hljs-number">0.9600659608840942</span>),<br> (<span class="hljs-symbol">&#x27;fun</span>&#x27;, <span class="hljs-number">0.9331520795822144</span>),<br> (<span class="hljs-symbol">&#x27;fantastic</span>&#x27;, <span class="hljs-number">0.9313924312591553</span>),<br> (<span class="hljs-symbol">&#x27;perfect</span>&#x27;, <span class="hljs-number">0.9243415594100952</span>)]<br></code></pre></td></tr></table></figure><ul><li><p>诸如BERT，ROBERTA和ALBERT之类的<strong>屏蔽语言模型</strong> Transformer模型已使用称为“屏蔽语言模型”的预置任务在大量文本上进行了训练，其中该模型必须根据上下文预测屏蔽词。</p><p>这可以用来扩充一些文本。例如，我们可以使用预训练的BERT模型，对文本的某些部分进行遮罩，然后要求BERT模型为遮罩预测令牌。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-bert-mlm.png" alt="掩蔽词预测"></p><p>因此，我们可以使用遮罩预测来生成文本的变体。与以前的方法相比，生成的文本在语法上更加连贯，因为模型在进行预测时会考虑上下文。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-bert-augmentations.png" alt="使用BERT的文本增强"></p><p>使用诸如<a href="https://huggingface.co/transformers/">转换器之</a>类的开源库很容易实现拥抱脸。您可以设置要替换的令牌<code>&lt;mask&gt;</code>并生成预测。</p></li></ul><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br>nlp = pipeline(<span class="hljs-string">&#x27;fill-mask&#x27;</span>)<br>nlp(<span class="hljs-string">&#x27;This is &lt;mask&gt; cool&#x27;</span>)<br>[&#123;<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.515411913394928</span>,<br>  <span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;&lt;s&gt; This is pretty cool&lt;/s&gt;&#x27;</span>,<br>  <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">1256</span>&#125;,<br> &#123;<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.1166248694062233</span>,<br>  <span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;&lt;s&gt; This is really cool&lt;/s&gt;&#x27;</span>,<br>  <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">269</span>&#125;,<br> &#123;<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.07387523353099823</span>,<br>  <span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;&lt;s&gt; This is super cool&lt;/s&gt;&#x27;</span>,<br>  <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">2422</span>&#125;,<br> &#123;<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.04272908344864845</span>,<br>  <span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;&lt;s&gt; This is kinda cool&lt;/s&gt;&#x27;</span>,<br>  <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">24282</span>&#125;,<br> &#123;<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.034715913236141205</span>,<br>  <span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;&lt;s&gt; This is very cool&lt;/s&gt;&#x27;</span>,<br>  <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">182</span>&#125;]<br></code></pre></td></tr></table></figure><p>但是，此方法的一个警告是，确定要掩盖文本的哪一部分并非易事。您将必须使用启发式方法来确定掩码，否则生成的文本可能不会保留原始句子的含义。</p><ul><li><strong>基于TF-IDF的单词替换</strong><br>此扩展方法由<a href="https://arxiv.org/abs/1904.12848">Xie等人</a>提出<a href="https://arxiv.org/abs/1904.12848">。</a>在无监督数据增强论文中。基本思想是，TF-IDF分数较低的单词是无意义的，因此可以替换而不会影响句子的真实标签。</li></ul><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-tf-idf-word-replacement.png" alt="基于TF-IDF的单词替换"></p><p>通过计算整个文档中单词的TF-IDF得分并取最低得分来选择替换原始单词的单词。您可以在<a href="https://github.com/google-research/uda/blob/master/text/augmentation/word_level_augment.py">此处</a>的原始文件中参考此代码的实现。</p><h2 id="2-反向翻译"><a href="#2-反向翻译" class="headerlink" title="2.反向翻译"></a>2.反向翻译</h2><p>在这种方法中，我们利用机器翻译来释义文本，同时重新训练其含义。<a href="https://arxiv.org/abs/1904.12848">谢等</a>使用此方法来扩充未标记的文本，并仅使用20个标记的示例在IMDB数据集上学习半监督模型。他们的模型优于以前在25,000个带标签的示例上训练的最新模型。</p><p>反向翻译过程如下：</p><ul><li><p>用一些句子（例如英语）并翻译成另一种语言，例如法语</p></li><li><p>将法语句子翻译回英语句子</p></li><li><p>检查新句子是否与我们的原始句子不同。如果是这样，那么我们将这个新句子用作原始文本的增强版本。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-back-translation.png" alt="回译的想法"></p></li></ul><p>您还可以一次使用不同的语言进行反向翻译，以产生更多的变化。如下所示，我们将英语句子翻译成目标语言，然后再将英语翻译成三种目标语言：法语，普通话和意大利语。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-backtranslation-multi.png" alt="多步回译"></p><p>该技术还被用在Kaggle上的“有毒评论分类挑战” 的<a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52557">第一名解决方案</a>中。获胜者将其用于训练数据的扩充以及测试期间，在测试期间，英语句子的预测概率以及使用三种语言（法语，德语，西班牙语）的反向翻译的平均值将得到最终预测。</p><p>对于实施反向翻译，可使用 <code>python translate</code> 包和 <code>textblob</code> 包（少量翻译），或者使用百度翻译或谷歌翻译的api通过python实现。</p><p>反向翻译一般用于机器翻译和<a href="https://liweinlp.com/?p=5000">中文文本纠错任务</a>。</p><h2 id="3-文字表面转换"><a href="#3-文字表面转换" class="headerlink" title="3.文字表面转换"></a>3.文字表面转换</h2><p>这些是使用正则表达式应用的简单模式匹配转换，由<a href="https://arxiv.org/abs/1812.04718">Claude Coulombe</a>在他的论文中介绍。</p><p>在本文中，他提供了一个将言语形式从收缩转变为扩张，反之亦然的例子。我们可以通过应用此生成增强文本。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-contraction.png" alt="文本的收缩和扩展"></p><p>由于转换不应该改变句子的含义，因此我们可以看到，在展开歧义语言形式的情况下，这样做可能会失败：</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-contraction-ambiguity.png" alt="语言形式扩展中的歧义"></p><p>为解决此问题，本文建议我们允许歧义收缩，但跳过歧义扩展。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-contraction-solution.png" alt="解决语言形式扩展中的歧义"></p><p>您可以<a href="https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions">在此处</a>找到英语的收缩列表。为了扩展，您可以使用Python中的<a href="https://github.com/kootenpv/contractions">收缩</a>库。</p><h2 id="4-随机噪声注入"><a href="#4-随机噪声注入" class="headerlink" title="4.随机噪声注入"></a>4.随机噪声注入</h2><p>这些方法的思想是在文本中注入噪声，以便训练的模型对扰动具有鲁棒性。</p><ul><li><p><strong>拼写错误注入</strong><br>在这种方法中，我们向句子中的某些随机单词添加了拼写错误。这些拼写错误可以通过编程方式添加，也可以使用常见拼写错误的映射（例如英语<a href="https://github.com/makcedward/nlpaug/blob/master/model/spelling_en.txt">列表）</a>来添加。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-spelling-example.png" alt="拼写错误注入"></p></li><li><p><strong>QWERTY键盘错误注入</strong><br>此方法尝试模拟在QWERTY布局键盘上键入时由于彼此之间非常接近的键而发生的常见错误。根据键盘距离插入错误。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-keyboard-error-example.png" alt="QUERTY键盘错误注入"></p></li><li><p><strong>Unigram噪声</strong><br>此方法已由<a href="https://arxiv.org/abs/1703.02573">Xie等人使用。</a>和<a href="https://arxiv.org/abs/1904.12848">UDA</a>文件。这个想法是用从字法频率分布中采样的单词进行替换。这个频率基本上是每个单词在训练语料库中出现的次数。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-unigram-noise.png" alt="会标噪声"></p></li><li><p><strong>空白噪声</strong><br>该方法由<a href="https://arxiv.org/abs/1703.02573">Xie等人</a>提出<a href="https://arxiv.org/abs/1703.02573">。</a>在他们的论文中。这个想法是用一个占位符标记代替一些随机词。本文使用“ _”作为占位符标记。在本文中，他们将其用作避免在特定上下文上过度拟合的方法以及语言模型的平滑机制。该技术有助于提高困惑度和BLEU分数。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-blank-noising.png" alt="空白噪声"></p></li><li><p><strong>句子改组</strong><br>这是一种幼稚的技术，我们可以对训练文本中存在的<strong>句子进行改组</strong>以创建增强版本。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-sentence-shuffle.png" alt="句子改组"></p></li><li><p><strong>随机插入</strong><br>该技术由<a href="https://arxiv.org/abs/1901.11196">Wei等人</a>提出<a href="https://arxiv.org/abs/1901.11196">。</a>在他们的论文“轻松数据增强”中。在这种技术中，我们首先从不是停用词的句子中选择一个随机词。然后，找到其同义词，并将其插入句子中的随机位置。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-random-insertion.png" alt="随机插入"></p></li><li><p><strong>随机交换</strong><br>此技术也由<a href="https://arxiv.org/abs/1901.11196">Wei等人</a>提出<a href="https://arxiv.org/abs/1901.11196">。</a>在他们的论文“轻松数据增强”中。想法是随机交换句子中的任何两个单词。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-random-swap.png" alt="随机交换"></p></li><li><p><strong>随机删除</strong><br>该技术也是由<a href="https://arxiv.org/abs/1901.11196">Wei等人</a>提出的<a href="https://arxiv.org/abs/1901.11196">。</a>在他们的论文“轻松数据增强”中。在这种情况下，我们以一定概率p随机删除句子中的每个单词。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-random-deletion.png" alt="随机删除"></p></li></ul><h2 id="5-实例交叉扩展"><a href="#5-实例交叉扩展" class="headerlink" title="5.实例交叉扩展"></a>5.实例交叉扩展</h2><p>这项技术是<a href="https://arxiv.org/abs/1909.11241">Luque</a>在他对TASS 2019的情感分析的论文中引入的。它受到遗传学中发生的染色体交叉操作的启发。<br>在该方法中，一条推文被分为两半，并且两个极性相同（即正/负）的随机推文被互换。假设是，即使结果将是不合语法且语义上不合理的，新文本仍将保留情感。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-instance-crossover.png" alt="实例交叉扩展"></p><p>该技术对准确性没有影响，但有助于提高F1分数，表明该技术可帮助减少诸如Tweet的中性类等少数群体。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-instance-crossover-result.png" alt="实例交叉扩展对F1的影响"></p><h2 id="6-语法树操作"><a href="#6-语法树操作" class="headerlink" title="6.语法树操作"></a>6.语法树操作</h2><p>此技术已在<a href="https://arxiv.org/abs/1812.04718">Coulombe</a>的论文中使用。想法是解析并生成原始句子的依存关系树，使用规则对其进行转换并生成释义的句子。<br>例如，一种不改变句子含义的转换是从主动语态到被动语态的转换，反之亦然。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-syntax-tree-manipulation.png" alt="语音变化的语法树处理"></p><h2 id="7-文字混合"><a href="#7-文字混合" class="headerlink" title="7.文字混合"></a>7.文字混合</h2><p>混合是<a href="https://arxiv.org/abs/1710.09412">张等人</a>介绍的一种简单而有效的图像增强技术<a href="https://arxiv.org/abs/1710.09412">。</a>这是在2017年提出的。想法是将两个随机图像按一定比例组合在一个小批量中，以生成用于训练的合成示例。对于图像，这意味着将两种不同类别的图像像素组合在一起。它是培训期间的一种正规化形式。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-mixup-image.png" alt="视觉的原始混合算法"></p><p><a href="https://arxiv.org/abs/1905.08941">郭等人</a>将此思想带给了自然语言处理<a href="https://arxiv.org/abs/1905.08941">。</a>修改了Mixup以处理文本。他们提出了两种新颖的方法来将Mixup应用于文本：</p><ul><li><p><strong>wordMixup</strong>：<br>在这种方法中，在一个小批量中获取两个随机句子，并将它们零填充到相同的长度。然后，它们的词嵌入按一定比例组合。生成的单词嵌入将传递到常规流程以进行文本分类。对于给定比例的原始文本的两个标签，计算交叉熵损失。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-wordmixup.png" alt="单词嵌入混合"></p></li><li><p><strong>sentMixup</strong>：<br>在此方法中，采用两个句子并将它们零填充为相同的长度。然后，它们的词嵌入通过LSTM / CNN编码器传递，我们将最后的隐藏状态作为句子嵌入。这些嵌入按一定比例组合，然后传递到最终分类层。交叉熵损失是基于给定比例的原始句子的两个标签来计算的。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-sentmixup.png" alt="句子嵌入的混合"></p></li></ul><h2 id="8-生成方法"><a href="#8-生成方法" class="headerlink" title="8.生成方法"></a>8.生成方法</h2><p>此工作尝试在保留班级标签的同时生成其他培训数据。</p><ul><li><p><strong>条件预训练语言模型</strong><br>这项技术由Anaby-Tavor等人首先提出。在他们的论文<a href="https://arxiv.org/abs/1911.03118">“数据不足？深度学习进行救援！</a>。<a href="https://arxiv.org/abs/2003.02245">Kumar等人的</a>最新论文。在多个基于变压器的预训练模型中评估了这个想法。问题表述如下：</p><ul><li><p>将班级标签附加到训练数据中的每个文本</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-generation-training.png" alt="添加SEP和EOS令牌"></p></li><li><p>在修改后的训练数据上微调大型的预训练语言模型（BERT / GPT2 / BART）。对于GPT2，微调任务是生成，而对于BERT，目标将是屏蔽令牌预测。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-gpt2-finetuning.png" alt="在标签和文本上微调GPT-2"></p></li><li><p>使用微调的语言模型，可以通过使用类标签和少量的初始单词作为模型提示来生成新样本。本文使用每个训练文本的3个初始单词，并为训练数据中的每个点生成一个综合示例。</p><p><img src="https://ningshixian.github.io/resources/images/nlp-aug-gpt2.png" alt="使用GPT-2生成新样本"></p></li></ul></li></ul><h1 id="EDA工具"><a href="#EDA工具" class="headerlink" title="EDA工具"></a>EDA工具</h1><ul><li><p>诸如<a href="https://github.com/makcedward/nlpaug">nlpaug</a>和<a href="https://github.com/QData/TextAttack">textattack之</a>类的库提供了简单而一致的API，以便在Python中应用上述NLP数据增强方法。它们与框架无关，可以轻松集成到您的管道中。</p></li><li><p><strong><a href="https://github.com/Asia-Lee/EDA_NLP_for_Chinese">中文语料的EDA数据增强工具</a></strong>：提供了四种简单的操作来进行数据增强，可以防止过拟合，并提高模型的泛化能力。<a href="https://github.com/Asia-Lee/EDA_NLP_for_Chinese#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95">使用方法（推荐）</a></p></li><li><strong><a href="https://github.com/huyingxi/Synonyms/">Synonyms中文近义词工具包</a></strong></li></ul><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>我从文献综述中得出的结论是，这些NLP增强方法中的许多方法都是特定于任务的，并且仅针对某些特定用例研究了它们对性能的影响。系统地比较这些方法并分析它们对许多任务的性能的影响将是一项有趣的研究。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>谢其哲，等。<a href="https://arxiv.org/abs/1904.12848">“用于一致性培训的无监督数据增强”</a></li><li>Claude Coulombe <a href="https://arxiv.org/abs/1812.04718">“通过利用NLP Cloud API使文本数据增强变得简单”</a></li><li>焦小琦，等。<a href="https://arxiv.org/abs/1909.10351">“ TinyBERT：提炼BERT以了解自然语言”</a></li><li>张翔，等。<a href="https://arxiv.org/abs/1509.01626">“用于文本分类的字符级卷积网络”</a></li><li>佛朗哥·卢克（Franco M. Luque）<a href="https://arxiv.org/abs/1909.11241">“ ATALaya在TASS 2019：情感分析的数据增强和强大嵌入”</a></li><li>子ang等。<a href="https://arxiv.org/abs/1703.02573">“数据噪声作为神经网络语言模型中的平滑处理”</a></li><li>郭宏宇，等。<a href="https://arxiv.org/abs/1905.08941">“使用混合来扩充数据以进行句子分类：一项实证研究”</a></li><li>张鸿yi，等。<a href="https://arxiv.org/abs/1710.09412">“混合：超越经验风险最小化”</a></li><li>Varun Kumar等。<a href="https://arxiv.org/abs/2003.02245">“使用预训练的变压器模型进行数据增强”</a></li><li>Jason Wei等。<a href="https://arxiv.org/abs/1901.11196">“ EDA：简单的数据增强技术，可提高文本分类任务的性能”</a></li><li>Ateret Anaby-Tavor等。<a href="https://arxiv.org/abs/1911.03118">“数据不足？深度学习抢救！”</a></li></ul><p>EDA工具论文<a href="https://arxiv.org/abs/1901.11196">《EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》</a></p><p><a href="https://zhpmatrix.github.io/2019/03/08/preprocess-augmentation-in-nlp/">聊一聊，预处理和数据增强技术</a></p><p>相对全面的总结：<a href="https://zhuanlan.zhihu.com/p/75207641">让机器自动生成文本数据—NLP文本数据增强方法简述</a></p><p><a href="https://zhuanlan.zhihu.com/p/76957566?utm_source=qq&amp;utm_medium=social&amp;utm_oi=52727124066304">思考为什么要做预处理，预处理做到什么程度的文章，非常棒。</a></p><p><a href="https://github.com/quincyliang/nlp-data-augmentation">NLP中的数据增强总结，包括多个NLP的具体任务</a></p><p>总结的非常全面的NLP中的数据增强方法：<a href="https://amitness.com/2020/05/data-augmentation-for-nlp/">A Visual Survey of Data Augmentation in NLP</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>数据增强</tag>
      
      <tag>EDA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python操作数据库</title>
    <link href="/2020/06/19/2020-06-19-Python%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A6%82%E4%BD%95%E8%BF%9E%E6%8E%A5/"/>
    <url>/2020/06/19/2020-06-19-Python%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A6%82%E4%BD%95%E8%BF%9E%E6%8E%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="Python操作数据库"><a href="#Python操作数据库" class="headerlink" title="Python操作数据库"></a>Python操作数据库</h1><p>本文主要介绍如何使用pymysql库以及DBUtils库来连接MySQL数据库。</p><h2 id="一-准备"><a href="#一-准备" class="headerlink" title="一 准备"></a>一 准备</h2><h3 id="1-几个概念"><a href="#1-几个概念" class="headerlink" title="1 几个概念"></a>1 几个概念</h3><p><img src="https://marlous.github.io/2019/04/29/Python-%E4%BD%BF%E7%94%A8-pymysql-%E5%BA%93%E6%93%8D%E4%BD%9C-MySQL/%E5%9B%BE1.PNG" alt="几个概念"></p><h3 id="2-几个概念间关系"><a href="#2-几个概念间关系" class="headerlink" title="2 几个概念间关系"></a>2 几个概念间关系</h3><p><img src="https://marlous.github.io/2019/04/29/Python-%E4%BD%BF%E7%94%A8-pymysql-%E5%BA%93%E6%93%8D%E4%BD%9C-MySQL/%E5%9B%BE2.PNG" alt="几个概念间关系"></p><span id="more"></span><h2 id="二-安装模块"><a href="#二-安装模块" class="headerlink" title="二 安装模块"></a>二 安装模块</h2><p>​    Python 程序想要操作数据库，首先需要安装 模块 来进行操作，下面使用pip命令安装PyMSQL模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip3 install pymysql<br></code></pre></td></tr></table></figure><p>如果没有pip3命令那么需要确认环境变量是否有添加，安装完毕后测试是否安装完毕。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pymysql<br>&gt;&gt;&gt;<br><span class="hljs-comment"># 如果没有报错，则表示安装成功</span><br></code></pre></td></tr></table></figure><h2 id="三-Pymysql-基本使用"><a href="#三-Pymysql-基本使用" class="headerlink" title="三 Pymysql 基本使用"></a>三 Pymysql 基本使用</h2><p>连接数据库并执行sql语句的一般流程是：</p><ol><li>建立连接</li><li>获取游标(创建)</li><li>执行SQL语句</li><li>提交事务</li><li>释放资源</li></ol><p>对应到代码上的逻辑为：</p><ol><li>导入相应的Python模块</li><li>使用connect函数连接数据库，并返回一个Connection对象</li><li>通过Connection对象的cursor方法，返回一个Cursor对象</li><li>通过Cursor对象的execute方法执行SQL语句</li><li>如果执行的是查询语句，通过Cursor对象的fetchall语句获取返回结果</li><li>调用Cursor对象的close关闭Cursor</li><li>调用Connection对象的close方法关闭数据库连接</li></ol><p>connection 对象（生成对象的方法参数、对象方法）</p><p><img src="https://marlous.github.io/2019/04/29/Python-%E4%BD%BF%E7%94%A8-pymysql-%E5%BA%93%E6%93%8D%E4%BD%9C-MySQL/%E5%9B%BE2-1.PNG" alt="connection 对象（生成对象的方法参数、对象方法）"></p><p><img src="https://marlous.github.io/2019/04/29/Python-%E4%BD%BF%E7%94%A8-pymysql-%E5%BA%93%E6%93%8D%E4%BD%9C-MySQL/%E5%9B%BE3.PNG" alt="connection 对象（生成对象的方法参数、对象方法）"></p><p>cursor 对象（对象方法、fetch 方法）</p><p><img src="https://marlous.github.io/2019/04/29/Python-%E4%BD%BF%E7%94%A8-pymysql-%E5%BA%93%E6%93%8D%E4%BD%9C-MySQL/%E5%9B%BE4.PNG" alt="cursor 对象（对象方法）"></p><h3 id="3-1-实现"><a href="#3-1-实现" class="headerlink" title="3.1 实现"></a>3.1 实现</h3><p><strong>连接数据库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pymysql<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">connect_mysql</span>():<br>    conn = pymysql.connect(host=<span class="hljs-string">&#x27;127.0.0.1&#x27;</span>，<br>                       port=<span class="hljs-number">3306</span>，<br>                       user=<span class="hljs-string">&#x27;root&#x27;</span>，<br>                       password=<span class="hljs-string">&#x27;123&#x27;</span>，<br>                       database=<span class="hljs-string">&#x27;pooldb&#x27;</span>，<br>                       charset=<span class="hljs-string">&#x27;utf8&#x27;</span>)<br>   <span class="hljs-keyword">return</span> conn<br><br><span class="hljs-comment"># conn.begin：开始事务</span><br><span class="hljs-comment"># conn.commit：提交事务</span><br><span class="hljs-comment"># conn.rollback：回滚事务</span><br><span class="hljs-comment"># conn.cursor：返回一个Cursor对象</span><br><span class="hljs-comment"># conn.autocommit：设置事务是否自动提交</span><br><span class="hljs-comment"># conn.set_character_set：设置字符集编码</span><br><span class="hljs-comment"># conn.get_server_info：获取数据库版本信息</span><br><span class="hljs-comment"># conn.ping(reconnect=True): 测试数据库是否活着，reconnect表示断开与服务器连接后是否重连，连接关闭时抛出异常（一般用来测通断）</span><br></code></pre></td></tr></table></figure><p>PS<em>: 在实际的编程过程中，一般不会直接调用begin、commit和rollback函数，而是通过<code>上下文管理器实现事务的提交与回滚操作</code>。</em></p><p><strong>利用游标操作数据库</strong></p><p>查询类SQL</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">conn = connect_mysql()<br><span class="hljs-keyword">cursor</span> = conn.<span class="hljs-keyword">cursor</span>()    # 创建游标<br><span class="hljs-keyword">sql</span> = <span class="hljs-string">&#x27;select * from test.student where id = %s&#x27;</span><br><span class="hljs-keyword">cursor</span>.<span class="hljs-keyword">execute</span>(<span class="hljs-keyword">sql</span>,args=(<span class="hljs-number">2</span>,))# 参数化查询<br>result = <span class="hljs-keyword">cursor</span>.fetchall()  # 获取游标执行的所有结果<br><span class="hljs-keyword">cursor</span>.<span class="hljs-keyword">close</span>()<br>conn.<span class="hljs-keyword">close</span>()<br></code></pre></td></tr></table></figure><p>非查询类SQL</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">try</span>:<br>    conn = connect_mysql()<br>    cursor = conn.cursor()  <span class="hljs-comment"># 创建游标</span><br>    sql = <span class="hljs-string">r&quot;insert into test.student (id,name,age) VALUES (%s,%s,%s)&quot;</span><br>    cursor.execute(sql, args=(<span class="hljs-number">6</span>,<span class="hljs-string">&#x27;dahl&#x27;</span>, <span class="hljs-number">23</span>,))<br>    conn.commit()    <span class="hljs-comment"># 提交事务</span><br><span class="hljs-keyword">except</span>:<br>    conn.rollback()   <span class="hljs-comment"># 当SQL语句执行失败时，回滚</span><br><span class="hljs-keyword">finally</span>:<br>    <span class="hljs-keyword">if</span> cursor:   <br>        cursor.close()    <span class="hljs-comment"># 关闭游标</span><br>    <span class="hljs-keyword">if</span> conn:<br>        conn.close()   <span class="hljs-comment"># 关闭连接</span><br></code></pre></td></tr></table></figure><p><em>PS: 使用pymysql来连接数据库时，单线程应用完全没有问题，但如果涉及到多线程应用那么就需要加锁，一旦加锁那么连接势必就会排队等待，当请求比较多时，性能就会降低了。因此，实际使用中，通常会使用数据库的连接池技术，来访问数据库达到资源复用的目的。</em></p><h2 id="四-数据库连接池DBUtils"><a href="#四-数据库连接池DBUtils" class="headerlink" title="四 数据库连接池DBUtils"></a>四 数据库连接池DBUtils</h2><p>DBUtils 是一套用于管理数据库连接池的包，并允许对非线程安全的数据库接口进行线程安全包装。它能为高频度高并发的数据库访问提供更好的性能，可以自动管理连接对象的创建和释放。</p><p><img src="https://github.com/dachenzi/StudyNotes/blob/master/Learn_Python_from_Zero/%E7%AC%94%E8%AE%B0/photo/dbutils.png?raw=true" alt="db_utils"></p><p><strong>连接池对性能的提升：</strong></p><ol><li><p>使用连接池可以进行长连接，直接从一个空闲的连接中获取，不需要重新初始化连接，提升获取连接的速度</p></li><li><p>关闭连接的时候，把连接放回连接池，而不是真正的关闭，所以可以减少频繁地打开和关闭连接</p></li></ol><p><em>PS：如果不执行conn.close() 会导致线程不断创建连接，超过了连接池能容纳的最大连接数而报错: <code>pymysql.err.OperationalError</code>，<code>1040</code>， <code>&#39;Too many connections&#39;</code></em></p><p><strong>DBUtils提供两种外部接口：</strong></p><ul><li><code>PersistentDB</code>: 提供线程专用的数据库连接，并自动管理连接。（为每一个线程创建一个）</li><li><code>PooledDB</code>: 提供线程间可共享的数据库连接，并自动管理连接。（常用）</li></ul><p><code>PooledDB</code> 和 <code>PersistentDB</code> 都通过回收数据库连接，且<strong>即使数据库连接中断也能保持稳定性</strong>，从而达到提升数据库访问性能的目的。那么在现实场景中应该如何选择呢？</p><blockquote><p>PooledDB 常用于多线程的情况。如果你的程序<strong>频繁地启动和关闭线程</strong>，最好使用这个;</p><p>PersistentDB 常用于单线程的情况。PersistentDB 会为每个线程创建一个，资源消耗太大。如果你的程序只是在单个线程上进行<strong>频繁的数据库连接</strong>，最好使这个;</p></blockquote><h3 id="4-1-安装"><a href="#4-1-安装" class="headerlink" title="4.1 安装"></a>4.1 安装</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs javascript">pip install <span class="hljs-title class_">DBUtils</span><br>pip install pymysql<br></code></pre></td></tr></table></figure><h3 id="4-2-使用"><a href="#4-2-使用" class="headerlink" title="4.2 使用"></a>4.2 使用</h3><p>连接池对象只初始化一次，一般可以作为模块级代码来确保。</p><h4 id="PersistentDB"><a href="#PersistentDB" class="headerlink" title="PersistentDB"></a>PersistentDB</h4><p>实现了稳定，线程仿射(thread-affine)，持久化的数据库连接。某个线程第一次开启一个数据库连接时，该连接将用于此特定线程。即使在线程中关闭连接，连接也会保持打开状态，以便同一个线程的下一次连接请求直接使用。线程结束时该连接会自动关闭。<a href="https://segmentfault.com/img/bVbnuiV?w=400&amp;h=475">图解地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> DBUtils.PersistentDB <span class="hljs-keyword">import</span> PersistentDB<br><span class="hljs-keyword">import</span> pymysql<br><br>POOL = PersistentDB(<br>    creator=pymysql,<br>    ping=<span class="hljs-number">0</span>,<br>    closeable=<span class="hljs-literal">False</span>,<br>    threadlocal=<span class="hljs-literal">None</span>,<br>    host=<span class="hljs-string">&#x27;10.1.210.33&#x27;</span>,<br>    port=<span class="hljs-number">3306</span>,<br>    user=<span class="hljs-string">&#x27;root&#x27;</span>,<br>    password=<span class="hljs-string">&#x27;1234qwer&#x27;</span>,<br>    database=<span class="hljs-string">&#x27;devops&#x27;</span>,<br>    charset=<span class="hljs-string">&#x27;utf8&#x27;</span><br>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>():<br>    conn = POOL.connection()<br>    cursor = conn.cursor()<br>    cursor.execute(<span class="hljs-string">&#x27;select * from XXX&#x27;</span>)<br>    result = cursor.fetchall()<br>    <span class="hljs-built_in">print</span>(result)<br>    cursor.close()<br>    conn.close()<br>    <br><span class="hljs-keyword">if</span> __name__==<span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    query()<br></code></pre></td></tr></table></figure><p><strong>NOTE:</strong>需要在连接上调用<code>begin()</code>方法明确开启事务。这可以确保a.只在事务完成时才重新打开连接b.连接被同一个线程重用时回滚。</p><h4 id="PooledDB"><a href="#PooledDB" class="headerlink" title="PooledDB"></a>PooledDB</h4><p><code>DBUtils.PooledDB</code> 实现了稳定、线程安全的缓存连接池。<a href="https://segmentfault.com/img/bVbnuiY?w=730&amp;h=515">图解地址</a></p><p>相关配置参数说明：</p><div class="table-container"><table><thead><tr><th>PooledDB 参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mincached</td><td>初始化时，链接池中至少创建的空闲的链接，0表示不创建</td></tr><tr><td>maxcached</td><td>连接池中允许的闲置的最多连接数量(缺省值 0和None不限制)</td></tr><tr><td>maxshared</td><td>允许的最大共享连接数(0或None表示所有连接都是专用的)</td></tr><tr><td>maxconnections</td><td>连接池允许的最大连接数，0和None表示不限制连接数</td></tr><tr><td>blocking</td><td>连接池中如果没有可用连接后，是否阻塞等待。True：等待；False：抛出异常</td></tr><tr><td>maxusage</td><td>单个连接的最大允许复用次数(缺省值 0 或 None 代表不限制的复用).当达到最大数时，连接会自动重新连接(关闭和重新打开)</td></tr><tr><td>setsession</td><td>开始会话前执行的命令列表</td></tr><tr><td>ping</td><td>如果<code>ping()</code>方法可用，该值表示何时使用ping()方法检查连接(`0 = None = never)</td></tr><tr><td>closeable</td><td>如果设置为True，将允许手动close()连接，默认为False，忽略关闭连接的操作，只在线程终止时自动关闭</td></tr></tbody></table></div><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment">#!/usr/bin/env python3</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br>import pymysql<br><span class="hljs-keyword">from</span> DBUtils.PooledDB import PooledDB， SharedDBConnection<br><span class="hljs-built_in"></span><br><span class="hljs-built_in">POOL </span>= PooledDB(<br>    <span class="hljs-attribute">creator</span>=pymysql，<br>    <span class="hljs-attribute">host</span>=self.host，<br>    <span class="hljs-attribute">port</span>=self.port，<br>    <span class="hljs-attribute">user</span>=self.user，<br>    <span class="hljs-attribute">password</span>=self.password，<br>    <span class="hljs-attribute">db</span>=self.db，<br>    <span class="hljs-attribute">charset</span>=<span class="hljs-string">&quot;utf8&quot;</span>，<br>    <span class="hljs-attribute">mincached</span>=2，<br>    <span class="hljs-attribute">maxcached</span>=20，<br>    <span class="hljs-attribute">maxconnections</span>=0，<br>    <span class="hljs-attribute">blocking</span>=<span class="hljs-literal">True</span><br>)<br><br>def query():<br>    try:<br>        conn = POOL.connection()<br>        <br>        # 必须先调用begin来开启一个事务<br>        conn.begin()<br>        with conn.cursor() as cursor:<br>            cursor.execute(sql, values)<br>            res = cursor.fetchall()<br>        <br>        # 这里commit之后才会真正提交给数据库<br>        conn.commit()<br>        conn.close()<br>    except Exception as e:<br>    conn.rollback()<br>        raise Exception(<span class="hljs-string">&quot;连接或执行失败: &quot;</span> + str(e))<br>    finally:<br>        cursor.close()<br>        conn.close()<br></code></pre></td></tr></table></figure><p>NOTE: 对于不再使用的连接，调用close()方法回收到连接池。</p><p>NOTE: <strong>需要在连接上调用begin()方法明确开启事务</strong>。这可以确保：a.只在事务完成时才重新打开连接；b.连接被同一个线程重用时回滚；c.连接不会被其他线程共享</p><h3 id="4-3-遇到的问题"><a href="#4-3-遇到的问题" class="headerlink" title="4.3 遇到的问题"></a>4.3 遇到的问题</h3><ol><li>正常情况下，mysql的设置的timeout是8个小时(28800秒)，也就是说，如果一个连接8个小时都没有操作，那么mysql会主动的断开连接，当这个连接再次尝试查询的时候就会报个”<code>2013, &#39;Lost connection to MySQL server during query&#39;</code>错误”</li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">解决办法:<br>https:<span class="hljs-regexp">//</span>www.yangyanxing.com<span class="hljs-regexp">/article/</span>connect_short_problem.html<br><br><span class="hljs-number">1</span>. 解决的方法有两种，既然这里的超时是由于在规定时间内没有任何操作导致mysql主动的将链接关闭，pymysql的connection对象有一个ping()方法，可以检查连接是否有效，在每次执行查询操作之前先执行一下ping()方法,该方法默认的有个reconnect参数，默认是True，如果失去连接了会重连。<br><br><span class="hljs-number">2</span>. 还有一种方法是使用连接池，连接池中保持着指定数量的可用连接，每次重新获取一个有效的连接进行查询操作,pymysql本身不具有连接池功能，需要借住DBUtils<br></code></pre></td></tr></table></figure><ol><li>很多使用DBUtils.PooledDB模块建立连接池，使用cursor()来建立多线程连接，在执行SQL查询的时候会报错：<strong>2014， “Commands out of sync; you can’t run this command now”</strong></li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">究其原因是：<br><span class="hljs-number">1</span>、查询结果未释放，然又执行查询；<br><span class="hljs-number">2</span>、两次查询之间没有存储结果应该重复使用connection()，而不是cursor()；<br><br>更多请参考官网：https:<span class="hljs-regexp">//</span>cito.github.io<span class="hljs-regexp">/DBUtils/</span>UsersGuide.html。 <br></code></pre></td></tr></table></figure><h3 id="4-4-思维导图总结"><a href="#4-4-思维导图总结" class="headerlink" title="4.4 思维导图总结"></a>4.4 思维导图总结</h3><p><img src="https://upload-images.jianshu.io/upload_images/2887744-ec0ce39f74b53f83.png?imageMogr2/auto-orient/strip|imageView2/2/w/1024/format/webp" alt="img"></p><p>模式一</p><p><img src="https://upload-images.jianshu.io/upload_images/2887744-167d70d5325c6a5c.png?imageMogr2/auto-orient/strip|imageView2/2/w/1052/format/webp" alt="img"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>DBUtils-数据库连接池：<a href="https://www.jianshu.com/p/f94bc9ce0842">https://www.jianshu.com/p/f94bc9ce0842</a></p><p>数据库连接池DBUtils使用  <a href="https://cloud.tencent.com/developer/article/1578187">https://cloud.tencent.com/developer/article/1578187</a></p><p>Python DBUtils 连接池对象 (PooledDB)   <a href="https://my.oschina.net/u/4357988/blog/3347902">https://my.oschina.net/u/4357988/blog/3347902</a></p><p>《python DbUtils 使用教程》<a href="https://cloud.tencent.com/developer/article/1568031">https://cloud.tencent.com/developer/article/1568031</a></p><p>《python数据库连接工具DBUtils》<a href="https://segmentfault.com/a/1190000017952033">https://segmentfault.com/a/1190000017952033</a></p><p><a href="https://www.pythonf.cn/read/56464">https://www.pythonf.cn/read/56464</a></p><p>一个简单易懂的博客<a href="https://www.cnblogs.com/zhuminghui/p/10930846.html">https://www.cnblogs.com/zhuminghui/p/10930846.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 最佳实践</title>
    <link href="/2020/06/19/2020-06-19-Python%20%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    <url>/2020/06/19/2020-06-19-Python%20%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
    
    <content type="html"><![CDATA[<h1 id="Python-最佳实践"><a href="#Python-最佳实践" class="headerlink" title="Python 最佳实践"></a>Python 最佳实践</h1><p>Python 日常使用的最佳实践，包括：</p><ul><li>Python环境搭建</li><li>项目工程的架构</li><li>单文件的结构</li><li>pythonic代码风格</li><li>优质项目代码</li><li>文档</li><li>测试</li><li>日志记录</li><li>配置文件和数据</li></ul><span id="more"></span><h2 id="安装Python"><a href="#安装Python" class="headerlink" title="安装Python"></a>安装Python</h2><ol><li>从Python的官方网站下载Python 3.7对应的或<a href="https://www.python.org/ftp/python/3.8.0/python-3.8.0.exe">32位安装程序</a></li><li>然后，运行下载的exe安装包。特别要注意勾上<code>Add Python 3.7 to PATH</code>，然后点“Install Now”即可完成安装。</li></ol><h2 id="底层虚拟环境-virtualenv"><a href="#底层虚拟环境-virtualenv" class="headerlink" title="底层虚拟环境 virtualenv"></a>底层虚拟环境 virtualenv</h2><p><a href="https://learnku.com/docs/python-guide/2018/virtualenvs-lower-level-virtualenv/3257">参考链接1</a> <a href="https://pythonguidecn.readthedocs.io/zh/latest/writing/structure.html">参考链接2</a></p><blockquote><p> <a href="http://pypi.python.org/pypi/virtualenv">virtualenv</a> 是一个 Python 项目依赖管理工具</p><p> 建议在开发项目时使用virtualenv做依赖隔离，便于使用pip freeze自动生成requirements文件</p></blockquote><p>通过 pip 安装 virtualenv ：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>pip install virtualenv<br><span class="hljs-variable">$ </span>virtualenv --version<br></code></pre></td></tr></table></figure><p>为项目创建一个虚拟环境，名叫<code>my_project</code>：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs powershell"><span class="hljs-variable">$</span> <span class="hljs-built_in">cd</span> my_project_folder<br><span class="hljs-variable">$</span> virtualenv my_project<br></code></pre></td></tr></table></figure><p>virtualenv 会创建一个文件夹，其中包含使用 Python 项目所有所需的可执行文件。</p><p>开始使用虚拟环境前，需要先激活：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">source</span> my_project/bin/activate<span class="hljs-comment"># linux</span></span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">my_project\Scripts\activate<span class="hljs-comment"># windows</span></span><br></code></pre></td></tr></table></figure><p>安装包的话就与往常一样，如：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs php">$ pip install XXX<br></code></pre></td></tr></table></figure><p>如果你在虚拟环境中暂时完成了工作，可以这样停用它：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs php">$ deactivate<br></code></pre></td></tr></table></figure><p>为了保持环境的一致性，“冻结” 当前环境包的状态是正确的选择：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">$ pip <span class="hljs-keyword">freeze</span> &gt; requirements.txt<br></code></pre></td></tr></table></figure><p>该命令将创建一个 requirements.txt 文件，里面包含有当前环境所有包的简单列表及对应的版本，这样就能完全搭建出与之前一致的环境了：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">$ pip <span class="hljs-keyword">install</span> -r requirements.txt<br></code></pre></td></tr></table></figure><p>这样有助于在跨设备，跨部署，跨人员的情况下保证环境的一致性。</p><p>最后，记得将虚拟环境文件夹从源代码控制中排除，也就是将其添加到 ignore 列表中 ( 详见 <a href="http://docs.python-guide.org/en/latest/writing/gotchas/#version-control-ignores">Version Control Ignores</a>).</p><h2 id="写出优雅的-Python-代码"><a href="#写出优雅的-Python-代码" class="headerlink" title="写出优雅的 Python 代码"></a>写出优雅的 Python 代码</h2><h3 id="结构化您的工程"><a href="#结构化您的工程" class="headerlink" title="结构化您的工程"></a>结构化您的工程</h3><p>工程化的项目目录具备让项目跑起来的所有基本内容。它里边会包含你的<strong>项目文件布局、自动化测试代码，模组，以及安装脚本</strong>。当你建立一个新项目的时候，只要把这个目录复制过去，改改目录的名字，再编辑里边的文件就行了。</p><p>“结构化”意味着通过编写简洁的代码，并且正如文件系统中文件和目录的组织一样， 代码应该具有逻辑和依赖清晰。</p><p>目的：为了防止各个模块的依赖混乱，一般通过模块划分，对Python项目进行结构化。之后，就只剩下架构性的工作，包括设计、实现项目各个模块，并整理清他们之间 的交互关系。</p><h3 id="项目架构-amp-仓库"><a href="#项目架构-amp-仓库" class="headerlink" title="项目架构&amp;仓库"></a>项目架构&amp;仓库</h3><p>简单的仓库结构模板： <a href="https://github.com/kennethreitz/samplemod">可以在GitHub上找到</a> 。</p><div class="table-container"><table><thead><tr><th>布局</th><th>作用</th></tr></thead><tbody><tr><td>README.rst</td><td>项目介绍</td></tr><tr><td>LICENSE</td><td>许可证. 法律相关</td></tr><tr><td>setup.py</td><td>安装、部署、打包的脚本-分发管理<br/>使用 <code>python setup.py install</code> 安装</td></tr><tr><td>requirements.txt</td><td>项目所需的依赖库</td></tr><tr><td>sample/__init__.py<br />sample/core.py<br/>sample/helpers.py<br/>sample/setting.py<br />sample/configs/<br />sample/data/<br />sample/static/<br />sample/templates/</td><td>初始化应用并组合所有其它组件<br />核心代码/具体代码<br/>helpers 工具模块<br />setting 用来作变量和常量的初始化<br />configs 配置文件包<br />data 数据文件包<br />包括了公共CSS， Javascript等<br />放置应用的Jinja2模板</td></tr><tr><td>docs/conf.py<br/>docs/index.rst</td><td>项目的参考文档</td></tr><tr><td>tests/</td><td>包的集合和单元测试<br />上下文环境的文件 context.py -方便测试导包</td></tr><tr><td>Makefile</td><td>通用的管理任务</td></tr></tbody></table></div><p><strong>Makefile 模板：</strong></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs subunit">init:<br>    pip install -r requirements.txt<br><br><span class="hljs-keyword">test:</span><br><span class="hljs-keyword">    </span>py.test tests<br><br>.PHONY: init test<br></code></pre></td></tr></table></figure><p><strong>setup.py 模板：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> setuptools <span class="hljs-keyword">import</span> setup, find_packages<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;README.rst&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    readme = f.read()<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;LICENSE&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    license = f.read()<br>setup(<br>    name=<span class="hljs-string">&#x27;slot-extract&#x27;</span>,<br>    version=<span class="hljs-string">&#x27;0.1.0&#x27;</span>,<br>    description=<span class="hljs-string">&#x27;龙小湖对话机器人-槽位提取服务&#x27;</span>,<br>    long_description=readme,<br>    author=<span class="hljs-string">&#x27;Ning Shixian&#x27;</span>,<br>    author_email=<span class="hljs-string">&#x27;&#x27;</span>,<br>    url=<span class="hljs-string">&#x27;http://git.longhu.net/ningshixian/slot-extract&#x27;</span>,<br>    license=license,<br>    packages=find_packages(exclude=(<span class="hljs-string">&#x27;tests&#x27;</span>, <span class="hljs-string">&#x27;docs&#x27;</span>))<br>)<br></code></pre></td></tr></table></figure><h3 id="Python中global的用法"><a href="#Python中global的用法" class="headerlink" title="Python中global的用法"></a>Python中global的用法</h3><p>global 是python中的一个关键字，作用在变量上，该关键字通常放在函数块中，用来声明该变量为全局变量。</p><p>例如下面变量a，定义在函数外面的是全局变量a，定义在fun函数里面的a是另一个a，是局部变量a，两者没有任何关系。好比这个地区有个叫张三的人，公办室里有个另一个叫张三的人。他们是两个不同的人。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-number">10</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fun</span>():<br>    a = <span class="hljs-number">2</span><br>fun()<br><span class="hljs-built_in">print</span>(a) <span class="hljs-comment"># 输出 10</span><br></code></pre></td></tr></table></figure><p>如果想要函数里面的那个a就代表外面的全局变量a，那么就要将函数里面的a 用关键字 global 声明为全局变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-number">10</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fun</span>():<br>    <span class="hljs-keyword">global</span> a<br>    a = <span class="hljs-number">2</span><br>fun()<br><span class="hljs-built_in">print</span>(a)  <span class="hljs-comment"># 输出 2</span><br></code></pre></td></tr></table></figure><h2 id="单个文件结构"><a href="#单个文件结构" class="headerlink" title="单个文件结构)"></a><a href="[https://marlous.github.io/2019/04/03/Python-%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%E7%BB%84%E7%BB%87/](https://marlous.github.io/2019/04/03/Python-软件项目文件结构组织/">单个文件结构</a>)</h2><p><img src="https://marlous.github.io/2019/04/03/Python-%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%E7%BB%84%E7%BB%87/%E5%9B%BE1.PNG" alt="单个文件结构"></p><h3 id="包"><a href="#包" class="headerlink" title="包"></a>包</h3><p>Python提供非常简单的包管理系统，即简单地将模块管理机制扩展到一个目录上(目录扩 展为包)。</p><p><strong>任意包含 <code>__init__.py</code> 文件的目录都被认为是一个Python包</strong>。导入一个包里不同 模块的方式和普通的导入模块方式相似，特别的地方是 <code>__init__.py</code> 文件将集合 所有包范围内的定义。</p><h3 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h3><p><strong>Python模块对应的是一个<code>.py</code> 文件</strong>，是最主要的抽象层之一。抽象层允许将代码分为 不同部分，每个部分包含相关的数据与功能。</p><p>例如在项目中，一层控制<em>用户操作</em> 相关接口，另一层<em>处理底层数据</em> 操作。最自然分开这两 层的方式是，在一份文件里重组所有功能接口，并将所有底层操作封装到另一个文件中。这种情况下，接口文件需要导入封装底层操作的文件，可通过 <code>import</code> 和 <code>from ... import</code> 语句完成。一旦您使用 import 语句，就可以使用这个模块。</p><h3 id="类"><a href="#类" class="headerlink" title="类"></a>类</h3><p>包含函数、变量。类中有属性和方法。一个对象就是一个类的实例。</p><h3 id="可变和不可变类型"><a href="#可变和不可变类型" class="headerlink" title="可变和不可变类型"></a>可变和不可变类型</h3><p>Python提供两种内置或用户定义的类型。<strong>数字、字符串、元组是不可变的</strong>，<strong>列表、字典是可变的</strong>。</p><p>对不可变类型的变量重新赋值，实际上是重新创建一个不可变类型的对象，并将原来的变量重新指向新创建的对象（如果没有其他变量引用原有对象的话（即引用计数为0），原有对象就会被回收）。</p><p> 字符串是不可变类型。这意味着当需要组合一个 字符串时，将每一部分放到一个可变列表里，使用字符串时再组合 (‘join’) 起来的做法更高效。 </p><p><strong>差</strong></p><figure class="highlight hsp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs hsp"><span class="hljs-meta"># 创建将<span class="hljs-number">0</span>到<span class="hljs-number">19</span>连接起来的字符串 (例 <span class="hljs-string">&quot;012..1819&quot;</span>)</span><br>nums = <span class="hljs-string">&quot;&quot;</span><br><span class="hljs-keyword">for</span> n in range(<span class="hljs-number">20</span>):<br>    nums += <span class="hljs-keyword">str</span>(n)   <span class="hljs-meta"># 慢且低效</span><br><span class="hljs-keyword">print</span> nums<br></code></pre></td></tr></table></figure><p><strong>好</strong></p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs axapta"><span class="hljs-meta"># 创建将0到19连接起来的字符串 (例 &quot;012..1819&quot;)</span><br>nums = [<span class="hljs-built_in">str</span>(n) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">20</span>)]<br><span class="hljs-keyword">print</span> <span class="hljs-string">&quot;&quot;</span>.<span class="hljs-keyword">join</span>(nums)<br></code></pre></td></tr></table></figure><h3 id="函数的参数"><a href="#函数的参数" class="headerlink" title="函数的参数"></a>函数的参数</h3><p>函数的参数可以使用四种不同的方式传递给函数。</p><ol><li><strong>必选参数</strong> 是没有默认值的必填的参数 <code>point(x, y)</code></li><li><strong>关键字参数</strong> 是非强制的，且有默认值  <code>point(x, y, z=None)</code></li><li><strong>任意参数列表</strong> 如果函数的参数数量是动态的，该函数可以被定义成 <code>*args</code> 的结构 <code>send(message, *args)</code></li><li><strong>任意关键字参数字典</strong> 如果函数要求一系列待定的命名参数，我们可以使用 <code>**kwargs</code> 的结构。在函数体中， kwargs 是一个字典，它包含所有传递给函数但没有被其他关键字参数捕捉的命名参数</li></ol><h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h3><p>当一个函数在其正常运行过程中有多个主要出口点时，它会变得难以调试其返回结果，所以保持单个出口点可能会更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">complex_function</span>(<span class="hljs-params">a, b, c</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> a:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># 抛出一个异常可能会更好</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> b:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># 抛出一个异常可能会更好</span><br><br>    <span class="hljs-comment"># 一些复杂的代码试着用 a,b,c 来计算x</span><br>    <span class="hljs-comment"># 如果成功了，抵制住返回 x 的诱惑</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> x:<br>        <span class="hljs-comment"># 使用其他的方法来计算出 x</span><br>    <span class="hljs-keyword">return</span> x  <span class="hljs-comment"># 返回值 x 只有一个出口点有利于维护代码</span><br></code></pre></td></tr></table></figure><h3 id="pythonic风格"><a href="#pythonic风格" class="headerlink" title="pythonic风格"></a>pythonic风格</h3><h4 id="代码风格"><a href="#代码风格" class="headerlink" title="代码风格:"></a>代码风格:</h4><ol><li><p>每个缩进层级使用4个空格</p></li><li><p>每行最多79个字符</p></li><li><p>顶层函数或类的定义之间空两行（特别容易漏，漏的话，是报E302 expected 2 blank lines, found 1）</p></li><li><p>采用ASCII或者UTF-8编码文件</p></li><li><p>每条import导入一个模块，导入放在代码顶端，导入顺序是先标准库，第三方库，本地库</p></li><li><p>小括号，大括号，中括号之间的逗号没有额外的空格</p></li><li><p>类命名采用骆驼命名法，CamelCase;函数用小写字符</p></li><li><p>函数命名使用小写字符，例如xxx_xxx_xxx; 用下划线开头定义私有的属性或方法，如_xxx</p></li></ol><h4 id="命名风格"><a href="#命名风格" class="headerlink" title="命名风格:"></a>命名风格:</h4><ol><li><p>类名使用 UpperCamelCase 风格，必须遵从驼峰形式，如：XmlService</p></li><li><p>方法名、参数名、成员变量、局部变量都统一使用 lowerCamelCase 风格，必须遵从驼峰形式：localValue / getHttpMessage()</p></li><li><p>常量命名全部大写，单词间用下划线隔开</p></li><li><p>包名统一使用小写、单数、_</p></li><li><p>命名时，使用尽量完整的单词组合来表达其意</p></li><li><p>方法命名规约：</p><pre><code class="hljs">1） 获取单个对象的方法用 get 做前缀。   2） 获取多个对象的方法用 list 做前缀。   3） 获取统计值的方法用 count 做前缀。   4） 插入的方法用 save/insert 做前缀。   5） 删除的方法用 remove/delete 做前缀。   6） 修改的方法用 update 做前缀。</code></pre></li><li><p>异常处理</p><p>对大段代码进行 try-catch，这是不负责任的表现。<br>捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之<br>finally 块必须对资源对象、流对象进行关闭<br>日志文件推荐至少保存 15 天</p></li><li><p>单元测试</p><p>保证测试粒度足够小，方法级别<br>必须使用 assert 来验证<br>编写单元测试代码遵守 BCDE 原则，以保证被测试模块的交付质量</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-variable">B</span>： <span class="hljs-variable">Border</span>，边界值测试，包括循环边界、特殊取值、特殊时间点、数据顺序等。<br><span class="hljs-built_in">C</span>： <span class="hljs-variable">Correct</span>，正确的输入，并得到预期的结果。<br><span class="hljs-built_in">D</span>： <span class="hljs-variable">Design</span>，与设计文档相结合，来编写单元测试。<br><span class="hljs-built_in">E</span>： <span class="hljs-variable">Error</span>，强制错误信息输入（如：非法数据、异常流程、非业务允许输入等），并得到预期的结果。<br></code></pre></td></tr></table></figure></li><li><p>MySQL 数据库</p><p>表达是与否概念的字段，必须使用 is_xxx 的方式命名，数据类型是 unsigned tinyint<br>表名、字段名必须使用小写字母或数字，如：aliyun_admin<br>小数类型为 decimal，禁止使用 float 和 double<br>varchar 是可变长字符串，不预先分配存储空间<br>表必备三字段： id, gmt_create, gmt_modified<br>表的命名最好是加上“业务名称_表的作用”    </p></li></ol><h1 id="阅读优质的代码"><a href="#阅读优质的代码" class="headerlink" title="阅读优质的代码"></a>阅读优质的代码</h1><ul><li><a href="https://github.com/gleitz/howdoi">Howdoi</a> Howdoi 使用 Python 实现的代码搜索工具。</li><li><a href="https://github.com/mitsuhiko/flask">Flask</a> Flask 是基于 Werkzeug and Jinja2 的 Python 微框架。 它的目的是快速入门并开发实现你头脑中的好主意。</li><li><a href="https://github.com/python-diamond/Diamond">Diamond</a> Diamond 是使用 python 实现的用于收集监控数据的工具，主要收集 metrics 类型的数据，并将其发布到 Graphite 或其他后台。它能够收集 cpu ， 内存， 网络， i/o ，负载和磁盘 metrics 数据。此外，它还提供 API 用以实现自定义收集器从任意来源中收集指标数据。</li><li><a href="https://github.com/mitsuhiko/werkzeug">Werkzeug</a> Werkzeug 最初是 WSGI 应用程序的各种实用工具的简单集合，并已成为最先进的 WSGI 实用程序模块之一。它包括强大的调试器、功能齐全的请求和响应对象、处理实体标记的 HTTP 实用程序、缓存控制头、HTTP 日期、cookie 处理、文件上传、强大的 URL 路由系统和一群社区贡献的插件模块。</li><li><a href="https://github.com/kennethreitz/requests">Requests</a> Requests 是一个用 Python 实现的 Apache2 授权的 HTTP 库供大家使用。</li><li>Tablib 是用 Python 实现的无格式的表格数据集库。</li></ul><h1 id="项目文档"><a href="#项目文档" class="headerlink" title="项目文档"></a>项目文档</h1><p>建议提供相关函数的更多信息，包括它是做什么的， 所抛的任何异常，返回的内容或参数的相关细节。</p><p>通常称为 <a href="http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html">Numpy style</a> Docstrings</p><h1 id="代码测试"><a href="#代码测试" class="headerlink" title="代码测试"></a>代码测试</h1><p>py.test 测试工具功能完备，并且可扩展，语法简单</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>pip install pytest<br><span class="hljs-variable">$ </span>pytest tests.py<br></code></pre></td></tr></table></figure><h1 id="日志记录"><a href="#日志记录" class="headerlink" title="日志记录"></a>日志记录</h1><p><a href="https://www.jianshu.com/p/df0343c59d74">python项目的日志体系构建实现</a></p><p><a href="https://www.jianshu.com/p/2c0fb3e1103d">Python日志最佳实践</a></p><p>日志记录一般有两个目的：</p><ul><li><strong>诊断日志</strong> 记录与应用程序操作相关的日志。例如，当用户遇到程序报错时， 可通过搜索诊断日志以获得上下文信息。</li><li><strong>审计日志</strong> 为商业分析而记录的日志。从审计日志中，可提取用户的交易信息， 并结合其他用户资料构成用户报告，或者用来作为优化商业目标的数据支撑。</li></ul><p>Python的logging模块提供了通用的日志系统，包括logger，handler，filter，formatter这四个方法：</p><ol><li>logger提供日志接口，供应用代码使用。<br> logger最长用的操作有两类：配置和发送日志消息。可以通过logging.getLogger(name)获取logger对象，如果不指定name则返回root对象，多次使用相同的name调用getLogger方法返回同一个logger对象。</li><li>handler<br> 将日志记录（log record）发送到合适的目的地（destination）,比如文件，socket等。一个logger对象可以通过addHandler方法添加0到多个handler，每个handler又可以定义不同日志级别，以实现日志分级过滤显示。</li><li>filter<br> 提供一种优雅的方式决定一个日志记录是否发送到handler。</li><li>formatter<br> 指定日志记录输出的具体格式。formatter的构造方法需要两个参数：消息的格式字符串和日期字符串，这两个参数都是可选的。</li></ol><p>总的一个逻辑是这样的，我们需要创建一个logger,这样在代码执行中，可以使用logger.debug/info/warning/error等方法，将日志输出到指定的位置（可以是控制台、文件，可多选）；创建logger之后，需要给logger添加处理句柄addHandler()，每个句柄就对应一种输出，比如StreamHandler表示输出到控制台（当然我这里简单化了），FileHandler输出到指定文件。然后给句柄设置格式Format，这就是你想要看见的格式。</p><h2 id="1-logger的定义"><a href="#1-logger的定义" class="headerlink" title="1. logger的定义"></a>1. logger的定义</h2><p>Logging.Logger：Logger是Logging模块的主体，进行以下三项工作：</p><ol><li>为程序提供记录日志的接口</li><li>判断日志所处级别，并判断是否要过滤</li><li>根据其日志级别将该条日志分发给不同handler<br> 常用函数有：<br> <code>Logger.setLevel()</code>设置日志级别<br> <code>Logger.addHandler()</code> 和<code>Logger.removeHandler()</code>添加和删除一个Handler<br> <code>Logger.addFilter()</code>添加一个Filter,过滤作用</li></ol><h2 id="2-handler的使用"><a href="#2-handler的使用" class="headerlink" title="2. handler的使用"></a>2. handler的使用</h2><p>所有的handler汇总，按需自取</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs cpp">StreamHandler：logging.StreamHandler；日志输出到流，可以是sys.stderr，sys.stdout或者文件<br>FileHandler：logging.FileHandler；日志输出到文件<br>BaseRotatingHandler：logging.handlers.BaseRotatingHandler；基本的日志回滚方式<br>RotatingHandler：logging.handlers.RotatingHandler；日志回滚方式，支持日志文件最大数量和日志文件回滚<br>TimeRotatingHandler：logging.handlers.TimeRotatingHandler；日志回滚方式，在一定时间区域内回滚日志文件<br>SocketHandler：logging.handlers.SocketHandler；远程输出日志到TCP/IP sockets<br>DatagramHandler：logging.handlers.DatagramHandler；远程输出日志到UDP sockets<br>SMTPHandler：logging.handlers.SMTPHandler；远程输出日志到邮件地址<br>SysLogHandler：logging.handlers.SysLogHandler；日志输出到syslog<br>NTEventLogHandler：logging.handlers.NTEventLogHandler；远程输出日志到Windows NT/<span class="hljs-number">2000</span>/XP的事件日志<br>MemoryHandler：logging.handlers.MemoryHandler；日志输出到内存中的指定buffer<br>HTTPHandler：logging.handlers.HTTPHandler；通过<span class="hljs-string">&quot;GET&quot;</span>或者<span class="hljs-string">&quot;POST&quot;</span>远程输出到HTTP服务器<br></code></pre></td></tr></table></figure><h2 id="3-Formater的使用"><a href="#3-Formater的使用" class="headerlink" title="3. Formater的使用"></a>3. Formater的使用</h2><p>format：指定输出的格式和内容，format可以输出很多有用的信息</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs ruby">参数：作用<br><span class="hljs-string">%(levelno)</span>s：打印日志级别的数值<br><span class="hljs-string">%(levelname)</span>s：打印日志级别的名称<br><span class="hljs-string">%(pathname)</span>s：打印当前执行程序的路径，其实就是sys.argv[<span class="hljs-number">0</span>]<br><span class="hljs-string">%(filename)</span>s：打印当前执行程序名<br><span class="hljs-string">%(funcName)</span>s：打印日志的当前函数<br><span class="hljs-string">%(lineno)</span>d：打印日志的当前行号<br><span class="hljs-string">%(asctime)</span>s：打印日志的时间<br><span class="hljs-string">%(thread)</span>d：打印线程<span class="hljs-variable constant_">ID</span><br><span class="hljs-string">%(threadName)</span>s：打印线程名称<br><span class="hljs-string">%(process)</span>d：打印进程<span class="hljs-variable constant_">ID</span><br><span class="hljs-string">%(message)</span>s：打印日志信息<br></code></pre></td></tr></table></figure><p>举个例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">console_fmt = <span class="hljs-string">&#x27;%(asctime)-15s [%(TASK)s] %(message)s&#x27;</span><br>console_formatter = logging.Formatter(console_fmt)<br></code></pre></td></tr></table></figure><h2 id="4-Level级别"><a href="#4-Level级别" class="headerlink" title="4. Level级别"></a>4. Level级别</h2><p>level：设置日志级别，默认为logging.WARNNING；</p><p>可以给logger和handler设置不同的级别，比如logger设置为debug，然后handler1设置为error,这样这个输出只会记录error以上的日志级别信息，另一个handler2设置为info,记录info级别以上的信息。有下面这几种可以设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs undefined">日志等级：使用范围<br><br>FATAL：致命错误<br>CRITICAL：特别糟糕的事情，如内存耗尽、磁盘空间为空，一般很少使用<br>ERROR：发生错误时，如IO操作失败或者连接问题<br>WARNING：发生很重要的事件，但是并不是错误时，如用户登录密码错误<br>INFO：处理请求或者状态变化等日常事务<br>DEBUG：调试过程中使用DEBUG等级，如算法中每个循环的中间状态<br>NOTSET：如果设置了日志级别为NOTSET,那么这里可以采取debug、info的级别的内容也可以显示在控制台上了<br></code></pre></td></tr></table></figure><h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">import logging<br>import auxiliary_module<br><br># 创建名为&#x27;spam_application&#x27;的记录器<br>logger = logging.get<span class="hljs-constructor">Logger(&#x27;<span class="hljs-params">spam_application</span>&#x27;)</span><br>logger.set<span class="hljs-constructor">Level(<span class="hljs-params">logging</span>.DEBUG)</span><br><br># 创建级别为DEBUG的日志处理器<br>fh = logging.<span class="hljs-constructor">FileHandler(&#x27;<span class="hljs-params">spam</span>.<span class="hljs-params">log</span>&#x27;)</span><br>fh.set<span class="hljs-constructor">Level(<span class="hljs-params">logging</span>.DEBUG)</span><br><br># 创建级别为ERROR的控制台日志处理器<br>ch = logging.<span class="hljs-constructor">StreamHandler()</span><br>ch.set<span class="hljs-constructor">Level(<span class="hljs-params">logging</span>.ERROR)</span><br><br># 创建格式器，加到日志处理器中<br>formatter = logging.<span class="hljs-constructor">Formatter(&#x27;%(<span class="hljs-params">asctime</span>)</span>s - %(name)s - %(levelname)s - %(message)s&#x27;)<br>fh.set<span class="hljs-constructor">Formatter(<span class="hljs-params">formatter</span>)</span><br>ch.set<span class="hljs-constructor">Formatter(<span class="hljs-params">formatter</span>)</span><br><br>logger.add<span class="hljs-constructor">Handler(<span class="hljs-params">fh</span>)</span><br>logger.add<span class="hljs-constructor">Handler(<span class="hljs-params">ch</span>)</span><br><br>logger.debug(&#x27;debug message&#x27;)<br>logger.info(&#x27;info message&#x27;)<br>logger.warn(&#x27;warn message&#x27;)<br>logger.error(&#x27;error message&#x27;)<br>logger.critical(&#x27;critical message&#x27;)<br></code></pre></td></tr></table></figure><h1 id="配置文件和数据"><a href="#配置文件和数据" class="headerlink" title="配置文件和数据"></a>配置文件和数据</h1><ol><li>如框架无特殊规定，配置文件应放置于<strong>项目根目录下的<code>config</code>文件夹中</strong></li><li>配置文件在部署、预发布、生产环境、开发环境等环境中会有很大差异，因此请<strong>不要将配置文件在上传到git、svn等版本库中</strong>， 而是建议在版本库中上传一个配置的<strong>示例文件</strong>（如：config.example）</li><li>上传到版本库中配置示例文件<strong>不允许出现密码、证书、token等敏感信息</strong></li><li>数据和程序应该尽量分离，不要将数据写在代码中，需要持久化存储数据必须使用数据库</li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://learnku.com/docs/python-guide/2018"><strong>Python 最佳实践指南 2018</strong></a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FastAPI 学习手册</title>
    <link href="/2020/06/16/2020-06-16-FastAPI%20%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/"/>
    <url>/2020/06/16/2020-06-16-FastAPI%20%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/</url>
    
    <content type="html"><![CDATA[<p>在这篇教程里，我们会一起搭建一个用于生产环境的 <a href="https://fastapi.tiangolo.com/">FastAPI</a> 服务器。写好之后，整个应用技术栈会是这样的：</p><p><img src="https://python-gino.org/docs/zh/master/_images/gino-fastapi.svg" alt="../_images/gino-fastapi.svg"></p><span id="more"></span><h1 id="FastAPI-简介"><a href="#FastAPI-简介" class="headerlink" title="FastAPI 简介"></a>FastAPI 简介</h1><p>官方文档：<a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com/</a></p><p>FastAPI是一种现代，快速（高性能）的Web框架，用于基于标准Python类型提示使用Python 3.6+构建API。</p><p>主要功能是：</p><ul><li><strong>快速</strong>：非常高的性能，看齐NodeJS和Go。<strong>现有最快的Python框架之一。</strong></li><li><strong>快速编码</strong>：将功能开发速度提高约200％至300％。</li><li><strong>更少的错误</strong>：减少约40％的人为错误（开发人员）。</li><li><strong>直观</strong>：强大的编辑器支持。完成无处不在。调试时间更少。</li><li><strong>简易</strong>：旨在易于使用和学习。减少阅读文档的时间。</li><li><strong>短</strong>：最小化代码重复。每个参数声明中的多个功能。更少的错误。</li><li><strong>健壮</strong>：获取可用于生产的代码。具有自动交互式文档。</li><li><strong>基于标准</strong>：基于（并完全兼容）API的开放标准：<a href="https://github.com/OAI/OpenAPI-Specification">OpenAPI</a>（以前称为Swagger）和<a href="http://json-schema.org/">JSON Schema</a>。</li></ul><h1 id="FastAPI-教程"><a href="#FastAPI-教程" class="headerlink" title="FastAPI 教程"></a>FastAPI 教程</h1><h3 id="FastAPI-基础学习"><a href="#FastAPI-基础学习" class="headerlink" title="FastAPI 基础学习"></a>FastAPI 基础学习</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/12807379.html">FastApi 基础学习(一) 概述</a> </p><p><a href="https://www.cnblogs.com/mazhiyong/p/12808529.html">FastApi 基础学习(二) 开发环境安装及交互式API文档 </a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12808825.html">FastApi 基础学习(三) Pydantic 做类型强制检查</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12871349.html">FastAPI 基础学习(四) 路径参数</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12900239.html">FastAPI 基础学习(五) 请求参数</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12902149.html">FastAPI 基础学习(六) Request Body(I)</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12905387.html">FastAPI 基础学习(七) Request Body(II)</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12906795.html">FastAPI 基础学习(八) 参数附加信息 (一)</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12911618.html">FastAPI 基础学习(九) 参数附加信息 (二)</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12923751.html">FastAPI 基础学习(十) Pydantic复杂模型</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12928963.html">FastAPI 基础学习(十一) 复杂数据类型</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12930555.html">FastAPI 基础学习(十二) Cookie操作</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12930938.html">FastAPI 基础学习(十三) Header操作</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12957121.html">FastAPI 基础学习(十四) Response自定义状态码</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13345076.html">FastAPI 基础学习(十五) 直接使用Request</a></p><h3 id="FastAPI-Response"><a href="#FastAPI-Response" class="headerlink" title="FastAPI Response"></a>FastAPI Response</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/12942241.html">FastAPI Response(一) Response模型</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13279543.html">FastAPI Response(二) 直接返回Response对象</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13293036.html">FastAPI Response(三) 定制化的Response</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13298832.html">FastAPI Response(四) 高级定制的Response</a></p><h3 id="FastAPI-中间件"><a href="#FastAPI-中间件" class="headerlink" title="FastAPI 中间件"></a>FastAPI 中间件</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/12988001.html">FastAPI 中间件(一) 自定义中间件</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12988413.html">FastAPI 中间件(二) 高级中间件</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12987619.html">FastAPI 中间件(三) 跨域资源共享中间件</a></p><h3 id="FastAPI-依赖注入系统"><a href="#FastAPI-依赖注入系统" class="headerlink" title="FastAPI 依赖注入系统"></a>FastAPI 依赖注入系统</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/13050551.html">FastAPI 依赖注入系统(一) 简介</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13066065.html">FastAPI 依赖注入系统(二) 依赖项类</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13066271.html">FastAPI 依赖注入系统(三) 子依赖项</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13072728.html">FastAPI 依赖注入系统(四) 基于路径操作装饰器的依赖项</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13073754.html">FastAPI 依赖注入系统(五) 带有yield功能的依赖项</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13323133.html">FastAPI 依赖注入系统(六) 可参数化的依赖项</a></p><h3 id="FastAPI-安全机制"><a href="#FastAPI-安全机制" class="headerlink" title="FastAPI 安全机制"></a>FastAPI 安全机制</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/13086051.html">FastAPI 安全机制(一) 简介 </a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13214643.html">FastAPI 安全机制(二) 基于OAuth2和JWT的Token认证机制(一)生成token</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13219300.html">FastAPI 安全机制(三) 基于OAuth2和JWT的Token认证机制(二)用户登陆及验证</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13328363.html">FastAPI 安全机制(四) OAuth2 scopes</a> </p><h3 id="FastAPI-数据库访问"><a href="#FastAPI-数据库访问" class="headerlink" title="FastAPI 数据库访问"></a>FastAPI 数据库访问</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/13225738.html">FastAPI 数据库访问（一）使用SQLAlchemy访问关系数据库</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13267119.html">FastAPI 数据库访问（二）使用SQLAlchemy异步访问关系数据库 </a></p><h3 id="FastAPI-进阶知识"><a href="#FastAPI-进阶知识" class="headerlink" title="FastAPI 进阶知识"></a>FastAPI 进阶知识</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/12957658.html">FastAPI 进阶知识(一) 表单数据</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12965542.html">FastAPI 进阶知识(二) JSON兼容编码</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/12965973.html">FastAPI 进阶知识(三) 错误处理</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13273293.html">FastAPI 进阶知识(四) 后台任务</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13370669.html">FastAPI 进阶知识(五) 子应用</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13372006.html">FastAPI 进阶知识(六) 启动-关闭事件</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13433214.html">FastAPI 进阶知识(七) 在Request中存储附加信息</a></p><h3 id="FastAPI-WebSockets"><a href="#FastAPI-WebSockets" class="headerlink" title="FastAPI WebSockets"></a>FastAPI WebSockets</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/13371186.html">FastAPI WebSockets</a></p><h3 id="FastAPI-工程管理"><a href="#FastAPI-工程管理" class="headerlink" title="FastAPI 工程管理"></a>FastAPI 工程管理</h3><p><a href="https://www.cnblogs.com/mazhiyong/p/13384373.html">FastAPI 工程管理(一) 工程目录管理</a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13375949.html">FastAPI 工程管理(二) 工程设置 </a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13384785.html">FastAPI 工程管理(三) 工程部署 </a></p><p><a href="https://www.cnblogs.com/mazhiyong/p/13446743.html">FastAPI 工程管理(四) 工程示例</a></p><h1 id="FastAPI异步-async-await"><a href="#FastAPI异步-async-await" class="headerlink" title="FastAPI异步 async/await"></a>FastAPI异步 async/await</h1><blockquote><p>无论你是否使用async，FastAPI都会采用异步的方式处理。</p><p><strong>但是，如果你定义了async函数，函数体却是同步的调用，将导致函数执行过程变成串行。</strong></p></blockquote><p><strong>async使用指导：</strong></p><ul><li>如果这个请求比较慢，比如连接数据库读取数据、文件IO、rpc调用等一般加上async，我想普通项目里大部分请求都会多多少少跟数据库打交道，所以加的往往比较多；</li><li>求稳不求快的，那就定义为普通函数 <code>def</code> （因为它可以采用多线程的方式解决）</li><li>分不清加不加就不加，毕竟async是要开销的。如果定义了async函数，里面却是同步的调用，那么这将慢的是灾难！；</li></ul><p><strong>异步编程：</strong></p><p>Asynchronous Code之所以称为“异步”，是因为计算机/程序不必与速度较慢的任务“同步”，而是在等待中去执行其它工作。文中作者详细列举了相对于cpu和内存操作来说，慢的多的操作如下：</p><ul><li>网络通信</li><li>文件读写</li><li>远程api调用如rpc调用</li><li>数据库操作和查询</li></ul><p>异步是相对同步编程来说的，在执行顺序上并没有改变。实际效果就是同一台机器如果有比较多的慢操作使用异步编程可以提高吞吐率。这里再吐槽一下，大部分语言比如python、csharp、go等的async都是协程实现的，只有java是线程实现的。代价：进程&gt;线程&gt;协程，java不是不能用协程来实现完全是因为历史包袱，线程相对也不会丢失太多的性能。</p><p><strong>异步实验代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> fastapi <span class="hljs-keyword">import</span> FastAPI<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-keyword">import</span> os<br><br>app = FastAPI()<br><br><span class="hljs-meta">@app.get(<span class="hljs-params"><span class="hljs-string">&quot;/async_slowest&quot;</span></span>)</span><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">async_slowest</span>():<br>   time.sleep(<span class="hljs-number">1</span>)<br>   <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;message&quot;</span>: <span class="hljs-string">&quot;异步模式，但是同步执行sleep函数，执行过程是串行的&quot;</span>&#125;<br>  <br><span class="hljs-meta">@app.get(<span class="hljs-params"><span class="hljs-string">&quot;/async_sleep_in_thread&quot;</span></span>)</span><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">async_sleep_in_thread</span>():<br>   loop = asyncio.get_event_loop()<br>   <span class="hljs-keyword">await</span> loop.run_in_executor(<span class="hljs-literal">None</span>, time.sleep, <span class="hljs-number">1</span>)<br>   <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;message&quot;</span>: <span class="hljs-string">&quot;线程池中运行sleep函数&quot;</span>&#125;<br>  <br><span class="hljs-meta">@app.get(<span class="hljs-params"><span class="hljs-string">&quot;/async_sleep&quot;</span></span>)</span><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">async_sleep</span>():<br>   <span class="hljs-keyword">await</span> asyncio.sleep(<span class="hljs-number">1</span>)<br>   <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;message&quot;</span>: <span class="hljs-string">&quot;异步模式，且异步执行sleep函数&quot;</span>&#125;<br><br><span class="hljs-meta">@app.get(<span class="hljs-params"><span class="hljs-string">&quot;/sync&quot;</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sync_sleep</span>():<br>   time.sleep(<span class="hljs-number">1</span>)<br>   <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;message&quot;</span>: <span class="hljs-string">&quot;同步模式，但是FastAPI会放在线程池中运行，所以很快&quot;</span>&#125;<br></code></pre></td></tr></table></figure><p>我们用ab工具，总量100，并发100进行测试。</p><p>这4个函数，最慢的就是第一个async_slowest。我们可以看到，它几乎是一个接一个的串联输出。原因是：fastapi框架会将async函数会放到event loop中运行。如果函数没有运行或有await，则其他函数无法运行。所以这里是一个串联的效果，总时间需要100s</p><p>为了解决这个问题，第二个函数引入了一个线程池中去运行，所以不会出现阻塞。这个函数，1秒可以全部运行完。</p><p>第三个函数是最正宗的实现。它使用异步的sleep取代了原版同步的sleep。这也是最快的实现。1秒可以运行完</p><p>第四个函数是唯一一个不是async的普通函数。它的运行时间是多少呢？我的电脑是3秒运行完！为什么？这就是fastapi精彩的地方。前面提到，async函数会放到event loop中执行。那么，普通的函数会放到哪里呢？答案是，放到thread pool中。那么为什么是3秒呢。这是因为我的电脑是逻辑8核。线程池的默认配置是核数*5，所以是40线程。我的测试是100个并发，所以一共是3秒完成。40-&gt;40-&gt;20</p><h1 id="FastAPI-vs-Flask"><a href="#FastAPI-vs-Flask" class="headerlink" title="FastAPI vs. Flask"></a>FastAPI vs. Flask</h1><p><a href="https://amitness.com/2020/06/fastapi-vs-flask/#data-validation">二者的区别在哪？</a></p><ol><li><p>FastAPI是最快的Python框架之一，其性能比Flask高出300％以上：</p><p><img src="https://ningshixian.github.io/resources/images/web.png" alt=""></p><p>在总体延迟和吞吐量方面，FastAPI都更具优势！</p></li><li><p>对异步具有本机支持</p><p>Flask（专为诸如Gunicorn之类的WSGI服务器设计）不具有本地异步支持；而FastAPI建立在ASGI服务器Uvicorn的基础上，使运行异步事件循环（对传入请求进行计数）变得容易。</p></li><li><p>转换成本低</p><p>就语法而言，它在设计上与Flask非常相似，从Flask到FastAPI的最初过渡只需要极少的重写量</p></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://calmcode.io/fastapi/hello-world.html"><img src="https://calmcode.io/images/fastapi.svg" alt="..." style="zoom:25%;" /> fastapi.</a></p><p><a href="https://blog.csdn.net/qq_31989047/article/details/104531860">fastapi教程翻译（搬运自用）</a></p><p><a href="https://my.oschina.net/u/4326175/blog/4255332/print">FastAPI框架入门 基本使用, 模版渲染, form表单数据交互, 上传文件, 静态文件配置</a></p><p><a href="https://github.com/tiangolo/fastapi/issues/731">How can I prevent “307 Temporary Redirect” while accessing FastAPI via an Android Emulator on local machine #731 </a></p><p><a href="https://www.pythonf.cn/read/82324">当碰到<strong>URL结尾包含斜杠“/”</strong>这种情况，会让客户端重定向到相应没有尾斜杠的URI（也有可能会返回301 - 用来资源重定向）</a></p><p><a href="[https://deathfeeling.gitee.io/myblog/2020/03/25/FastAPI%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/](https://deathfeeling.gitee.io/myblog/2020/03/25/FastAPI项目实战/">FastAPI项目实战</a>)</p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FastAPI工程部署Gunicorn+Uvicorn</title>
    <link href="/2020/06/16/2020-06-16-FastAPI%E5%B7%A5%E7%A8%8B%E9%83%A8%E7%BD%B2Gunicorn+Uvicorn/"/>
    <url>/2020/06/16/2020-06-16-FastAPI%E5%B7%A5%E7%A8%8B%E9%83%A8%E7%BD%B2Gunicorn+Uvicorn/</url>
    
    <content type="html"><![CDATA[<ul><li>Uvicorn</li><li>Gunicorn</li><li>Supervisor</li></ul><span id="more"></span><h1 id="Gunicorn"><a href="#Gunicorn" class="headerlink" title="Gunicorn"></a>Gunicorn</h1><ol><li>什么是 Gunicorn ？</li></ol><p>答：<a href="https://gunicorn.org/">Gunicorn (‘Green Unicorn’)</a> 是一个 UNIX 下的纯 Python WSGI 服务器。好的，那这是什么意思呢？</p><ul><li>Gunicorn 启动了被分发到的一个主线程，然后因此产生的子线程就是对应的 worker。</li><li>主进程的作用是确保 worker 数量与设置中定义的数量相同。因此如果任何一个 worker 挂掉，主线程都可以通过分发它自身而另行启动。</li><li>worker 的角色是处理 HTTP 请求。</li><li>这个 预 in 预分发 就意味着主线程在处理 HTTP 请求之前就创建了 worker。</li><li>操作系统的内核就负责处理 worker 进程之间的负载均衡。</li></ul><p>它没有其它依赖，容易安装和使用。它所在的位置通常是在反向代理（如 Nginx）或者 负载均衡（如 AWS ELB）和一个 web 应用（比如 Django 或者 Flask）之间.</p><ol><li>什么是WSGI？</li></ol><p>Wsgi是同步通信服务规范，客户端请求一项服务，并等待服务完成，只有当它收到服务的结果时，它才会继续工作。当然了，可以定义一个超时时间，如果服务在规定的时间内没有完成，则认为调用失败，调用方继续工作。</p><p><img src="https://www.godhearing.cn/posts/17778/image-20210120124037533.png" alt="img" style="zoom:33%;" /></p><h2 id="Gunicorn参数说明"><a href="#Gunicorn参数说明" class="headerlink" title="Gunicorn参数说明"></a><a href="https://www.itnotebooks.com/?p=531">Gunicorn参数说明</a></h2><ul><li>安装</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> gunicorn<br>pip <span class="hljs-keyword">install</span> gevent<br></code></pre></td></tr></table></figure><ul><li>启动参数说明</li></ul><div class="table-container"><table><thead><tr><th><code>-c CONFIG, --config=CONFIG</code></th><th></th><th>指定配置文件</th></tr></thead><tbody><tr><td><code>-b BIND, --bind=BIND</code></td><td></td><td>绑定运行的主机加端口</td></tr><tr><td><code>-w INT, --workers INT</code></td><td></td><td>用于处理工作进程的数量，整数，默认为1</td></tr><tr><td><code>-k STRTING, --worker-class STRTING</code></td><td></td><td>要使用的工作模式，默认为sync异步，类型：sync, eventlet, gevent, tornado, gthread, gaiohttp</td></tr><tr><td><code>--threads INT</code></td><td></td><td>处理请求的工作线程数，使用指定数量的线程运行每个worker。为正整数，默认为1</td></tr><tr><td><code>--worker-connections INT</code></td><td></td><td>最大客户端并发数量，默认1000</td></tr><tr><td><code>--backlog int</code></td><td></td><td>等待连接的最大数，默认2048</td></tr><tr><td><code>-p FILE, --pid FILE</code></td><td></td><td>设置pid文件的文件名，如果不设置将不会创建pid文件</td></tr><tr><td><code>--access-logfile FILE</code></td><td></td><td>日志文件路径</td></tr><tr><td><code>--access-logformat STRING</code></td><td></td><td>日志格式，<code>--access_log_format &#39;%(h)s %(l)s %(u)s %(t)s&#39;</code></td></tr><tr><td><code>--error-logfile FILE, --log-file FILE</code></td><td></td><td>错误日志文件路径</td></tr><tr><td><code>--log-level LEVEL</code></td><td></td><td>日志输出等级</td></tr><tr><td><code>--limit-request-line INT</code></td><td></td><td>限制HTTP请求行的允许大小，默认4094。取值范围0~8190，此参数可以防止任何DDOS攻击</td></tr><tr><td><code>--limit-request-fields INT</code></td><td></td><td>限制HTTP请求头字段的数量以防止DDOS攻击，与limit-request-field-size一起使用可以提高安全性。默认100，最大值32768</td></tr><tr><td><code>--limit-request-field-size INT</code></td><td></td><td>限制HTTP请求中请求头的大小，默认8190。值是一个整数或者0，当该值为0时，表示将对请求头大小不做限制</td></tr><tr><td><code>-t INT, --timeout INT</code></td><td></td><td>超过设置后工作将被杀掉并重新启动，默认30s，nginx默认60s</td></tr><tr><td><code>--reload</code></td><td></td><td>在代码改变时自动重启，默认False</td></tr><tr><td><code>--daemon</code></td><td></td><td>是否以守护进程启动，默认False</td></tr><tr><td><code>--chdir</code></td><td></td><td>在加载应用程序之前切换目录</td></tr><tr><td><code>--graceful-timeout INT</code></td><td></td><td>默认30，在超时(从接收到重启信号开始)之后仍然活着的工作将被强行杀死；一般默认</td></tr><tr><td><code>--keep-alive INT</code></td><td></td><td>在keep-alive连接上等待请求的秒数，默认情况下值为2。一般设定在1~5秒之间</td></tr><tr><td><code>--spew</code></td><td></td><td>打印服务器执行过的每一条语句，默认False。此选择为原子性的，即要么全部打印，要么全部不打印</td></tr><tr><td><code>--check-config</code></td><td></td><td>显示当前的配置，默认False，即显示</td></tr><tr><td><code>-e ENV, --env ENV</code></td><td></td><td>设置环境变量</td></tr></tbody></table></div><ul><li>配置文件示例</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 并行进程数</span><br><span class="hljs-attr">workers</span> = <span class="hljs-number">4</span><br> <br><span class="hljs-comment"># 指定每个工作的线程数</span><br><span class="hljs-attr">threads</span> = <span class="hljs-number">2</span><br><br><span class="hljs-comment"># worker超时时间，超时重启</span><br><span class="hljs-attr">timeout</span> = <span class="hljs-number">30</span><br><br><span class="hljs-comment"># 监听端口8000</span><br><span class="hljs-attr">bind</span> = <span class="hljs-string">&#x27;0.0.0.0:8000&#x27;</span><br> <br><span class="hljs-comment"># 守护进程,将进程交给supervisor管理</span><br><span class="hljs-attr">daemon</span> = <span class="hljs-string">&#x27;false&#x27;</span><br> <br><span class="hljs-comment"># 工作模式协程</span><br><span class="hljs-comment"># worker_class = &#x27;gevent&#x27;</span><br><span class="hljs-attr">worker_class</span> = <span class="hljs-string">&quot;uvicorn.workers.UvicornWorker&quot;</span><br> <br><span class="hljs-comment"># 最大并发量</span><br><span class="hljs-attr">worker_connections</span> = <span class="hljs-number">1000</span><br> <br><span class="hljs-comment"># 进程文件</span><br><span class="hljs-attr">pidfile</span> = <span class="hljs-string">&#x27;/var/run/gunicorn.pid&#x27;</span><br> <br><span class="hljs-comment"># 访问日志和错误日志</span><br><span class="hljs-attr">accesslog</span> = <span class="hljs-string">&#x27;/var/log/gunicorn_acess.log&#x27;</span><br><span class="hljs-attr">errorlog</span> = <span class="hljs-string">&#x27;/var/log/gunicorn_error.log&#x27;</span><br> <br><span class="hljs-comment"># 日志级别</span><br><span class="hljs-attr">loglevel</span> = <span class="hljs-string">&#x27;info&#x27;</span><br></code></pre></td></tr></table></figure><ul><li>启动</li></ul><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stata">gunicorn -c gunicorn.<span class="hljs-keyword">conf</span> main:<span class="hljs-keyword">app</span><br></code></pre></td></tr></table></figure><h2 id="关于如何配置-Gunicorn-的实用建议"><a href="#关于如何配置-Gunicorn-的实用建议" class="headerlink" title="关于如何配置 Gunicorn 的实用建议"></a><a href="https://juejin.cn/post/6844903850713825287">关于如何配置 Gunicorn 的实用建议</a></h2><p>为了提高使用 Gunicorn 时的性能，我们必须牢记 3 种并发方式：</p><ol><li><p>第一种并发方式（workers 模式，又名 UNIX 进程模式）</p><p>每个 <code>worker</code> 都是一个加载 Python 应用程序的 UNIX 进程。<code>worker</code>之间没有共享内存。</p><p>建议的 <code>workers</code> 数量是 <code>(2*CPU)+1</code>。</p><p>对于一个双核（两个CPU）机器，5 就是建议的 <code>worker</code> 数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">gunicorn --workers=<span class="hljs-number">5</span> main:app<br></code></pre></td></tr></table></figure></li><li><p>第二种并发方式（多线程）</p><p>Gunicorn 还允许每个 worker 拥有多个线程。在这种场景下，Python 应用程序每个 worker 都会加载一次，同一个 worker 生成的每个线程共享相同的内存空间。</p><p>为了在 Gunicorn 中使用多线程。我们使用了 <code>threads</code> 模式。每一次我们使用 <code>threads</code> 模式，worker 的类就会是 <code>gthread</code>：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">gunicorn <span class="hljs-attribute">--workers</span>=5 <span class="hljs-attribute">--threads</span>=2 main:app<br></code></pre></td></tr></table></figure><p>在我们的例子里面最大的并发请求数就是 <code>worker * 线程</code>，也就是10。</p><p>在使用 <code>worker</code> 和多线程模式时建议的最大并发数量仍然是<code>(2*CPU)+1</code>。</p></li><li><p>第三种并发方式（“伪线程”）</p><p>有一些 Python 库比如（<code>gevent 和 Asyncio</code>）可以在 Python 中启用多并发。那是基于协程实现的“伪线程”。</p><p>Gunicrn 允许通过设置对应的 worker 类来使用这些异步 Python 库。</p><p>这里的设置适用于我们想要在单核机器上运行的<code>gevent</code>：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">gunicorn <span class="hljs-attribute">--worker-class</span>=gevent <span class="hljs-attribute">--worker-connections</span>=1000 <span class="hljs-attribute">--workers</span>=3 main:app<br></code></pre></td></tr></table></figure><p>在这种情况下，最大的并发请求数量是 <code>3000</code>。（<code>3 个 worker * 1000 个连接/worker</code>）</p></li></ol><h2 id="Uvicorn与Gunicorn一起使用"><a href="#Uvicorn与Gunicorn一起使用" class="headerlink" title="Uvicorn与Gunicorn一起使用"></a>Uvicorn与Gunicorn一起使用</h2><p>Uvicorn包括一个Gunicorn工人类，它使您可以运行ASGI应用程序，同时具有Uvicorn的所有性能优势，同时还为您提供Gunicorn的全功能进程管理。</p><p>在生产环境中，Guicorn 大概是最简单的方式来管理 Uvicorn 了，生产环境部署我们推荐使用 Guicorn 和 Uvicorn 的 worker 类，这样的话，可以动态增加或减少进程数量，平滑地重启工作进程，或者升级服务器而无需停机</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">gunicorn</span> example:app -w <span class="hljs-number">4</span> -k uvicorn.workers.UvicornWorker<br></code></pre></td></tr></table></figure><h1 id="Uvicorn"><a href="#Uvicorn" class="headerlink" title="Uvicorn"></a>Uvicorn</h1><ol><li>什么是 Uvicorn ？</li></ol><p>答：Uvicorn 是基于 uvloop 和 httptools 构建的非常快速的 <strong>ASGI 服务器</strong>。</p><ol><li>什么是 uvloop 和 httptools ？</li></ol><p>答：uvloop 用于替换标准库 asyncio 中的事件循环，使用 Cython 实现，它非常快，可以使 asyncio 的速度提高 2-4 倍。asyncio 不用我介绍吧，写异步代码离不开它。httptools 是 nodejs HTTP 解析器的 Python 实现。</p><ol><li>什么是ASGI？</li></ol><p>Asgi是异步通信服务规范。客户端发起服务呼叫，但不等待结果。调用方立即继续其工作，并不关心结果。如果调用方对结果感兴趣，有一些机制可以让其随时被回调方法返回结果。</p><p><img src="https://www.godhearing.cn/posts/17778/image-20210120124131615.png" alt="img" style="zoom: 50%;" /></p><ol><li><code>uvicorn</code> 设计的初衷？</li></ol><ul><li>使用 <a href="https://github.com/MagicStack/uvloop"><code>uvloop</code></a>和 <a href="https://github.com/MagicStack/httptools"><code>httptools</code></a>实现一个极速的 <code>asyncio</code> 服务器。</li><li>实现一个基于 <a href="http://channels.readthedocs.io/en/stable/asgi.html"><code>ASGI(异步服务器网关接口)</code></a>的最小的应用程序接口。</li></ul><ol><li>uvicorn 命令行选项</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs stylus">Usage: uvicorn <span class="hljs-selector-attr">[OPTIONS]</span> APP<br><br>Options:<br>  <span class="hljs-attr">--host</span> TEXT                     绑定套接字到ip地址，默认<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span><br>  <span class="hljs-attr">--port</span> INTEGER                  绑定套接字到端口号，默认<span class="hljs-number">8000</span><br>  <span class="hljs-attr">--uds</span> TEXT                      绑定到UNIX域套接字<br>  <span class="hljs-attr">--fd</span> INTEGER                    从此文件描述符绑定到套接字<br>  <span class="hljs-attr">--reload</span>                        启用自动重载（内容修改后）<br>  <span class="hljs-attr">--reload-dir</span> TEXT               显式设置重载目录，而不使用当前工作目录<br>  <span class="hljs-attr">--workers</span> INTEGER               工作进程数。不适用于<span class="hljs-attr">--reload</span>启用<br>  <span class="hljs-attr">--loop</span> <span class="hljs-selector-attr">[auto|asyncio|uvloop|iocp]</span> 事件循环实现，默认：auto<br>  <span class="hljs-attr">--http</span> <span class="hljs-selector-attr">[auto|h11|httptools]</span>     HTTP协议实现，默认：auto<br>  <span class="hljs-attr">--ws</span> <span class="hljs-selector-attr">[auto|none|websockets|wsproto]</span> WebSocket协议实现，默认：auto<br>  <span class="hljs-attr">--lifespan</span> <span class="hljs-selector-attr">[auto|on|off]</span>        生命周期实施，默认：auto<br>  <span class="hljs-attr">--interface</span> <span class="hljs-selector-attr">[auto|asgi3|asgi2|wsgi]</span> 应用程序界面选择，默认：auto<br>  <span class="hljs-attr">--env-file</span> PATH                 环境配置文件<br>  <span class="hljs-attr">--log-config</span> PATH               日志配置文件<br>  <span class="hljs-attr">--log-level</span> <span class="hljs-selector-attr">[critical|error|warning|info|debug|trace]</span><br>                                  Log level. <span class="hljs-selector-attr">[default: info]</span><br>  <span class="hljs-attr">--access-log</span> / <span class="hljs-attr">--no-access-log</span>  启用/禁用访问日志<br>  <span class="hljs-attr">--use-colors</span> / <span class="hljs-attr">--no-use-colors</span>  启用/禁用彩色日志记录<br>  <span class="hljs-attr">--proxy-headers</span> / <span class="hljs-attr">--no-proxy-headers</span><br>                                  启用/禁用X-Forwarded-Proto，X-Forwarded-For，X-Forwarded-Port以填充远程地址信息<br>  <span class="hljs-attr">--forwarded-allow-ips</span> TEXT      逗号分隔的IP列表，以信任代理标头。如果可用，则默认为$ FORWARDED_ALLOW_IPS环境变量，或者为<span class="hljs-string">&#x27;127.0.0.1&#x27;</span><br>  <span class="hljs-attr">--root-path</span> TEXT                <br>  <span class="hljs-attr">--limit-concurrency</span> INTEGER     在发出HTTP <span class="hljs-number">503</span>响应之前允许的最大并发连接数或任务数。<br>                                  <br>  <span class="hljs-attr">--backlog</span> INTEGER               积压的最大连接数<br>  <span class="hljs-attr">--limit-max-requests</span> INTEGER    终止进程之前服务的最大请求数<br>  <span class="hljs-attr">--timeout-keep-alive</span> INTEGER    如果在此超时时间内未收到新数据，则关闭“保持活动”连接。 <span class="hljs-selector-attr">[默认值：5]</span><br>  <span class="hljs-attr">--ssl-keyfile</span> TEXT              SSL key file<br>  <span class="hljs-attr">--ssl-certfile</span> TEXT             SSL certificate file<br>  <span class="hljs-attr">--ssl-version</span> INTEGER           要使用的SSL版本 <span class="hljs-selector-attr">[默认值：2]</span><br>  <span class="hljs-attr">--ssl-cert-reqs</span> INTEGER         是否需要客户端证书 <span class="hljs-selector-attr">[默认值：0]</span><br>  <span class="hljs-attr">--ssl-ca-certs</span> TEXT             CA证书文件<br>  <span class="hljs-attr">--ssl-ciphers</span> TEXT              要使用的密码 <span class="hljs-selector-attr">[默认值：TLSv1]</span><br>  <span class="hljs-attr">--header</span> TEXT                   自定义默认HTTP响应标头<br>  <span class="hljs-attr">--help</span>                          显示帮助消息并退出<br></code></pre></td></tr></table></figure><p>​    有关更多信息，请参阅<a href="https://www.uvicorn.org/settings/">设置文档</a>。</p><ol><li>使用方法</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pip install uvicorn</span><br>uvicorn demo:app --host <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span> --port <span class="hljs-number">8080</span> --workers <span class="hljs-number">10</span> --limit-concurrency <span class="hljs-number">100</span> --timeout-keep-alive <span class="hljs-number">5</span> --reload<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> uvicorn<br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">app</span>(<span class="hljs-params">scope, receive, send</span>):<br>    ...<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    uvicorn.run(<span class="hljs-string">&quot;example:app&quot;</span>, host=<span class="hljs-string">&quot;127.0.0.1&quot;</span>, port=<span class="hljs-number">5000</span>, log_level=<span class="hljs-string">&quot;info&quot;</span>)<br></code></pre></td></tr></table></figure><h1 id="supervisor"><a href="#supervisor" class="headerlink" title="supervisor"></a>supervisor</h1><p>supervisor 是一款基于Python的进程管理工具，可以很方便的管理服务器上部署的应用程序。supervisor的功能如下:</p><p>　　a. 启动、重启、关闭包括但不限于python进程。</p><p>　　b.查看进程的运行状态。</p><p>　　c.批量维护多个进程。</p><p>为什么要用supervisor来管理进程，是因为相对于linux传统的进程管理(即系统自带的init 进程管理)方式来说，它有很多的优势：</p><p><strong>1) 简单方便</strong></p><p>a）supervisor管理进程，只要在supervisor的配置文件中，把要管理的进程的可执行文件的路径写进去就OK了；b）被管理进程作为supervisor的子进程，当子进程挂掉的时候，父进程可以准确获取子进程挂掉的信息的，所以也就可以对挂掉的子进程进行自动重启了, 至于重启还是不重启，也要看配置文件里面有没有设置autostart=true。</p><p><strong>2) 精确</strong></p><p>linux对进程状态的反馈有时候不太准确, 也就是说linux进程通常很难获得准确的up/down状态, Pidfiles经常说谎! 而supervisor监控子进程，得到的子进程状态无疑是准确的。supervisord将进程作为子进程启动，所以它总是知道其子进程的正确的up/down状态，可以方便的对这些数据进行查询. </p><p><strong>3) 进程分组</strong></p><p>进程支持分组启动和停止，也支持启动顺序，即‘优先级’，supervisor允许为进程分配优先级，并允许用户通过supervisorctl客户端发出命令，如“全部启动”和”重新启动所有“，它们以预先分配的优先级顺序启动。还可以将进程分为”进程组“，一组逻辑关联的进程可以作为一个单元停止或启动。进程组supervisor可以对进程组统一管理，也就是说我们可以把需要管理的进程写到一个组里面，然后把这个组作为一个对象进行管理，如启动，停止，重启等等操作。而linux系统则是没有这种功能的，想要停止一个进程，只能一个一个的去停止，要么就自己写个脚本去批量停止。</p><p><strong>4) 集中式管理</strong></p><p>supervisor管理的进程，进程组信息，全部都写在一个ini格式的文件里就OK了。管理supervisor时, 可以在本地进行管理，也可以远程管理，而且supervisor提供了一个web界面，可以在web界面上监控，管理进程。 当然了，本地，远程和web管理的时候，需要调用supervisor的xml_rpc接口。</p><p><strong>5) 可扩展性</strong></p><p>supervisor有一个简单的事件（event）通知协议，还有一个用于控制的XML-RPC接口，可以用Python开发人员来扩展构建。</p><p><strong>6) 权限</strong></p><p>总所周知, linux的进程特别是侦听在1024端口之下的进程，一般用户大多数情况下，是不能对其进行控制的。想要控制的话，必须要有root权限。然而supervisor提供了一个功能，可以为supervisord或者每个子进程，设置一个非root的user，这个user就可以管理它对应的进程了。</p><p><strong>7) 兼容性，稳定性</strong></p><p>supervisor由Python编写，在除Windows操作系统以外基本都支持，如linux，Mac OS x,solaris,FreeBSD系统</p> <!-- more --><h2 id="1-组成部分"><a href="#1-组成部分" class="headerlink" title="1.组成部分"></a>1.组成部分</h2><p>Supervisor 包括以下四个组件:</p><ul><li><strong>supervisord</strong> 服务端程序，主要功能是启动 supervisord 服务及其管理的子进程，记录日志，重启崩溃的子进程，等。</li><li><strong>supervisorctl</strong> supervisor命令行的客户端名称是supervisorctl。它为supervisord提供了一个类似于shell的交互界面。使用supervisorctl，用户可以查看不同的supervisord进程列表，获取控制子进程的状态，如停止和启动子进程</li><li><strong>Web Server</strong> 实现在界面上管理进程，还能查看进程日志和清除日志。</li><li><strong>XML-RPC 接口</strong> 可以通过 XML_RPC 协议对 supervisord 进行远程管理，达到和 supervisorctl 以及 Web Server 一样的管理功能。</li></ul><p>Supervisor 管理的进程运行状态图： <img src="https://img2018.cnblogs.com/blog/1050393/201812/1050393-20181213162311893-349412006.png" alt="img"></p><ul><li>running：进程处于运行状态</li><li>starting：Supervisor 收到启动请求后，进程处于正在启动过程中</li><li>stopped：进程处于关闭状态</li><li>stopping：Supervisor 收到关闭请求后，进程处于正在关闭过程中</li><li>backoff：进程进入 starting 状态后，由于马上就退出导致没能进入 running 状态</li><li>fatal：进程没有正常启动</li><li>exited：进程从 running 状态退出</li></ul><h2 id="2-安装"><a href="#2-安装" class="headerlink" title="2.安装"></a>2.安装</h2><p>centos系统下可以直接yum安装, 前提是需要下载epel源, 下载地址: <a href="http://dl.fedoraproject.org/pub/epel/">http://dl.fedoraproject.org/pub/epel/</a></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">yum <span class="hljs-keyword">install</span> supervisor<br></code></pre></td></tr></table></figure><h2 id="3-常用命令"><a href="#3-常用命令" class="headerlink" title="3.常用命令"></a>3.常用命令</h2><div class="table-container"><table><thead><tr><th>supervisord</th><th>初始启动Supervisord，启动、管理配置中设置的进程</th></tr></thead><tbody><tr><td>supervisorctl stop programxxx</td><td>停止某一个进程</td></tr><tr><td>supervisorctl start programxxx</td><td>启动某个进程</td></tr><tr><td>supervisorctl restart programxxx</td><td>重启某个进程</td></tr><tr><td>supervisorctl stop groupworker</td><td>停止所有属于名为groupworker这个分组的进程(start,restart同理)</td></tr><tr><td>supervisorctl stop all</td><td>停止全部进程，注：start、restart、stop都不会载入最新的配置文件</td></tr><tr><td>supervisorctl reload</td><td>载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程</td></tr><tr><td>supervisorctl update</td><td>根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启</td></tr><tr><td></td><td></td></tr><tr><td>supervisord -c /etc/supervisor/supervisord.conf</td><td>制定让其读取的配置文件</td></tr><tr><td>supervisorctl status programxxx</td><td>查看状态</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td></tr></tbody></table></div><p>设置开机启动</p><p>centos7下：新建文件supervisord.service</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment">#supervisord.service</span><br><br><span class="hljs-section">[Unit]</span> <br><span class="hljs-attr">Description</span>=Supervisor daemon<br><br><span class="hljs-section">[Service]</span> <br><span class="hljs-attr">Type</span>=forking <br><span class="hljs-attr">ExecStart</span>=/usr/bin/supervisord -c /etc/supervisord.conf <br><span class="hljs-attr">ExecStop</span>=/usr/bin/supervisorctl shutdown <br><span class="hljs-attr">ExecReload</span>=/usr/bin/supervisorctl reload <br><span class="hljs-attr">KillMode</span>=process <br><span class="hljs-attr">Restart</span>=<span class="hljs-literal">on</span>-failure <br><span class="hljs-attr">RestartSec</span>=<span class="hljs-number">42</span>s<br><br><span class="hljs-section">[Install]</span> <br><span class="hljs-attr">WantedBy</span>=multi-user.target<br></code></pre></td></tr></table></figure><p>将文件拷贝到/usr/lib/systemd/system/</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">cp supervisord.service <span class="hljs-regexp">/usr/</span>lib<span class="hljs-regexp">/systemd/</span>system/<br></code></pre></td></tr></table></figure><p>启动服务</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">systemctl <span class="hljs-built_in">enable</span> supervisord<br></code></pre></td></tr></table></figure><p>验证一下是否为开机启动</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs actionscript">systemctl <span class="hljs-keyword">is</span>-enabled supervisord<br></code></pre></td></tr></table></figure><h2 id="4-配置参数说明"><a href="#4-配置参数说明" class="headerlink" title="4.配置参数说明"></a>4.配置参数说明</h2><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br></pre></td><td class="code"><pre><code class="hljs llvm">[unix_http_server]            <br>file<span class="hljs-operator">=</span>/tmp/supervisor.sock   <span class="hljs-comment">; socket文件的路径，supervisorctl用XML_RPC和supervisord通信就是通过它进行</span><br>                              的。如果不设置的话，supervisorctl也就不能用了  <br>                              不设置的话，默认为none。 非必须设置        <br><span class="hljs-comment">;chmod=0700                 ; 这个简单，就是修改上面的那个socket文件的权限为0700</span><br>                              不设置的话，默认为<span class="hljs-number">0700</span>。 非必须设置<br><span class="hljs-comment">;chown=nobody:nogroup       ; 这个一样，修改上面的那个socket文件的属组为user.group</span><br>                              不设置的话，默认为启动supervisord进程的用户及属组。非必须设置<br><span class="hljs-comment">;username=user              ; 使用supervisorctl连接的时候，认证的用户</span><br>                               不设置的话，默认为不需要用户。 非必须设置<br><span class="hljs-comment">;password=123               ; 和上面的用户名对应的密码，可以直接使用明码，也可以使用SHA加密</span><br>                              如：&#123;SHA&#125;<span class="hljs-number">82</span>ab<span class="hljs-number">876</span>d<span class="hljs-number">1387</span>bfafe<span class="hljs-number">46</span><span class="hljs-keyword">cc</span><span class="hljs-number">1</span><span class="hljs-keyword">c</span><span class="hljs-number">8</span>a<span class="hljs-number">2</span>ef<span class="hljs-number">074</span>eae<span class="hljs-number">50</span>cb<span class="hljs-number">1</span>d<br>                              默认不设置。。。非必须设置<br><span class="hljs-comment">;[inet_http_server]         ; 侦听在TCP上的socket，Web Server和远程的supervisorctl都要用到他</span><br>                              不设置的话，默认为不开启。非必须设置<br><span class="hljs-comment">;port=127.0.0.1:9001        ; 这个是侦听的IP和端口，侦听所有IP用 :9001或*:9001。</span><br>                              这个必须设置，只要上面的[inet_http_server]开启了，就必须设置它<br><span class="hljs-comment">;username=user              ; 这个和上面的uinx_http_server一个样。非必须设置</span><br><span class="hljs-comment">;password=123               ; 这个也一个样。非必须设置</span><br>[supervisord]                <span class="hljs-comment">;这个主要是定义supervisord这个服务端进程的一些参数的</span><br>                              这个必须设置，不设置，supervisor就不用干活了<br>logfile<span class="hljs-operator">=</span>/tmp/supervisord.log <span class="hljs-comment">; 这个是supervisord这个主进程的日志路径，注意和子进程的日志不搭嘎。</span><br>                               默认路径$CWD/supervisord.log，$CWD是当前目录。。非必须设置<br>logfile_maxbytes<span class="hljs-operator">=</span><span class="hljs-number">50</span>MB        <span class="hljs-comment">; 这个是上面那个日志文件的最大的大小，当超过50M的时候，会生成一个新的日 </span><br>                               志文件。当设置为<span class="hljs-number">0</span>时，表示不限制文件大小<br>                               默认值是<span class="hljs-number">50</span>M，非必须设置。              <br>logfile_backups<span class="hljs-operator">=</span><span class="hljs-number">10</span>           <span class="hljs-comment">; 日志文件保持的数量，上面的日志文件大于50M时，就会生成一个新文件。文件</span><br>                               数量大于<span class="hljs-number">10</span>时，最初的老文件被新文件覆盖，文件数量将保持为<span class="hljs-number">10</span><br>                               当设置为<span class="hljs-number">0</span>时，表示不限制文件的数量。<br>                               默认情况下为<span class="hljs-number">10</span>。。。非必须设置<br>loglevel<span class="hljs-operator">=</span>info                <span class="hljs-comment">; 日志级别，有critical, error, warn, info, debug, trace, or blather等</span><br>                               默认为info。。。非必须设置项<br>pidfile<span class="hljs-operator">=</span>/tmp/supervisord.pid <span class="hljs-comment">; supervisord的pid文件路径。</span><br>                               默认为$CWD/supervisord.pid。。。非必须设置<br>nodaemon<span class="hljs-operator">=</span><span class="hljs-keyword">false</span>               <span class="hljs-comment">; 如果是true，supervisord进程将在前台运行</span><br>                               默认为<span class="hljs-keyword">false</span>，也就是后台以守护进程运行。。。非必须设置<br>minfds<span class="hljs-operator">=</span><span class="hljs-number">1024</span>                  <span class="hljs-comment">; 这个是最少系统空闲的文件描述符，低于这个值supervisor将不会启动。</span><br>                               系统的文件描述符在这里设置cat /proc/sys/fs/file-<span class="hljs-keyword">max</span><br>                               默认情况下为<span class="hljs-number">1024</span>。。。非必须设置<br>minprocs<span class="hljs-operator">=</span><span class="hljs-number">200</span>                 <span class="hljs-comment">; 最小可用的进程描述符，低于这个值supervisor也将不会正常启动。</span><br>                              ulimit  -u这个命令，可以查看linux下面用户的最大进程数<br>                              默认为<span class="hljs-number">200</span>。。。非必须设置<br><span class="hljs-comment">;umask=022                   ; 进程创建文件的掩码</span><br>                               默认为<span class="hljs-number">022</span>。。非必须设置项<br><span class="hljs-comment">;user=chrism                 ; 这个参数可以设置一个非root用户，当我们以root用户启动supervisord之后。</span><br>                               我这里面设置的这个用户，也可以对supervisord进行管理<br>                               默认情况是不设置。。。非必须设置项<br><span class="hljs-comment">;identifier=supervisor       ; 这个参数是supervisord的标识符，主要是给XML_RPC用的。当你有多个</span><br>                               supervisor的时候，而且想调用XML_RPC统一管理，就需要为每个<br>                               supervisor设置不同的标识符了<br>                               默认是supervisord。。。非必需设置<br><span class="hljs-comment">;directory=/tmp              ; 这个参数是当supervisord作为守护进程运行的时候，设置这个参数的话，启动</span><br>                               supervisord进程之前，会先切换到这个目录<br>                               默认不设置。。。非必须设置<br><span class="hljs-comment">;nocleanup=true              ; 这个参数当为false的时候，会在supervisord进程启动的时候，把以前子进程</span><br>                               产生的日志文件(路径为AUTO的情况下)清除掉。有时候咱们想要看历史日志，当 <br>                               然不想日志被清除了。所以可以设置为<span class="hljs-keyword">true</span><br>                               默认是<span class="hljs-keyword">false</span>，有调试需求的同学可以设置为<span class="hljs-keyword">true</span>。。。非必须设置<br><span class="hljs-comment">;childlogdir=/tmp            ; 当子进程日志路径为AUTO的时候，子进程日志文件的存放路径。</span><br>                               默认路径是这个东西，执行下面的这个命令看看就OK了，处理的东西就默认路径<br>                               python -<span class="hljs-keyword">c</span> <span class="hljs-string">&quot;import tempfile;print tempfile.gettempdir()&quot;</span><br>                               非必须设置<br><span class="hljs-comment">;environment=KEY=&quot;value&quot;     ; 这个是用来设置环境变量的，supervisord在linux中启动默认继承了linux的</span><br>                               环境变量，在这里可以设置supervisord进程特有的其他环境变量。<br>                               supervisord启动子进程时，子进程会拷贝父进程的内存空间内容。 所以设置的<br>                               这些环境变量也会被子进程继承。<br>                               小例子：environment<span class="hljs-operator">=</span>name<span class="hljs-operator">=</span><span class="hljs-string">&quot;haha&quot;</span><span class="hljs-punctuation">,</span>age<span class="hljs-operator">=</span><span class="hljs-string">&quot;hehe&quot;</span><br>                               默认为不设置。。。非必须设置<br><span class="hljs-comment">;strip_ansi=false            ; 这个选项如果设置为true，会清除子进程日志中的所有ANSI 序列。什么是ANSI</span><br>                               序列呢？就是我们的\n<span class="hljs-punctuation">,</span>\t这些东西。<br>                               默认为<span class="hljs-keyword">false</span>。。。非必须设置<br><span class="hljs-comment">; the below section must remain in the config file for RPC</span><br><span class="hljs-comment">; (supervisorctl/web interface) to work, additional interfaces may be</span><br><span class="hljs-comment">; added by defining them in separate rpcinterface: sections</span><br>[rpcinterface:supervisor]    <span class="hljs-comment">;这个选项是给XML_RPC用的，当然你如果想使用supervisord或者web server 这 </span><br>                              个选项必须要开启的<br>supervisor.rpcinterface_factory <span class="hljs-operator">=</span> supervisor.rpcinterface:make_main_rpcinterface <br>[supervisorctl]              <span class="hljs-comment">;这个主要是针对supervisorctl的一些配置  </span><br>serverurl<span class="hljs-operator">=</span>unix:///tmp/supervisor.sock <span class="hljs-comment">; 这个是supervisorctl本地连接supervisord的时候，本地UNIX socket</span><br>                                        路径，注意这个是和前面的[unix_http_server]对应的<br>                                        默认值就是unix:///tmp/supervisor.sock。。非必须设置<br><span class="hljs-comment">;serverurl=http://127.0.0.1:9001 ; 这个是supervisorctl远程连接supervisord的时候，用到的TCP socket路径</span><br>                                   注意这个和前面的[inet_http_server]对应<br>                                   默认就是http://<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">9001</span>。。。非必须项<br>                               <br><span class="hljs-comment">;username=chris              ; 用户名</span><br>                               默认空。。非必须设置<br><span class="hljs-comment">;password=123                ; 密码</span><br>                              默认空。。非必须设置<br><span class="hljs-comment">;prompt=mysupervisor         ; 输入用户名密码时候的提示符</span><br>                               默认supervisor。。非必须设置<br><span class="hljs-comment">;history_file=~/.sc_history  ; 这个参数和shell中的history类似，我们可以用上下键来查找前面执行过的命令</span><br>                               默认是no file的。。所以我们想要有这种功能，必须指定一个文件。。。非<br>                               必须设置<br><span class="hljs-comment">; The below sample program section shows all possible program subsection values,</span><br><span class="hljs-comment">; create one or more &#x27;real&#x27; program: sections to be able to control them under</span><br><span class="hljs-comment">; supervisor.</span><br><span class="hljs-comment">;[program:theprogramname]      ;这个就是咱们要管理的子进程了，&quot;:&quot;后面的是名字，最好别乱写和实际进程</span><br>                                有点关联最好。这样的program我们可以设置一个或多个，一个program就是<br>                                要被管理的一个进程<br><span class="hljs-comment">;command=/bin/cat              ; 这个就是我们的要启动进程的命令路径了，可以带参数</span><br>                                例子：/home/test.py -a &#x27;hehe&#x27;<br>                                有一点需要注意的是，我们的command只能是那种在终端运行的进程，不能是<br>                                守护进程。这个想想也知道了，比如说command<span class="hljs-operator">=</span>service httpd start。<br>                                httpd这个进程被linux的service管理了，我们的supervisor再去启动这个命令<br>                                这已经不是严格意义的子进程了。<br>                                这个是个必须设置的项<br><span class="hljs-comment">;process_name=%(program_name)s ; 这个是进程名，如果我们下面的numprocs参数为1的话，就不用管这个参数</span><br>                                 了，它默认值%(program_name)s也就是上面的那个program冒号后面的名字，<br>                                 但是如果numprocs为多个的话，那就不能这么干了。想想也知道，不可能每个<br>                                 进程都用同一个进程名吧。<br>                                <br><span class="hljs-comment">;numprocs=1                    ; 启动进程的数目。当不为1时，就是进程池的概念，注意process_name的设置</span><br>                                 默认为<span class="hljs-number">1</span>    。。非必须设置<br><span class="hljs-comment">;directory=/tmp                ; 进程运行前，会前切换到这个目录</span><br>                                 默认不设置。。。非必须设置<br><span class="hljs-comment">;umask=022                     ; 进程掩码，默认none，非必须</span><br><span class="hljs-comment">;priority=999                  ; 子进程启动关闭优先级，优先级低的，最先启动，关闭的时候最后关闭</span><br>                                 默认值为<span class="hljs-number">999</span> 。。非必须设置<br><span class="hljs-comment">;autostart=true                ; 如果是true的话，子进程将在supervisord启动后被自动启动</span><br>                                 默认就是<span class="hljs-keyword">true</span>   。。非必须设置<br><span class="hljs-comment">;autorestart=unexpected        ; 这个是设置子进程挂掉后自动重启的情况，有三个选项，false,unexpected</span><br>                                 和<span class="hljs-keyword">true</span>。如果为<span class="hljs-keyword">false</span>的时候，无论什么情况下，都不会被重新启动，<br>                                 如果为unexpected，只有当进程的退出码不在下面的exitcodes里面定义的退 <br>                                 出码的时候，才会被自动重启。当为<span class="hljs-keyword">true</span>的时候，只要子进程挂掉，将会被无<br>                                 条件的重启<br><span class="hljs-comment">;startsecs=1                   ; 这个选项是子进程启动多少秒之后，此时状态如果是running，则我们认为启</span><br>                                 动成功了<br>                                 默认值为<span class="hljs-number">1</span> 。。非必须设置<br><span class="hljs-comment">;startretries=3                ; 当进程启动失败后，最大尝试启动的次数。。当超过3次后，supervisor将把</span><br>                                 此进程的状态置为FAIL<br>                                 默认值为<span class="hljs-number">3</span> 。。非必须设置<br><span class="hljs-comment">;exitcodes=0,2                 ; 注意和上面的的autorestart=unexpected对应。。exitcodes里面的定义的</span><br>                                 退出码是expected的。<br><span class="hljs-comment">;stopsignal=QUIT               ; 进程停止信号，可以为TERM, HUP, INT, QUIT, KILL, USR1, or USR2等信号</span><br>                                  默认为TERM 。。当用设定的信号去干掉进程，退出码会被认为是expected<br>                                  非必须设置<br><span class="hljs-comment">;stopwaitsecs=10               ; 这个是当我们向子进程发送stopsignal信号后，到系统返回信息</span><br>                                 给supervisord，所等待的最大时间。 超过这个时间，supervisord会向该<br>                                 子进程发送一个强制kill的信号。<br>                                 默认为<span class="hljs-number">10</span>秒。。非必须设置<br><span class="hljs-comment">;stopasgroup=false             ; 这个东西主要用于，supervisord管理的子进程，这个子进程本身还有</span><br>                                 子进程。那么我们如果仅仅干掉supervisord的子进程的话，子进程的子进程<br>                                 有可能会变成孤儿进程。所以咱们可以设置可个选项，把整个该子进程的<br>                                 整个进程组都干掉。 设置为<span class="hljs-keyword">true</span>的话，一般killasgroup也会被设置为<span class="hljs-keyword">true</span>。<br>                                 需要注意的是，该选项发送的是stop信号<br>                                 默认为<span class="hljs-keyword">false</span>。。非必须设置。。<br><span class="hljs-comment">;killasgroup=false             ; 这个和上面的stopasgroup类似，不过发送的是kill信号</span><br><span class="hljs-comment">;user=chrism                   ; 如果supervisord是root启动，我们在这里设置这个非root用户，可以用来</span><br>                                 管理该program<br>                                 默认不设置。。。非必须设置项<br><span class="hljs-comment">;redirect_stderr=true          ; 如果为true，则stderr的日志会被写入stdout日志文件中</span><br>                                 默认为<span class="hljs-keyword">false</span>，非必须设置<br><span class="hljs-comment">;stdout_logfile=/a/path        ; 子进程的stdout的日志路径，可以指定路径，AUTO，none等三个选项。</span><br>                                 设置为none的话，将没有日志产生。设置为AUTO的话，将随机找一个地方<br>                                 生成日志文件，而且当supervisord重新启动的时候，以前的日志文件会被<br>                                 清空。当 redirect_stderr<span class="hljs-operator">=</span><span class="hljs-keyword">true</span>的时候，sterr也会写进这个日志文件<br><span class="hljs-comment">;stdout_logfile_maxbytes=1MB   ; 日志文件最大大小，和[supervisord]中定义的一样。默认为50</span><br><span class="hljs-comment">;stdout_logfile_backups=10     ; 和[supervisord]定义的一样。默认10</span><br><span class="hljs-comment">;stdout_capture_maxbytes=1MB   ; 这个东西是设定capture管道的大小，当值不为0的时候，子进程可以从stdout</span><br>                                 发送信息，而supervisor可以根据信息，发送相应的event。<br>                                 默认为<span class="hljs-number">0</span>，为<span class="hljs-number">0</span>的时候表达关闭管道。。。非必须项<br><span class="hljs-comment">;stdout_events_enabled=false   ; 当设置为ture的时候，当子进程由stdout向文件描述符中写日志的时候，将</span><br>                                 触发supervisord发送PROCESS_LOG_STDOUT类型的event<br>                                 默认为<span class="hljs-keyword">false</span>。。。非必须设置<br><span class="hljs-comment">;stderr_logfile=/a/path        ; 这个东西是设置stderr写的日志路径，当redirect_stderr=true。这个就不用</span><br>                                 设置了，设置了也是白搭。因为它会被写入stdout_logfile的同一个文件中<br>                                 默认为AUTO，也就是随便找个地存，supervisord重启被清空。。非必须设置<br><span class="hljs-comment">;stderr_logfile_maxbytes=1MB   ; 这个出现好几次了，就不重复了</span><br><span class="hljs-comment">;stderr_logfile_backups=10     ; 这个也是</span><br><span class="hljs-comment">;stderr_capture_maxbytes=1MB   ; 这个一样，和stdout_capture一样。 默认为0，关闭状态</span><br><span class="hljs-comment">;stderr_events_enabled=false   ; 这个也是一样，默认为false</span><br><span class="hljs-comment">;environment=A=&quot;1&quot;,B=&quot;2&quot;       ; 这个是该子进程的环境变量，和别的子进程是不共享的</span><br><span class="hljs-comment">;serverurl=AUTO                ; </span><br><span class="hljs-comment">; The below sample eventlistener section shows all possible</span><br><span class="hljs-comment">; eventlistener subsection values, create one or more &#x27;real&#x27;</span><br><span class="hljs-comment">; eventlistener: sections to be able to handle event notifications</span><br><span class="hljs-comment">; sent by supervisor.</span><br><span class="hljs-comment">;[eventlistener:theeventlistenername] ;这个东西其实和program的地位是一样的，也是suopervisor启动的子进</span><br>                                       程，不过它干的活是订阅supervisord发送的event。他的名字就叫<br>                                       listener了。我们可以在listener里面做一系列处理，比如报警等等<br>                                       楼主这两天干的活，就是弄的这玩意<br><span class="hljs-comment">;command=/bin/eventlistener    ; 这个和上面的program一样，表示listener的可执行文件的路径</span><br><span class="hljs-comment">;process_name=%(program_name)s ; 这个也一样，进程名，当下面的numprocs为多个的时候，才需要。否则默认就</span><br>                                 OK了<br><span class="hljs-comment">;numprocs=1                    ; 相同的listener启动的个数</span><br><span class="hljs-comment">;events=EVENT                  ; event事件的类型，也就是说，只有写在这个地方的事件类型。才会被发送</span><br>                      <br>                                 <br><span class="hljs-comment">;buffer_size=10                ; 这个是event队列缓存大小，单位不太清楚，楼主猜测应该是个吧。当buffer</span><br>                                 超过<span class="hljs-number">10</span>的时候，最旧的event将会被清除，并把新的event放进去。<br>                                 默认值为<span class="hljs-number">10</span>。。非必须选项<br><span class="hljs-comment">;directory=/tmp                ; 进程执行前，会切换到这个目录下执行</span><br>                                 默认为不切换。。。非必须<br><span class="hljs-comment">;umask=022                     ; 淹没，默认为none，不说了</span><br><span class="hljs-comment">;priority=-1                   ; 启动优先级，默认-1，也不扯了</span><br><span class="hljs-comment">;autostart=true                ; 是否随supervisord启动一起启动，默认true</span><br><span class="hljs-comment">;autorestart=unexpected        ; 是否自动重启，和program一个样，分true,false,unexpected等，注意</span><br>                                  unexpected和exitcodes的关系<br><span class="hljs-comment">;startsecs=1                   ; 也是一样，进程启动后跑了几秒钟，才被认定为成功启动，默认1</span><br><span class="hljs-comment">;startretries=3                ; 失败最大尝试次数，默认3</span><br><span class="hljs-comment">;exitcodes=0,2                 ; 期望或者说预料中的进程退出码，</span><br><span class="hljs-comment">;stopsignal=QUIT               ; 干掉进程的信号，默认为TERM，比如设置为QUIT，那么如果QUIT来干这个进程</span><br>                                 那么会被认为是正常维护，退出码也被认为是expected中的<br><span class="hljs-comment">;stopwaitsecs=10               ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="hljs-comment">;stopasgroup=false             ; send stop signal to the UNIX process group (default false)</span><br><span class="hljs-comment">;killasgroup=false             ; SIGKILL the UNIX process group (def false)</span><br><span class="hljs-comment">;user=chrism                   ;设置普通用户，可以用来管理该listener进程。</span><br>                                默认为空。。非必须设置<br><span class="hljs-comment">;redirect_stderr=true          ; 为true的话，stderr的log会并入stdout的log里面</span><br>                                默认为<span class="hljs-keyword">false</span>。。。非必须设置<br><span class="hljs-comment">;stdout_logfile=/a/path        ; 这个不说了，好几遍了</span><br><span class="hljs-comment">;stdout_logfile_maxbytes=1MB   ; 这个也是</span><br><span class="hljs-comment">;stdout_logfile_backups=10     ; 这个也是</span><br><span class="hljs-comment">;stdout_events_enabled=false   ; 这个其实是错的，listener是不能发送event</span><br><span class="hljs-comment">;stderr_logfile=/a/path        ; 这个也是</span><br><span class="hljs-comment">;stderr_logfile_maxbytes=1MB   ; 这个也是</span><br><span class="hljs-comment">;stderr_logfile_backups        ; 这个不说了</span><br><span class="hljs-comment">;stderr_events_enabled=false   ; 这个也是错的，listener不能发送event</span><br><span class="hljs-comment">;environment=A=&quot;1&quot;,B=&quot;2&quot;       ; 这个是该子进程的环境变量</span><br>                                 默认为空。。。非必须设置<br><span class="hljs-comment">;serverurl=AUTO                ; override serverurl computation (childutils)</span><br><span class="hljs-comment">; The below sample group section shows all possible group values,</span><br><span class="hljs-comment">; create one or more &#x27;real&#x27; group: sections to create &quot;heterogeneous&quot;</span><br><span class="hljs-comment">; process groups.</span><br><span class="hljs-comment">;[group:thegroupname]  ;这个东西就是给programs分组，划分到组里面的program。我们就不用一个一个去操作了</span><br>                         我们可以对组名进行统一的操作。 注意：program被划分到组里面之后，就相当于原来<br>                         的配置从supervisor的配置文件里消失了。。。supervisor只会对组进行管理，而不再<br>                         会对组里面的单个program进行管理了<br><span class="hljs-comment">;programs=progname1,progname2  ; 组成员，用逗号分开</span><br>                                 这个是个必须的设置项<br><span class="hljs-comment">;priority=999                  ; 优先级，相对于组和组之间说的</span><br>                                 默认<span class="hljs-number">999</span>。。非必须选项<br><span class="hljs-comment">; The [include] section can just contain the &quot;files&quot; setting.  This</span><br><span class="hljs-comment">; setting can list multiple files (separated by whitespace or</span><br><span class="hljs-comment">; newlines).  It can also contain wildcards.  The filenames are</span><br><span class="hljs-comment">; interpreted as relative to this file.  Included files *cannot*</span><br><span class="hljs-comment">; include files themselves.</span><br><span class="hljs-comment">;[include]                         ;这个东西挺有用的，当我们要管理的进程很多的时候，写在一个文件里面</span><br>                                    就有点大了。我们可以把配置信息写到多个文件中，然后include过来<br><span class="hljs-comment">;files = relative/directory/*.ini</span><br></code></pre></td></tr></table></figure><h2 id="5-demo"><a href="#5-demo" class="headerlink" title="5.demo"></a>5.demo</h2><p>python程序进程一般都用supervisor进行管理</p><p>分享一个线上曾经用过的supervisor监控python程序的配置</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[program:xcspam]</span><br><span class="hljs-attr">command</span>=/usr/bin/python /app/web/xcspam/bin/main.py --port=<span class="hljs-number">93</span>%(process_num)<span class="hljs-number">02</span>d<br><span class="hljs-attr">process_name</span>=%(program_name)s_%(process_num)<span class="hljs-number">02</span>d<br><span class="hljs-attr">numprocs</span>=<span class="hljs-number">16</span><br><span class="hljs-attr">numprocs_start</span>=<span class="hljs-number">1</span><br><span class="hljs-attr">directory</span>=/app/web/xcspam<br><span class="hljs-attr">user</span>=work<br><span class="hljs-attr">autostart</span>=<span class="hljs-literal">true</span><br><span class="hljs-attr">autorestart</span>=<span class="hljs-literal">true</span><br><span class="hljs-attr">stopsignal</span>=QUIT<br><span class="hljs-attr">stdout_logfile</span>=/data/log/xcspam/xcspam.log<br><span class="hljs-attr">stderr_logfile</span>=/data/log/xcspam/xcspam_error.log<br><span class="hljs-attr">stdout_logfile_maxbytes</span>=<span class="hljs-number">0</span><br><span class="hljs-attr">stderr_logfile_maxbytes</span>=<span class="hljs-number">0</span><br><span class="hljs-attr">environment</span>=PYTHONPATH=/app/web/xcspam, KEVIN_ENV=production<br><br><span class="hljs-section">[program:uwsgi]</span><br><span class="hljs-attr">command</span>=/usr/local/python3/bin/uwsgi /usr/local/nginx/conf/uwsgi.ini<br><span class="hljs-attr">directory</span>=/data/www/APPServer/<br><span class="hljs-attr">startsecs</span>=<span class="hljs-number">0</span><br><span class="hljs-attr">stopwaitsecs</span>=<span class="hljs-number">0</span><br><span class="hljs-attr">autostart</span>=<span class="hljs-literal">true</span><br><span class="hljs-attr">autorestart</span>=<span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>重启supervisor服务</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs inform7"><span class="hljs-comment">[root@localhost ~]</span># /etc/init.d/supervisord restart<br>Stopping supervisord:                                      <span class="hljs-comment">[  OK  ]</span><br>Starting supervisord:                                        <span class="hljs-comment">[  OK  ]</span> <br></code></pre></td></tr></table></figure><p> 查看supervisor管理的两个进程状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># supervisorctl</span><br>main           RUNNING    pid 16183, <span class="hljs-built_in">uptime</span> 0:00:12<br>uwsgi          RUNNING    pid 19043, <span class="hljs-built_in">uptime</span> 0:00:13<br>supervisor&gt; status<br>main           RUNNING    pid 16183, <span class="hljs-built_in">uptime</span> 0:00:17<br>uwsgi          RUNNING    pid 19043, <span class="hljs-built_in">uptime</span> 0:00:18<br></code></pre></td></tr></table></figure><p> 查看管理的这两进程的运行情况</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs gradle">[root@localhost ~]# ps -ef|<span class="hljs-keyword">grep</span> main<br>root     <span class="hljs-number">16183</span> <span class="hljs-number">16181</span>  <span class="hljs-number">0</span> <span class="hljs-number">21</span>:<span class="hljs-number">38</span> ?        <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/bin/m</span>ain<br>root     <span class="hljs-number">16207</span> <span class="hljs-number">15953</span>  <span class="hljs-number">0</span> <span class="hljs-number">21</span>:<span class="hljs-number">42</span> pts/<span class="hljs-number">0</span>    <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> <span class="hljs-keyword">grep</span> main<br> <br>[root@localhost ~]# ps -ef|<span class="hljs-keyword">grep</span> uwsgi<br>root      <span class="hljs-number">16595</span>  <span class="hljs-number">16064</span>  <span class="hljs-number">0</span> <span class="hljs-number">21</span>:<span class="hljs-number">40</span> pts/<span class="hljs-number">0</span>    <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> <span class="hljs-keyword">grep</span> --color=auto uwsgi<br>root      <span class="hljs-number">19043</span>  <span class="hljs-number">17056</span>  <span class="hljs-number">0</span> <span class="hljs-number">21</span>:<span class="hljs-number">44</span> ?          <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">04</span>  <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/python3/</span>bin<span class="hljs-regexp">/uwsgi /u</span>sr<span class="hljs-regexp">/local/</span>nginx<span class="hljs-regexp">/conf/u</span>wsgi.ini<br></code></pre></td></tr></table></figure><h2 id="6-开启Supervisor-Web-管理界面"><a href="#6-开启Supervisor-Web-管理界面" class="headerlink" title="6.开启Supervisor Web 管理界面"></a>6.开启Supervisor Web 管理界面</h2><p>在<code>supervisord.conf</code>配置中添加以下配置：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[inet_http_server]</span><br><span class="hljs-attr">port</span>=*:<span class="hljs-number">9000</span><br><span class="hljs-attr">username</span>=user<br><span class="hljs-attr">password</span>=<span class="hljs-number">123</span><br></code></pre></td></tr></table></figure><h2 id="7-错误收集"><a href="#7-错误收集" class="headerlink" title="7.错误收集"></a>7.错误收集</h2><p> ps：supervisor 比较适合监控业务应用，且只能监控前台程序，如果你的程序是以daemon的方式启动，就不能使用supervisor监控了,那么执行：supervisor status 会提示：BACKOFF Exited too quickly (process log may have details)。</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">FATAL   Exited too quickly <span class="hljs-comment">(process log may have details)</span><br></code></pre></td></tr></table></figure><p>出现这个问题原因：a、配置出错，查看错误日志；b.Supervisor 管理的进程不能设置为 daemon 模式如我们使用supervisor守护redis:如果 Redis 无法正常启动，可以查看一下 Redis 的配置，并将<code>daemonize</code>选项设置为 no。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://juejin.im/post/5ce8cab8e51d4577523f22f8">[译] 通过优化 Gunicorn 配置提高性能√</a></p><p><a href="https://juejin.im/post/5e83f4f5e51d4546fa45175a">Python 异步 ASGI 服务器及框架</a></p><p><a href="https://github.com/encode/uvicorn/issues/463">uvicorn 启动多线程异常停止问题</a></p><p><a href="https://www.cnblogs.com/kevingrace/p/7525200.html">*Supervisor (进程管理利器) 使用说明 - 运维笔记</a></p><p><a href="https://www.cnblogs.com/freely/p/10087950.html">https://www.cnblogs.com/freely/p/10087950.html</a></p><p><a href="https://www.jianshu.com/p/8f2382b3b9aa">Centos+Gunicorn+Nginx+Supervisor部署Flask – 简书</a></p><p><a href="https://gist.github.com/binderclip/f6b6f5ed4d71fa64c7c5">Flask Gunicorn Supervisor Nginx 项目部署小总结</a></p><p><a href="https://www.crifan.com/centos_python_2_pip_install_supervisor_can_not_find_etc_supervisor_conf">【已解决】CentOS中用python2的pip去安装supervisor后找不到/etc/supervisor中的默认配置文件supervisord.conf</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>FastAPI</tag>
      
      <tag>Gunicorn</tag>
      
      <tag>Uvicorn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重温DFS&amp;BFS</title>
    <link href="/2020/06/12/2020-06-12-%E9%87%8D%E6%B8%A9DFS&amp;BFS/"/>
    <url>/2020/06/12/2020-06-12-%E9%87%8D%E6%B8%A9DFS&amp;BFS/</url>
    
    <content type="html"><![CDATA[<h1 id="重温DFS-amp-BFS"><a href="#重温DFS-amp-BFS" class="headerlink" title="重温DFS&amp;BFS"></a>重温DFS&amp;BFS</h1><h2 id="深度优先搜索"><a href="#深度优先搜索" class="headerlink" title="深度优先搜索"></a>深度优先搜索</h2><p>DFS（Depth-First-Search）,是盲目搜索算法的一种。常常用在树的遍历及图的处理上。假设当前搜索的节点记为k，深度优先搜索表示，继续探寻k节点的所有的边。搜索过程中，遇到满足条件的k+1节点，则继续搜索探寻k+1节点的所有的边。最后回溯至节点k。这个过程一直进行到已发现从源节点开始可以到达的所有节点位置。</p><h2 id="广度优先搜索"><a href="#广度优先搜索" class="headerlink" title="广度优先搜索"></a>广度优先搜索</h2><p>BFS（Breadth-First-Search），BFS同样属于盲目搜索算法，常常使用队列（先进先出）的数据结构来辅助实现。在python中，可以使用堆栈（pop（0））的性质进行实现。BFS是从源节点开始，沿着树(图)的宽度遍历树(图)的节点。将未被访问的节点依次压入队列，再依次读取进行遍历，直到队列为空，所有节点均被访问，算法终止。</p><span id="more"></span><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p>给定一个由 ‘1’（陆地）和 ‘0’（水）组成的的二维网格，计算岛屿的数量。一个岛被水包围，并且它是通过水平方向或垂直方向上相邻的陆地连接而成的。</p></blockquote><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">示例 1</span><span class="hljs-punctuation">:</span><br><span class="hljs-punctuation"></span><br><span class="hljs-attribute">输入</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">11110</span><br><span class="hljs-attribute">11010</span><br><span class="hljs-attribute">11000</span><br><span class="hljs-attribute">00000</span><br><span class="hljs-attribute"></span><br><span class="hljs-attribute">输出</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1</span><br><span class="hljs-attribute">示例 2</span><span class="hljs-punctuation">:</span><br><span class="hljs-punctuation"></span><br><span class="hljs-attribute">输入</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">11000</span><br><span class="hljs-attribute">11000</span><br><span class="hljs-attribute">00100</span><br><span class="hljs-attribute">00011</span><br><span class="hljs-attribute"></span><br><span class="hljs-attribute">输出</span><span class="hljs-punctuation">:</span> <span class="hljs-string">3</span><br></code></pre></td></tr></table></figure><p>注意点：</p><ol><li>无论DFS还是BFS，在涉及到图中上下左右搜索的问题时，一般都要建立方向数组，表示向四个方向依次遍历（dx = [-1,1,0,0]、dy = [0,0,-1,1]）</li><li>python中的可变对象和不可变对象传值问题。list、dict为可变对象，作为参数传递时，对象位置不发生改变。string、tuple以及int为不可变对象。</li></ol><h3 id="DFS算法思路："><a href="#DFS算法思路：" class="headerlink" title="DFS算法思路："></a>DFS算法思路：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numIslands</span>(<span class="hljs-params">self, grid</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :type grid: List[List[str]]</span><br><span class="hljs-string">        :rtype: int</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        res = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(grid) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> res<br>        m = <span class="hljs-built_in">len</span>(grid)<br>        n = <span class="hljs-built_in">len</span>(grid[<span class="hljs-number">0</span>])<br><br>        mark = [[<span class="hljs-number">0</span>] * n <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m)]<br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                <span class="hljs-keyword">if</span> mark[i][j] == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> grid[i][j] == <span class="hljs-string">&#x27;1&#x27;</span>:<br>                    self.dfs(mark, grid, i, j)<br>                    res += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> res<br><br>    <span class="hljs-comment"># 深度优先搜索:时间优于宽度优先搜索</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">dfs</span>(<span class="hljs-params">self, mark, grid, x, y</span>):<br>        mark[x][y] = <span class="hljs-number">1</span><br>        dx = [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 方向数组</span><br>        dy = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br>        m = <span class="hljs-built_in">len</span>(grid)<br>        n = <span class="hljs-built_in">len</span>(grid[<span class="hljs-number">0</span>])<br>        <span class="hljs-comment"># 遍历上下左右四个方向</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>            newx = dx[i] + x<br>            newy = dy[i] + y<br>            <span class="hljs-keyword">if</span> newx &lt; <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> newx &gt;= m <span class="hljs-keyword">or</span> newy &gt;= n <span class="hljs-keyword">or</span> newy &lt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">if</span> mark[newx][newy] == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> grid[newx][newy] == <span class="hljs-string">&#x27;1&#x27;</span>:<br>                self.dfs(mark, grid, newx, newy)<br><br><br><br>s = Solution()<br>nums = [[<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;1&#x27;</span>]]<br><span class="hljs-built_in">print</span>(s.numIslands(nums))<br></code></pre></td></tr></table></figure><h3 id="BFS算法思路"><a href="#BFS算法思路" class="headerlink" title="BFS算法思路"></a>BFS算法思路</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numIslands</span>(<span class="hljs-params">self, grid</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :type grid: List[List[str]]</span><br><span class="hljs-string">        :rtype: int</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        res = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(grid) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> res<br>        m = <span class="hljs-built_in">len</span>(grid)<br>        n = <span class="hljs-built_in">len</span>(grid[<span class="hljs-number">0</span>])<br><br>        mark = [[<span class="hljs-number">0</span>] * n <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m)]<br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                <span class="hljs-keyword">if</span> mark[i][j] == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> grid[i][j] == <span class="hljs-string">&#x27;1&#x27;</span>:<br>                    self.bfs(mark, grid, i, j)<br>                    res += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> res<br><br>    <span class="hljs-comment"># 深度优先搜索:时间优于宽度优先搜索</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">bfs</span>(<span class="hljs-params">self, mark, grid, x, y</span>):<br>        Q = []<br>        Q.append([x,y])<br>        mark[x][y] = <span class="hljs-number">1</span><br>        dx = [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 方向数组</span><br>        dy = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br>        m = <span class="hljs-built_in">len</span>(grid)<br>        n = <span class="hljs-built_in">len</span>(grid[<span class="hljs-number">0</span>])<br>        <span class="hljs-comment"># 遍历上下左右四个方向</span><br>        <span class="hljs-keyword">while</span> Q:<br>            temp = Q.pop(<span class="hljs-number">0</span>)<br>            x = temp[<span class="hljs-number">0</span>]<br>            y = temp[<span class="hljs-number">1</span>]<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                newx = dx[i]+x<br>                newy = dy[i]+y<br>                <span class="hljs-keyword">if</span> newx &lt; <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> newx &gt;= m <span class="hljs-keyword">or</span> newy &gt;= n <span class="hljs-keyword">or</span> newy &lt; <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">continue</span><br>                <span class="hljs-keyword">if</span> mark[newx][newy] == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> grid[newx][newy] == <span class="hljs-string">&#x27;1&#x27;</span>:<br>                    Q.append([newx, newy])  <span class="hljs-comment"># 将新位置进入队列</span><br>                    mark[newx][newy] = <span class="hljs-number">1</span><br><br>s = Solution()<br>nums = [[<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;1&#x27;</span>,<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;1&#x27;</span>]]<br><span class="hljs-built_in">print</span>(s.numIslands(nums))<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/limingqi/p/12014044.html">leetcode—200—python（深度广度优先遍历实现代码）</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>逻辑回归详述</title>
    <link href="/2020/06/08/2020-06-08-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%A6%E8%BF%B0/"/>
    <url>/2020/06/08/2020-06-08-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%A6%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="Logistic回归-详细概述"><a href="#Logistic回归-详细概述" class="headerlink" title="Logistic回归-详细概述"></a>Logistic回归-详细概述</h1><p><img src="https://ningshixian.github.io/resources/images/logistic regression model.png" alt=""></p><p>图1：Logistic回归模型</p><p>Logistic回归在20世纪初被用于许多社会科学应用中。当因变量（目标）是分类时，可以使用逻辑回归。</p><p>例如，</p><ul><li>预测电子邮件是垃圾邮件（1）还是（0）</li><li>肿瘤是否为恶性（1）（0）</li></ul><p>考虑一种情况，在这种情况下，我们需要对电子邮件是否为垃圾邮件进行分类。如果我们对这个问题使用线性回归，则需要根据可完成的分类设置阈值。假设实际类别为恶性，预测连续值为0.4，阈值为0.5，则该数据点将归类为非恶性，这可能导致严重的实时后果。</p><span id="more"></span><p><strong>简单逻辑回归</strong></p><p><a href="[https](https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb">完整的源代码</a> : <a href="https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb">//github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb</a>)</p><p><strong>模型</strong></p><p>输出= 0或1</p><p>假设=&gt; Z = WX + B</p><p>hΘ（x）= Sigmoid（Z）</p><p><strong>Sigmoid Function</strong></p><p><img src="https://ningshixian.github.io/resources/images/sigmoid function.png" alt=""></p><p>图2：sigmoid激活函数</p><p>如果’Z’变为无穷大，则Y（预测）将变为1，如果’Z’变为负无穷大，则Y（预测）将变为0。</p><p><strong><em>Logistic回归的类型\</em></strong></p><ol><li>二元Logistic回归</li></ol><p>类别响应只有两个2种可能的结果。示例：是否为垃圾邮件</p><ol><li>多项逻辑回归</li></ol><p>三个或更多类别，无需订购。示例：预测哪种食物更受欢迎（蔬菜，非蔬菜，素食主义者）</p><ol><li>有序逻辑回归</li></ol><p>带有订购的三个或更多类别。示例：电影分级从1到5</p><p><strong><em>决策边界\</em></strong></p><p>为了预测数据属于哪个类别，可以设置一个阈值。基于该阈值，将获得的估计概率分类。</p><p>比如，predicted_value≥0.5，则将电子邮件分类为垃圾邮件，否则分类为非垃圾邮件。</p><p>决策边界可以是线性的或非线性的。可以增加多项式阶数以获得复杂的决策边界。</p><p>为什么已经用于线性的成本函数不能用于物流？</p><p>线性回归使用均方误差作为成本函数。如果将其用于逻辑回归，则它将是参数（theta）的非凸函数。仅当函数为凸函数时，梯度下降才会收敛到全局最小值。</p><p><img src="https://ningshixian.github.io/resources/images/convex vs non-convex.png" alt=""></p><p>图5：凸和非凸成本函数</p><p><strong><em>成本函数说明\</em></strong></p><p><img src="https://ningshixian.github.io/resources/images/cost function 1.jpeg" alt=""></p><p>图6：成本函数第1部分</p><p><img src="https://ningshixian.github.io/resources/images/cost function 2.jpeg" alt=""></p><p>图7：成本函数第2部分</p><p><strong><em>简化成本函数\</em></strong></p><p><img src="https://ningshixian.github.io/resources/images/simiplified cost function.png" alt=""></p><p>图8：简化的成本函数</p><p><strong><em>为什么要使用此成本函数？\</em></strong></p><p><img src="https://ningshixian.github.io/resources/images/maximum likelihood 1.jpeg" alt=""></p><p>图9：最大似然说明第1部分</p><p><img src="https://ningshixian.github.io/resources/images/maximum likelihood 2.jpeg" alt=""></p><p>图10：最大似然说明第2部分</p><p>这个负函数是因为当我们训练时，我们需要通过最小化损失函数来最大化概率。假设从相同独立的分布中抽取样本，则降低成本将增加最大可能性。</p><p><strong><em>推导梯度下降算法的公式\</em></strong></p><p><img src="https://ningshixian.github.io/resources/images/gradient1.jpeg" alt=""></p><p>图11：梯度下降算法第1部分</p><p><img src="https://ningshixian.github.io/resources/images/gradient2.jpeg" alt=""></p><p>图12：梯度下降第2部分</p><p><strong><em>Python实现\</em></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">weightInitialization</span>(<span class="hljs-params">n_features</span>):<br>    w = np.zeros((<span class="hljs-number">1</span>,n_features))<br>    b = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">return</span> w,b<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_activation</span>(<span class="hljs-params">result</span>):<br>    final_result = <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-result))<br>    <span class="hljs-keyword">return</span> final_result<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_optimize</span>(<span class="hljs-params">w, b, X, Y</span>):<br>    m = X.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment">#Prediction</span><br>    final_result = sigmoid_activation(np.dot(w,X.T)+b)<br>    Y_T = Y.T<br>    cost = (-<span class="hljs-number">1</span>/m)*(np.<span class="hljs-built_in">sum</span>((Y_T*np.log(final_result)) + ((<span class="hljs-number">1</span>-Y_T)*(np.log(<span class="hljs-number">1</span>-final_result)))))<br>    <span class="hljs-comment">#</span><br>    <br>    <span class="hljs-comment">#Gradient calculation</span><br>    dw = (<span class="hljs-number">1</span>/m)*(np.dot(X.T, (final_result-Y.T).T))<br>    db = (<span class="hljs-number">1</span>/m)*(np.<span class="hljs-built_in">sum</span>(final_result-Y.T))<br>    <br>    grads = &#123;<span class="hljs-string">&quot;dw&quot;</span>: dw, <span class="hljs-string">&quot;db&quot;</span>: db&#125;<br>    <br>    <span class="hljs-keyword">return</span> grads, cost<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_predict</span>(<span class="hljs-params">w, b, X, Y, learning_rate, no_iterations</span>):<br>    costs = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(no_iterations):<br>        <span class="hljs-comment">#</span><br>        grads, cost = model_optimize(w,b,X,Y)<br>        <span class="hljs-comment">#</span><br>        dw = grads[<span class="hljs-string">&quot;dw&quot;</span>]<br>        db = grads[<span class="hljs-string">&quot;db&quot;</span>]<br>        <span class="hljs-comment">#weight update</span><br>        w = w - (learning_rate * (dw.T))<br>        b = b - (learning_rate * db)<br>        <span class="hljs-comment">#</span><br>        <br>        <span class="hljs-keyword">if</span> (i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>):<br>            costs.append(cost)<br>            <span class="hljs-comment">#print(&quot;Cost after %i iteration is %f&quot; %(i, cost))</span><br>    <br>    <span class="hljs-comment">#final parameters</span><br>    coeff = &#123;<span class="hljs-string">&quot;w&quot;</span>: w, <span class="hljs-string">&quot;b&quot;</span>: b&#125;<br>    gradient = &#123;<span class="hljs-string">&quot;dw&quot;</span>: dw, <span class="hljs-string">&quot;db&quot;</span>: db&#125;<br>    <br>    <span class="hljs-keyword">return</span> coeff, gradient, costs<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">final_pred, m</span>):<br>    y_pred = np.zeros((<span class="hljs-number">1</span>,m))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(final_pred.shape[<span class="hljs-number">1</span>]):<br>        <span class="hljs-keyword">if</span> final_pred[<span class="hljs-number">0</span>][i] &gt; <span class="hljs-number">0.5</span>:<br>            y_pred[<span class="hljs-number">0</span>][i] = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> y_pred<br></code></pre></td></tr></table></figure><p>Cost vs Number_of_Iterations</p><p><img src="https://ningshixian.github.io/resources/images/cost reduction.png" alt=""></p><p>系统的训练和测试精度为100％</p><p>此实现用于二进制逻辑回归。对于具有2个以上类别的数据，必须使用softmax回归。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc">Logistic Regression — Detailed Overview</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL总结</title>
    <link href="/2020/04/29/2020-04-29-MySQL%E6%80%BB%E7%BB%93/"/>
    <url>/2020/04/29/2020-04-29-MySQL%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="MySQL总结"><a href="#MySQL总结" class="headerlink" title="MySQL总结"></a>MySQL总结</h1><p><a href="https://github.com/Snailclimb/JavaGuide/blob/master/docs/database/MySQL.md#什么是mysql">什么是MySQL?</a></p><p><a href="https://www.w3schools.com/sql/exercise.asp">SQL语句在线练习</a> （非常不错）</p><p>主要内容如下：</p><ul><li>什么是MySQL?</li><li>MySQL基础</li><li>MySQL语句总结</li><li>MySQL架构</li><li><p>附录：SQL注入安全</p><span id="more"></span></li></ul><h2 id="什么是MySQL"><a href="#什么是MySQL" class="headerlink" title="什么是MySQL?"></a>什么是MySQL?</h2><p>MySQL 是一个关系型数据库管理系统，由瑞典 MySQL AB 公司开发，目前属于 Oracle 公司。MySQL 是一种关联数据库管理系统，关联数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。</p><ul><li>MySQL 是开源的，目前隶属于 Oracle 旗下产品。</li><li>MySQL 支持大型的数据库。可以处理拥有上千万条记录的大型数据库。</li><li>MySQL 使用标准的 SQL 数据语言形式。</li><li>MySQL 可以运行于多个系统上，并且支持多种语言。这些编程语言包括 C、C++、Python、Java、Perl、PHP、Eiffel、Ruby 和 Tcl 等。</li><li>MySQL 对PHP有很好的支持，PHP 是目前最流行的 Web 开发语言。</li><li>MySQL 支持大型数据库，支持 5000 万条记录的数据仓库，32 位系统表文件最大可支持 4GB，64 位系统支持最大的表文件为8TB。</li><li>MySQL 是可以定制的，采用了 GPL 协议，你可以修改源码来开发自己的 MySQL 系统。</li></ul><h2 id="MySQL基础"><a href="#MySQL基础" class="headerlink" title="MySQL基础"></a>MySQL基础</h2><h3 id="SQL-DB-DBMS-分别是什么？"><a href="#SQL-DB-DBMS-分别是什么？" class="headerlink" title="SQL/DB/DBMS 分别是什么？"></a>SQL/DB/DBMS 分别是什么？</h3><blockquote><p>DBMS 负责执行 sql 语句， 通过执行 sql 语句来操作 db 中的数据 </p><p>DBMS -（执行）-&gt; SQL 语句 -（操作）-&gt; DB</p></blockquote><p>DB：DataBase (数据库， 实际上在硬盘上以文件的形似存在)</p><p>DBMS：Database Management System（数据库管理系统，常见的有：MySQL, Oracle DB2 Sybase, SqlServer…）</p><p>SQL:结构化查询语言， 是一门标准通用语言， 标准的sql语句适用于所有的数据库产偶</p><h3 id="什么是表？"><a href="#什么是表？" class="headerlink" title="什么是表？"></a>什么是表？</h3><p>表是数据库的基本组成单元， 所有的数据都以表格的格式组织 一个表包括行和列： 行： 被称为数据/记录 列：被称为字段</p><ul><li>每个字段名包含哪些信息： 字段名， 数据类型， 相关约束</li></ul><h2 id="MySQL语句总结"><a href="#MySQL语句总结" class="headerlink" title="MySQL语句总结"></a>MySQL语句总结</h2><p><a href="https://www.runoob.com/sql/sql-tutorial.html">SQL 教程（菜鸟教程）</a></p><p><a href="http://www.runoob.com/MySQL/MySQL-tutorial.html">MySQL 教程（菜鸟教程）</a></p><p><strong>插入</strong></p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lasso">INSERT <span class="hljs-keyword">INTO</span> table_name ( field1, field2,<span class="hljs-params">...</span>fieldN )<br>                       VALUES<br>                       ( value1, value2,<span class="hljs-params">...</span>valueN );<br></code></pre></td></tr></table></figure><p><strong>查询</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">column_name</span>,<span class="hljs-built_in">column_name</span><br><span class="hljs-keyword">FROM</span> <span class="hljs-built_in">table_name</span><br>[<span class="hljs-keyword">WHERE</span> Clause]<br>[<span class="hljs-keyword">LIMIT</span> N][ <span class="hljs-keyword">OFFSET</span> M]<br></code></pre></td></tr></table></figure><p><strong>更新</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">UPDATE</span> <span class="hljs-built_in">table_name</span> <span class="hljs-keyword">SET</span> field1=<span class="hljs-built_in">new</span>-value1, field2=<span class="hljs-built_in">new</span>-value2<br>[<span class="hljs-keyword">WHERE</span> Clause]<br></code></pre></td></tr></table></figure><p><strong>删除</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">DELETE</span> <span class="hljs-keyword">FROM</span> <span class="hljs-built_in">table_name</span> [<span class="hljs-keyword">WHERE</span> Clause]<br></code></pre></td></tr></table></figure><p><strong>LIKE</strong></p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">SELECT</span> field1, field2,...fieldN <br><span class="hljs-keyword">FROM</span> table_name<br><span class="hljs-keyword">WHERE</span> field1 <span class="hljs-keyword">LIKE</span> <span class="hljs-string">&#x27;%xxx%&#x27;</span><br></code></pre></td></tr></table></figure><p><strong>UNION 语句</strong>：用于将不同表中相同列中查询的数据展示出来；（不包括重复数据）</p><p><strong>UNION ALL 语句</strong>：用于将不同表中相同列中查询的数据展示出来；（包括重复数据）</p><p>使用形式如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">SELECT</span> 列名称 <span class="hljs-keyword">FROM</span> 表名称 <span class="hljs-keyword">UNION</span> <span class="hljs-keyword">SELECT</span> 列名称 <span class="hljs-keyword">FROM</span> 表名称 <span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> 列名称；<br><span class="hljs-keyword">SELECT</span> 列名称 <span class="hljs-keyword">FROM</span> 表名称 <span class="hljs-keyword">UNION</span> <span class="hljs-keyword">ALL</span> <span class="hljs-keyword">SELECT</span> 列名称 <span class="hljs-keyword">FROM</span> 表名称 <span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> 列名称；<br></code></pre></td></tr></table></figure><p><strong>排序 order</strong></p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> table_name1 <br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> 排序字段 [<span class="hljs-keyword">ASC</span> [<span class="hljs-keyword">DESC</span>][默认 <span class="hljs-keyword">ASC</span>]]<br></code></pre></td></tr></table></figure><p><strong>分组 group</strong></p><p>在分组的列上我们可以使用 COUNT, SUM, AVG,等函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mysql">select 字段名，分组名<br>from 表名<br>where 筛选条件  // 分组前筛选<br>group by 分组名<br>having 分组后筛选条件  //分组后筛选<br></code></pre></td></tr></table></figure><p><strong>联合多表查询 JOIN</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs mysql">select 字段名<br>from 表名 as 别名<br>【连接类型】join 表名 as 别名<br>on 连接条件<br>where 筛选条件<br>order by 排序字段<br></code></pre></td></tr></table></figure><p>其中【连接类型包括】大致分为如下三类：</p><ul><li><strong>INNER JOIN（内连接,或等值连接）</strong>：获取两个表中字段匹配关系的记录</li><li><strong>LEFT JOIN（左连接）：</strong>获取左表所有记录，即使右表没有对应匹配的记录；左表是主表</li><li><strong>RIGHT JOIN（右连接）：</strong> 与 LEFT JOIN 相反；右边是主表</li></ul><p><strong>过滤重复数据</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">DISTINCT</span> last_name, first_name<br><span class="hljs-keyword">FROM</span> <span class="hljs-keyword">table</span>;<br></code></pre></td></tr></table></figure><p><strong>NULL 值处理</strong></p><p>为了处理这种情况，MySQL提供了三大运算符:</p><ul><li><strong>IS NULL:</strong> 当列的值是 NULL,此运算符返回 true。</li><li><strong>IS NOT NULL:</strong> 当列的值不为 NULL, 运算符返回 true。</li><li><strong>&lt;=&gt;:</strong> 比较操作符（不同于 = 运算符），当比较的的两个值相等或者都为 NULL 时返回 true。</li></ul><p><strong>视图 view</strong></p><blockquote><p>多个地方用到同样的查询且SQL相对复杂。不保存结果，只保存SQL。</p><p>优点：重用SQL，保护和封装数据。</p><p>可以进行插入或者修改，不建议这样，视图最好只有‘只读’权限。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mysql">create view 视图名<br>as<br>查询语句；<br></code></pre></td></tr></table></figure><h3 id="MySQL-函数"><a href="#MySQL-函数" class="headerlink" title="MySQL 函数"></a>MySQL 函数</h3><p><strong>MySQL 字符串函数</strong></p><div class="table-container"><table><thead><tr><th>函数</th><th>描述</th></tr></thead><tbody><tr><td>CHAR_LENGTH(s)</td><td>返回字符串 s 的字符数</td></tr><tr><td>CONCAT(s1,s2…sn)</td><td>字符串 s1,s2 等多个字符串合并为一个字符串</td></tr><tr><td>FIND_IN_SET(s1,s2)</td><td>返回在字符串s2中与s1匹配的字符串的位置</td></tr><tr><td>LOCATE(s1,s)</td><td>从字符串 s 中获取 s1 的开始位置</td></tr><tr><td>LOWER(s)</td><td>将字符串 s 的所有字母变成小写字母</td></tr><tr><td>STRCMP(s1,s2)</td><td>比较字符串 s1 和 s2，如果 s1 与 s2 相等返回 0 ，<br />如果 s1&gt;s2 返回 1，如果 s1&lt;s2 返回 -1</td></tr></tbody></table></div><p><strong>MySQL 数字函数</strong></p><div class="table-container"><table><thead><tr><th style="text-align:left">函数名</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">ABS(x)</td><td style="text-align:left">返回 x 的绝对值</td></tr><tr><td style="text-align:left">AVG(expression)</td><td style="text-align:left">返回一个表达式的平均值</td></tr><tr><td style="text-align:left">CEIL(x)</td><td style="text-align:left">返回大于或等于 x 的最小整数</td></tr><tr><td style="text-align:left">COUNT(expression)</td><td style="text-align:left">返回查询的记录总数</td></tr><tr><td style="text-align:left">FLOOR(x)</td><td style="text-align:left">返回小于或等于 x 的最大整数</td></tr><tr><td style="text-align:left">MAX(expression)</td><td style="text-align:left">返回字段 expression 中的最大值</td></tr><tr><td style="text-align:left">MIN(expression)</td><td style="text-align:left">返回字段 expression 中的最小值</td></tr><tr><td style="text-align:left">MOD(x,y)</td><td style="text-align:left">返回 x 除以 y 以后的余数</td></tr></tbody></table></div><p><strong>MySQL 日期函数</strong></p><div class="table-container"><table><thead><tr><th style="text-align:left">函数名</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">CURDATE()</td><td style="text-align:left">返回当前日期</td></tr><tr><td style="text-align:left">CURRENT_TIME</td><td style="text-align:left">返回当前时间</td></tr><tr><td style="text-align:left">CURRENT_TIMESTAMP()</td><td style="text-align:left">返回当前日期和时间</td></tr><tr><td style="text-align:left">DATE()</td><td style="text-align:left">从日期或日期时间表达式中提取日期值</td></tr><tr><td style="text-align:left">DATE_FORMAT(d,f)</td><td style="text-align:left">按表达式 f的要求显示日期 d</td></tr><tr><td style="text-align:left">DAY(d)</td><td style="text-align:left">返回日期值 d 的日期部分</td></tr><tr><td style="text-align:left"></td></tr></tbody></table></div><h2 id="MySQL架构"><a href="#MySQL架构" class="headerlink" title="MySQL架构"></a>MySQL架构</h2><p>和其它数据库相比，MySQL有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎的架构上，<strong>插件式的存储引擎架构将查询处理和其它的系统任务以及数据的存储提取相分离</strong>。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。</p><p><img src="https:////upload-images.jianshu.io/upload_images/19895418-141f44f4a1dd139f?imageMogr2/auto-orient/strip|imageView2/2/w/1047/format/webp" alt="img"></p><p>从上图中，我们可以看出，<code>MySQL</code>体系中包含以下几个重要的部分：</p><ul><li><strong>连接层</strong>：最上层是一些客户端和连接服务。<strong>主要完成一些类似于连接处理、授权认证、及相关的安全方案</strong>。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。</li><li><strong>服务层</strong>：第二层服务层，主要完成大部分的核心服务功能， 包括查询解析、分析、优化、缓存、以及所有的内置函数，所有跨存储引擎的功能也都在这一层实现，包括触发器、存储过程、视图等</li><li><strong>引擎层</strong>：第三层存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取</li><li><strong>存储层</strong>：第四层为数据存储层，主要是将数据存储在运行于该设备的文件系统之上，并完成与存储引擎的交互</li></ul><h3 id="存储引擎"><a href="#存储引擎" class="headerlink" title="存储引擎"></a>存储引擎</h3><p>存储引擎是MySQL的组件，用于处理不同表类型的SQL操作。不同的存储引擎提供不同的存储机制、索引技巧、锁定水平等功能，使用不同的存储引擎，还可以获得特定的功能。</p><p>MySQL服务器使用<strong>可插拔</strong>的存储引擎体系结构，可以从运行中的 MySQL 服务器加载或卸载存储引擎 。常见的存储引擎就 InnoDB、MyISAM、Memory、NDB。InnoDB 现在是 MySQL 默认的存储引擎，支持<strong>事务、行级锁定和外键</strong>，它具有以下特性：</p><ul><li>☆通过多版本并发（<code>MVCC</code>）实现高并发性能</li><li>☆支持<code>SQL</code>标准的4种隔离级别，默认为<code>Repatable</code>级别</li><li>☆通过间隙锁（<code>Next-Key locking</code>）来避免幻读</li><li>提供插入缓冲（<code>insert-buffer</code>）</li><li>支持二次写（<code>double write</code>）</li><li>自适应哈希索引（<code>adaptive hash index</code>）</li><li>预读（<code>read ahead</code>）</li><li>…</li></ul><h3 id="MySQL-索引"><a href="#MySQL-索引" class="headerlink" title="MySQL 索引"></a>MySQL 索引</h3><p>MYSQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构，所以说<strong>索引的本质是：数据结构</strong>。</p><p>索引的目的在于提高查询效率，可以类比字典、 火车站的车次表、图书的目录等 。</p><p>可以简单的理解为“排好序的快速查找数据结构”，数据本身之外，<strong>数据库还维护者一个满足特定查找算法的数据结构</strong>，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。下图是一种可能的索引方式示例。</p><p><img src="https:////upload-images.jianshu.io/upload_images/19895418-abe72d9a126cff9e?imageMogr2/auto-orient/strip|imageView2/2/w/605/format/webp" alt="img"></p><p>左边的数据表，一共有两列七条记录，最左边的是数据记录的物理地址。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值，和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在一定的复杂度内获取到对应的数据，从而快速检索出符合条件的记录。</p><p>索引本身也很大，不可能全部存储在内存中，<strong>一般以索引文件的形式存储在磁盘上</strong></p><p>在<code>InnoDB</code>中，主要包含3种索引：</p><ul><li>B+树索引</li><li>全文索引</li><li>哈希索引</li></ul><p>在<code>InnoDB</code>中，索引通过<code>B+</code>树来实现，<code>B+</code>树作为索引的优点：</p><ul><li>非叶子节点不存储数据，使得每页能存储更多索引，减少<code>IO</code>的次数</li><li>叶子节点存在相互索引的指针，便于范围查找</li><li>性能稳定，每次查询都需要通过相同的次数才能获取到需要的数据</li></ul><p><strong>优势</strong></p><ul><li><strong>提高数据检索效率，降低数据库IO成本</strong></li><li><strong>降低数据排序的成本，降低CPU的消耗</strong></li></ul><p><strong>劣势</strong></p><ul><li>索引也是一张表，保存了主键和索引字段，并指向实体表的记录，所以也需要占用内存</li><li>虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段， 都会调整因为更新所带来的键值变化后的索引信息</li></ul><h3 id="查询缓存的使用"><a href="#查询缓存的使用" class="headerlink" title="查询缓存的使用"></a>查询缓存的使用</h3><blockquote><p>执行查询语句的时候，会先查询缓存。不过，MySQL 8.0 版本后移除，因为这个功能不太实用</p></blockquote><h3 id="MySQL-事务"><a href="#MySQL-事务" class="headerlink" title="MySQL 事务"></a>MySQL 事务</h3><h4 id="什么是事务"><a href="#什么是事务" class="headerlink" title="什么是事务?"></a>什么是事务?</h4><p><strong>事务是逻辑上的一组操作，要么都执行，要么都不执行。</strong></p><p>事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。</p><h4 id="事务的四大特性-ACID"><a href="#事务的四大特性-ACID" class="headerlink" title="事务的四大特性(ACID)"></a>事务的四大特性(ACID)</h4><ol><li><strong>原子性（Atomicity）：</strong> 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；</li><li><strong>一致性（Consistency）：</strong> 执行事务后，数据库从一个正确的状态变化到另一个正确的状态；</li><li><strong>隔离性（Isolation）：</strong> 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；</li><li><strong>持久性（Durability）：</strong> 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。</li></ol><p><strong>隔离性</strong></p><p><code>InnoDB</code>的隔离性则通过锁和<code>MVCC</code>来实现，由于完整的隔离性，只能通过互斥锁来实现，而互斥锁带来的问题便是性能急剧下降，因此处于对性能的妥协和结合日常业务的使用，根据加锁的程度不同，又将隔离性分为以下四个级别：</p><ul><li>读未提交(<code>READ UNCOMMITTED</code>)： 会出现脏读的问题</li><li>读已提交(<code>READ COMMITTED</code>)： 不会出现脏读，但是会出现不可能重复读的问题</li><li>可重复读(<code>REPEATABLE READ</code>)： 不会出现不可重复读，但是会出现幻读的问题</li><li>串行化(<code>SERIALIZABLE</code>)： 不会出现幻读问题</li></ul><h4 id="并发事务带来哪些问题"><a href="#并发事务带来哪些问题" class="headerlink" title="并发事务带来哪些问题?"></a>并发事务带来哪些问题?</h4><p>在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。</p><ul><li><strong>脏读（Dirty read）:</strong> 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。</li><li><strong>丢失修改（Lost to modify）:</strong> 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。</li><li><strong>不可重复读（Unrepeatableread）:</strong> 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。</li><li><strong>幻读（Phantom read）:</strong> 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。</li></ul><h4 id="并发事务处理问题的解决办法"><a href="#并发事务处理问题的解决办法" class="headerlink" title="并发事务处理问题的解决办法"></a>并发事务处理问题的解决办法</h4><ul><li>“更新丢失”通常是应该完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。</li><li>“脏读” 、 “不可重复读”和“幻读” ，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决：</li><li>一种是加锁：在读取数据前，对其加锁，阻止其他事务对数据进行修改。</li><li>另一种是数据多版本并发控制（MultiVersion Concurrency Control，简称 <strong>MVCC</strong>或 MCC），也称为多版本数据库：不用加任何锁， 通过一定机制生成一个数据请求时间点的一致性数据快照 （Snapshot)， 并用这个快照来提供一定级别 （语句级或事务级） 的一致性读取。从用户的角度来看，好象是数据库可以提供同一数据的多个版本。</li></ul><h3 id="大表优化"><a href="#大表优化" class="headerlink" title="大表优化"></a>大表优化</h3><p>当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：</p><h4 id="1-限定数据的范围"><a href="#1-限定数据的范围" class="headerlink" title="1. 限定数据的范围"></a>1. 限定数据的范围</h4><p>务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；</p><h4 id="2-读-写分离"><a href="#2-读-写分离" class="headerlink" title="2. 读/写分离"></a>2. 读/写分离</h4><p>经典的数据库拆分方案，主库负责写，从库负责读；</p><h4 id="3-垂直分区"><a href="#3-垂直分区" class="headerlink" title="3. 垂直分区"></a>3. 垂直分区</h4><p><strong>根据数据库里面数据表的相关性进行拆分。</strong></p><h3 id="什么是池化-什么是数据库连接池-为什么需要数据库连接池"><a href="#什么是池化-什么是数据库连接池-为什么需要数据库连接池" class="headerlink" title="什么是池化?什么是数据库连接池?为什么需要数据库连接池?"></a>什么是池化?什么是数据库连接池?为什么需要数据库连接池?</h3><p>池化设计应该不是一个新名词。我们常见的如java线程池、jdbc连接池、redis连接池等就是这类设计的代表实现。这种设计会初始预设资源，解决的问题就是抵消每次获取资源的消耗，如创建线程的开销，获取远程连接的开销等。就好比你去食堂打饭，打饭的大妈会先把饭盛好几份放那里，你来了就直接拿着饭盒加菜即可，不用再临时又盛饭又打菜，效率就高了。除了初始化资源，池化设计还包括如下这些特征：池子的初始值、池子的活跃值、池子的最大值等，这些特征可以直接映射到java线程池和数据库连接池的成员属性中。这篇文章对<a href="https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&amp;mid=2247485679&amp;idx=1&amp;sn=57dbca8c9ad49e1f3968ecff04a4f735&amp;chksm=cea24724f9d5ce3212292fac291234a760c99c0960b5430d714269efe33554730b5f71208582&amp;token=1141994790&amp;lang=zh_CN#rd">池化设计思想</a>介绍的还不错，直接复制过来，避免重复造轮子了。</p><p>数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存。我们可以把数据库连接池是看做是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。<strong>在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中</strong>。 连接池还减少了用户必须等待建立与数据库的连接的时间。</p><h3 id="一条SQL语句在MySQL中如何执行的"><a href="#一条SQL语句在MySQL中如何执行的" class="headerlink" title="一条SQL语句在MySQL中如何执行的"></a>一条SQL语句在MySQL中如何执行的</h3><p>客户端请求 —-&gt; 连接器（验证用户身份，给予权限）  —-&gt; 查询缓存（存在缓存则直接返回，不存在则执行后续操作） —-&gt; 分析器（对SQL进行词法分析和语法分析操作）  —-&gt; 优化器（主要对执行的sql优化选择最优的执行方案方法）  —-&gt; 执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口） —-&gt; 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果）图：极客时间</p><p><img src="https:////upload-images.jianshu.io/upload_images/19895418-e2bb5f705402c3f6?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp" alt="img"></p><p><strong>分析器执行顺序</strong></p><ul><li>首先，应该明确数据来源，因此需要解析<code>FROM</code>，找到对应的数据表</li><li>找到数据表后，下一步便是通过过滤条件，获取具体的数据行，因此需要解析<code>WHERE</code></li><li>拿到具体的数据行之后，接下来需要处理数据行，因此需要解析<code>GROUP BY</code></li><li>通过<code>GROUP BY</code>分组之后，就可以将同一个组的数据进行聚合，因此解析<code>count(),sum(),arg()</code>等</li><li>通过聚合函数之后，则可以将聚合之后的数据进行过滤，因此解析<code>HAVING</code></li><li>处理完数据行之后，接下来查看用户需要的数据列，因此解析<code>SELECT</code></li><li>剩下的数据便是用户真真需要的数据，但是<code>SQL</code>可能会对数据进行排序等，因此处理 <code>DISTINCT</code>以及<code>ORDER</code></li><li>最后，排序完成后，查看用户是否需要返回指定的行，因此处理<code>LIMIT</code></li></ul><p>总结出来执行顺序便是：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">FROM`-&gt;`WHERE`-&gt;`GROUP BY`-&gt;`count()`-&gt;`HAVING`-&gt;`SELECT`-&gt;`DISTINCT`-&gt;`ORDER`-&gt;`LIMIT<br></code></pre></td></tr></table></figure><h3 id="一条SQL语句执行得很慢的原因有哪些？"><a href="#一条SQL语句执行得很慢的原因有哪些？" class="headerlink" title="一条SQL语句执行得很慢的原因有哪些？"></a>一条SQL语句执行得很慢的原因有哪些？</h3><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&amp;mid=2247485185&amp;idx=1&amp;sn=66ef08b4ab6af5757792223a83fc0d45&amp;chksm=cea248caf9d5c1dc72ec8a281ec16aa3ec3e8066dbb252e27362438a26c33fbe842b0e0adf47&amp;token=79317275&amp;lang=zh_CN#rd">腾讯面试：一条SQL语句执行得很慢的原因有哪些？—-不看后悔系列</a></p><h3 id="CHAR-和-VARCHAR-的区别？"><a href="#CHAR-和-VARCHAR-的区别？" class="headerlink" title="CHAR 和 VARCHAR 的区别？"></a>CHAR 和 VARCHAR 的区别？</h3><p>char是固定长度，varchar长度可变：</p><p>char(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数，比如 CHAR(30) 就可以存储 30 个字符。</p><p>存储时，前者不管实际存储数据的长度，直接按 char 规定的长度分配存储空间；而后者会根据实际存储的数据分配最终的存储空间</p><h3 id="Mysql-的内连接、左连接、右连接有什么区别？"><a href="#Mysql-的内连接、左连接、右连接有什么区别？" class="headerlink" title="Mysql 的内连接、左连接、右连接有什么区别？"></a>Mysql 的内连接、左连接、右连接有什么区别？</h3><p><img src="https://upload-images.jianshu.io/upload_images/19895418-1eed858856a1b34a?imageMogr2/auto-orient/strip|imageView2/2/w/966/format/webp" alt="img"></p><h1 id="附录：SQL注入安全"><a href="#附录：SQL注入安全" class="headerlink" title="附录：SQL注入安全"></a>附录：SQL注入安全</h1><h3 id="什么是sql注入呢"><a href="#什么是sql注入呢" class="headerlink" title="什么是sql注入呢?"></a>什么是sql注入呢?</h3><p>SQL注入即是指web应用程序对用户输入数据的合法性没有判断或过滤不严，攻击者可以在web应用程序中事先定义好的查询语句的结尾上添加额外的SQL语句，在管理员不知情的情况下实现非法操作，以此来实现欺骗数据库服务器执行非授权的任意查询，从而进一步得到相应的数据信息。 </p><!-- more --><h3 id="如何避免-sql-注入风险"><a href="#如何避免-sql-注入风险" class="headerlink" title="如何避免 sql 注入风险"></a>如何避免 sql 注入风险</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs javascript"> <span class="hljs-number">1.</span>永远不要信任用户的输入。对用户的输入进行校验，能够通过正则表达式，或限制长度；对单引号和<br>双<span class="hljs-string">&quot;-&quot;</span>进行转换等。检查输入的数据是否具有所期望的数据格式，严格限制变量的类型，例如使用regexp包进行一些匹配处理，<br>或者使用strconv包对字符串转化成其他基本类型的数据进行判断。<br><br><span class="hljs-number">2.</span>永远不要使用动态拼装sql，能够使用參数化的sql或者直接使用存储过程进行数据查询存取。<br><br><span class="hljs-number">3.</span>永远不要使用管理员权限的数据库连接，为每一个应用使用单独的权限有限的数据库连接。<br><br><span class="hljs-number">4.</span>不要把机密信息直接存放。加密或者hash掉password和敏感的信息。对进入数据库的特殊字符（<span class="hljs-string">&#x27;&quot;\尖括号&amp;*;等）进行转义处理，</span><br><span class="hljs-string">或编码转换</span><br><span class="hljs-string"></span><br><span class="hljs-string">5.应用的异常信息应该给出尽可能少的提示，最好使用自己定义的错误信息对原始错误信息进行包装,避免网站打印出SQL错误信息，</span><br><span class="hljs-string">比如类型错误、字段不匹配等，把代码里的SQL语句暴露出来，以防止攻击者利用这些错误信息进行SQL注入。</span><br><span class="hljs-string"></span><br><span class="hljs-string">6.sql注入的检測方法一般採取辅助软件或站点平台来检測。软件一般採用sql注入检測工具jsky，站点平台就有亿思站点安全平台检測工具。</span><br><span class="hljs-string">MDCSOFT SCAN等。採用MDCSOFT-IPS能够有效的防御SQL注入。XSS攻击等。</span><br><span class="hljs-string"></span><br><span class="hljs-string">7.严格限制Web应用的数据库的操作权限，给此用户提供仅仅能够满足其工作的最低权限，从而最大限度的减少注入攻击对数据库的危害。</span><br><span class="hljs-string"></span><br><span class="hljs-string">8.在应用发布之前建议使用专业的SQL注入检测工具进行检测，以及时修补被发现的SQL注入漏洞。网上有很多这方面的开源工具，</span><br><span class="hljs-string">例如sqlmap、SQLninja等。</span><br><span class="hljs-string"></span><br><span class="hljs-string">9.所有的查询语句建议使用数据库提供的参数化查询接口，参数化的语句使用参数而不是将用户输入变量嵌入到SQL语句中，</span><br><span class="hljs-string">即不要直接拼接SQL语句。例如使用database/sql里面的查询函数Prepare和Query，或者Exec(query string, args ...interface&#123;&#125;)。</span><br></code></pre></td></tr></table></figure><h3 id="pymysql-简单规避注入风险示列"><a href="#pymysql-简单规避注入风险示列" class="headerlink" title="pymysql 简单规避注入风险示列"></a>pymysql 简单规避注入风险示列</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs javascript">#错误示范  不要自己去拼接账户和密码<br><span class="hljs-keyword">import</span> pymysql<br>conn = pymysql.<span class="hljs-title function_">connect</span>(host=<span class="hljs-string">&#x27;127.0.0.1&#x27;</span>, user=<span class="hljs-string">&#x27;root&#x27;</span>, password=<span class="hljs-string">&#x27;318&#x27;</span>, database=<span class="hljs-string">&#x27;ftp&#x27;</span>)<br>cur = conn.<span class="hljs-title function_">cursor</span>()<br>username = <span class="hljs-title function_">input</span>(<span class="hljs-string">&#x27;user &gt;&gt;&gt;&#x27;</span>)<br>password = <span class="hljs-title function_">input</span>(<span class="hljs-string">&#x27;passwd &gt;&gt;&gt;&#x27;</span>)<br>sql = <span class="hljs-string">&quot;select * from userinfo where name = %s and password =  %s ;&quot;</span>% (username, password)<br>cur.<span class="hljs-title function_">execute</span>(sql)<br><span class="hljs-title function_">print</span>(cur.<span class="hljs-title function_">fetchone</span>())<br>cur.<span class="hljs-title function_">close</span>()<br>conn.<span class="hljs-title function_">close</span>()<br><br>user &gt;&gt;&gt;<span class="hljs-string">&#x27;我不知道账号&#x27;</span> or <span class="hljs-number">1</span>=<span class="hljs-number">1</span>;--<br>passwd &gt;&gt;&gt;我也不知道密码<br>(<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;凯歌318&#x27;</span>, <span class="hljs-string">&#x27;666&#x27;</span>)<br><span class="hljs-title class_">Process</span> finished <span class="hljs-keyword">with</span> exit code <span class="hljs-number">0</span><br><br><br>#正确方法   cur.<span class="hljs-title function_">execute</span>(sql, (username, password)) 把密码和账户交给 execute去拼接<br><span class="hljs-keyword">import</span> pymysql<br>conn = pymysql.<span class="hljs-title function_">connect</span>(host=<span class="hljs-string">&#x27;127.0.0.1&#x27;</span>, user=<span class="hljs-string">&#x27;root&#x27;</span>, password=<span class="hljs-string">&#x27;318&#x27;</span>, database=<span class="hljs-string">&#x27;ftp&#x27;</span>)<br>cur = conn.<span class="hljs-title function_">cursor</span>()<br>username = <span class="hljs-title function_">input</span>(<span class="hljs-string">&#x27;user &gt;&gt;&gt;&#x27;</span>)<br>password = <span class="hljs-title function_">input</span>(<span class="hljs-string">&#x27;pwd &gt;&gt;&gt;&#x27;</span>)<br>sql = <span class="hljs-string">&quot;select * from userinfo where name = %s and pwd = %s&quot;</span><br>cur.<span class="hljs-title function_">execute</span>(sql, (username, password))<br><span class="hljs-title function_">print</span>(cur.<span class="hljs-title function_">fetchone</span>())<br>cur.<span class="hljs-title function_">close</span>()<br>conn.<span class="hljs-title function_">close</span>()<br><br>user &gt;&gt;&gt;<span class="hljs-string">&#x27;我不知道账号&#x27;</span> or <span class="hljs-number">1</span>=<span class="hljs-number">1</span>;--<br>pwd &gt;&gt;&gt;也不知道密码<br><span class="hljs-title class_">None</span><br><br><span class="hljs-title class_">Process</span> finished <span class="hljs-keyword">with</span> exit code <span class="hljs-number">0</span><br><br>user &gt;&gt;&gt;凯歌<span class="hljs-number">318</span><br>pwd &gt;&gt;&gt;<span class="hljs-number">666</span><br>(<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;凯歌318&#x27;</span>, <span class="hljs-string">&#x27;666&#x27;</span>)<br><span class="hljs-title class_">Process</span> finished <span class="hljs-keyword">with</span> exit code <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h3 id="特殊情况-IN-不定长参数"><a href="#特殊情况-IN-不定长参数" class="headerlink" title="特殊情况 - IN - 不定长参数"></a>特殊情况 - IN - 不定长参数</h3><p>有一个值列表，我想在一个<code>IN</code>子句中执行的 SQL 语句。 例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">files = [<span class="hljs-string">&#x27;file1&#x27;</span>, <span class="hljs-string">&#x27;file2&#x27;</span>, ...]  <span class="hljs-comment"># this list can have a variable number of elements</span><br>con = pymysql.connect(...)<br>cur = conn.cursor()<br><br>result = cur.execute(<span class="hljs-string">&#x27;SELECT * FROM sometable WHERE file_name IN (?)&#x27;</span>, files)<br></code></pre></td></tr></table></figure><p>但是当我执行上面的语句时，我得到一个错误，例如：</p><blockquote><p>ProgrammingError：（’SQL包含1个参数标记，但提供了18个参数’，’HY000’）</p></blockquote><p>我可以使用以下内容生成变量参数字符串：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">params = <span class="hljs-string">&quot;,&quot;</span>.join([<span class="hljs-string">&quot;%s&quot;</span>] * <span class="hljs-built_in">len</span>(files))<br>query = <span class="hljs-string">&#x27;SELECT * FROM sometable WHERE file_name IN (&#123;&#125;)&#x27;</span>.<span class="hljs-built_in">format</span>(params)<br>con.ping(reconnect=<span class="hljs-literal">True</span>)<br><br>result = cur.execute(query, files)<br></code></pre></td></tr></table></figure><p>这样就可以避免SQL注入的风险。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="[https://stackoom.com/question/3tkvV/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%B8%A6%E6%9C%89IN%E5%AD%90%E5%8F%A5%E7%9A%84SQL%E5%8F%82%E6%95%B0%E4%B8%BApyodbc%E6%8F%90%E4%BE%9B%E5%8F%AF%E5%8F%98%E6%95%B0%E9%87%8F%E7%9A%84%E5%80%BC](https://stackoom.com/question/3tkvV/如何使用带有IN子句的SQL参数为pyodbc提供可变数量的值">如何使用带有IN子句的SQL参数为pyodbc提供可变数量的值？</a>)</p><p><a href="https://cloud.tencent.com/developer/article/1514599">sql 注入风险</a></p><p><a href="https://www.jianshu.com/p/24e1179ef563">MySQL 三万字精华总结 + 面试100 问，吊打面试官绰绰有余（收藏系列）</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL语法</title>
    <link href="/2020/04/29/2020-04-29-SQL%E8%AF%AD%E6%B3%95/"/>
    <url>/2020/04/29/2020-04-29-SQL%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="一、基础"><a href="#一、基础" class="headerlink" title="一、基础"></a>一、基础</h2><p>模式定义了数据如何存储、存储什么样的数据以及数据如何分解等信息，数据库和表都有模式。</p><p>主键的值不允许修改，也不允许复用（不能将已经删除的主键值赋给新数据行的主键）。</p><p>SQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。</p><p>SQL 语句不区分大小写，但是数据库表名、列名和值是否区分依赖于具体的 DBMS 以及配置。</p><p>SQL 支持以下三种注释：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql">## 注释<br><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> mytable; <span class="hljs-comment">-- 注释</span><br><span class="hljs-comment">/* 注释1</span><br><span class="hljs-comment">   注释2 */</span><br></code></pre></td></tr></table></figure><p>数据库创建与使用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> DATABASE test;<br>USE test;<br></code></pre></td></tr></table></figure><h2 id="二、创建表"><a href="#二、创建表" class="headerlink" title="二、创建表"></a>二、创建表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> mytable (<br>  # <span class="hljs-type">int</span> 类型，不为空，自增<br>  id <span class="hljs-type">INT</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span> AUTO_INCREMENT,<br>  # <span class="hljs-type">int</span> 类型，不可为空，默认值为 <span class="hljs-number">1</span>，不为空<br>  col1 <span class="hljs-type">INT</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">DEFAULT</span> <span class="hljs-number">1</span>,<br>  # 变长字符串类型，最长为 <span class="hljs-number">45</span> 个字符，可以为空<br>  col2 <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">45</span>) <span class="hljs-keyword">NULL</span>,<br>  # 日期类型，可为空<br>  col3 <span class="hljs-type">DATE</span> <span class="hljs-keyword">NULL</span>,<br>  # 设置主键为 id<br>  <span class="hljs-keyword">PRIMARY</span> KEY (`id`));<br></code></pre></td></tr></table></figure><h2 id="三、修改表"><a href="#三、修改表" class="headerlink" title="三、修改表"></a>三、修改表</h2><p>添加列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> mytable<br><span class="hljs-keyword">ADD</span> col <span class="hljs-type">CHAR</span>(<span class="hljs-number">20</span>);<br></code></pre></td></tr></table></figure><p>删除列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> mytable<br><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">COLUMN</span> col;<br></code></pre></td></tr></table></figure><p>删除表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">TABLE</span> mytable;<br></code></pre></td></tr></table></figure><h2 id="四、插入"><a href="#四、插入" class="headerlink" title="四、插入"></a>四、插入</h2><p>普通插入</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> mytable(col1, col2)<br><span class="hljs-keyword">VALUES</span>(val1, val2);<br></code></pre></td></tr></table></figure><p>插入检索出来的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> mytable1(col1, col2)<br><span class="hljs-keyword">SELECT</span> col1, col2<br><span class="hljs-keyword">FROM</span> mytable2;<br></code></pre></td></tr></table></figure><p>将一个表的内容插入到一个新表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> newtable <span class="hljs-keyword">AS</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> mytable;<br></code></pre></td></tr></table></figure><h2 id="五、更新"><a href="#五、更新" class="headerlink" title="五、更新"></a>五、更新</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">UPDATE</span> mytable<br><span class="hljs-keyword">SET</span> col <span class="hljs-operator">=</span> val<br><span class="hljs-keyword">WHERE</span> id <span class="hljs-operator">=</span> <span class="hljs-number">1</span>;<br></code></pre></td></tr></table></figure><h2 id="六、删除"><a href="#六、删除" class="headerlink" title="六、删除"></a>六、删除</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">DELETE</span> <span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">WHERE</span> id <span class="hljs-operator">=</span> <span class="hljs-number">1</span>;<br></code></pre></td></tr></table></figure><p><strong>TRUNCATE TABLE</strong>   可以清空表，也就是删除所有行。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">TRUNCATE</span> <span class="hljs-keyword">TABLE</span> mytable;<br></code></pre></td></tr></table></figure><p>使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。</p><h2 id="七、查询"><a href="#七、查询" class="headerlink" title="七、查询"></a>七、查询</h2><h3 id="DISTINCT"><a href="#DISTINCT" class="headerlink" title="DISTINCT"></a>DISTINCT</h3><p>相同值只会出现一次。它作用于所有列，也就是说所有列的值都相同才算相同。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">DISTINCT</span> col1, col2<br><span class="hljs-keyword">FROM</span> mytable;<br></code></pre></td></tr></table></figure><h3 id="LIMIT"><a href="#LIMIT" class="headerlink" title="LIMIT"></a>LIMIT</h3><p>限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。</p><p>返回前 5 行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> mytable<br>LIMIT <span class="hljs-number">5</span>;<br></code></pre></td></tr></table></figure><h3 id="模糊查询：POSITION、LOCATE和INSTR"><a href="#模糊查询：POSITION、LOCATE和INSTR" class="headerlink" title="模糊查询：POSITION、LOCATE和INSTR"></a>模糊查询：POSITION、LOCATE和INSTR</h3><p>一.POSITION()函数<br>语法: POSITION(substr IN str)<br>返回字符串str中第一次出现子字符串substr的位置。<br><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex">SELECT position(&#x27;a&#x27; IN &#x27;nanana&#x27;);   <span class="hljs-params">#</span> 2<br></code></pre></td></tr></table></figure></p><p>二.LOCATE()函数<br>语法: LOCATE(substr,str,[pos])<br>回从位置pos开始的字符串str中第一次出现子字符串substr的位置。 如果substr不在str中，则返回0。 如果substr或str为NULL，则返回NULL。<br><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs latex">SELECT locate(&#x27;a&#x27;, &#x27;nanana&#x27;);       <span class="hljs-params">#</span> 2<br>SELECT locate(&#x27;a&#x27;, &#x27;nanana&#x27;, 3);    <span class="hljs-params">#</span> 4<br>SELECT locate(&#x27;b&#x27;, &#x27;nanana&#x27;);       <span class="hljs-params">#</span> 0<br>SELECT locate(10, &#x27;nanana&#x27;);        <span class="hljs-params">#</span> 0<br>SELECT locate(NULL , &#x27;nanana&#x27;);     <span class="hljs-params">#</span> null<br>SELECT locate(&#x27;a&#x27; , NULL );         <span class="hljs-params">#</span> null<br></code></pre></td></tr></table></figure></p><p>三.INSTR()函数<br>语法: INSTR(str,substr)<br>返回字符串str中第一次出现子字符串substr的位置。<br>INSTR()与LOCATE（）的双参数形式相同，只是参数的顺序相反。<br><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs latex">SELECT instr(&#x27;nanana&#x27;, &#x27;a&#x27;);        <span class="hljs-params">#</span> 2<br>SELECT instr(&#x27;nanana&#x27;, &#x27;e&#x27;);        <span class="hljs-params">#</span> 0<br></code></pre></td></tr></table></figure></p><h2 id="八、排序"><a href="#八、排序" class="headerlink" title="八、排序"></a>八、排序</h2><ul><li><strong>ASC</strong>  ：升序（默认）</li><li><strong>DESC</strong>  ：降序</li></ul><p>可以按多个列进行排序，并且为每个列指定不同的排序方式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> col1 <span class="hljs-keyword">DESC</span>, col2 <span class="hljs-keyword">ASC</span>;<br></code></pre></td></tr></table></figure><h2 id="九、过滤"><a href="#九、过滤" class="headerlink" title="九、过滤"></a>九、过滤</h2><p>不进行过滤的数据非常大，导致通过网络传输了多余的数据，从而浪费了网络带宽。因此尽量使用 SQL 语句来过滤不必要的数据，而不是传输所有的数据到客户端中然后由客户端进行过滤。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">WHERE</span> col <span class="hljs-keyword">IS</span> <span class="hljs-keyword">NULL</span>;<br></code></pre></td></tr></table></figure><p>下表显示了 WHERE 子句可用的操作符</p><div class="table-container"><table><thead><tr><th>操作符</th><th>说明</th></tr></thead><tbody><tr><td>=</td><td>等于</td></tr><tr><td>&lt;</td><td>小于</td></tr><tr><td>&gt;</td><td>大于</td></tr><tr><td>&lt;&gt; !=</td><td>不等于</td></tr><tr><td>&lt;= !&gt;</td><td>小于等于</td></tr><tr><td>&gt;= !&lt;</td><td>大于等于</td></tr><tr><td>BETWEEN</td><td>在两个值之间</td></tr><tr><td>IS NULL</td><td>为 NULL 值</td></tr></tbody></table></div><p>应该注意到，NULL 与 0、空字符串都不同。</p><p><strong>AND 和 OR</strong>   用于连接多个过滤条件。优先处理 AND，当一个过滤表达式涉及到多个 AND 和 OR 时，可以使用 () 来决定优先级，使得优先级关系更清晰。</p><p><strong>IN</strong>   操作符用于匹配一组值，其后也可以接一个 SELECT 子句，从而匹配子查询得到的一组值。</p><p><strong>NOT</strong>   操作符用于否定一个条件。</p><h2 id="十、通配符"><a href="#十、通配符" class="headerlink" title="十、通配符"></a>十、通配符</h2><p>通配符也是用在过滤语句中，但它只能用于文本字段。</p><ul><li><strong>%</strong>   匹配 &gt;=0 个任意字符； </li><li><strong>_</strong>   匹配 ==1 个任意字符； </li><li><strong>[ ]</strong>   可以匹配集合内的字符，例如 [ab] 将匹配字符 a 或者 b。用脱字符 ^ 可以对其进行否定，也就是不匹配集合内的字符。 </li></ul><p>使用 Like 来进行通配符匹配。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">WHERE</span> col <span class="hljs-keyword">LIKE</span> <span class="hljs-string">&#x27;[^AB]%&#x27;</span>; <span class="hljs-comment">-- 不以 A 和 B 开头的任意文本</span><br></code></pre></td></tr></table></figure><p>不要滥用通配符，通配符位于开头处匹配会非常慢。</p><h2 id="十一、计算字段"><a href="#十一、计算字段" class="headerlink" title="十一、计算字段"></a>十一、计算字段</h2><p>在数据库服务器上完成数据的转换和格式化的工作往往比客户端上快得多，并且转换和格式化后的数据量更少的话可以减少网络通信量。</p><p>计算字段通常需要使用   <strong>AS</strong>   来取别名，否则输出的时候字段名为计算表达式。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> col1 <span class="hljs-operator">*</span> col2 <span class="hljs-keyword">AS</span> alias<br><span class="hljs-keyword">FROM</span> mytable;<br></code></pre></td></tr></table></figure><p><strong>CONCAT()</strong>   用于连接两个字段。许多数据库会使用空格把一个值填充为列宽，因此连接的结果会出现一些不必要的空格，使用 <strong>TRIM()</strong> 可以去除首尾空格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> CONCAT(<span class="hljs-built_in">TRIM</span>(col1), <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-built_in">TRIM</span>(col2), <span class="hljs-string">&#x27;)&#x27;</span>) <span class="hljs-keyword">AS</span> concat_col<br><span class="hljs-keyword">FROM</span> mytable;<br></code></pre></td></tr></table></figure><h2 id="十二、函数"><a href="#十二、函数" class="headerlink" title="十二、函数"></a>十二、函数</h2><p>各个 DBMS 的函数都是不相同的，因此不可移植，以下主要是 MySQL 的函数。</p><h3 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h3><div class="table-container"><table><thead><tr><th>函 数</th><th>说 明</th></tr></thead><tbody><tr><td>AVG()</td><td>返回某列的平均值</td></tr><tr><td>COUNT()</td><td>返回某列的行数</td></tr><tr><td>MAX()</td><td>返回某列的最大值</td></tr><tr><td>MIN()</td><td>返回某列的最小值</td></tr><tr><td>SUM()</td><td>返回某列值之和</td></tr></tbody></table></div><p>AVG() 会忽略 NULL 行。</p><p>使用 DISTINCT 可以汇总不同的值。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">AVG</span>(<span class="hljs-keyword">DISTINCT</span> col1) <span class="hljs-keyword">AS</span> avg_col<br><span class="hljs-keyword">FROM</span> mytable;<br></code></pre></td></tr></table></figure><h3 id="文本处理"><a href="#文本处理" class="headerlink" title="文本处理"></a>文本处理</h3><div class="table-container"><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>LEFT()</td><td>左边的字符</td></tr><tr><td>RIGHT()</td><td>右边的字符</td></tr><tr><td>LOWER()</td><td>转换为小写字符</td></tr><tr><td>UPPER()</td><td>转换为大写字符</td></tr><tr><td>LTRIM()</td><td>去除左边的空格</td></tr><tr><td>RTRIM()</td><td>去除右边的空格</td></tr><tr><td>LENGTH()</td><td>长度</td></tr><tr><td>SOUNDEX()</td><td>转换为语音值</td></tr></tbody></table></div><p>其中，  <strong>SOUNDEX()</strong>   可以将一个字符串转换为描述其语音表示的字母数字模式。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">WHERE</span> SOUNDEX(col1) <span class="hljs-operator">=</span> SOUNDEX(<span class="hljs-string">&#x27;apple&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="日期和时间处理"><a href="#日期和时间处理" class="headerlink" title="日期和时间处理"></a>日期和时间处理</h3><ul><li>日期格式：YYYY-MM-DD</li><li>时间格式：HH:<zero-width space>MM:SS<br>| 函 数 | 说 明 |<br>| —- | —- |<br>| ADDDATE() | 增加一个日期（天、周等） |<br>| ADDTIME() | 增加一个时间（时、分等） |<br>| CURDATE() | 返回当前日期 |<br>| CURTIME() | 返回当前时间 |<br>| DATE() | 返回日期时间的日期部分 |<br>| DATEDIFF() | 计算两个日期之差 |<br>| DATE_ADD() | 高度灵活的日期运算函数 |<br>| DATE_FORMAT() | 返回一个格式化的日期或时间串 |<br>| DAY() | 返回一个日期的天数部分 |<br>| DAYOFWEEK() | 对于一个日期，返回对应的星期几 |<br>| HOUR() | 返回一个时间的小时部分 |<br>| MINUTE() | 返回一个时间的分钟部分 |<br>| MONTH() | 返回一个日期的月份部分 |<br>| NOW() | 返回当前日期和时间 |<br>| SECOND() | 返回一个时间的秒部分 |<br>| TIME() | 返回一个日期时间的时间部分 |<br>| YEAR() | 返回一个日期的年份部分 |</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql">mysql<span class="hljs-operator">&gt;</span> <span class="hljs-keyword">SELECT</span> NOW();<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2018</span>-<span class="hljs-number">4</span>-<span class="hljs-number">14</span> <span class="hljs-number">20</span>:<span class="hljs-number">25</span>:<span class="hljs-number">11</span><br></code></pre></td></tr></table></figure><h3 id="数值处理"><a href="#数值处理" class="headerlink" title="数值处理"></a>数值处理</h3><div class="table-container"><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>SIN()</td><td>正弦</td></tr><tr><td>COS()</td><td>余弦</td></tr><tr><td>TAN()</td><td>正切</td></tr><tr><td>ABS()</td><td>绝对值</td></tr><tr><td>SQRT()</td><td>平方根</td></tr><tr><td>MOD()</td><td>余数</td></tr><tr><td>EXP()</td><td>指数</td></tr><tr><td>PI()</td><td>圆周率</td></tr><tr><td>RAND()</td><td>随机数</td></tr></tbody></table></div><h2 id="十三、分组"><a href="#十三、分组" class="headerlink" title="十三、分组"></a>十三、分组</h2><p>把具有相同的数据值的行放在同一组中。</p><p>可以对同一分组数据使用汇总函数进行处理，例如求分组数据的平均值等。</p><p>指定的分组字段除了能按该字段进行分组，也会自动按该字段进行排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> col, <span class="hljs-built_in">COUNT</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">AS</span> num<br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> col;<br></code></pre></td></tr></table></figure><p>GROUP BY 自动按分组字段进行排序，ORDER BY 也可以按汇总字段来进行排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> col, <span class="hljs-built_in">COUNT</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">AS</span> num<br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> col<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> num;<br></code></pre></td></tr></table></figure><p>WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> col, <span class="hljs-built_in">COUNT</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">AS</span> num<br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">WHERE</span> col <span class="hljs-operator">&gt;</span> <span class="hljs-number">2</span><br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> col<br><span class="hljs-keyword">HAVING</span> num <span class="hljs-operator">&gt;=</span> <span class="hljs-number">2</span>;<br></code></pre></td></tr></table></figure><p>分组规定：</p><ul><li>GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前；</li><li>除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出；</li><li>NULL 的行会单独分为一组；</li><li>大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型。</li></ul><h2 id="十四、子查询"><a href="#十四、子查询" class="headerlink" title="十四、子查询"></a>十四、子查询</h2><p>子查询中只能返回一个字段的数据。</p><p>可以将子查询的结果作为 WHRER 语句的过滤条件：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> mytable1<br><span class="hljs-keyword">WHERE</span> col1 <span class="hljs-keyword">IN</span> (<span class="hljs-keyword">SELECT</span> col2<br>               <span class="hljs-keyword">FROM</span> mytable2);<br></code></pre></td></tr></table></figure><p>下面的语句可以检索出客户的订单数量，子查询语句会对第一个查询检索出的每个客户执行一次：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> cust_name, (<span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">COUNT</span>(<span class="hljs-operator">*</span>)<br>                   <span class="hljs-keyword">FROM</span> Orders<br>                   <span class="hljs-keyword">WHERE</span> Orders.cust_id <span class="hljs-operator">=</span> Customers.cust_id)<br>                   <span class="hljs-keyword">AS</span> orders_num<br><span class="hljs-keyword">FROM</span> Customers<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> cust_name;<br></code></pre></td></tr></table></figure><h2 id="十五、连接"><a href="#十五、连接" class="headerlink" title="十五、连接"></a>十五、连接</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/349339670">https://zhuanlan.zhihu.com/p/349339670</a></p></blockquote><p>连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。</p><p>连接可以替换子查询，并且比子查询的效率一般会更快。</p><p>可以用 AS 给列名、计算字段和表名取别名，给表名取别名是为了简化 SQL 语句以及连接相同表。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/8420697/1661412273396-2e954e9e-2842-4d06-abd4-6ea11445ab01.png#clientId=ub7e28fad-3d12-4&amp;from=paste&amp;height=380&amp;id=ud6f5e640&amp;name=image.png&amp;originHeight=760&amp;originWidth=966&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=445399&amp;status=done&amp;style=none&amp;taskId=u6d496b48-8153-4f79-bbd1-8e9256a49b9&amp;title=&amp;width=483" alt="image.png"></p><h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><p>内连接又称等值连接，使用 INNER JOIN 关键字。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> A.value, B.value<br><span class="hljs-keyword">FROM</span> tablea <span class="hljs-keyword">AS</span> A <span class="hljs-keyword">INNER</span> <span class="hljs-keyword">JOIN</span> tableb <span class="hljs-keyword">AS</span> B<br><span class="hljs-keyword">ON</span> A.key <span class="hljs-operator">=</span> B.key;<br></code></pre></td></tr></table></figure><p>可以不明确使用 INNER JOIN，而使用普通查询并在 WHERE 中将两个表中要连接的列用等值方法连接起来。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> A.value, B.value<br><span class="hljs-keyword">FROM</span> tablea <span class="hljs-keyword">AS</span> A, tableb <span class="hljs-keyword">AS</span> B<br><span class="hljs-keyword">WHERE</span> A.key <span class="hljs-operator">=</span> B.key;<br></code></pre></td></tr></table></figure><h3 id="自连接"><a href="#自连接" class="headerlink" title="自连接"></a>自连接</h3><p>自连接可以看成内连接的一种，只是连接的表是自身而已。</p><p>一张员工表，包含员工姓名和员工所属部门，要找出与 Jim 处在同一部门的所有员工姓名。</p><p>子查询版本</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> name<br><span class="hljs-keyword">FROM</span> employee<br><span class="hljs-keyword">WHERE</span> department <span class="hljs-operator">=</span> (<br>      <span class="hljs-keyword">SELECT</span> department<br>      <span class="hljs-keyword">FROM</span> employee<br>      <span class="hljs-keyword">WHERE</span> name <span class="hljs-operator">=</span> &quot;Jim&quot;);<br></code></pre></td></tr></table></figure><p>自连接版本</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> e1.name<br><span class="hljs-keyword">FROM</span> employee <span class="hljs-keyword">AS</span> e1 <span class="hljs-keyword">INNER</span> <span class="hljs-keyword">JOIN</span> employee <span class="hljs-keyword">AS</span> e2<br><span class="hljs-keyword">ON</span> e1.department <span class="hljs-operator">=</span> e2.department<br>      <span class="hljs-keyword">AND</span> e2.name <span class="hljs-operator">=</span> &quot;Jim&quot;;<br></code></pre></td></tr></table></figure><h3 id="自然连接"><a href="#自然连接" class="headerlink" title="自然连接"></a>自然连接</h3><p>自然连接是把同名列通过等值测试连接起来的，同名列可以有多个。</p><p>内连接和自然连接的区别：内连接提供连接的列，而自然连接自动连接所有同名列。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> A.value, B.value<br><span class="hljs-keyword">FROM</span> tablea <span class="hljs-keyword">AS</span> A <span class="hljs-keyword">NATURAL</span> <span class="hljs-keyword">JOIN</span> tableb <span class="hljs-keyword">AS</span> B;<br></code></pre></td></tr></table></figure><h3 id="外连接"><a href="#外连接" class="headerlink" title="外连接"></a>外连接</h3><p>外连接保留了没有关联的那些行。分为左外连接，右外连接以及全外连接，左外连接就是保留左表没有关联的行。</p><p>检索所有顾客的订单信息，包括还没有订单信息的顾客。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> Customers.cust_id, Customer.cust_name, Orders.order_id<br><span class="hljs-keyword">FROM</span> Customers <span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">OUTER</span> <span class="hljs-keyword">JOIN</span> Orders<br><span class="hljs-keyword">ON</span> Customers.cust_id <span class="hljs-operator">=</span> Orders.cust_id;<br></code></pre></td></tr></table></figure><p>customers 表：</p><div class="table-container"><table><thead><tr><th>cust_id</th><th>cust_name</th></tr></thead><tbody><tr><td>1</td><td>a</td></tr><tr><td>2</td><td>b</td></tr><tr><td>3</td><td>c</td></tr></tbody></table></div><p>orders 表：</p><div class="table-container"><table><thead><tr><th>order_id</th><th>cust_id</th></tr></thead><tbody><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>1</td></tr><tr><td>3</td><td>3</td></tr><tr><td>4</td><td>3</td></tr></tbody></table></div><p>结果：</p><div class="table-container"><table><thead><tr><th>cust_id</th><th>cust_name</th><th>order_id</th></tr></thead><tbody><tr><td>1</td><td>a</td><td>1</td></tr><tr><td>1</td><td>a</td><td>2</td></tr><tr><td>3</td><td>c</td><td>3</td></tr><tr><td>3</td><td>c</td><td>4</td></tr><tr><td>2</td><td>b</td><td>Null</td></tr></tbody></table></div><h2 id="十六、组合查询"><a href="#十六、组合查询" class="headerlink" title="十六、组合查询"></a>十六、组合查询</h2><p>使用   <strong>UNION</strong>   来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。</p><p>每个查询必须包含相同的列、表达式和聚集函数。</p><p>默认会去除相同行，如果需要保留相同行，使用 UNION ALL。</p><p>只能包含一个 ORDER BY 子句，并且必须位于语句的最后。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> col<br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">WHERE</span> col <span class="hljs-operator">=</span> <span class="hljs-number">1</span><br><span class="hljs-keyword">UNION</span><br><span class="hljs-keyword">SELECT</span> col<br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">WHERE</span> col <span class="hljs-operator">=</span><span class="hljs-number">2</span>;<br></code></pre></td></tr></table></figure><h2 id="十七、视图"><a href="#十七、视图" class="headerlink" title="十七、视图"></a>十七、视图</h2><p>视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。</p><p>对视图的操作和对普通表的操作一样。</p><p>视图具有如下好处：</p><ul><li>简化复杂的 SQL 操作，比如复杂的连接；</li><li>只使用实际表的一部分数据；</li><li>通过只给用户访问视图的权限，保证数据的安全性；</li><li>更改数据格式和表示。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">VIEW</span> myview <span class="hljs-keyword">AS</span><br><span class="hljs-keyword">SELECT</span> Concat(col1, col2) <span class="hljs-keyword">AS</span> concat_col, col3<span class="hljs-operator">*</span>col4 <span class="hljs-keyword">AS</span> compute_col<br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">WHERE</span> col5 <span class="hljs-operator">=</span> val;<br></code></pre></td></tr></table></figure><h2 id="十八、存储过程"><a href="#十八、存储过程" class="headerlink" title="十八、存储过程"></a>十八、存储过程</h2><p>存储过程可以看成是对一系列 SQL 操作的批处理。</p><p>使用存储过程的好处：</p><ul><li>代码封装，保证了一定的安全性；</li><li>代码复用；</li><li>由于是预先编译，因此具有很高的性能。</li></ul><p>命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。</p><p>包含 in、out 和 inout 三种参数。</p><p>给变量赋值都需要用 select into 语句。</p><p>每次只能给一个变量赋值，不支持集合的操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs sql">delimiter <span class="hljs-operator">/</span><span class="hljs-operator">/</span><br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">procedure</span> myprocedure( <span class="hljs-keyword">out</span> ret <span class="hljs-type">int</span> )<br>    <span class="hljs-keyword">begin</span><br>        <span class="hljs-keyword">declare</span> y <span class="hljs-type">int</span>;<br>        <span class="hljs-keyword">select</span> <span class="hljs-built_in">sum</span>(col1)<br>        <span class="hljs-keyword">from</span> mytable<br>        <span class="hljs-keyword">into</span> y;<br>        <span class="hljs-keyword">select</span> y<span class="hljs-operator">*</span>y <span class="hljs-keyword">into</span> ret;<br>    <span class="hljs-keyword">end</span> <span class="hljs-operator">/</span><span class="hljs-operator">/</span><br><br>delimiter ;<br></code></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">call</span> myprocedure(<span class="hljs-variable">@ret</span>);<br><span class="hljs-keyword">select</span> <span class="hljs-variable">@ret</span>;<br></code></pre></td></tr></table></figure><h2 id="十九、游标"><a href="#十九、游标" class="headerlink" title="十九、游标"></a>十九、游标</h2><p>在存储过程中使用游标可以对一个结果集进行移动遍历。</p><p>游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。</p><p>使用游标的四个步骤：</p><ol><li>声明游标，这个过程没有实际检索出数据；</li><li>打开游标；</li><li>取出数据；</li><li>关闭游标；</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs sql">delimiter <span class="hljs-operator">/</span><span class="hljs-operator">/</span><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">procedure</span> myprocedure(<span class="hljs-keyword">out</span> ret <span class="hljs-type">int</span>)<br>    <span class="hljs-keyword">begin</span><br>        <span class="hljs-keyword">declare</span> done <span class="hljs-type">boolean</span> <span class="hljs-keyword">default</span> <span class="hljs-number">0</span>;<br><br>        <span class="hljs-keyword">declare</span> mycursor <span class="hljs-keyword">cursor</span> <span class="hljs-keyword">for</span><br>        <span class="hljs-keyword">select</span> col1 <span class="hljs-keyword">from</span> mytable;<br>        # 定义了一个 continue handler，当 <span class="hljs-keyword">sqlstate</span> <span class="hljs-string">&#x27;02000&#x27;</span> 这个条件出现时，会执行 <span class="hljs-keyword">set</span> done <span class="hljs-operator">=</span> <span class="hljs-number">1</span><br>        <span class="hljs-keyword">declare</span> continue handler <span class="hljs-keyword">for</span> <span class="hljs-keyword">sqlstate</span> <span class="hljs-string">&#x27;02000&#x27;</span> <span class="hljs-keyword">set</span> done <span class="hljs-operator">=</span> <span class="hljs-number">1</span>;<br><br>        <span class="hljs-keyword">open</span> mycursor;<br><br>        repeat<br>            <span class="hljs-keyword">fetch</span> mycursor <span class="hljs-keyword">into</span> ret;<br>            <span class="hljs-keyword">select</span> ret;<br>        until done <span class="hljs-keyword">end</span> repeat;<br><br>        <span class="hljs-keyword">close</span> mycursor;<br>    <span class="hljs-keyword">end</span> <span class="hljs-operator">/</span><span class="hljs-operator">/</span><br> delimiter ;<br></code></pre></td></tr></table></figure><h2 id="二十、触发器"><a href="#二十、触发器" class="headerlink" title="二十、触发器"></a>二十、触发器</h2><p>触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。</p><p>触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。</p><p>INSERT 触发器包含一个名为 NEW 的虚拟表。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TRIGGER</span> mytrigger AFTER <span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">ON</span> mytable<br><span class="hljs-keyword">FOR</span> <span class="hljs-keyword">EACH</span> <span class="hljs-type">ROW</span> <span class="hljs-keyword">SELECT</span> NEW.col <span class="hljs-keyword">into</span> <span class="hljs-variable">@result</span>;<br><br><span class="hljs-keyword">SELECT</span> <span class="hljs-variable">@result</span>; <span class="hljs-comment">-- 获取结果</span><br></code></pre></td></tr></table></figure><p>DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。</p><p>UPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改的，而 OLD 是只读的。</p><p>MySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。</p><h2 id="二十一、事务管理"><a href="#二十一、事务管理" class="headerlink" title="二十一、事务管理"></a>二十一、事务管理</h2><p>基本术语：</p><ul><li>事务（transaction）指一组 SQL 语句；</li><li>回退（rollback）指撤销指定 SQL 语句的过程；</li><li>提交（commit）指将未存储的 SQL 语句结果写入数据库表；</li><li>保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。</li></ul><p>不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。</p><p>MySQL 的事务提交默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。</p><p>设置 autocommit 为 0 可以取消自动提交；autocommit 标记是针对每个连接而不是针对服务器的。</p><p>如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处；如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">START</span> TRANSACTION<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> ...<br><span class="hljs-keyword">SAVEPOINT</span> delete1<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> ...<br><span class="hljs-keyword">ROLLBACK</span> <span class="hljs-keyword">TO</span> delete1<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> ...<br><span class="hljs-keyword">COMMIT</span><br></code></pre></td></tr></table></figure><h2 id="二十二、字符集"><a href="#二十二、字符集" class="headerlink" title="二十二、字符集"></a>二十二、字符集</h2><p>基本术语：</p><ul><li>字符集为字母和符号的集合；</li><li>编码为某个字符集成员的内部表示；</li><li>校对字符指定如何比较，主要用于排序和分组。</li></ul><p>除了给表指定字符集和校对外，也可以给列指定：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> mytable<br>(col <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">10</span>) <span class="hljs-type">CHARACTER</span> <span class="hljs-keyword">SET</span> latin <span class="hljs-keyword">COLLATE</span> latin1_general_ci )<br><span class="hljs-keyword">DEFAULT</span> <span class="hljs-type">CHARACTER</span> <span class="hljs-keyword">SET</span> hebrew <span class="hljs-keyword">COLLATE</span> hebrew_general_ci;<br></code></pre></td></tr></table></figure><p>可以在排序、分组时指定校对：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> mytable<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> col <span class="hljs-keyword">COLLATE</span> latin1_general_ci;<br></code></pre></td></tr></table></figure><h2 id="二十三、权限管理"><a href="#二十三、权限管理" class="headerlink" title="二十三、权限管理"></a>二十三、权限管理</h2><p>MySQL 的账户信息保存在 mysql 这个数据库中。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">USE mysql;<br><span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">user</span> <span class="hljs-keyword">FROM</span> <span class="hljs-keyword">user</span>;<br></code></pre></td></tr></table></figure><p><strong>创建账户</strong></p><p>新创建的账户没有任何权限。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">USER</span> myuser IDENTIFIED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;mypassword&#x27;</span>;<br></code></pre></td></tr></table></figure><p><strong>修改账户名</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql">RENAME <span class="hljs-keyword">USER</span> myuser <span class="hljs-keyword">TO</span> newuser;<br></code></pre></td></tr></table></figure><p><strong>删除账户</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">USER</span> myuser;<br></code></pre></td></tr></table></figure><p><strong>查看权限</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SHOW</span> GRANTS <span class="hljs-keyword">FOR</span> myuser;<br></code></pre></td></tr></table></figure><p><strong>授予权限</strong></p><p>账户用 username<a href="/host">@host </a> 的形式定义，username@% 使用的是默认主机名。 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">SELECT</span>, <span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">ON</span> mydatabase.<span class="hljs-operator">*</span> <span class="hljs-keyword">TO</span> myuser;<br></code></pre></td></tr></table></figure><p><strong>删除权限</strong></p><p>GRANT 和 REVOKE 可在几个层次上控制访问权限：</p><ul><li>整个服务器，使用 GRANT ALL 和 REVOKE ALL；</li><li>整个数据库，使用 ON database.*；</li><li>特定的表，使用 ON database.table；</li><li>特定的列；</li><li>特定的存储过程。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">REVOKE</span> <span class="hljs-keyword">SELECT</span>, <span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">ON</span> mydatabase.<span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> myuser;<br></code></pre></td></tr></table></figure><p><strong>更改密码</strong></p><p>必须使用 Password() 函数进行加密。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SET</span> PASSWROD <span class="hljs-keyword">FOR</span> myuser <span class="hljs-operator">=</span> Password(<span class="hljs-string">&#x27;new_password&#x27;</span>);<br></code></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>BenForta. SQL 必知必会 [M]. 人民邮电出版社, 2013.</li></ul><h2 id="附录：SQL注入安全"><a href="#附录：SQL注入安全" class="headerlink" title="附录：SQL注入安全"></a>附录：SQL注入安全</h2><h3 id="什么是sql注入呢"><a href="#什么是sql注入呢" class="headerlink" title="什么是sql注入呢?"></a>什么是sql注入呢?</h3><p>SQL注入即是指web应用程序对用户输入数据的合法性没有判断或过滤不严，攻击者可以在web应用程序中事先定义好的查询语句的结尾上添加额外的SQL语句，在管理员不知情的情况下实现非法操作，以此来实现欺骗数据库服务器执行非授权的任意查询，从而进一步得到相应的数据信息。</p><h3 id="如何避免-sql-注入风险"><a href="#如何避免-sql-注入风险" class="headerlink" title="如何避免 sql 注入风险"></a>如何避免 sql 注入风险</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs javascript"> <span class="hljs-number">1.</span>永远不要信任用户的输入。对用户的输入进行校验，能够通过正则表达式，或限制长度；对单引号和<br>双<span class="hljs-string">&quot;-&quot;</span>进行转换等。检查输入的数据是否具有所期望的数据格式，严格限制变量的类型，例如使用regexp包进行一些匹配处理，<br>或者使用strconv包对字符串转化成其他基本类型的数据进行判断。<br><br><span class="hljs-number">2.</span>永远不要使用动态拼装sql，能够使用參数化的sql或者直接使用存储过程进行数据查询存取。<br><br><span class="hljs-number">3.</span>永远不要使用管理员权限的数据库连接，为每一个应用使用单独的权限有限的数据库连接。<br><br><span class="hljs-number">4.</span>不要把机密信息直接存放。加密或者hash掉password和敏感的信息。对进入数据库的特殊字符（<span class="hljs-string">&#x27;&quot;\尖括号&amp;*;等）进行转义处理，</span><br><span class="hljs-string">或编码转换</span><br><span class="hljs-string"></span><br><span class="hljs-string">5.应用的异常信息应该给出尽可能少的提示，最好使用自己定义的错误信息对原始错误信息进行包装,避免网站打印出SQL错误信息，</span><br><span class="hljs-string">比如类型错误、字段不匹配等，把代码里的SQL语句暴露出来，以防止攻击者利用这些错误信息进行SQL注入。</span><br><span class="hljs-string"></span><br><span class="hljs-string">6.sql注入的检測方法一般採取辅助软件或站点平台来检測。软件一般採用sql注入检測工具jsky，站点平台就有亿思站点安全平台检測工具。</span><br><span class="hljs-string">MDCSOFT SCAN等。採用MDCSOFT-IPS能够有效的防御SQL注入。XSS攻击等。</span><br><span class="hljs-string"></span><br><span class="hljs-string">7.严格限制Web应用的数据库的操作权限，给此用户提供仅仅能够满足其工作的最低权限，从而最大限度的减少注入攻击对数据库的危害。</span><br><span class="hljs-string"></span><br><span class="hljs-string">8.在应用发布之前建议使用专业的SQL注入检测工具进行检测，以及时修补被发现的SQL注入漏洞。网上有很多这方面的开源工具，</span><br><span class="hljs-string">例如sqlmap、SQLninja等。</span><br><span class="hljs-string"></span><br><span class="hljs-string">9.所有的查询语句建议使用数据库提供的参数化查询接口，参数化的语句使用参数而不是将用户输入变量嵌入到SQL语句中，</span><br><span class="hljs-string">即不要直接拼接SQL语句。例如使用database/sql里面的查询函数Prepare和Query，或者Exec(query string, args ...interface&#123;&#125;)。</span><br></code></pre></td></tr></table></figure><h3 id="pymysql-简单规避注入风险示列"><a href="#pymysql-简单规避注入风险示列" class="headerlink" title="pymysql 简单规避注入风险示列"></a>pymysql 简单规避注入风险示列</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs javascript">#错误示范  不要自己去拼接账户和密码<br><span class="hljs-keyword">import</span> pymysql<br>conn = pymysql.<span class="hljs-title function_">connect</span>(host=<span class="hljs-string">&#x27;127.0.0.1&#x27;</span>, user=<span class="hljs-string">&#x27;root&#x27;</span>, password=<span class="hljs-string">&#x27;318&#x27;</span>, database=<span class="hljs-string">&#x27;ftp&#x27;</span>)<br>cur = conn.<span class="hljs-title function_">cursor</span>()<br>username = <span class="hljs-title function_">input</span>(<span class="hljs-string">&#x27;user &gt;&gt;&gt;&#x27;</span>)<br>password = <span class="hljs-title function_">input</span>(<span class="hljs-string">&#x27;passwd &gt;&gt;&gt;&#x27;</span>)<br>sql = <span class="hljs-string">&quot;select * from userinfo where name = %s and password =  %s ;&quot;</span>% (username, password)<br>cur.<span class="hljs-title function_">execute</span>(sql)<br><span class="hljs-title function_">print</span>(cur.<span class="hljs-title function_">fetchone</span>())<br>cur.<span class="hljs-title function_">close</span>()<br>conn.<span class="hljs-title function_">close</span>()<br><br>user &gt;&gt;&gt;<span class="hljs-string">&#x27;我不知道账号&#x27;</span> or <span class="hljs-number">1</span>=<span class="hljs-number">1</span>;--<br>passwd &gt;&gt;&gt;我也不知道密码<br>(<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;凯歌318&#x27;</span>, <span class="hljs-string">&#x27;666&#x27;</span>)<br><span class="hljs-title class_">Process</span> finished <span class="hljs-keyword">with</span> exit code <span class="hljs-number">0</span><br><br><br>#正确方法   cur.<span class="hljs-title function_">execute</span>(sql, (username, password)) 把密码和账户交给 execute去拼接<br><span class="hljs-keyword">import</span> pymysql<br>conn = pymysql.<span class="hljs-title function_">connect</span>(host=<span class="hljs-string">&#x27;127.0.0.1&#x27;</span>, user=<span class="hljs-string">&#x27;root&#x27;</span>, password=<span class="hljs-string">&#x27;318&#x27;</span>, database=<span class="hljs-string">&#x27;ftp&#x27;</span>)<br>cur = conn.<span class="hljs-title function_">cursor</span>()<br>username = <span class="hljs-title function_">input</span>(<span class="hljs-string">&#x27;user &gt;&gt;&gt;&#x27;</span>)<br>password = <span class="hljs-title function_">input</span>(<span class="hljs-string">&#x27;pwd &gt;&gt;&gt;&#x27;</span>)<br>sql = <span class="hljs-string">&quot;select * from userinfo where name = %s and pwd = %s&quot;</span><br>cur.<span class="hljs-title function_">execute</span>(sql, (username, password))<br><span class="hljs-title function_">print</span>(cur.<span class="hljs-title function_">fetchone</span>())<br>cur.<span class="hljs-title function_">close</span>()<br>conn.<span class="hljs-title function_">close</span>()<br><br>user &gt;&gt;&gt;<span class="hljs-string">&#x27;我不知道账号&#x27;</span> or <span class="hljs-number">1</span>=<span class="hljs-number">1</span>;--<br>pwd &gt;&gt;&gt;也不知道密码<br><span class="hljs-title class_">None</span><br><br><span class="hljs-title class_">Process</span> finished <span class="hljs-keyword">with</span> exit code <span class="hljs-number">0</span><br><br>user &gt;&gt;&gt;凯歌<span class="hljs-number">318</span><br>pwd &gt;&gt;&gt;<span class="hljs-number">666</span><br>(<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;凯歌318&#x27;</span>, <span class="hljs-string">&#x27;666&#x27;</span>)<br><span class="hljs-title class_">Process</span> finished <span class="hljs-keyword">with</span> exit code <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h3 id="特殊情况-IN-不定长参数"><a href="#特殊情况-IN-不定长参数" class="headerlink" title="特殊情况 - IN - 不定长参数"></a>特殊情况 - IN - 不定长参数</h3><p>有一个值列表，我想在一个<code>IN</code>子句中执行的 SQL 语句。 例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">files = [<span class="hljs-string">&#x27;file1&#x27;</span>, <span class="hljs-string">&#x27;file2&#x27;</span>, ...]  <span class="hljs-comment"># this list can have a variable number of elements</span><br>con = pymysql.connect(...)<br>cur = conn.cursor()<br><br>result = cur.execute(<span class="hljs-string">&#x27;SELECT * FROM sometable WHERE file_name IN (?)&#x27;</span>, files)<br></code></pre></td></tr></table></figure><p>但是当我执行上面的语句时，我得到一个错误，例如：</p><blockquote><p>ProgrammingError：（’SQL包含1个参数标记，但提供了18个参数’，’HY000’）</p></blockquote><p>我可以使用以下内容生成变量参数字符串：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">params = <span class="hljs-string">&quot;,&quot;</span>.join([<span class="hljs-string">&quot;%s&quot;</span>] * <span class="hljs-built_in">len</span>(files))<br>query = <span class="hljs-string">&#x27;SELECT * FROM sometable WHERE file_name IN (&#123;&#125;)&#x27;</span>.<span class="hljs-built_in">format</span>(params)<br>con.ping(reconnect=<span class="hljs-literal">True</span>)<br><br>result = cur.execute(query, files)<br></code></pre></td></tr></table></figure><p>这样就可以避免SQL注入的风险。</p>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python敏感信息加密</title>
    <link href="/2020/04/28/2020-04-28-python%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E5%8A%A0%E5%AF%86/"/>
    <url>/2020/04/28/2020-04-28-python%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E5%8A%A0%E5%AF%86/</url>
    
    <content type="html"><![CDATA[<h2 id="python敏感信息加密"><a href="#python敏感信息加密" class="headerlink" title="python敏感信息加密"></a>python敏感信息加密</h2><h3 id="base64加密"><a href="#base64加密" class="headerlink" title="base64加密"></a>base64加密</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> base64<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">AskData的标准权限管理接口</span><br><span class="hljs-string">测试环境：http://10.231.135.146:8060/auth</span><br><span class="hljs-string">Request参数:</span><br><span class="hljs-string">&#123;&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">jiami</span>(<span class="hljs-params">xx</span>):<br>unkownPassword=base64.b64encode(<span class="hljs-built_in">bytes</span>(xx,<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;加密后：&quot;</span>+<span class="hljs-built_in">str</span>(unkownPassword,<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">jiemi</span>(<span class="hljs-params">xx</span>):<br>kownPassword=<span class="hljs-built_in">str</span>(base64.b64decode(xx),<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;解密后：&#x27;</span>+kownPassword)<br><br>cfg = ConfigParser()<br>cfg.read(<span class="hljs-string">&quot;config.ini&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>server = cfg[<span class="hljs-string">&quot;server67&quot;</span>]<br>username = base64.b64decode(<span class="hljs-string">b&quot;%s&quot;</span> % server.get(<span class="hljs-string">&quot;user&quot;</span>).encode()).decode(<span class="hljs-string">&quot;utf-8&quot;</span>)<br>password = base64.b64decode(<span class="hljs-string">b&quot;%s&quot;</span> % server.get(<span class="hljs-string">&quot;passwd&quot;</span>).encode()).decode(<span class="hljs-string">&quot;utf-8&quot;</span>)<br></code></pre></td></tr></table></figure><span id="more"></span><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs routeros">pool = PooledDB(<br>    <span class="hljs-attribute">creator</span>=pymysql,<br>    <span class="hljs-attribute">host</span>=DB_CONFIG.get(&quot;host&quot;),<br>    <span class="hljs-attribute">port</span>=DB_CONFIG.getint(&quot;port&quot;),<br>    <span class="hljs-attribute">user</span>=DB_CONFIG.get(&quot;user&quot;),<br>    <span class="hljs-attribute">password</span>=DB_CONFIG.get(&quot;passwd&quot;),<br>    <span class="hljs-attribute">db</span>=DB_CONFIG.get(&quot;db&quot;),<br>    <span class="hljs-attribute">charset</span>=<span class="hljs-string">&quot;utf8&quot;</span>,<br>    <span class="hljs-attribute">mincached</span>=1,  # 启动时开启的闲置连接数量<br>    <span class="hljs-attribute">maxcached</span>=3,  # 连接池中允许的闲置的最多连接数量<br>    <span class="hljs-attribute">maxconnections</span>=5,  # 创建连接池的最大数量<br>    <span class="hljs-attribute">blocking</span>=<span class="hljs-literal">True</span>,  # 设置在连接池达到最大数量时的行为<br>    <span class="hljs-attribute">maxusage</span>=0,  # 单个连接的最大允许复用次数(缺省值 0 或 <span class="hljs-literal">False</span> 代表不限制的复用)<br>)<br>self.db = pool.connection()  # 获取链接池中的mysql链接<br></code></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs routeros">db = pymysql.connect(<br>    <span class="hljs-attribute">host</span>=DB_CONFIG.get(&quot;host&quot;),<br>    <span class="hljs-attribute">port</span>=DB_CONFIG.getint(&quot;port&quot;),<br>    <span class="hljs-attribute">user</span>=DB_CONFIG.get(&quot;user&quot;),<br>    <span class="hljs-attribute">passwd</span>=DB_CONFIG.get(&quot;passwd&quot;),<br>    <span class="hljs-attribute">db</span>=DB_CONFIG.get(&quot;db&quot;),<br>    <span class="hljs-attribute">charset</span>=<span class="hljs-string">&quot;utf8&quot;</span>,<br>    # <span class="hljs-attribute">cursorclass</span>=pymysql.cursors.DictCursor,  # 以字典形式返回查询记录<br>    )<br>    # db.autocommit(1) # 执行完SQL语句后, 自动提交到真正的数据库（慎用）<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;数据库连接成功~~~~~~~&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NN调参炼丹上分手册</title>
    <link href="/2020/04/24/2020-04-24-%E8%B0%83%E5%8F%82%E7%82%BC%E4%B8%B9%E6%89%8B%E5%86%8C/"/>
    <url>/2020/04/24/2020-04-24-%E8%B0%83%E5%8F%82%E7%82%BC%E4%B8%B9%E6%89%8B%E5%86%8C/</url>
    
    <content type="html"><![CDATA[<p><strong>常见调参技巧</strong></p><ul><li>Learning Rate</li><li>优化器选择</li><li>Dropout</li><li>Batch Size</li><li>激活函数</li><li>BN</li><li>…</li></ul><p><strong>问题和解决方法</strong></p><ul><li>关于过拟合问题的讨论</li><li>Loss为NAN</li><li>Loss为负数</li><li>Loss不下降</li><li>模型训练加速</li><li>OOM</li></ul><p><strong>keras相关经验</strong></p><ol><li><p>训练集，验证集和测试集</p></li><li><p>查看模型的评价指标</p></li><li><p>保存keras输出的loss，val</p></li><li><p>绘制精度和损失曲线</p></li><li><p>将整型 label 转换成 one-hot 形式</p></li><li><p>自制回调函数 callback</p></li><li><p>网格超参数搜索</p></li><li><p>编写自己的层</p></li><li><p>keras保存和加载自定义损失模型</p></li><li><p>PRF 值计算</p></li><li><p>keras 获取中间层的输出</p></li><li><p>categorical_crossentropy vs. sparse_categorical_crossentropy</p></li><li><p>通过生成器的方式训练模型，节省内存</p></li><li><p>多类别预测概率转换</p></li><li><p>CNN+LSTM的思考</p></li><li><p>使用预训练模型的权重</p></li></ol><span id="more"></span><h1 id="常见调参技巧"><a href="#常见调参技巧" class="headerlink" title="常见调参技巧"></a>常见调参技巧</h1><blockquote><p>超参上，learning rate 最重要，推荐了解 cosine learning rate 和 cyclic learning rate，其次是 batchsize 和 weight decay。当你的模型还不错的时候，可以试着做数据增广和改损失函数锦上添花了。</p></blockquote><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><blockquote><p>推荐一篇fastai首席设计师「Sylvain Gugger」的一篇博客：How Do You Find A Good Learning Rate[1]</p><p>以及相关的论文Cyclical Learning Rates for Training Neural Networks[2]。</p></blockquote><p><strong>一般来说，越大的batch-size使用越大的学习率（一般来说Batch Size变成原始几倍，学习率就增加几倍）。</strong>原理很简单，越大的batch-size意味着我们学习的时候，收敛方向的confidence越大，我们前进的方向更加坚定，而小的batch-size则显得比较杂乱，毫无规律性，因为相比批次大的时候，批次小的情况下无法照顾到更多的情况，所以需要小的学习率来保证不至于出错。</p><p>在显存足够的条件下，最好采用较大的batch-size进行训练，找到合适的学习率后，可以加快收敛速度。</p><p>另外，较大的batch-size可以避免batch normalization出现的一些小问题</p><ul><li>采用较小的学习率，则<strong>收敛缓慢</strong>，也有可能收敛到<strong>局部最优</strong>解，导致loss没变化；</li><li>采用较大的学习率，会使得 loss 上下波动较大，导致loss爆炸=Nan或者无法收敛；</li></ul><p>调参技巧：</p><ul><li>优化器推荐使用<strong>AdamW</strong>或者<strong>SGD with Momentum</strong><ul><li>初始值建议 3e-4 (SGD可以选0.1)</li></ul></li><li><p>学习率衰减策略采用<strong>cosine learning rate 和 cyclic learning rate</strong></p><ul><li>参考 <a href="TF/Keras Learning Rate &amp; Schedulers">TF/Keras Learning Rate &amp; Schedulers</a>、 <a href="https://blog.csdn.net/Light2077/article/details/106629697">Tensorflow2.0学习率衰减详细汇总</a>、<a href="https://blog.csdn.net/qxqsunshine/article/details/107486709?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-8.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-8.control">Cosine decay with warmup和 周期性学习率（CLR）</a>、<a href="https://blog.csdn.net/zaf0516/article/details/90720759">Tensorflow 中 learning rate decay 的奇技淫巧</a></li></ul></li><li><p>使用动态的学习率：<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/103379602">CLR、余弦退火、SGDR、switch Adam to SGD等</a>(含代码)</p></li><li><strong><a href="https://zhuanlan.zhihu.com/p/114869268">Warm up</a></strong>：用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，假如一开始就采用较大的学习率容易出现数值不稳定，这也是为什么要使用Warm up。然后等到训练过程基本上稳定了就可以使用原始的初始学习率进行训练了</li><li>训练策略：使用<strong>cosine learning rate+warmup</strong>的方法（最终结果差不太多）</li></ul><h3 id="优化器选择"><a href="#优化器选择" class="headerlink" title="优化器选择"></a>优化器选择</h3><blockquote><p> “Optimization” refers to the process of adjusting a model to get the best performance possible on the training data (the “learning” in “machine learning”), </p></blockquote><ul><li><p>优化方法使用</p><ul><li>Adam，Adade，RMSprop结果都差不多，Nadam因为是adam的动量添加的版本，在收敛效果上会更出色。</li><li>优化器公用参数 clipnorm 和 clipvalue<ul><li>参数一：clipnorm 对梯度进行裁剪，最大值为1</li><li>参数二：clipvalue 对梯度范围进行裁剪，范围（-x，x）</li></ul></li><li><strong>一般的起手式: Adam（<a href="https://www.zhihu.com/question/25097993">推荐使用3e-4</a>）</strong></li><li>Keras 推薦 RNN 使用 RMSProp<ul><li>在訓練 RNN 需要注意 explosive gradient 的問題 =&gt; clip gradient 的暴力美學</li><li><code>opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)</code></li></ul></li><li>SGD+monmentum：</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sgd = SGD(<span class="hljs-attribute">lr</span>=learning_rate, <span class="hljs-attribute">decay</span>=learning_rate/nb_epoch, <span class="hljs-attribute">momentum</span>=0.9, <span class="hljs-attribute">nesterov</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><blockquote><p>随机丢弃，抑制过拟合，提高模型鲁棒性。</p></blockquote><p>dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是「暂时」，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。</p><p>通常我们在全连接层部分使用dropout，在卷积层则不使用。在全连接层部分，采用较大概率的dropout而在卷积层采用低概率或者不采用dropout。<strong>dropout对小数据防止过拟合有很好的效果,值一般设为0.2-0.5</strong></p><p><strong>Dropout作用</strong></p><ul><li>一方面缓解过拟合，另一方面引入的随机性，可以平缓训练过程，加速训练过程，处理outliers</li><li>Dropout可以看做ensemble，特征采样，相当于bagging很多子网络；训练过程中动态扩展拥有类似variation的输入数据集。（在单层网络中，类似折中Naiive bayes(所有特征权重独立)和logistic regression(所有特征之间有关系)；</li><li>一般对于越复杂的大规模网络，Dropout效果越好，是一个强regularizer！</li><li>最好的防止over-fitting就是有大量不重复数据</li></ul><h3 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a><strong>Batch Size</strong></h3><ul><li>太小可能导致不收敛（梯度更新方向变化太大）</li><li>太大可能收敛慢</li><li>一般选择<strong>64</strong></li></ul><p>如果可以容忍训练时间过长<strong>，最好开始使用尽量小的batch size(16,8,1)</strong>。<strong>batch size=1是一个很不错的选择</strong>, 起码在某些task上,这也有可能是很多人无法复现alex graves实验结果的原因之一，因为他总是把batch size设成1。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><blockquote><p>RELU用极简的方式实现非线性激活，缓解梯度消失</p></blockquote><ul><li>尽量不要用sigmoid，可以用relu之类的激活函数.</li><li>最后一层不要用relu，例如分类问题最后一层用softmax，回归问题可以不用。</li><li><strong>PReLU</strong>是一个不错的选择</li></ul><h3 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h3><blockquote><p>BatchNormalization可以加快收敛速度。</p></blockquote><ul><li><p>有BN的全连接层没必要加Dropout</p></li><li><p>Batchnormalization层的放置问题</p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs llvm"><span class="hljs-keyword">x</span> <span class="hljs-operator">=</span> (<span class="hljs-keyword">x</span> - <span class="hljs-keyword">x</span>.mean()) / <span class="hljs-keyword">x</span>.std()<br></code></pre></td></tr></table></figure><p>BN层针对数据分布进行优化，对于BN来说其不但可以防止过拟合，还可以防止梯度消失等问题，并且可以加快模型的收敛速度，但是加了BN，模型训练往往会变得慢些。具体放置位置试！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">BatchNormalization(mode=<span class="hljs-number">0</span>, axis=<span class="hljs-number">1</span>)<span class="hljs-comment"># 输入是形如（samples，channels，rows，cols）的4D图像张量，需要设置axis=1</span><br>Dense()<br>BatchNormalization(mode=<span class="hljs-number">1</span>)<span class="hljs-comment"># 按样本规范化，该模式默认输入为2D</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="加深网络"><a href="#加深网络" class="headerlink" title="加深网络"></a>加深网络</h3><p>都说深度网络精度更高，但深度不是盲目堆起来的，<strong>一定要在浅层网络有一定效果的基础上，增加深度。深度增加是为了增加模型的准确率</strong>，如果浅层都学不到东西，深了也没效果。开始一般用3-8层，当效果不错时，为了得到更高的准确率，再尝试加深网络</p><h3 id="隐层神经元的数量"><a href="#隐层神经元的数量" class="headerlink" title="隐层神经元的数量"></a>隐层神经元的数量</h3><blockquote><p>太多：训练慢，难去除噪声(over-fitting)</p><p>太少：拟合能力下降</p><p>一般：256-1024</p></blockquote><p>调参技巧：</p><ul><li>分类任务：初始尝试5-10倍类别个数</li><li>回归任务：初始尝试2-3倍输入/输出特征数</li><li>这里直觉很重要</li><li>最终影响其实不大，只是训练过程比较慢，多尝试</li></ul><h3 id="CNN的trick"><a href="#CNN的trick" class="headerlink" title="CNN的trick"></a>CNN的trick</h3><ul><li>pooling或卷积尺寸和步长不一样，增加数据多样性</li><li>data augumentation，避免过拟合，提高泛化，加噪声扰动</li><li>weight regularization</li><li>SGD使用decay的训练方法</li><li>最后使用pooling（avgpooling）代替全连接，减少参数量</li><li>maxpooling代替avgpooling，避免avgpooling带来的模糊化效果</li><li>2个3x3代替一个5x5等，减少参数，增加非线性映射，使CNN对特征学习能力强</li><li>3x3,2x2窗口</li><li>预训练方法等</li><li>数据预处理后(PCA,ZCA)喂给模型</li><li>输出结果窗口ensemble</li><li>中间节点作为辅助输出节点，相当于模型融合，同时增加反向传播的梯度信号，提供了额外的正则化</li><li>1x1卷积，夸通道组织信息，提高网络表达，可对输出降维，低成本，性价比高，增加非线性映射，符合Hebbian原理</li><li>NIN增加网络对不同尺度的适应性，类似Multi-Scale思想</li><li>Factorization into small convolution，7x7用1x7和7x1代替，节约参数，增加非线性映射</li><li>BN减少Internal Covariance Shift问题，提高学习速度，减少过拟合，可以取消dropout，增大学习率，减轻正则，减少光学畸变的数据增强</li><li>模型遇到退化问题考虑shortcut结构，增加深度</li><li>等等</li></ul><h3 id="RNN的trick"><a href="#RNN的trick" class="headerlink" title="RNN的trick"></a>RNN的trick</h3><p>小的细节和其他很像，简单说两句个人感觉的其他方面吧，其实RNN也是shortcut结构</p><ul><li>一般用LSTM结构防止BPTT的梯度消失，GRU拥有更少的参数，可以优先考虑</li><li>预处理细节，padding，序列长度设定，罕见词语处理等</li><li>一般语言模型的数据量一定要非常大</li><li>Gradient Clipping</li><li>Seq2Seq结构考虑attention，前提数据量大</li><li>序列模型考率性能优良的CNN+gate结构</li><li>一般生成模型可以参考GAN，VAE，产生随机变量</li><li>RL的框架结合</li><li>数据量少考虑简单的MLP</li><li>预测采用层级结构降低训练复杂度</li><li>设计采样方法，增加模型收敛速度</li><li>增加多级shortcut结构</li></ul><h1 id="问题和解决方法"><a href="#问题和解决方法" class="headerlink" title="问题和解决方法"></a>问题和解决方法</h1><h3 id="关于过拟合问题的讨论"><a href="#关于过拟合问题的讨论" class="headerlink" title="关于过拟合问题的讨论"></a>关于过拟合问题的讨论</h3><blockquote><p>当数据集较小而模型较大时会出现过拟合现象，作者指出了为避免过拟合的经验规律，也即<strong>当我们将模型大小扩大8倍时需要将数据集大小扩大5倍。</strong></p></blockquote><ul><li>防止过拟合的方法<ul><li>第一种就是添加dropout层，dropout可以放在很多类层的后面，用来抑制过拟合现象，常见的可以直接放在Dense层后面，一般在Dropout设置0.5。Dropout相当于Ensemble，dropout过大相当于多个模型的结合，一些差模型会拉低训练集的精度。<ul><li>通常只加在 hidden layer，不會加在 output layer，因為影響太大了，除非 output layer 的 dimension 很大。</li><li>Dropout 會讓 training performance 變差</li><li>參數少時，regularization</li></ul></li><li>第二种是使用参数正则化，也就是在一些层的声明中加入L1或L2正则化系数，在一定程度上提升了模型的泛化能力。<code>kernel_regularizer=regularizers.l2(0.001)</code></li><li>Reducing the network’s size： The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model</li><li>Early Stopping<ul><li>希望在 Model overfitting 之前就停止 training</li><li>Early Stopping in Keras<ul><li><code>from keras.callbacks import EarlyStopping</code></li><li><code>early_stopping=EarlyStopping(monitor=&#39;val_loss&#39;, patience=3)</code></li></ul></li></ul></li></ul></li></ul><h3 id="loss的值为NAN"><a href="#loss的值为NAN" class="headerlink" title="loss的值为NAN"></a>loss的值为NAN</h3><ul><li>学习率太高: loss爆炸, 或者nan</li><li>学习率太小: 半天loss没反映</li><li>relu作为激活函数?</li><li><strong>training sample中出现了脏数据&amp;异常值(nan, inf等)</strong>！措施：重整你的数据集</li><li>如果是自己定义的损失函数，这时候可能是你设计的损失函数有问题</li><li><a href="https://zhuanlan.zhihu.com/p/89588946">https://zhuanlan.zhihu.com/p/89588946</a></li></ul><h3 id="loss为负数"><a href="#loss为负数" class="headerlink" title="loss为负数"></a>loss为负数</h3><ul><li>如果出现loss为负，是因为之前多分类的标签哪些设置不对，现在是5分类的，写成了2分类之后导致了Loss为负数</li><li>也可能是损失函数选择错误导致</li></ul><h3 id="Loss不下降"><a href="#Loss不下降" class="headerlink" title="Loss不下降"></a><a href="https://blog.csdn.net/weixin_40267472/article/details/82216668">Loss不下降</a></h3><ul><li><p>train loss 不断下降，test loss不断下降，说明网络仍在学习;</p></li><li><p>train loss 不断下降，test loss趋于不变，说明网络过拟合;</p></li><li><p>train loss 趋于不变，test loss不断下降，说明数据集100%有问题;</p></li><li><p>train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;</p></li><li><p>train loss 不断上升，test loss不断上升，说明网络结构设计或超参数设置不当，数据集经过清洗等问题。</p></li><li><p>train loss下降一点后不再下降，学习率过大过小都不收敛</p></li></ul><h3 id="Loss维持在0-69附近（二分类）"><a href="#Loss维持在0-69附近（二分类）" class="headerlink" title="Loss维持在0.69附近（二分类）"></a><a href="https://www.jianshu.com/p/45c2180cab17">Loss维持在0.69附近（二分类）</a></h3><p>loss下降到0.69附近就不下降了，一直停在那里，acc在0.5左右？</p><ul><li><p>0.69是个什么数？</p><p>一般采用的都是cross entropy loss value,定义如下：</p></li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/6671823-ad7303e0dd58d6a3.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/353/format/webp" alt="img"></p><p>​    发现就是网络预测给出的二类概率向量为[0.5,0.5]，即a和1-a都是0.5，不管y取值0/1，整个的平均loss就是-ln(0.5)=0.69. <strong>也就是训练过程中，无论如何调节网络都不收敛。</strong></p><ul><li><p>为啥预测的a老是为0.5呢？</p><p>a的值是softmax的输出，在二分类的情况下为0.5，表明输入softmax的值x是(近似)相等的。</p></li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/6671823-d0014bc7e8a172b2.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/257/format/webp" alt="img"></p><p>​    进一步观察发现，x几乎都很小，随着训练的进行，还有进一步变小的趋势，可怕！</p><ul><li><p>解决办法</p><ol><li><p>调整初始化和激活函数无法间接保证与调节数据分布，那就强上BN层</p><ul><li>即在网络的每一层都加上Batch Normalization层操作，归一化强力保证其分布，果然彻底解决了0.69问题。</li></ul></li><li><p>改了Dense层的初始化方式×</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = Dense(<span class="hljs-number">1</span>,kernel_initializer=<span class="hljs-string">&#x27;he_normal&#x27;</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>,kernel_regularizer=l2(<span class="hljs-number">0.00001</span>))(x)<br></code></pre></td></tr></table></figure></li><li><p>可能是激活层的激活方式与损失函数不匹配。</p><p>一般使用sigmoid，损失函数使用binary_crossentropy ；使用softmax，损失函数使用categorical_crossentropy</p><ul><li><p><strong>改为softmax loss + sparse_categorical_crossentropy！</strong>×</p></li><li><p>contrastive_loss ×</p></li><li>取消relu激活 ×</li></ul></li><li><p>训练数据需要打乱，要检查每此batch是否都是一个类别，如果是，则没有办法优化；×</p></li><li>检查网络是不是没有回传梯度，而是只做了前向运算；×</li><li>二分类问题中 0.5 的 acc 接近随机猜测的值，可以检查下标签是否标错；×</li><li><strong>尝试不同的 Learning Rate (1e-6、2e-5、3e-4)；</strong>×</li><li>检查是否在 logit 那层加了激活函数，导致 logits 有问题，例如全为 0，经过 softmax 后就是 0.5了<ul><li>修改欧式距离为cos距离×</li><li>改为差和乘积的拼接；×</li></ul></li><li><strong>过拟合？尝试Dropout(0.5)</strong>×</li><li>BERT模型无法共享参数使用？</li><li>数据集本身的问题<ol><li>数据本身以及label是否有异常</li><li>数据是否过于脏乱，没有经过清洗</li><li>数据输入是否有问题，比如图片与label是否一致</li><li>数据经过预处理后，是否丢失特征或者因预处理而出现别的问题</li><li>数据量是否过少，网络出现过拟合的现象</li></ol></li></ol></li></ul><h3 id="Bad-Gradient-Dead-Neurons"><a href="#Bad-Gradient-Dead-Neurons" class="headerlink" title="Bad Gradient(Dead Neurons)"></a><strong>Bad Gradient(Dead Neurons)</strong></h3><p>使用ReLU激活函数，由于其在小于零范围梯度为0，可能会影响模型性能，甚至模型不会在更新<br><strong>当发现模型随着epoch进行，训练error不变化，可能所以神经元都“死”了。这时尝试更换激活函数如leaky ReLU，ELU，再看训练error变化</strong></p><ul><li>使用ReLU时需要给参数加一点噪声，打破完全对称避免0梯度，甚至给biases加噪声</li><li>相对而言对于sigmoid，因为其在0值附近最敏感，梯度最大，初始化全为0就可以啦</li><li>任何关于梯度的操作，比如clipping, rounding, max/min都可能产生类似的问题</li><li>ReLU相对Sigmoid优点：单侧抑制；宽阔的兴奋边界；稀疏激活性；解决梯度消失</li></ul><h3 id="模型训练加速"><a href="#模型训练加速" class="headerlink" title="模型训练加速"></a>模型训练加速</h3><p>关于模型训练加速，论文提到了2点，一是<strong>使用更大的Batch Size</strong>，二是<strong>使用低精度(如FP16)进行训练</strong>（也是我们常说的混合精度训练）。关于使用更大的Batch Size进行训练加速，作者指出一般只增加Batch Size的话，效果不会太理想，需要结合如下调参方案：</p><ul><li><strong>增大学习率</strong>。因为更大的Batch Size意味着每个Batch数据计算得到的梯度更加贴近整个数据集，从数学上来说就是方差更小，因此当更新方向更加准确之后，迈的步子也可以更大，一般来说Batch Size变成原始几倍，学习率就增加几倍。</li><li><strong>Warm up</strong>。Warm up指的是用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，假如一开始就采用较大的学习率容易出现数值不稳定，这也是为什么要使用Warm up。然后等到训练过程基本上稳定了就可以使用原始的初始学习率进行训练了。作者在使用Warm up的过程中使用线性增加的策略。举个例子假如Warm up阶段的初始学习率是0，warmup阶段共需要训练m个batch的数据（论文实现中m个batch共5个<code>epoch</code>），假设训练阶段的初始学习率是L，那么在第<img src="https://www.zhihu.com/equation?tex=i" alt="[公式]">个batch的学习率就设置为<img src="https://www.zhihu.com/equation?tex=i%5Ctimes+L+%2F+m" alt="[公式]">。</li><li><strong>每一个残差块后的最后一个BN层的<img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="[公式]">参数初始化为0</strong>。我们知道BN层的<img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="[公式]">，<img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="[公式]">参数是用来对标注化后的数据做线性变换的，公式表示为：<img src="https://www.zhihu.com/equation?tex=y%3D%5Cgamma+x%5E+%2B%5Cbeta" alt="[公式]">，其中我们一般会把<img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="[公式]">设为1，而这篇论文提出初始化为<img src="https://www.zhihu.com/equation?tex=0" alt="[公式]">则更容易训练。</li><li><strong>不对Bias参数做权重惩罚</strong>。但是对权重还是要做的。。</li></ul><h3 id="ResourceExhaustedError-OOM"><a href="#ResourceExhaustedError-OOM" class="headerlink" title="ResourceExhaustedError: OOM"></a>ResourceExhaustedError: OOM</h3><ul><li>意思就是GPU的内存不够了</li><li>解决：检查下是否有其他程序占用，不行就重启下IDE，或kill 进程ID</li></ul><h1 id="Keras相关经验"><a href="#Keras相关经验" class="headerlink" title="Keras相关经验"></a>Keras相关经验</h1><h2 id="1-训练集，验证集和测试集"><a href="#1-训练集，验证集和测试集" class="headerlink" title="1. 训练集，验证集和测试集"></a>1. 训练集，验证集和测试集</h2><ul><li>验证集是从训练集中抽取出来用于调参的，在validation_split中设置<ul><li>用 Keras 的 <code>validation_split</code> 之前要記得把資料先弄亂，因為它會從資料的最尾端開始取，如果沒有弄亂的話切出來的資料 bias 會很大。可以使用 <code>np.shuffle</code> 來弄亂</li></ul></li><li>测试集是和训练集无交集的，用于测试所选参数用于该模型的效果的。在evaluate函数里设置</li><li>尽量对数据做shuffle</li></ul><h2 id="2-查看模型的评价指标"><a href="#2-查看模型的评价指标" class="headerlink" title="2. 查看模型的评价指标"></a>2. 查看模型的评价指标</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">history_dict = history.history<br>history_dict.keys()<br>dict_keys([<span class="hljs-string">&#x27;val_acc&#x27;</span>, <span class="hljs-string">&#x27;acc&#x27;</span>, <span class="hljs-string">&#x27;val_loss&#x27;</span>, <span class="hljs-string">&#x27;loss’])</span><br></code></pre></td></tr></table></figure><h2 id="3-保存keras输出的loss，val"><a href="#3-保存keras输出的loss，val" class="headerlink" title="3. 保存keras输出的loss，val"></a>3. 保存keras输出的loss，val</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">hist=model.fit(train_set_x,train_set_y,batch_size=<span class="hljs-number">256</span>,shuffle=<span class="hljs-literal">True</span>,nb_epoch=nb_epoch,validation_split=<span class="hljs-number">0.1</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;log_sgd_big_32.txt&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(<span class="hljs-built_in">str</span>(hist.history))<br></code></pre></td></tr></table></figure><h2 id="4-绘制精度和损失曲线"><a href="#4-绘制精度和损失曲线" class="headerlink" title="4. 绘制精度和损失曲线"></a>4. 绘制精度和损失曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot</span>(<span class="hljs-params">history</span>):<br>   plt.figure(figsize=(<span class="hljs-number">16</span>,<span class="hljs-number">7</span>))<br>   plt.subplot(<span class="hljs-number">121</span>)<br>   plt.xlabel(<span class="hljs-string">&#x27;epoch&#x27;</span>)<br>   plt.ylabel(<span class="hljs-string">&#x27;acc&#x27;</span>)<br>   plt.plot(history.epoch, history.history[<span class="hljs-string">&#x27;acc&#x27;</span>], <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&quot;acc&quot;</span>)<br>   plt.plot(history.epoch, history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>], <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&quot;val_acc&quot;</span>)<br>   plt.scatter(history.epoch, history.history[<span class="hljs-string">&#x27;acc&#x27;</span>], marker=<span class="hljs-string">&#x27;*&#x27;</span>)<br>   plt.scatter(history.epoch, history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>])<br>   plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br><br>   plt.subplot(<span class="hljs-number">122</span>)<br>   plt.xlabel(<span class="hljs-string">&#x27;epoch&#x27;</span>)<br>   plt.ylabel(<span class="hljs-string">&#x27;loss&#x27;</span>)<br>   plt.plot(history.epoch, history.history[<span class="hljs-string">&#x27;loss&#x27;</span>], <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&quot;loss&quot;</span>)<br>   plt.plot(history.epoch, history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>], <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&quot;val_loss&quot;</span>)<br>   plt.scatter(history.epoch, history.history[<span class="hljs-string">&#x27;loss&#x27;</span>], marker=<span class="hljs-string">&#x27;*&#x27;</span>)<br>   plt.scatter(history.epoch, history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>], marker=<span class="hljs-string">&#x27;*&#x27;</span>)<br>   plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>   plt.show()<br><br><span class="hljs-comment"># 或者</span><br>history.loss_plot(<span class="hljs-string">&#x27;epoch&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="5-将整型-label-转换成-one-hot-形式"><a href="#5-将整型-label-转换成-one-hot-形式" class="headerlink" title="5. 将整型 label 转换成 one-hot 形式"></a>5. 将整型 label 转换成 one-hot 形式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">to_one_hot</span>(<span class="hljs-params">labels, dimension=<span class="hljs-number">46</span></span>):<br>    results = np.zeros((<span class="hljs-built_in">len</span>(labels), dimension))<br>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):<br>        results[i, label] = <span class="hljs-number">1.</span><br>    <span class="hljs-keyword">return</span> results<br></code></pre></td></tr></table></figure><h2 id="6-自制回调函数-callback"><a href="#6-自制回调函数-callback" class="headerlink" title="6. 自制回调函数 callback"></a>6. 自制回调函数 callback</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 该回调函数将在每个epoch后保存概率文件</span><br><span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> Callback<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WritePRF</span>(<span class="hljs-title class_ inherited__">Callback</span>):<br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data</span>):<br>      <span class="hljs-built_in">super</span>(WritePRF, self).__init__()<br>      self.data = data<br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">on_epoch_end</span>(<span class="hljs-params">self, epoch, logs=<span class="hljs-literal">None</span></span>):<br>      <br><span class="hljs-comment"># 该回调函数将在每个迭代后保存的最好模型</span><br><span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> ModelCheckpoint  <br><br>checkpoint = ModelCheckpoint(  <br>    <span class="hljs-string">&#x27;model.h5&#x27;</span>,  <br>    monitor = <span class="hljs-string">&#x27;val_loss&#x27;</span>,  <br>    verbose = <span class="hljs-number">1</span>,  <br>    save_best_only = <span class="hljs-literal">True</span>,  <br>    mode = <span class="hljs-string">&#x27;min&#x27;</span>,  <br>) <br></code></pre></td></tr></table></figure><h2 id="7-网格超参数搜索"><a href="#7-网格超参数搜索" class="headerlink" title="7. 网格超参数搜索"></a>7. 网格超参数搜索</h2><p><a href="http://geek.csdn.net/news/detail/95494">Keras/Python深度学习中的网格搜索超参数调优（附源码）</a></p><p>先grid search，再random search（由粗到细）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_model</span>():<br><span class="hljs-comment"># create model</span><br>model = Sequential()<br>model.add(Dense(<span class="hljs-number">12</span>, input_dim=<span class="hljs-number">8</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><span class="hljs-comment"># Compile model</span><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>, optimizer=optimizer, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><span class="hljs-keyword">return</span> model<br> <br><span class="hljs-comment"># create model</span><br>model = KerasClassifier(build_fn=create_model, verbose=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># define the grid search parameters</span><br>batch_size = [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">40</span>, <span class="hljs-number">60</span>, <span class="hljs-number">80</span>, <span class="hljs-number">100</span>]<br>epochs = [<span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>]<br><span class="hljs-comment"># define the grid search parameters</span><br>optimizer = [<span class="hljs-string">&#x27;SGD&#x27;</span>, <span class="hljs-string">&#x27;RMSprop&#x27;</span>, <span class="hljs-string">&#x27;Adagrad&#x27;</span>, <span class="hljs-string">&#x27;Adadelta&#x27;</span>, <span class="hljs-string">&#x27;Adam&#x27;</span>, <span class="hljs-string">&#x27;Adamax&#x27;</span>, <span class="hljs-string">&#x27;Nadam&#x27;</span>]<br><span class="hljs-comment"># define the grid search parameters</span><br>learn_rate = [<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>]<br>dropout_rate = [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>]<br>neurons = [<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30</span>]<br><br>param_grid = <span class="hljs-built_in">dict</span>(batch_size=batch_size, epochs=epochs, optimizer=optimizer)<br>grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=<span class="hljs-number">1</span>)<br>grid_result = grid.fit(X, Y)<br><span class="hljs-comment"># summarize results</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Best: %f using %s&quot;</span> % (grid_result.best_score_, grid_result.best_params_))<br>means = grid_result.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]<br>stds = grid_result.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]<br>params = grid_result.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]<br><span class="hljs-keyword">for</span> mean, stdev, param <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(means, stds, params):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;%f (%f) with: %r&quot;</span> % (mean, stdev, param))<br></code></pre></td></tr></table></figure><h2 id="8-编写自己的层"><a href="#8-编写自己的层" class="headerlink" title="8. 编写自己的层"></a>8. 编写自己的层</h2><p>对于简单的定制操作，我们或许可以通过使用layers.core.Lambda层来完成。要定制自己的层，你需要实现下面三个方法:</p><ul><li>build(input_shape)：这是定义权重的方法</li><li>call(x)：这是定义层功能的方法，除非你希望你写的层支持masking，否则你只需要关心call的第一个参数：输入张量</li><li>compute_output_shape(input_shape)：如果你的层修改了输入数据的shape，你应该在这里指定shape变化的方法，这个函数使得Keras可以做自动shape推断</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-keyword">from</span> keras.engine.topology <span class="hljs-keyword">import</span> Layer<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyLayer</span>(<span class="hljs-title class_ inherited__">Layer</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, output_dim, **kwargs</span>):<br>        self.output_dim = output_dim<br>        <span class="hljs-built_in">super</span>(MyLayer, self).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self, input_shape</span>):<br>        <span class="hljs-comment"># Create a trainable weight variable for this layer.</span><br>        self.kernel = self.add_weight(name=<span class="hljs-string">&#x27;kernel&#x27;</span>, <br>                                      shape=(input_shape[<span class="hljs-number">1</span>], self.output_dim),<br>                                      initializer=<span class="hljs-string">&#x27;uniform&#x27;</span>,<br>                                      trainable=<span class="hljs-literal">True</span>)<br>        <span class="hljs-built_in">super</span>(MyLayer, self).build(input_shape)  <span class="hljs-comment"># Be sure to call this somewhere!</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> K.dot(x, self.kernel)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_output_shape</span>(<span class="hljs-params">self, input_shape</span>):<br>        <span class="hljs-keyword">return</span> (input_shape[<span class="hljs-number">0</span>], self.output_dim)<br></code></pre></td></tr></table></figure><h2 id="9-keras保存和加载自定义损失模型"><a href="#9-keras保存和加载自定义损失模型" class="headerlink" title="9. keras保存和加载自定义损失模型"></a>9. keras保存和加载自定义损失模型</h2><p>如果使用了自定义的loss函数， 则需要在加载模型的时候，指定load_model函数提供的一个custom_objects参数：在custom_objects参数词典里加入keras的未知参数，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">custom_objects=&#123;<span class="hljs-string">&#x27;ChainCRF&#x27;</span>: ClassWrapper, <span class="hljs-string">&#x27;loss&#x27;</span>: loss, <span class="hljs-string">&#x27;sparse_loss&#x27;</span>: sparse_loss&#125;<br>model = load_model(<span class="hljs-string">&#x27;model/tmpModel.h5&#x27;</span>, custom_objects=create_custom_objects())<br></code></pre></td></tr></table></figure><h2 id="10-PRF-值计算"><a href="#10-PRF-值计算" class="headerlink" title="10. PRF 值计算"></a>10. PRF 值计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate</span>(<span class="hljs-params">predictions, test_label, RESULT_FILE</span>):<br>   num = <span class="hljs-built_in">len</span>(predictions)<br>   <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(RESULT_FILE, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>      <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num):<br>         <span class="hljs-keyword">if</span> predictions[i][<span class="hljs-number">1</span>] &gt; predictions[i][<span class="hljs-number">0</span>]:<br>            predict = +<span class="hljs-number">1</span><br>         <span class="hljs-keyword">else</span>:<br>            predict = -<span class="hljs-number">1</span><br>         f.write(<span class="hljs-built_in">str</span>(predictions[i][<span class="hljs-number">0</span>]) + <span class="hljs-string">&#x27; &#x27;</span> + <span class="hljs-built_in">str</span>(predictions[i][<span class="hljs-number">1</span>]) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>      <span class="hljs-comment"># f.write(str(predict) + str(predictions[i]) + &#x27;\n&#x27;)</span><br><br>   TP = <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span><br>           predictions[i][<span class="hljs-number">1</span>] &gt; predictions[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> (test_label[i] == np.asarray([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])).<span class="hljs-built_in">all</span>()])<br>   FP = <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span><br>           predictions[i][<span class="hljs-number">1</span>] &gt; predictions[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> (test_label[i] == np.asarray([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>])).<span class="hljs-built_in">all</span>()])<br>   FN = <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span><br>           predictions[i][<span class="hljs-number">1</span>] &lt; predictions[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> (test_label[i] == np.asarray([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])).<span class="hljs-built_in">all</span>()])<br>   TN = <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span><br>           predictions[i][<span class="hljs-number">1</span>] &lt; predictions[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> (test_label[i] == np.asarray([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>])).<span class="hljs-built_in">all</span>()])<br><br>   precision = recall = Fscore = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>   <span class="hljs-keyword">try</span>:<br>      precision = TP / (<span class="hljs-built_in">float</span>)(TP + FP)  <span class="hljs-comment"># ZeroDivisionError: float division by zero</span><br>      recall = TP / (<span class="hljs-built_in">float</span>)(TP + FN)<br>      Fscore = (<span class="hljs-number">2</span> * precision * recall) / (precision + recall)<br>   <span class="hljs-keyword">except</span> ZeroDivisionError <span class="hljs-keyword">as</span> exc:<br>      <span class="hljs-built_in">print</span>(exc.message)<br><br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt;&gt; Report the result ...&quot;</span>)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-1 --&gt; &quot;</span>, <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span> predictions[i][<span class="hljs-number">1</span>] &lt; predictions[i][<span class="hljs-number">0</span>]]))<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;+1 --&gt; &quot;</span>, <span class="hljs-built_in">len</span>([<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num) <span class="hljs-keyword">if</span> predictions[i][<span class="hljs-number">1</span>] &gt; predictions[i][<span class="hljs-number">0</span>]]))<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;TP=&quot;</span>, TP, <span class="hljs-string">&quot;  FP=&quot;</span>, FP, <span class="hljs-string">&quot; FN=&quot;</span>, FN, <span class="hljs-string">&quot; TN=&quot;</span>, TN)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;precision= &quot;</span>, precision)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;recall= &quot;</span>, recall)<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Fscore= &quot;</span>, Fscore)<br></code></pre></td></tr></table></figure><h2 id="11-keras-获取中间层的输出"><a href="#11-keras-获取中间层的输出" class="headerlink" title="11. keras 获取中间层的输出"></a>11. keras 获取中间层的输出</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载权重到当前模型</span><br>model = load_model(model_path)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;获取中间层的输出&#x27;&#x27;&#x27;</span><br>layer_name = <span class="hljs-string">&#x27;my_layer&#x27;</span><br>intermediate_layer_model = Model(<span class="hljs-built_in">input</span>=model.<span class="hljs-built_in">input</span>,<br>                         output=model.get_layer(layer_name).output)<br><br>intermediate_output = intermediate_layer_model.predict(X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(intermediate_output))<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;intermediate_output.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> intermediate_output:<br>      f.write(i)<br></code></pre></td></tr></table></figure><h2 id="12-keras指定显卡且限制显存用量"><a href="#12-keras指定显卡且限制显存用量" class="headerlink" title="12. keras指定显卡且限制显存用量"></a>12. keras指定显卡且限制显存用量</h2><p>keras在使用GPU的时候有个特点，就是默认全部占满显存。需要修改后端代码：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-comment"># 方法1:显存占用会随着epoch的增长而增长</span><br><span class="hljs-comment"># 也就是后面的epoch会去申请新的显存,前面已完成的并不会释放,为了防止碎片化</span><br><br><span class="hljs-built_in">config</span> = tf.ConfigProto()<br><span class="hljs-built_in">config</span>.gpu_options.allow_growth = True  <span class="hljs-comment"># 按需求增长</span><br>sess = tf.Session(<span class="hljs-built_in">config</span>=<span class="hljs-built_in">config</span>)<br>set_session(sess)<br></code></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># # 方法2:只允许使用x%的显存,其余的放着不动</span><br><br><span class="hljs-attr">config</span> = tf.ConfigProto()<br><span class="hljs-attr">config.gpu_options.per_process_gpu_memory_fraction</span> = <span class="hljs-number">0.5</span>    <span class="hljs-comment"># 按比例</span><br><span class="hljs-attr">sess</span> = tf.Session(config=config)<br></code></pre></td></tr></table></figure><p>PS: 需要注意的是，虽然代码或配置层面设置了对显存占用百分比阈值，但在实际运行中如果达到了这个阈值，程序有需要的话还是会突破这个阈值。换而言之如果跑在一个大数据集上还是会用到更多的显存。以上的显存限制仅仅为了在跑小数据集时避免对显存的浪费而已。</p><h2 id="13-Keras-切换后端（Theano和TensorFlow）"><a href="#13-Keras-切换后端（Theano和TensorFlow）" class="headerlink" title="13. Keras 切换后端（Theano和TensorFlow）"></a>13. Keras 切换后端（Theano和TensorFlow）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">vi ~/.keras/keras.json<br><br>&#123;<br>    <span class="hljs-string">&quot;image_dim_ordering&quot;</span>: <span class="hljs-string">&quot;tf&quot;</span>, <br>    <span class="hljs-string">&quot;epsilon&quot;</span>: <span class="hljs-number">1e-07</span>, <br>    <span class="hljs-string">&quot;floatx&quot;</span>: <span class="hljs-string">&quot;float32&quot;</span>, <br>    <span class="hljs-string">&quot;backend&quot;</span>: <span class="hljs-string">&quot;tensorflow&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="14-categorical-crossentropy-vs-sparse-categorical-crossentropy"><a href="#14-categorical-crossentropy-vs-sparse-categorical-crossentropy" class="headerlink" title="14. categorical_crossentropy vs. sparse_categorical_crossentropy"></a>14. categorical_crossentropy vs. sparse_categorical_crossentropy</h2><p>There are two ways to handle labels in multi-class classification: Encoding the labels via “categorical encoding” (also known as “one-hot encoding”) and using <code>categorical_crossentropy</code> as your loss function. Encoding the labels as integers and using the <code>sparse_categorical_crossentropy</code> loss function.</p><h2 id="15-通过生成器的方式训练模型，节省内存"><a href="#15-通过生成器的方式训练模型，节省内存" class="headerlink" title="15. 通过生成器的方式训练模型，节省内存"></a>15. 通过生成器的方式训练模型，节省内存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#从节省内存的角度，通过生成器的方式来训练</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generator</span>(<span class="hljs-params">data, chars, targets, data_a, chars_a, targets_a, batch_size</span>): <br>        idx = np.arange(<span class="hljs-built_in">len</span>(data))<br>        np.random.shuffle(idx)<br>        batches = [idx[<span class="hljs-built_in">range</span>(batch_size*i, <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(data), batch_size*(i+<span class="hljs-number">1</span>)))] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data)//batch_size)]<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> batches:<br>                xx, yy = np.array(data[i]), np.array(targets[i])<br>                char = np.array(chars[i])<br>                xx_a, yy_a = np.array(data_a[i]), np.array(targets_a[i])<br>                char_a = np.array(chars_a[i])<br>                <span class="hljs-keyword">yield</span> ([xx, char, xx_a, char_a], [yy, yy_a])<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Build model...&#x27;</span>)<br>    model = buildModel(max_word)<br><br>    generator = data_generator(train_x, train_char, train_y, aux_train_x, aux_train_char, aux_train_y, batch_size)<br>    samples_per_epoch = <span class="hljs-built_in">len</span>(train_x)<br>    steps_per_epoch = samples_per_epoch // batch_size<br>    <span class="hljs-comment"># StopIteration: dataset fully readed before fit end</span><br>    history = model.fit_generator(generator, steps_per_epoch=steps_per_epoch, epochs=epochs)<br></code></pre></td></tr></table></figure><h2 id="16-CNN-LSTM的思考"><a href="#16-CNN-LSTM的思考" class="headerlink" title="16. CNN+LSTM的思考"></a>16. CNN+LSTM的思考</h2><p>Because RNNs are extremely expensive for processing very long sequences, but 1D convnets are cheap, it can be a good idea to use a 1D convnet as a preprocessing step before a RNN, shortening the sequence and extracting useful representations for the RNN to process.</p><h2 id="17-使用预训练模型的权重"><a href="#17-使用预训练模型的权重" class="headerlink" title="17. 使用预训练模型的权重"></a>17. 使用预训练模型的权重</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">WEIGHTS_PATH = <span class="hljs-string">&#x27;bottleneck_fc_model.h5&#x27;</span><br>model1.save_weights(WEIGHTS_PATH)<br>model2.load_weights(WEIGHTS_PATH)<br><span class="hljs-comment"># layer.trainable = False</span><br>model2.fit()<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247493509&amp;idx=3&amp;sn=032d0e4e3739bc311b09c6cd388cdf14&amp;chksm=ebb7df51dcc05647ab13b837ec0619d36b970225586c74e0ed8819fddd7bee538e91add623e0&amp;scene=0&amp;xtrack=1#rd">【神经网络训练】trick总结</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247513519&amp;idx=3&amp;sn=10192952888d4a6fb6d71518c788aae1&amp;chksm=ebb78d7bdcc0046da4155b767fec826816fb2f9ddd6f1b5d8799b6227f8ab6fdf1060951287c&amp;scene=126&amp;sessionid=1604041737&amp;key=e4f3c199ec5f11ddce50d5ee0a0958370d1a089556ec124a47f5cb97dd2078bd1269fb9f9878c30ea81c70a9355cc5cbe41593ba0bf0523fde58563694078048dd9b838bdb16eaf0646915443f66b6c9afda97d05025da77c1b7b47bb632a621292b67e4aa2b848e56e842ca91fd421517e467857cbe0d80839c1b2e42c77a72&amp;ascene=14&amp;uin=MjM2MDA1NjcyMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A8QWRj5C0ouaywX0FtXc4WQ%3D&amp;pass_ticket=943CnHv8LXB1GZ0a9Q4gwHdZ0KtBvMEBhT4bPNqTZNi64VF8LD95Wvjqw5jVQfHP&amp;wx_header=0">深度学习调参tricks总结！</a></p><p><a href="https://zhuanlan.zhihu.com/p/352971645">大道至简：算法工程师炼丹Trick手册</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Keras</tag>
      
      <tag>炼丹</tag>
      
      <tag>调参</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动混合精度训练</title>
    <link href="/2020/04/21/2020-04-21-%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/"/>
    <url>/2020/04/21/2020-04-21-%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>《MIXED PRECISION TRAINING》这篇论文是百度&amp;Nvidia研究院一起发表的，结合N卡底层计算优化，提出了一种灰常有效的神经网络训练加速方法，不仅是预训练，在全民finetune BERT的今天变得异常有用哇。而且小夕调研了一下，发现不仅百度的paddle框架支持混合精度训练，在Tensorflow和Pytorch中也有相应的实现。下面我们先来讲讲<strong>理论</strong>，后面再分析混合精度训练在三大深度学习框架中的<strong>打开方式</strong>。</p><p>另外，推荐一个省显存的优化器文章<a href="https://mp.weixin.qq.com/s/hHMGQn8_9ZTPVxtWjnNU0A">《硬核推导Google AdaFactor：一个省显存的宝藏优化器》</a> 以及<a href="https://spaces.ac.cn/archives/7367">节省显存的重计算技巧也有了Keras版了</a></p><span id="more"></span><h2 id="理论原理"><a href="#理论原理" class="headerlink" title="理论原理"></a>理论原理</h2><p>训练过神经网络的小伙伴都知道，神经网络的参数和中间结果绝大部分都是<strong>单精度浮点数</strong>（即float32）存储和计算的，当网络变得超级大时，降低浮点数精度，比如使用<strong>半精度浮点数，</strong>显然是提高计算速度，降低存储开销的一个很直接的办法。然而副作用也很显然，如果我们直接降低浮点数的精度直观上必然导致模型训练精度的损失。但是呢，天外有天，这篇文章用了<strong>三种机制</strong>有效地防止了模型的精度损失。</p><ul><li><strong>权重备份(master weights)</strong></li><li><strong>损失放缩（loss scaling）</strong></li><li><strong>运算精度（precison of ops）</strong></li></ul><p><strong>总结陈词</strong>混合精度训练做到了在前向和后向计算过程中均使用半精度浮点数，并且没有像之前的一些工作一样还引入额外超参，而且重要的是，实现非常简单却能带来非常显著的收益，在<strong>显存half</strong>以及<strong>速度double</strong>的情况下保持模型的精度，简直不能再厉害啦。</p><p>混合精度训练[5]不是很难理解，但要注意以下几点：</p><ol><li>混合精度训练不是单纯地把FP32转成FP16去计算就可以了，只用FP16会造成80%的精度损失</li><li>Loss scaling：由于梯度值都很小，用FP16会下溢，因此先用FP32存储loss并放大，使得梯度也得到放大，可以用FP16存储，更新时变成FP32再缩放</li><li>在涉及到累加操作时，比如BatchNorm、Softmax，FP16会上溢，需要用FP32保存，一般使用GPU中TensorCore的FP16*FP16+FP32=FP32运算</li></ol><p><strong>整体流程：FP32权重 -&gt; FP16权重 -&gt; FP16计算前向 -&gt; FP32的loss，扩大 -&gt; 转为FP16 -&gt; FP16反向计算梯度 -&gt; 缩放为FP32的梯度更新权重</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulIOYfhm4iaiaP0ibt6ucyWbu2aDAWmS1bHbVVtQKRFN6X4m2eNzdcv28LLr4PPUhTgLZXJica9MU9zTbA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h2 id="三大深度学习框架的打开方式"><a href="#三大深度学习框架的打开方式" class="headerlink" title="三大深度学习框架的打开方式"></a>三大深度学习框架的打开方式</h2><h3 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a><strong>Pytorch</strong></h3><p>导入Automatic Mixed Precision (AMP)，不要998不要288，只需3行无痛使用！</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">from</span> apex <span class="hljs-keyword">import</span> amp<br>model, optimizer = amp.<span class="hljs-title function_">initialize</span>(model, optimizer, opt_level=<span class="hljs-string">&quot;O1&quot;</span>) # 这里是“欧一”，不是“零一”<br><span class="hljs-keyword">with</span> amp.<span class="hljs-title function_">scale_loss</span>(loss, optimizer) <span class="hljs-keyword">as</span> <span class="hljs-attr">scaled_loss</span>:<br>    scaled_loss.<span class="hljs-title function_">backward</span>()<br></code></pre></td></tr></table></figure><p>来看个例子，将上面三行按照正确的位置插入到自己原来的代码中就可以实现酷炫的半精度训练啦！</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> apex <span class="hljs-keyword">import</span> amp<br>model = ... <br>optimizer = ...<br><br>#包装model和optimizer<br>model, optimizer = amp.<span class="hljs-title function_">initialize</span>(model, optimizer, opt_level=<span class="hljs-string">&quot;O1&quot;</span>)<br><br><span class="hljs-keyword">for</span> data, label <span class="hljs-keyword">in</span> <span class="hljs-attr">data_iter</span>: <br>    out = <span class="hljs-title function_">model</span>(data) <br>    loss = <span class="hljs-title function_">criterion</span>(out, label) <br>    optimizer.<span class="hljs-title function_">zero_grad</span>() <br>    <br>    #loss scaling，代替loss.<span class="hljs-title function_">backward</span>()<br>    <span class="hljs-keyword">with</span> amp.<span class="hljs-title function_">scaled_loss</span>(loss, optimizer) <span class="hljs-keyword">as</span> <span class="hljs-attr">scaled_loss</span>:   <br>        scaled_loss.<span class="hljs-title function_">backward</span>() <br>optimizer.<span class="hljs-title function_">step</span>()<br></code></pre></td></tr></table></figure><h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><p>一句话实现混合精度训练之<strong>修改环境变量，</strong>在python脚本中设置环境变量</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">os<span class="hljs-selector-class">.environ</span><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;TF_ENABLE_AUTO_MIXED_PRECISION&#x27;</span>]</span> = <span class="hljs-string">&#x27;1&#x27;</span><br></code></pre></td></tr></table></figure><p>另外还需要对优化器(Optimizer)作如下修改：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">opt</span> = <span class="hljs-keyword">tf</span>.train.AdamOptimizer(learning_rate=learning_rate)<br><span class="hljs-keyword">opt</span> = <span class="hljs-keyword">tf</span>.train.experimental.enable_mixed_precision_graph_rewrite(<br>    <span class="hljs-keyword">opt</span>,<br>    loss_scale=<span class="hljs-string">&#x27;dynamic&#x27;</span>) # 需要添加这句话，该例子是tf1.<span class="hljs-number">14.0</span>版本,不同版本可能不一样<br></code></pre></td></tr></table></figure><p><strong>Keras-based示例</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs javascript">opt = tf.<span class="hljs-property">keras</span>.<span class="hljs-property">optimizers</span>.<span class="hljs-title class_">Adam</span>()<br>opt = tf.<span class="hljs-property">train</span>.<span class="hljs-property">experimental</span>.<span class="hljs-title function_">enable_mixed_precision_graph_rewrite</span>(<br>            opt,<br>            loss_scale=<span class="hljs-string">&#x27;dynamic&#x27;</span>)<br>            <br>model.<span class="hljs-title function_">compile</span>(loss=loss, optimizer=opt)<br>model.<span class="hljs-title function_">fit</span>(...)<br></code></pre></td></tr></table></figure><h3 id="PaddlePaddle"><a href="#PaddlePaddle" class="headerlink" title="PaddlePaddle"></a><strong>PaddlePaddle</strong></h3><p>一句话实现混合精度训练之<strong>添加config</strong>（惊呆?毕竟混合精度训练是百度家提出的，内部早就熟练应用了叭）</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs javascript">--use_fp16=<span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>举个栗子，基于BERT finetune XNLI任务时，只需在执行时设置use_fp16为true即可。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">export</span> FLAGS_sync_nccl_allreduce=<span class="hljs-number">0</span><br><span class="hljs-keyword">export</span> FLAGS_eager_delete_tensor_gb=<span class="hljs-number">1</span><br><span class="hljs-keyword">export</span> <span class="hljs-variable constant_">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span><br><br><span class="hljs-variable constant_">BERT_BASE_PATH</span>=<span class="hljs-string">&quot;chinese_L-12_H-768_A-12&quot;</span><br><span class="hljs-variable constant_">TASK_NAME</span>=<span class="hljs-string">&#x27;XNLI&#x27;</span><br><span class="hljs-variable constant_">DATA_PATH</span>=<span class="hljs-regexp">/path/</span>to/xnli/data/<br><span class="hljs-variable constant_">CKPT_PATH</span>=<span class="hljs-regexp">/path/</span>to/save/checkpoints/<br><br>python -u run_classifier.<span class="hljs-property">py</span> --task_name $&#123;<span class="hljs-variable constant_">TASK_NAME</span>&#125; \<br>                   --use_fp16=<span class="hljs-literal">true</span> \  #!!!!!!add a line<br>                   --use_cuda <span class="hljs-literal">true</span> \<br>                   --do_train <span class="hljs-literal">true</span> \<br>                   --do_val <span class="hljs-literal">true</span> \<br>                   --do_test <span class="hljs-literal">true</span> \<br>                   --batch_size <span class="hljs-number">32</span> \<br>                   --in_tokens <span class="hljs-literal">false</span> \<br>                   --init_pretraining_params $&#123;<span class="hljs-variable constant_">BERT_BASE_PATH</span>&#125;/params \<br>                   --data_dir $&#123;<span class="hljs-variable constant_">DATA_PATH</span>&#125; \<br>                   --vocab_path $&#123;<span class="hljs-variable constant_">BERT_BASE_PATH</span>&#125;/vocab.<span class="hljs-property">txt</span> \<br>                   --checkpoints $&#123;<span class="hljs-variable constant_">CKPT_PATH</span>&#125; \<br>                   --save_steps <span class="hljs-number">1000</span> \<br>                   --weight_decay  <span class="hljs-number">0.01</span> \<br>                   --warmup_proportion <span class="hljs-number">0.1</span> \<br>                   --validation_steps <span class="hljs-number">100</span> \<br>                   --epoch <span class="hljs-number">3</span> \<br>                   --max_seq_len <span class="hljs-number">128</span> \<br>                   --bert_config_path $&#123;<span class="hljs-variable constant_">BERT_BASE_PATH</span>&#125;/bert_config.<span class="hljs-property">json</span> \<br>                   --learning_rate <span class="hljs-number">5e-5</span> \<br>                   --skip_steps <span class="hljs-number">10</span> \<br>                   --num_iteration_per_drop_scope <span class="hljs-number">10</span> \<br>                   --verbose <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cloud.tencent.com/developer/article/1587462">【DL】模型训练太慢？显存不够用？这个算法让你的GPU老树开新花</a></p><p><a href="https://www.cnblogs.com/marsggbo/p/11686184.html">用于深度学习的自动混合精度</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
      <tag>Keras</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux后台运行（nohup&amp;screen）</title>
    <link href="/2020/04/20/2020-04-20-linux%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%EF%BC%88nohup&amp;screen%EF%BC%89/"/>
    <url>/2020/04/20/2020-04-20-linux%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%EF%BC%88nohup&amp;screen%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="linux后台运行（nohup-amp-screen）"><a href="#linux后台运行（nohup-amp-screen）" class="headerlink" title="linux后台运行（nohup&amp;screen）"></a>linux后台运行（nohup&amp;screen）</h1><h2 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">nohup <span class="hljs-keyword">python</span> -<span class="hljs-keyword">u</span> flask_test.<span class="hljs-keyword">py</span> &gt; <span class="hljs-built_in">log</span>.txt <span class="hljs-number">2</span>&gt;&amp;<span class="hljs-number">1</span> &amp;<br></code></pre></td></tr></table></figure><p>为避免python的输出缓冲，将程序中print的内容写入日志，使用-u参数，使得python不启用缓冲.</p><h2 id="screen"><a href="#screen" class="headerlink" title="screen"></a>screen</h2><p>screen 是一个非常有用的命令，提供从单个 SSH 会话中使用多个 shell 窗口（会话）的能力。当会话被分离或网络中断时，screen 会话中启动的进程仍将运行，你可以随时重新连接到 screen 会话</p><p>安装Screen</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">yum -y <span class="hljs-keyword">install </span><span class="hljs-keyword">screen</span><br><span class="hljs-keyword"></span>apt-get -y <span class="hljs-keyword">install </span><span class="hljs-keyword">screen</span><br></code></pre></td></tr></table></figure><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs gauss">创建窗口<br><span class="hljs-keyword">screen</span> -S intent<br>python xxx.py<br><br>查看哪些窗口正在运行<br><span class="hljs-keyword">screen</span> -ls<br><span class="hljs-keyword">screen</span> -list<br><span class="hljs-keyword">screen</span> -r ex 程序执行命令<br><br>恢复会话窗口<br><span class="hljs-keyword">screen</span> -r &lt;线程ID&gt;/&lt;作业名称&gt; <br><br>会话分离<br>ctrl+a 然后 d<br><br>杀死会话窗口<br>kill <span class="hljs-number">-9</span> 线程号<br><span class="hljs-keyword">screen</span> -wipe<br><br>查看手册<br>man <span class="hljs-keyword">screen</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>预训练语言模型小酌</title>
    <link href="/2020/04/10/2020-04-10-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8C/"/>
    <url>/2020/04/10/2020-04-10-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%85%8C/</url>
    
    <content type="html"><![CDATA[<ul><li>预训练原理<ul><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#语言表示学习">语言表示学习</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#神经上下文编码器">神经上下文编码器</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#为什么预训练">为什么预训练</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#预训练任务">预训练任务</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#应用于下游任务">应用于下游任务</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#开放资源">开放资源</a></li></ul></li><li>预训练模型<ul><li>Transformer介绍</li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#gpt-2018-radford2018improving">GPT (2018)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#bert-2018-devlin2018bert">BERT (2018)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#unilm-2019-dong2019unified">UniLM (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#transformer-xl-2019-dai2019transformer">Transformer-XL (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#xlnet-2019-yang2019xlnet">XLNet (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#mass-2019-song2019mass">MASS (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#roberta-2019-liu2019roberta">RoBERTa (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#bart-2019-lewis2019bart">BART (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#t5-2019-raffel2019exploring">T5 (2019)</a></li><li><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#ernie-baidu-2019-sun2019ernie-sun2019ernie2">ERNIE (Baidu, 2019)</a></li><li>Albert</li><li>BERT 拓展</li></ul></li></ul><blockquote><p>本文为 Pre-trained Models for Natural Language Processing: A Survey 和相关模型的笔记</p></blockquote><p>在当下的 NLP 研究领域，随着计算机算力的不断增强，越来越多的通用语言表征的预训练模型（Pre-trained Models，PTMs）逐渐涌现出来。这对下游的 NLP 任务非常有帮助，可以避免大量从零开始训练新的模型。PTM 大致可以分为两代：</p><ul><li>第一代 PTM 旨在学习词嵌入。由于下游任务不在需要这些模型，因此为了计算效率，这些模型往往采用浅层模型，例如 Skip-Gram，GloVe等。尽管这些模型可以捕获词的语义，但由于未基于上下文环境，因此不能够捕捉到更深层次的概念，例如：句法结构，语义角色，指代等等。</li><li>第二代 PTM 专注于学习基于上下文的词嵌入，例如 CoVe <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:mccann2017learned">4</a>，ELMo <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:peters2018deep">5</a>，OpenAI GPT <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:radford2018improving">6</a> 和 BERT <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:devlin2018bert">7</a> 等。这些学习到的编码器在下游任务中仍会用于词在上下文中的语义表示。</li></ul><span id="more"></span><h1 id="预训练原理"><a href="#预训练原理" class="headerlink" title="预训练原理"></a>预训练原理</h1><h2 id="语言表示学习"><a href="#语言表示学习" class="headerlink" title="语言表示学习"></a>语言表示学习</h2><blockquote><p>参考<a href="https://mp.weixin.qq.com/s/Jyqwf0_cfPiTdjb6tnDANw">《语言模型：过去、现在、未来》</a></p></blockquote><ul><li><p><strong>独热编码（One-hot Encoding）</strong>：最简单的一种单词表示法，一个单词用长度为n的向量表示，其中只有一个位置为1，其余位置为0，n为语料中词库的大小。</p></li><li><p><strong>马尔可夫与语言模型</strong>：人类历史上第一个对语言模型进行研究，他提出n元模型(n-gram model)，假设序列上每个位置的单词只依赖于前n−1个位置的单词 $p(w1,w2,…,w_n)=\prod_{i=1}^np(w_i|w_{i-n+1},w_{i-n+2},…,w_{i-1})$​ </p></li><li><p><strong>香农与语言模型</strong>：假设语言（单词序列）是由一个随机过程产生的数据，n元模型的熵的定义为：$H_n(p,q)=-\sum{p(w1,w2,…,w_n)·q(w1,w2,…,w_n)}$​ ，这里p表示生成数据的真实概率分布。熵表示一个概率分布的不确定性，交叉熵表示一个概率分布相对于另一个概率分布的不确定性。熵是交叉熵的下界。如果一个语言模型比另一个语言模型能更准确地预测单词序列数据，那么它就应该有更小的交叉熵。香农的研究为语言模型学习提供了评价工具。</p></li><li><p><strong>神经语言模型</strong>：n元模型的表示和学习能力是有限的。传统的方法是从语料中统计n元模型中的条件概率，对未见过的n元组的概率通过平滑的方法估算。模型的参数个数是指数级的 $O(V^n)$，其中 $V$ 是词表的大小。当$n$ 增大时，无法准确地学到模型的参数。Bengio等人的神经语言模型从两个方面对n元模型予以改进。一是用一个低维的实值向量表示一个单词或单词的组合；二是在使用词向量的基础上，通过神经网络来表示语言模型，大幅减少模型的参数.  $p(w_i|w_{i-n+1},w_{i-n+2},…,w_{i-1})=f_\theta(w_{i-n+1},w_{i-n+2},…,w_{i-1})$​ </p></li><li><p><strong>预训练语言模型</strong>：上述类型的嵌入主要有两个缺陷：一是嵌入是静态的，词在不同的上下文中的嵌入表示是相同的，因此无法处理一词多义；二是未登录词（out-of-vocabulary，OOV）问题。为了解决上述问题，基于上下文的动态词嵌入出现了。优势：可作为初始词向量、解决处理一词多义问题、能增强模型的泛化能力</p></li></ul><h2 id="神经上下文编码器"><a href="#神经上下文编码器" class="headerlink" title="神经上下文编码器"></a>神经上下文编码器</h2><p>神经上下文编码器大致可以分为 3 类：</p><ol><li><p>基于卷积的模型：基于卷积的模型通过卷积操作从一个词的邻居中聚合局部信息来捕获这个词的含义</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/convolutional-model.png" alt="img"></p></li><li><p>基于序列的模型：基于序列的模型采用 RNNs（LSTM和 GRU） 来捕获词的上下文信息。实际中，我们采用双向的 RNNs 从词的两端收集信息，不过整体效果容易收到长期依赖问题的影响。</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/sequential-model.png" alt="img"></p></li><li><p>基于图的模型：基于图的模型将字作为图中的一个节点来学习上下文表示，这个图通常是一个词之间预定义的语言结构，例如：语法结构或语义关系。尽管基于语言学的图结构能提供有用的信息，但如何构建一个好的图结构则成为了难题。除此之外，基于语言学的图结构需要依赖专家知识和外部工具，例如：依存句法分析等。事实上，我们会采用一个更直接的方式去学习任意两个词之间的关系，通常连接的权重可以通过自注意力机制自动计算得出。Transformer是一个采用了全链接自注意力架构的实现，同时也采用了位置嵌入（positional embedding），层标准化（layer normalization）和残差连接（residual connections）等网络设计理念。</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/fully-connected-graph-based-model.png" alt="img"></p></li></ol><h2 id="为什么预训练"><a href="#为什么预训练" class="headerlink" title="为什么预训练"></a>为什么预训练</h2><p>对于大多数的 NLP 任务，构建一个大规模的有标签的数据集是一项很大的挑战。相反，大规模的无标签语料是相对容易构建的，为了充分利用这些无标签数据，我们可以先利用它们获取一个好的语言表示，再将这些表示用于其他任务。预训练的好处如下：</p><ol><li>预训练可以从大规模语料中学习得到通用的语言表示，并用于下游任务。</li><li>预训练提供了更优的模型初始化方法，有助于提高模型的泛化能力和加速模型收敛。</li><li>预训练可以当作是在小数据集上一种避免过拟合的正则化方法。</li></ol><h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><p>预训练任务对于学习语言的通用表示来说至关重要。通常情况下，预训练任务具有挑战性，同时需要大量训练数据。我们将预训练任务划分为 3 类：</p><ol><li><strong>监督学习</strong>，即从包含输入输出对的训练数据中学习一个由输入到输出的映射函数。</li><li><strong>非监督学习</strong>，即从无标签数据获取一些固有的知识，例如：聚类，密度，潜在表征等。</li><li><strong>自监督学习</strong>，是监督学习和非监督学习的混合体，核心思想是对于输入的一部分利用其他部分进行预测。</li></ol><p>下图展示了预训练模型的分类和部分代表模型：</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/ptms.png" alt="预训练模型分类及代表性模型"></p><h2 id="应用于下游任务"><a href="#应用于下游任务" class="headerlink" title="应用于下游任务"></a>应用于下游任务</h2><h3 id="选择合适的预训练任务，模型架构和语料"><a href="#选择合适的预训练任务，模型架构和语料" class="headerlink" title="选择合适的预训练任务，模型架构和语料"></a>选择合适的预训练任务，模型架构和语料</h3><p>不同的 PTMs 在相同的下游任务上有着不同的效果，这是因为 PTMs 有着不同的预训练任务，模型架构和语料。</p><ol><li>目前，语言模型是最流行的预训练任务，同时也可以有效地解决很多 NLP 问题。但是不同的预训练任务有着自己的侧重，在不同的任务上会有不同的效果。例如：NSP 任务使得 PTM 可以理解两句话之间的关系，因此 PTM 可以在例如问答（Question Answering，QA）和自然语言推理（Natural Language Inference，NLI）等下游任务上表现更好。</li><li>PTM 的网络架构对下游任务也至关重要。例如：尽管 BERT 可以处理大多数自然语言理解任务，对其很难生成语言。</li><li>下游任务的数据分布应该和 PTM 训练所用语料相似。目前，大量现成的 PTM 仅可以快速地用于特定领域或特定语言的下游任务上。</li></ol><h3 id="选择合适的网络层"><a href="#选择合适的网络层" class="headerlink" title="选择合适的网络层"></a>选择合适的网络层</h3><p>给定一个预训练的模型，不同的网络层捕获了不同的信息，例如：词性标记（POS tagging），语法（parsing），长期依赖（long-term dependencies），语义角色（semantic roles），指代（coreference）等。Tenney <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:tenney2019bert">22</a> 等人发现 BERT 表示方式类似传统的 NLP 流程：基础的句法信息出现在浅层的网络中，高级的语义信息出现在更高的层级中。</p><p>令 H(l)(1≤l≤L) 表示共 L 层的预训练模型的第 l 层表示，g(⋅) 表示用于特定任务的的模型。一般有 3 中情况选择表示：</p><ol><li>Embedding Only：一种情况是仅选用预训练模型的静态嵌入，模型的其他部分仍需作为一个任务从头训练。这种情况不能够获取到一些有用的深层信息，词嵌入仅能够捕获词的语义信息。</li><li>Top Layer：最简单有效的方式是将网络的顶层表示输入到模型中 g(H(L))。</li><li>All Layers：另一种更灵活的方式是自动选择最合适的层，例如 ELMo：(5)rt=γ∑l=1Lαlht(l)其中 αl 是层 l 的 softmax 归一的权重，γ 是用于缩放预训练模型输出向量的一个标量值，再将不同层的混合输出输入到后续模型中 g(rt)。</li></ol><h3 id="是否微调"><a href="#是否微调" class="headerlink" title="是否微调"></a>是否微调</h3><p>目前，主要有两种方式进行模型迁移：特征提取（预训练模型的参数是固定的）和模型微调（预训练模型的参数是经过微调的）。当采用特征提取时，预训练模型可以被看作是一个特征提取器。除此之外，我们应该采用内部层作为特征，因为他们通常是最适合迁移的特征。尽管两种不同方式都能对大多数 NLP 任务效果有显著提升，但以特征提取的方式需要更复杂的特定任务的架构。因此，微调是一种更加通用和方便的处理下游任务的方式。</p><h2 id="开放资源"><a href="#开放资源" class="headerlink" title="开放资源"></a>开放资源</h2><h3 id="PTMs-开源实现："><a href="#PTMs-开源实现：" class="headerlink" title="PTMs 开源实现："></a>PTMs 开源实现：</h3><div class="table-container"><table><thead><tr><th>项目</th><th>框架</th><th>PTMs</th></tr></thead><tbody><tr><td><a href="https://github.com/tmikolov/word2vec">word2vec</a></td><td>-</td><td>CBOW, Skip-Gram</td></tr><tr><td><a href="https://nlp.stanford.edu/projects/glove">GloVe</a></td><td>-</td><td>Pre-trained word vectors</td></tr><tr><td><a href="https://github.com/facebookresearch/fastText">FastText</a></td><td>-</td><td>Pre-trained word vectors</td></tr><tr><td><a href="https://github.com/huggingface/transformers">Transformers</a></td><td><em>PyTorch</em> &amp; <em>TF</em></td><td>BERT, GPT-2, RoBERTa, XLNet, etc.</td></tr><tr><td><a href="https://github.com/pytorch/fairseq">Fairseq</a></td><td><em>PyTorch</em></td><td>English LM, German LM, RoBERTa, etc.</td></tr><tr><td><a href="https://github.com/ﬂairNLP/ﬂair">Flair</a></td><td><em>PyTorch</em></td><td>BERT, ELMo, GPT, RoBERTa, XLNet, etc.</td></tr><tr><td><a href="https://github.com/allenai/allennlp">AllenNLP</a></td><td><em>PyTorch</em></td><td>ELMo, BERT, GPT-2, etc.</td></tr><tr><td><a href="https://github.com/fastnlp/fastNLP">FastNLP</a></td><td><em>PyTorch</em></td><td>BERT, RoBERTa, GPT, etc.</td></tr><tr><td><a href="https://github.com/ymcui/Chinese-BERT-wwm">Chinese-BERT</a></td><td>-</td><td>BERT, RoBERTa, etc. (for Chinese)</td></tr><tr><td><a href="https://github.com/google-research/bert">BERT</a></td><td><em>TF</em></td><td>BERT, BERT-wwm</td></tr><tr><td><a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta">RoBERTa</a></td><td><em>PyTorch</em></td><td></td></tr><tr><td><a href="https://github.com/zihangdai/xlnet">XLNet</a></td><td><em>TF</em></td><td></td></tr><tr><td><a href="https://github.com/google-research/ALBERT">ALBERT</a></td><td><em>TF</em></td><td></td></tr><tr><td><a href="https://github.com/google-research/text-to-text-transfer-transformer">T5</a></td><td><em>TF</em></td><td></td></tr><tr><td><a href="https://github.com/thunlp/ERNIE">ERNIE(THU)</a></td><td><em>PyTorch</em></td><td></td></tr><tr><td><a href="https://github.com/PaddlePaddle/ERNIE">ERNIE(Baidu)</a></td><td>PaddlePaddle</td><td></td></tr><tr><td><a href="https://github.com/huggingface/transformers">Hugging Face</a></td><td><em>PyTorch</em> &amp; <em>TF</em></td><td>很多…</td></tr></tbody></table></div><h3 id="论文列表和-PTMs-相关资源："><a href="#论文列表和-PTMs-相关资源：" class="headerlink" title="论文列表和 PTMs 相关资源："></a>论文列表和 PTMs 相关资源：</h3><div class="table-container"><table><thead><tr><th>资源</th><th>URL</th></tr></thead><tbody><tr><td>论文列表</td><td><a href="https://github.com/thunlp/PLMpapers">https://github.com/thunlp/PLMpapers</a></td></tr><tr><td>论文列表</td><td><a href="https://github.com/tomohideshibata/BERT-related-papers">https://github.com/tomohideshibata/BERT-related-papers</a></td></tr><tr><td>论文列表</td><td><a href="https://github.com/cedrickchee/awesome-bert-nlp">https://github.com/cedrickchee/awesome-bert-nlp</a></td></tr><tr><td>Bert Lang Street</td><td><a href="https://bertlang.unibocconi.it/">https://bertlang.unibocconi.it</a></td></tr><tr><td>BertViz</td><td><a href="https://github.com/jessevig/bertviz">https://github.com/jessevig/bertviz</a></td></tr></tbody></table></div><h1 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h1><h2 id="1-Transformer-💡"><a href="#1-Transformer-💡" class="headerlink" title="1. Transformer 💡"></a>1. Transformer 💡</h2><blockquote><p> 参考《 <a href="https://kexue.fm/archives/4765">Attention is All You Need》浅读（简介+代码）</a>》、<a href="https://zhuanlan.zhihu.com/p/345680792">Self-attenstion</a>、 [Transformer中的Self-attenstion、<a href="https://zhuanlan.zhihu.com/p/347709112">Transformer实现细节</a>、<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> “带注释”版本的pytorch代码详细走读</p></blockquote><p>① Transformer摆脱了nlp任务对于rnn，lstm的依赖，在长距离上的建模能力更强；② 使用了 <code>self-attention</code> 可以并行化地对上下文进行建模，提高了训练和推理的速度；③ Transformer也是后续更强大的nlp预训练模型的基础（bert系列使用了transformer的encoder，gpt系列transformer的decoder）</p><p>为了方便学习，我将编码器分为 <code>4</code> 个部分，依次讲解。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/v1JN0W4OpXjr1iaicSWjfKiasqX6Af1z4ibPUoLrpSwvmUUSX7mmCtZu1vRkjcQxser5UGW8wd4Q6esqD7yYVWn2xQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><h3 id="1-1-Position-Embedding"><a href="#1-1-Position-Embedding" class="headerlink" title="1.1 Position Embedding"></a>1.1 Position Embedding</h3><blockquote><p><a href="https://kexue.fm/archives/8130">让研究人员绞尽脑汁的Transformer位置编码</a></p></blockquote><p>我们知道，文字的先后顺序，很重要。</p><p>比如<code>吃饭没</code>、<code>没吃饭</code>、<code>没饭吃</code>、<code>饭吃没</code>、<code>饭没吃</code>，同样三个字，顺序颠倒，所表达的含义就不同了。</p><p>文字的位置信息很重要，<code>Tranformer</code> 没有类似 <code>RNN</code> 的循环结构，没有捕捉顺序序列的能力。</p><blockquote><p>注：因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，导致模型<strong>并不能捕捉序列的顺序</strong>！</p></blockquote><p>为了保留这种位置信息交给 <code>Tranformer</code> 学习，我们需要用到<strong>位置嵌入</strong>。</p><p>加入位置信息的方式非常多，最简单的可以是直接将绝对坐标 <code>0,1,2</code> 编码。</p><p><code>Tranformer</code> 采用的是 <code>sin-cos</code> 规则，使用了 <code>sin</code> 和 <code>cos</code> 函数的线性变换来提供给模型位置信息：</p><script type="math/tex; mode=display">\begin{aligned} P E_{(\text {pos}, 2 i)} &=\sin \left(\text {pos} / 10000^{2 i / d_{\text {model }}}\right) \\ P E_{(\text {pos}, 2 i+1)} &=\cos \left(\text {pos} / 10000^{2 i / d_{\text {model }}}\right) \end{aligned}</script><p>上式中 <code>pos</code> 指的是句中字的位置，取值范围是 <code>[0, 𝑚𝑎𝑥 𝑠𝑒𝑞𝑢𝑒𝑛𝑐𝑒 𝑙𝑒𝑛𝑔𝑡ℎ)</code>，<code>i</code> 指的是字嵌入的维度, 取值范围是 <code>[0, 𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛)</code>。 就是 <code>𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛</code> 的大小。</p><p>上面有 <code>sin</code> 和 <code>cos</code> 一组公式，也就是对应着 <code>𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛</code> 维度的一组奇数和偶数的序号的维度，从而产生不同的周期性变化。</p><p>可以用代码，简单看下效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入依赖库</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_positional_encoding</span>(<span class="hljs-params">max_seq_len, embed_dim</span>):<br>    <span class="hljs-comment"># 初始化一个positional encoding</span><br>    <span class="hljs-comment"># embed_dim: 字嵌入的维度</span><br>    <span class="hljs-comment"># max_seq_len: 最大的序列长度</span><br>    positional_encoding = np.array([<br>        [pos / np.power(<span class="hljs-number">10000</span>, <span class="hljs-number">2</span> * i / embed_dim) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(embed_dim)]<br>        <span class="hljs-keyword">if</span> pos != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> np.zeros(embed_dim) <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_seq_len)])<br>    positional_encoding[<span class="hljs-number">1</span>:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = np.sin(positional_encoding[<span class="hljs-number">1</span>:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>])  <span class="hljs-comment"># dim 2i 偶数</span><br>    positional_encoding[<span class="hljs-number">1</span>:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = np.cos(positional_encoding[<span class="hljs-number">1</span>:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>])  <span class="hljs-comment"># dim 2i+1 奇数</span><br>    <span class="hljs-comment"># 归一化, 用位置嵌入的每一行除以它的模长</span><br>    <span class="hljs-comment"># denominator = np.sqrt(np.sum(position_enc**2, axis=1, keepdims=True))</span><br>    <span class="hljs-comment"># position_enc = position_enc / (denominator + 1e-8)</span><br>    <span class="hljs-keyword">return</span> positional_encoding<br>    <br>positional_encoding = get_positional_encoding(max_seq_len=<span class="hljs-number">100</span>, embed_dim=<span class="hljs-number">16</span>)<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>sns.heatmap(positional_encoding)<br>plt.title(<span class="hljs-string">&quot;Sinusoidal Function&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;hidden dimension&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;sequence length&quot;</span>)<br></code></pre></td></tr></table></figure><p>最后，将字向量和 <code>位置嵌入</code> 相加，送给下一层。</p><blockquote><p>绝对位置编码 vs. 相对位置编码</p><ul><li>绝对位置编码是相对简单的一种方案，①可以直接将位置编码当作可训练参数，比如最大长度为512，编码维度为768，那么就初始化一个512×768的矩阵作为位置向量，让它随着训练过程更新。现在的BERT、GPT等模型所用的就是这种位置编码；②三角函数式位置编码，一般也称为Sinusoidal位置编码，Transformer所使用的方法；③先接一层RNN学习位置信息，然后再接Transformer，那么理论上就不需要加位置编码了。</li><li>相对位置并没有完整建模每个输入的位置信息，而是在算Attention的时候考虑当前位置与被Attention的位置的相对距离，由于自然语言一般更依赖于相对位置，所以相对位置编码通常也有着优秀的表现。对于相对位置编码来说，它的灵活性更大，更加体现出了研究人员的“天马行空”。</li></ul></blockquote><h3 id="1-1-Attention层"><a href="#1-1-Attention层" class="headerlink" title="1.1 Attention层"></a>1.1 Attention层</h3><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwfrkul6pxj316f0u0dmb.jpg" alt="CFQz0to"></p><p><strong>Attention层的好处是能够==一步到位捕捉到全局的联系==，因为它直接把序列两两比较（代价是计算量变为 $O(n^2)$，当然由于是纯矩阵运算，这个计算量相当也不是很严重）；相比之下，RNN需要一步步递推才能捕捉到，而CNN则需要通过层叠来扩大感受野，这是Attention层的明显优势。</strong> </p><p>Google给出的Attention的定义:</p><script type="math/tex; mode=display">{Attention}(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>逐个向量来看:</p><script type="math/tex; mode=display">Attention \left(\boldsymbol{q}_{t}, \boldsymbol{K}, \boldsymbol{V}\right)=\sum_{s=1}^{m} \frac{1}{Z} \exp \left(\frac{\left\langle\boldsymbol{q}_{t}, \boldsymbol{k}_{s}\right\rangle}{\sqrt{d_{k}}}\right) \boldsymbol{v}_{s}</script><p>其中，$\boldsymbol{Q} \in \mathbb{R}^{n \times d_{k}}, \boldsymbol{K} \in \mathbb{R}^{m \times d_{k}}, \boldsymbol{V} \in \mathbb{R}^{m \times d_{v}} , Z是归一化因子 $</p><p>过程：$q,k,v$ 分别是 $query,key,value$ 的简写，$K,V$ 是一一对应的，它们就像是key-value的关系，那么上式的意思就是==通过 $qt$ 这个query，通过与各个 $ks$ 内积的并softmax的方式，来得到 $qt$ 与各个$vs$ 的相似度，然后加权求和，得到一个 $dv$ 维的向量==。其中因子 $\sqrt{d_k}$ 起到调节作用，使得内积不至于太大（太大的话softmax后就非0即1了，不够“soft”了）。 </p><p>结果：将$n×d_k$的序列$Q$编码成了一个新的$n×dv$​的序列.</p><p>所谓Self Attention，其实就是$Attention(X,X,X)$，$X$ 就是前面说的输入序列。也就是说，<u>在序列内部做Attention，寻找序列内部的联系</u>。 </p><h3 id="1-2-Multi-Head-Attention"><a href="#1-2-Multi-Head-Attention" class="headerlink" title="1.2 Multi-Head Attention"></a>1.2 Multi-Head Attention</h3><p>把$Q,K,V$ 通过参数矩阵映射一下，然后再做 Attention，把这个过程重复做 $h$ 次，结果拼接起来: </p><script type="math/tex; mode=display">{head}_{i} = {Attention}(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V})\\{MultiHead}(Q, K, V) ={Concat}({head}_{1}, \cdots, {head}_{h})</script><p>最后得到一个 $n×(hd_v)$ 的序列。==<strong>所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接</strong>== </p><h3 id="Transformer-Encoder-的具体流程"><a href="#Transformer-Encoder-的具体流程" class="headerlink" title="Transformer Encoder 的具体流程"></a>Transformer Encoder 的具体流程</h3><p><img src="https://ningshixian.github.io/resources/images/transformer_architecture.jpg" alt=""></p><ol><li><p>Inputs是经过padding的输入数据，大小是 <em>[batch size, max seq length]</em></p></li><li><p>初始化embedding matrix，通过==embedding lookup==将Inputs映射成token embedding，大小是<em>[batch size, max seq length, embedding size]</em></p></li><li><p>通过sin和cos函数构造位置编码，并加到token embedding中，然后dropout。</p></li><li><p>多头注意力机制</p><ol><li><p>输入token embedding $X$，通过Dense生成 $Q，K，V$，大小是[<em>batch size, max seq length, embedding size]</em>，然后按第2维<em>split</em>成heads的个数，并按第0维拼接，生成新的 $Q，K，V$，大小是<em>[num heads\</em>batch size, max seq length, embedding size/num heads]*，完成多头的操作</p></li><li><p>将 $K$ 的第1维和第2维进行转置，然后 $Q$ 和转置后的 $K$ 的进行点积，结果的大小是 <em>[num heads \</em>batch size, max seq length, max seq length]*</p></li><li><p>将<4.2>的结果除以hidden size的开方(<u>在transformer中，hidden size=embedding size</u>)，完成scale的操作。</p><script type="math/tex; mode=display">a_{1,i}=q^1k^i/\sqrt{d}</script></li><li><p>将<4.3>中padding的点积结果置成一个很小的数 $(-2^{32}+1)$，完成mask操作，后续softmax对padding的结果就可以忽略不计了 √</p></li><li><p>将经过mask的结果进行softmax操作:</p></li></ol></li></ol><script type="math/tex; mode=display">a`_{1,i}=exp(a_{1,i})/\sum_j{exp(a_{1,j})}</script><ul><li><ol><li>将softmax的结果和 $V$ 进行点积，得到attention的结果，大小是 <em>[num heads\</em>batch size, max seq length, hidden size/num heads]*:</li></ol><script type="math/tex; mode=display">b^1=\sum_i{a`_{1,i}v^i}</script></li></ul><ol><li>将attention的结果按第0维split成heads的个数，并按第2维concat，生成multi-head attention的结果，大小是 <em>[batch size, max seq length, hidden size]</em>:</li></ol><script type="math/tex; mode=display">{head}_{i} = {Attention}(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V})  \\  {MultiHead}(Q, K, V) ={Concat}({head}_{1}, \cdots, {head}_{h})</script><ol><li>将token embedding和multi-head attention的结果相加（残差），并进行Layer Normalization。</li><li>将<5>的结果经过2层Dense，其中第1层的 <code>activation=relu</code>，第2层 <code>activation=None</code>。</li></ol><h3 id="transformer一问一答"><a href="#transformer一问一答" class="headerlink" title="transformer一问一答"></a><strong>transformer一问一答</strong></h3><ul><li><p>Q：为什么存在Positional Embedding？在该结构中以何种形式表示？</p><p>A：Attention机制与CNN结构一样，无法表示文本的时序型，因此相比于LSTM结构，在NLP领域效果要差一些，而加入位置信息，相当于给予了时序特性。</p></li><li><p>Q：为什么<4.2>在做attention时，需要进行Scale？</p><p>QK进行点击之后，值之间的方差会较大，也就是大小差距会较大；如果直接通过Softmax操作，会导致大的更大，小的更小；进行缩放，会使参数更平滑，训练效果更好。</p></li><li><p>Q：为什么<5>要将multi-head attention的输入和输出相加？</p><p>类似于resnet中的残差学习单元，有ensemble的思想在里面，解决网络退化问题</p></li><li><p>Q：为什么multi-head attention后面要加一个ffn？</p><p>类比cnn网络中，cnn block和fc交替连接，效果更好。相比于单独的multi-head attention，在后面加一个ffn，可以提高整个block的<u>非线性变换</u>的能力。</p></li></ul><h2 id="2-GPT-2018"><a href="#2-GPT-2018" class="headerlink" title="2. GPT (2018)"></a>2. GPT (2018)</h2><blockquote><p><a href="https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.4.8c42358a11926b2f.md">https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.4.8c42358a11926b2f.md</a></p><p>gpt在bert之前就发表了，使用了transformer decoder作为预训练的框架。在看到了decoder只能get上文信息，不能get下文信息的缺点之后，bert改用了transformer encoder作为预训练的框架，能够同时get上下文信息，获得了巨大的成功。</p></blockquote><p>给定一个语料 $U={u_1,…,u_n}$，使用标准的语言建模目标来最大化如下似然：</p><script type="math/tex; mode=display">L_1(U)=∑_ilog⁡P(u_i∣u_{i−k},…,u_{i−1};Θ)</script><p>其中，k 为上下文窗口的大小，条件概率 P 通过参数为 Θ 的神经网络进行建模。GPT 中使用了一个多层的 Transformer Decoder 作为语言模型。模型首先对输入上下文词条应用多头自注意力机制，再通过按位置的前馈层产生目标词条的输出分布：</p><script type="math/tex; mode=display">\begin{aligned}h_0=UW_e+W_p \\ h_l=transformer_black(h_{l−1}),∀i∈[1,n] \\ P(u)=softmax(h_nW_e^⊤)\end{aligned}</script><p>其中，$U=(u_{−k},…,u_{−1})$​ 为词条的上下文向量，n 为网络层数，$W_e$ 为词条的嵌入矩阵，$W_p$ 为位置嵌入矩阵。</p><p>给定一个有标签的数据集 C，其中包含了输入词条序列 $x_1,…,x_m$ 和对应的标签 y。利用上述预训练的模型获得输入对应的最后一个 Transformer 的激活输出 $h_l^m$，之后再将其输入到一个参数为 $W_y$ 的线性输入层中预测 y：</p><script type="math/tex; mode=display">P(y∣x_1,…,_xm)=softmax(h_l^mW_y)</script><p>模型通过最小化如下损失进行优化：</p><script type="math/tex; mode=display">L_2(C)=∑_{(x,y)}log⁡P(y∣x_1,…,x_m)</script><p>研究还发现将语言建模作为微调的附加目标可以帮助提高模型的泛化能力，同时可以加速模型收敛。GPT 中采用如下的优化目标：</p><script type="math/tex; mode=display">L_3(C)=L_2(C)+λL_1(C)</script><p>GPT 网络架构示意图如下：</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/gpt.png" alt="img"></p><h2 id="3-BERT-2018-💡"><a href="#3-BERT-2018-💡" class="headerlink" title="3. BERT (2018) 💡"></a>3. BERT (2018) 💡</h2><blockquote><p> <a href="https://zhuanlan.zhihu.com/p/46833276">https://zhuanlan.zhihu.com/p/46833276</a></p><ul><li>BERT BASE：L=12,H=768,A=12，参数总量为 100 M</li><li>BERT LARGE：L=24,H=1024,A=16，参数总量为 340 M</li></ul></blockquote><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h4><ul><li><p>BERT 是一种语境化的词（字）嵌入模型  </p></li><li><p>BERT 是基于Transformer的深度双向语言表征模型</p></li><li><p>BERT预训练过程包含两个不同的预训练任务，分别是<strong>Masked Language Model和Next Sentence Prediction</strong>任务。</p><ul><li><p>Masked Language Model（MLM）</p><p>通过随机掩盖一些词（替换为统一标记符[MASK]），然后预测这些被遮盖的词来训练双向语言模型</p></li><li><p>Next Sentence Prediction（NSP）</p><p>为了训练一个理解句子间关系的模型，引入一个下一句预测任务。</p></li></ul></li></ul><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="https://p0.meituan.net/travelcube/890083d95e1dc89888296e0991437dc7646898.png" alt="图2 BERT及Transformer网络结构示意图" style="zoom: 25%;" /></p><h4 id="两个预训练任务"><a href="#两个预训练任务" class="headerlink" title="两个预训练任务"></a>两个预训练任务</h4><p>在预训练阶段，BERT 采用了两个无监督预测任务：</p><h5 id="1-Masked-Language-Model"><a href="#1-Masked-Language-Model" class="headerlink" title="1. Masked Language Model"></a>1. Masked Language Model</h5><p>不同于一般的仅利用 <code>[MASK]</code> 进行遮挡，BERT 选择采用 80% 的 <code>[MASK]</code>，10% 的随机词和 10% 保留原始词的方式对随机选择的 15% 的词条进行遮挡处理。由于编码器不知会预测哪个词或哪个词被随机替换了，这迫使其必须保留每个输入词条的分布式上下文表示。同时 1.5% 的随机替换也不会过多的损害模型的理解能力。</p><ul><li>80%的时间中：将选中的词用[MASK]token来代替，例如</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">my dog is hairy → my dog is [MASK]<br></code></pre></td></tr></table></figure><ul><li>10%的时间中：将选中的词用任意的词来进行代替，例如</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">my dog is hairy → my dog is apple<br></code></pre></td></tr></table></figure><ul><li>10%的时间中：选中的词不发生变化，例如</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">my dog is hairy → my dog is hairy<br></code></pre></td></tr></table></figure><p>这样存在另一个问题在于在训练过程中只有15%的token被预测，正常的语言模型实际上是预测每个token的，因此Masked LM相比正常LM会收敛地慢一些，后面的实验也的确证实了这一点。</p><h5 id="2-Next-Sentence-Prediction"><a href="#2-Next-Sentence-Prediction" class="headerlink" title="2. Next Sentence Prediction"></a>2. Next Sentence Prediction</h5><p>一些重要的下游任务，例如问答（Question Answering，QA）和自然语言推断（Natural Language Inference，NLI）是基于两个句子之间关系的理解，这是语言建模无法直接捕获的。BERT 通过训练一个预测是否为下一个句子的二分类任务来实现，对于一个句子对 A 和 B，50% 的 B 是句子 A 真实的下一句，剩余 50% 为随机抽取的。</p><h4 id="输入表示"><a href="#输入表示" class="headerlink" title="输入表示"></a>输入表示</h4><p>BERT 的输入表示既可以表示一个单独的文本序列，也可以表示一对文本序列（例如：问题和答案）。对于一个给定的词条，其输入表示由对应的词条嵌入，分割嵌入和位置嵌入三部分加和构成，如下图所示：</p><ol><li>Token embedding 表示当前词的embedding</li><li>Segment Embedding 表示当前词所在句子的index embedding</li><li>Position Embedding 表示当前词所在位置的index embedding</li></ol><p><img src="https://p0.meituan.net/travelcube/348d5d3e5bcaec1b569f2af209ac62fd450645.png" alt="图3 BERT模型的输入表示"></p><p>为了一个输入能够针对两个任务，输入构造规则如下：</p><ol><li>为了能够同时表示单句子和句子对，多句子(例如QA中的Q/A)需要进行拼接作为单个句子，用segment embedding和[SEG]来进行区分</li><li>句子第一个token总是有特殊含义，例如分类问题中是类别，如果不是分类问题那么就忽略</li><li>三个embedding进行sum得到输入的向量</li></ol><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>BERT预训练对于算力有着极大要求:</p><ul><li>开启混合精度实现训练加速；</li><li>在通用中文语料基础上加入领域业务语料进行模型预训练，完成领域迁移；</li><li>预训练过程中尝试融入知识图谱中的实体信息，采用全词MASK训练策略；</li><li><p>通过在业务数据上进行微调，支持不同类型的业务需求。</p></li><li><p>基于Horovod的分布式训练方案…..</p></li><li>模型轻量化<ul><li>低精度量化。在模型训练和推理中使用低精度（FP16甚至INT8、二值网络）表示取代原有精度（FP32）表示。</li><li>模型裁剪和剪枝。减少模型层数和参数规模。</li><li>模型蒸馏。通过知识蒸馏方法[22]基于原始BERT模型蒸馏出符合上线要求的小模型。</li></ul></li></ul><h4 id="模型fine-tune"><a href="#模型fine-tune" class="headerlink" title="模型fine-tune"></a>模型fine-tune</h4><p>这里fine-tuning之前对模型的修改非常简单，例如针对sequence-level classification problem(例如情感分析)，取第一个token的输出表示，喂给一个softmax层得到分类结果输出；对于token-level classification(例如NER)，取所有token的最后层transformer输出，喂给softmax层做分类。总之不同类型的任务需要对模型做不同的修改，但是修改都是非常简单的，最多加一层神经网络即可。如下图所示</p><p><img src="https://ningshixian.github.io/resources/images/bert-fine-tune.jpg" style="zoom:50%;" /></p><h4 id="Bert和transformer在embedding上的差异"><a href="#Bert和transformer在embedding上的差异" class="headerlink" title="Bert和transformer在embedding上的差异"></a>Bert和transformer在embedding上的差异</h4><ol><li><p>transformer的embedding由<strong>2部分</strong>构成：</p><ul><li>一个是token embedding，通过embedding matrix lookup到token_ids上生成表示token的向量；</li><li>一个是position embedding，是通过sin和cos函数创建的定值向量。</li></ul><p>而bert的embedding由<strong>3部分</strong>构成：</p><ul><li>第一个同样是token embedding，通过embedding matrix lookup到token_ids上生成表示token的向量；</li><li>第二个是segment embedding，用来表达当前token是来自于第一个segment，还是第二个segment，因此segment vocab size是2；</li><li>第三个是position embedding，与transformer不同的是，bert创建了一个position embedding matrix，通过position embedding matrix lookup到token_ids的位置上生成表示token位置的位置向量。</li></ul></li><li><p>transformer在embedding之后跟了一个dropout，但是bert在embedding之后先跟了一个layer normalization，再跟了一个dropout。</p></li><li><p>bert在token序列之前加了一个特定的token<code>“[cls]”</code>，这个token对应的向量后续会用在分类任务上；如果是句子对的任务，那么两个句子间使用特定的token“[seq]”来分割。</p></li></ol><h4 id="Bert和transformer在loss上的差异"><a href="#Bert和transformer在loss上的差异" class="headerlink" title="Bert和transformer在loss上的差异"></a>Bert和transformer在loss上的差异</h4><ol><li><p>transformer的loss是在decoder阶段计算的，loss的计算方式是transformer的<19>。<br>bert预训练的loss由2部分构成，</p><ul><li>一部分是==NSP==的loss，就是token“[cls]”经过1层Dense，然后接一个二分类的loss，其中0表示segment B是segment A的下一句，1表示segment A和segment B来自2篇不同的文本；</li><li>另一部分是==MLM==的loss，segment中<strong>每个token都有15%的概率被mask，而被mask的token有80%的概率用“<mask>”表示，有10%的概率随机替换成某一个token，有10%的概率保留原来的token</strong>，被mask的token经过encoder后乘以embedding matrix的转置会生成在vocab上的分布，然后计算分布和真实的token的one-hot形式的cross entropy，最后sum起来当作loss。</li></ul><p>这两部分loss相加起来当作bert的 total loss，利用adam进行训练。bert fine-tune的loss会根据任务性质来设计，例如分类任务中就是token“[cls]”经过1层Dense，然后接了一个二分类的loss；例如问题回答任务中会在paragraph上的token中预测一个起始位置，一个终止位置，然后以起始位置和终止位置的预测分布和真实分布为基础设计loss；例如序列标注，预测每一个token的词性，然后以每一个token在词性的预测分布和真实分布为基础设计loss。</p></li><li><p>bert在encoder之后，在计算NSP和MLM的loss之前，分别对NSP和MLM的输入加了一个Dense操作，这部分参数只对预训练有用，对fine-tune没用。而transformer在decoder之后就直接计算loss了，中间没有Dense操作。</p></li></ol><h4 id="Bert的技术细节"><a href="#Bert的技术细节" class="headerlink" title="Bert的技术细节"></a>Bert的技术细节</h4><ul><li>为什么bert需要额外的segment embedding?</li></ul><p>  因为bert预训练的其中一个任务是判断segment A和segment B之间的关系，这就需要embedding中能包含当前token属于哪个segment的信息，然而无论是token embedding，还是position embedding都无法表示出这种信息，因此额外创建一个segment embedding matrix用来表示当前token属于哪个segment的信息，segment vocab size就是2，其中index=0表示token属于segment A，index=1表示token属于segment B。</p><ul><li>为什么transformer的embedding后面接了一个dropout，而bert是先接了一个layer normalization，再接dropout?</li></ul><p>  LN是为了解决梯度消失的问题，dropout是为了解决过拟合的问题。在embedding后面加LN有利于embedding matrix的收敛。</p><ul><li>为什么在multi-head attention中，bert不仅会concat<4.6>的attention的结果，还会把前N-1个encoder block中attention的结果都concat进来？</li></ul><p>  有ensemble的思路在里面，比起单纯只用第N个encoder block中的attention结果，将前N个encoder block中的attention结果concat起来显然能够get到更多的信息，而下一步的linear操作又将结果的大小重新变回[batch size, max seq length, hidden size]。该问题和transformer的问题3.4的本质是一样的，通过ensemble可以得到更多的信息。</p><ul><li><p>为什么token被mask的概率是15%？为什么被mask后，还要分3种情况？</p><p>15%的概率是通过实验得到的最好的概率，xlnet也是在这个概率附近，说明在这个概率下，既能有充分的mask样本可以学习，又不至于让segment的信息损失太多，以至于影响mask样本上下文信息的表达。然而因为在下游任务中不会出现token“<mask>”，所以预训练和fine-tune出现了不一致，为了减弱不一致性给模型带来的影响，被mask的token有80%的概率用“<mask>”表示，有10%的概率随机替换成某一个token，有10%的概率保留原来的token，这3个百分比也是多次实验得到的最佳组合，在这3个百分比的情况下，下游任务的fine-tune可以达到最佳的实验结果。</p></li></ul><h4 id="BERT-可以用在哪？"><a href="#BERT-可以用在哪？" class="headerlink" title="BERT 可以用在哪？"></a>BERT 可以用在哪？</h4><ul><li>Just take it as a kind of embedding (contextualized)</li><li>Wherever you use word embedding， use BERT instead!</li><li>Eg. Text classifacation, seqence labeling, sentence generation，machine reading comprehension， natural language inference …</li></ul><p><img src="https://p1.meituan.net/travelcube/5e5258267b2ca37cbbe662a4d415e021110023.png" alt="图4 MT-BERT整体技术框架"></p><p>细粒度情感分析</p><p><img src="https://p0.meituan.net/travelcube/54ffe0cbac52e59af8e8abd349476e47345216.jpg" alt="图9 基于MT-BERT的多任务细粒度情感分析模型架构"></p><h4 id="BERT的总结"><a href="#BERT的总结" class="headerlink" title="BERT的总结"></a>BERT的总结</h4><ol><li>BERT采用Masked LM + Next Sentence Prediction作为pre-training tasks, 完成了真正的Bidirectional LM</li><li>BERT模型能够很容易地Fine-tune，并且效果很好，并且BERT as additional feature效果也很好</li><li>模型足够泛化，覆盖了足够多的NLP tasks</li></ol><h2 id="4-UniLM-2019"><a href="#4-UniLM-2019" class="headerlink" title="4. UniLM (2019)"></a>4. UniLM (2019)</h2><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/unilm.png" alt="img" style="zoom: 50%;" /></p><p><strong>Q：UniLM模型介绍</strong></p><p>A：通过不同的掩码来控制预测单词的可见上下文词语数量，实现不同的语言模型的联合训练。</p><p>单向语言模型：分为从左到右和从右向左两种，从左到右，即仅通过被掩蔽token的左侧所有本文来预测被掩蔽的token；从右到左，则是仅通过被掩蔽token的右侧所有本文来预测被掩蔽的token。</p><p>双向语言模型：与BERT模型一致，在预测被掩蔽token时，可以观察到所有的token。</p><p>序列到序列语言模型：如果被掩蔽token在第一个文本序列中，那么仅可以使用第一个文本序列中所有token，不能使用第二个文本序列的任何信息；如果被掩蔽token在第二个文本序列中，那么使用一个文本序列中所有token和第二个文本序列中被掩蔽token的左侧所有token预测被掩蔽token。</p><p><strong>Q：UniLM模型如何掩码？</strong></p><p>A：token掩码的概率为15%，在被掩掉的token中，有80%使用[MASK]替换，10%使用字典中随机词进行替换，10%保持原来token不变，与BERT模型一致。此外，在80%的情况下，每次随机掩掉一个token，在剩余的20%情况下，掩掉一个二元token组或三元token组。</p><p><strong>Q：如何训练UniLM模型？</strong></p><p>A：使用1/3的数据进行双向语言模型优化，1/3的数据进行序列到序列语言模型优化，1/6的数据进行从左向右的单向语言模型优化，1/6的数据进行从右向左的单向语言模型优化</p><p><strong>Q：如何实现训练代码？</strong></p><p>A：每个batch为一个任务，2个双向语言模型任务，2个序列到序列语言模型任务，1个左向右的单向语言模型任务，1个从右向左的单向语言模型，每跑一个任务进行一次累计梯度，跑完一轮所有任务，执行一次反向传播。</p><p>UniLM论文见：<a href="https://arxiv.org/abs/1905.03197">https://arxiv.org/abs/1905.03197</a></p><p>UniLM论文解读见：<a href="https://zhuanlan.zhihu.com/p/113380840">https://zhuanlan.zhihu.com/p/113380840</a></p><h2 id="5-Transformer-XL-2019"><a href="#5-Transformer-XL-2019" class="headerlink" title="5. Transformer-XL (2019)"></a>5. Transformer-XL (2019)</h2><p>将 Transformer 或注意力机制应用到语言建模中的核心问题是如何训练 Transformer 使其有效地将一个任意长文本编码为一个固定长度的表示。Transformer-XL 将整个语料拆分为较短的段落，仅利用每段进行训练并忽略之前段落的上下文信息。这种方式称之为 Vanilla Model <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:airfou2019character">27</a>，如下图所示：</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/transformer-xl-vanilla-model.png" alt="img"></p><p>在这种训练模式下，无论是前向还是后向信息都不会跨越分割的段落进行传导。利用固定长度的上下文主要有两个弊端：</p><ol><li>这限制了最大依赖的长度，虽然自注意力机制不会像 RNN 一样受到梯度弥散的影响，但 Vanilla Model 也不能完全利用到这个优势。</li><li>虽然可以利用补全操作来实现句子或其他语义的分割，但实际上通常会简单的将一个长文本截断成一个固定长度的分割，这样会产生上下文分裂破碎的问题。</li></ol><p>为了解决这个问题，Transformer-XL 采用了一种循环机制的 Transformer。在训练阶段，在处理新的分割段落时，之前分割分部分的隐含状态序列将被<strong>固定（fixed）</strong>和<strong>缓存（cached）</strong>下来作为一个扩展的上下文被复用参与计算，如下图所示：</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/transformer-xl-model.png" alt="img"></p><p>虽然梯度仍仅限于这个分割段落内部，但网络可以从历史中获取信息，从而实现对长期依赖的建模。令两个长度为 L 的连续分割段落为 $\mathbf{s}_{\tau} = \left[x_{\tau, 1}, \dotsc, x_{\tau, L}\right]$ 和 $\mathbf{s}_{\tau + 1} = \left[x_{\tau + 1, 1}, \dotsc, x_{\tau + 1, L}\right]$，第 τ 段分割 sτ 的第 n 层隐含状态为 $\mathbf{h}^n_{\tau} \in \mathbb{R}^{L \times d}$，其中 d 为隐含维度。则对于分割段落 $s_{τ+1}$ 的第 n 层隐含状态通过如下方式进行计算：</p><script type="math/tex; mode=display">\begin{aligned}\tilde{\mathbf{h}}^{n-1}_{\tau + 1} &= \left[\text{SG} \left(\mathbf{h}^{n-1}_{\tau}\right) \circ \mathbf{h}^{n-1}_{\tau + 1} \right] \\\mathbf{q}^{n}_{\tau + 1}, \mathbf{k}^{n}_{\tau + 1}, \mathbf{v}^{n}_{\tau + 1} &= \mathbf{h}^{n-1}_{\tau + 1} \mathbf{W}^{\top}_{q}, \tilde{\mathbf{h}}^{n-1}_{\tau + 1} \mathbf{W}^{\top}_{k}, \tilde{\mathbf{h}}^{n-1}_{\tau + 1} \mathbf{W}^{\top}_{v} \\\mathbf{h}^{n}_{\tau + 1} &= \text{Transformer-Layer} \left(\mathbf{q}^{n}_{\tau + 1}, \mathbf{k}^{n}_{\tau + 1}, \mathbf{v}^{n}_{\tau + 1}\right)\end{aligned}</script><p>其中，$SG(⋅)$ 表示停止梯度，$\left[\mathbf{h}_u \circ \mathbf{h}_v\right]$ 表示将两个隐含序列按照长度维度进行拼接，W 为模型的参数。与一般的 Transformer 相比，最大的不同在于 $\mathbf{k}^n_{\tau + 1}$ 和 $\mathbf{v}^n_{\tau + 1}$ 不仅依赖于 $\tilde{\mathbf{h}}^{n-1}_{\tau - 1}$ 还依赖于之前分割段落的 $\mathbf{h}^{n-1}_{\tau}$ 缓存。</p><p>在标准的 Transformer 中，序列的顺序信息通过位置嵌入$\mathbf{U} \in \mathbb{R}^{L_{\max} \times d}$ 提供，其中第 i 行 $U_i$ 对应一个分割文本内部的第 i 个<strong>绝对</strong>位置，$L_max$ 为最大可能长度。在 Transformer-XL 中则是通过一种<strong>相对</strong>位置信息对其进行编码，构建一个相对位置嵌入 $\mathbf{R} \in \mathbb{R} ^{L_{\max} \times d}$，其中第 i 行 $R_i$ 表示两个位置之间相对距离为 i 的嵌入表示。</p><p>对于一般的 Transformer，一个分割段落内部的 qi 和 kj 之间的注意力分数可以分解为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{A}_{i, j}^{\mathrm{abs}} &=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)} \\&+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)}\end{aligned}</script><p>利用相对位置思想，变化如下：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{A}_{i, j}^{\mathrm{rel}} &=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} \textcolor{blue}{\mathbf{R}_{i-j}}}_{(b)} \\&+\underbrace{\textcolor{red}{u^{\top}} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\textcolor{red}{v^{\top}} \mathbf{W}_{k, R} \textcolor{blue}{\mathbf{R}_{i-j}}}_{(d)}\end{aligned}</script><ol><li>首先，利用相对位置 $R_{i−j}$ 替代绝对位置嵌入 Uj，这里 R 采用的是无需学习的 sinusoid 编码矩阵 <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:vaswani2017attention">14</a>。</li><li>其次，引入了一个可训练的参数 $\textcolor{red}{u} \in \mathbb{R}^d$ 用于替换 $\mathbf{U}^{\top}_i \mathbf{W}^{\top}_q$。类似的，对于$\mathbf{U}^{\top} \mathbf{W}^{\top}_q$ 使用一个可训练的 $\textcolor{red}{v} \in \mathbb{R}^d$ 替换。</li><li>最后，有意地划分了两个权重矩阵 $\mathbf{W}_{k, E}$ 和 $\mathbf{W}_{k, R}$ 用于生成基于内容的 Key 向量和基于位置的 Key 向量。</li></ol><p>这样，(a) 代表了基于内容的位置信息，(b) 捕获了内容无关的位置偏置，(c) 表示了一个全局的内容偏置，(d) 捕获了一个全局的位置偏置。</p><p>利用一个自注意力头计算 N 层的 Transformer-XL 的过程如下，对于 n=1,…,N 有：</p><script type="math/tex; mode=display">\begin{aligned}\widetilde{\mathbf{h}}_{\tau}^{n-1}=&\left[\mathrm{SG}\left(\mathbf{m}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau}^{n-1}\right] \\\mathbf{q}_{\tau}^{n}, \mathbf{k}_{\tau}^{n}, \mathbf{v}_{\tau}^{n}=& \mathbf{h}_{\tau}^{n-1} {\mathbf{W}_{q}^{n}}^{\top}, \widetilde{\mathbf{h}}_{\tau}^{n-1} {\mathbf{W}_{k, E}^{n}}^{\top}, \widetilde{\mathbf{h}}_{\tau}^{n-1} {\mathbf{W}_{v}^{n}}^{\top} \\\mathbf{A}_{\tau, i, j}^{n}=& {\mathbf{q}_{\tau, i}^{n}}^{\top} \mathbf{k}_{\tau, j}^{n} + {\mathbf{q}_{\tau, i}^{n}}^{\top} \mathbf{W}_{k, R}^{n} \mathbf{R}_{i-j} \\&+u^{\top} \mathbf{k}_{\tau, j}+v^{\top} \mathbf{W}_{k, R}^{n} \mathbf{R}_{i-j} \\\mathbf{a}_{\tau}^{n}=& \text { Masked-Softmax }\left(\mathbf{A}_{\tau}^{n}\right) \mathbf{v}_{\tau}^{n} \\\mathbf{o}_{\tau}^{n}=& \text { LayerNorm } \left(\text{Linear}\left(\mathbf{a}_{\tau}^{n}\right)+\mathbf{h}_{\tau}^{n-1}\right) \\\mathbf{h}_{\tau}^{n}=& \text { Positionwise-Feed-Forward }\left(\mathbf{o}_{\tau}^{n}\right)\end{aligned}</script><h2 id="6-XLNet-2019"><a href="#6-XLNet-2019" class="headerlink" title="6. XLNet (2019)"></a>6. XLNet (2019)</h2><p><a href="https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.7.8c42358a11926b2f.md">https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.7.8c42358a11926b2f.md</a></p><p>目前语言预训练模型的模式主要有2种，第一种是像gpt这种的auto-regressive自回归模型，每个时刻都依据之前所有时刻的token来预测下一个token，auto-regressive的loss的定义如下：</p><script type="math/tex; mode=display">\max _{\theta} \log p_{\theta}(\mathbf{x})=\sum_{t=1}^{T} \log p_{\theta}\left(x_{t} | \mathbf{x}_{<t}\right)=\sum_{t=1}^{T} \log \frac{\exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x^{\prime}\right)\right)}</script><p>第二种是像bert这种的auto-encoder自编码模型，随机mask掉句子中若干个token，然后依据上下文预测被mask掉的token，auto-encoder的loss的定义如下：</p><script type="math/tex; mode=display">\max _{\theta} \quad \log p_{\theta}(\overline{\mathbf{x}} | \hat{\mathbf{x}}) \approx \sum_{t=1}^{T} m_{t} \log p_{\theta}\left(x_{t} | \hat{\mathbf{x}}\right)=\sum_{t=1}^{T} m_{t} \log \frac{\exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x^{\prime}\right)\right)}</script><p>两种不同的预训练目标的优劣势如下：</p><ol><li><strong>独立假设</strong>：BERT 中联合条件概率 $p(\overline{\mathbf{x}} | \hat{\mathbf{x}})$ 假设在给定的 x^ 下，遮挡的词条 $\overline{\mathbf{x}}$ 是相关独立的，而 AR(auto-regressive) 语言模型则没有这样的假设。</li><li><strong>输入噪声</strong>：BERT 在预训练是使用了特殊标记 <code>[MASK]</code>，在下游任务微调时不会出现，而 AR 语言模型则不会存在这个问题。</li><li><strong>上下文依赖</strong>：AR 语言模型仅考虑了词条左侧的上下文，而 BERT 则可以捕获两个方向的上下文。</li></ol><p>为了利用 AR 语言模型和 BERT 的优点，XLNet 提出了排序语言模型。对于一个长度为 T 序列 x，共有 $T!$​ 种不同的方式进行 AR 分解，如果模型共享不同分解顺序的参数，那么模型就能学习到两侧所有位置的信息。令 $Z_T$ 为长度为 T 的索引序列 $[1,2,…,T]$​ 的所有可能排列，$z_t$ 和 $z_{&lt;t}$ 分别表示一个排列 $z∈Z_T$ 第 t 个和前 t−1 个元素。则排列语言模型的优化目标为：</p><script type="math/tex; mode=display">\max_{\theta} \quad \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\sum_{t=1}^{T} \log p_{\theta}\left(x_{z_{t}} | \mathbf{x}_{\mathbf{z}_{<t}}\right)\right]</script><p>根据标准的 Transformer，下一个词条的分布 $p_{\theta}\left(X_{z_{t}} | \mathbf{x}_{\mathbf{z}&lt;t}\right)$ 为：</p><script type="math/tex; mode=display">p_{\theta}\left(X_{z_{t}} = x | \mathbf{x}_{\mathbf{z}<t}\right)=\frac{\exp \left(e(x)^{\top} h_{\theta}\left(\mathbf{x}_{\mathbf{z}<t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} h_{\theta}\left(\mathbf{x}_{\mathbf{z}<t}\right)\right)}</script><p>其中，$h_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}\right)$​ 表示通过共享的 Transformer 产生的 $X_{Z&lt;t}$ 的隐含表示。该表示并不依赖于所预测的位置，为了避免这个问题，我们将位置 zt 加入到模型中：</p><script type="math/tex; mode=display">p_{\theta}\left(X_{z_{t}}=x | \mathbf{x}_{z_{<t}}\right)=\frac{\exp \left(e(x)^{\top} g_{\theta}\left(\mathbf{x}_{\mathbf{z}<t}, z_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} g_{\theta}\left(\mathbf{x}_{\mathbf{z}<t}, z_{t}\right)\right)}</script><p>对于 $g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)$ 进行建模需要满足如下两个要求：</p><ol><li>预测 $x_{zt}$ 时，$g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)$ 只能使用位置信息 zt 而不能使用内容信息 $x_{zt}$。</li><li>在预测 $x_{zt}$ 之后的词条时，$g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)$  又必须包含 $x_{zt}$ 的语义信息。</li></ol><p>为了解决这个问题，XLNet 提供了两种隐含表示：</p><ol><li>内容隐含表示 $h_{\theta}\left(\mathbf{x}_{\mathbf{z} \leq t}\right)$，简写为 hzt，它和标准的 Transformer 一样，既编码上下文也编码 xzt 的内容。</li><li>查询隐含表示 $g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)$，简写为 gzt，它仅编码上下文信息 XZ&lt;t 和位置信息 zt，不编码内容 xzt。</li></ol><p>模型的整个计算过程如下图所示：</p><p><img src="https://ningshixian.github.io/resources/images/xlnet.jpg" alt=""></p><p>虽然排列语言模型有很多优点，但是由于计算量很大，模型很难进行优化，因此我们通过仅预测一个句子后面的一些词条解决这个问题。将 z 分为两部分：非目标子序列 $\mathbf{z}_{\leq c}$ 和目标子序列 $z_{&gt;c}$，其中 c 为切分点。同时会设置一个超参数 K，表示仅 1/K 的词条会被预测，有 $|\mathbf{z}| /(|\mathbf{z}|-c) \approx K$。对于未被选择的词条，其查询隐状态无需被计算，从而节省计算时间和资源。</p><h2 id="7-MASS-2019"><a href="#7-MASS-2019" class="headerlink" title="7. MASS (2019)"></a>7. MASS (2019)</h2><p>MASS 是一个专门针对序列到序列的自然语言任务设计的预训练方法，对于一个给定的原始句子 $x∈X$，令 $x^{\setminus u:v}$ 表示将 x 从 u 到 v 位置进行遮挡处理，$k=v−u+1$ 为被遮挡词条的个数，$x^{u:v}$ 为从 u 到 v 位置被遮挡的部分。MASS 利用被遮挡的序列 $x^{\setminus u:v}$ 预测被遮挡的部分 $x^{u:v}$ ，目标函数的对数似然如下：</p><script type="math/tex; mode=display">\begin{aligned}L(\theta ; \mathcal{X}) &=\frac{1}{|\mathcal{X}|} \Sigma_{x \in \mathcal{X}} \log P\left(x^{u: v} | x^{\setminus u: v} ; \theta\right) \\&=\frac{1}{|\mathcal{X}|} \Sigma_{x \in \mathcal{X}} \log \prod_{t=u}^{v} P\left(x_{t}^{u: v} | x_{<t}^{u: v}, x^{\setminus u: v} ; \theta\right)\end{aligned}</script><p>对于一个具有 8 个词条的序列，$x_3 x_4 x_5 x_6$ 被遮挡的示例如下：</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/mass.png" alt="img"></p><p>模型仅预测遮挡的部分 $x_3 x_4 x_5 x_6$ ，对于解码器中位置 4−6 利用 $x_3 x_4 x_5$  作为输入，利用特殊遮挡符号 $\left[\mathbb{M}\right]$ 作为其他位置的输入。对于不同长度 k，MASS 包含了上文中提到的两种预训练模型：</p><div class="table-container"><table><thead><tr><th>长度</th><th>概率</th><th>模型</th></tr></thead><tbody><tr><td>k=1</td><td>$P\left(x^{u} \mid x^{\setminus u} ; \theta\right)$</td><td>masked LM in BERT</td></tr><tr><td>k=m</td><td>$P\left(x^{1:m} \mid x^{\setminus 1:m} ; \theta\right)$</td><td>masked LM in GPT</td></tr><tr><td>k∈(1,m)</td><td>$P\left(x^{u:v} \mid x^{\setminus u:v} ; \theta\right)$</td><td>两种之间</td></tr></tbody></table></div><p>对于不同 k 值，实验发现当 k 处于 m 的 50% 至 70% 之间时下游任务性能最优。</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/mass-k.png" alt="img"></p><p>当 $k=0.5m$​ 时，MASS 可以很好地平衡编码器和解码器的预训练。过度地偏向编码器（k=1，masked LM in BERT）和过度地偏向解码器（k=m，masked LM in GPT）均不能在下游的自然语言生成任务中取得很好的效果。</p><h2 id="8-RoBERTa-2019"><a href="#8-RoBERTa-2019" class="headerlink" title="8. RoBERTa (2019)"></a>8. RoBERTa (2019)</h2><p>Facebook的<a href="https://arxiv.org/pdf/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>，主要工作是复现 <code>BERT</code>，然后对 <code>BERT</code> 的模型架构、训练目标、训练细节（如数据集大小、训练时间）的重要性进行探索，从而提出了改进方案，这个改进方案称为 <code>RoBERTa</code> 。</p><p>roberta的创新点主要有4点：</p><ol><li>第1点是动态mask，之前bert使用的是静态mask，就是数据预处理的时候完成mask操作，之后训练的时候同一个样本都是相同的mask结果，动态mask就是在训练的时候每输入一个样本都要重新mask，动态mask相比静态mask有更多不同mask结果的数据用于训练，效果很好。</li><li>模型去掉了 NSP 任务，修改了样本的构造方式，发现可以略微提升下游任务的性能。将输入2个segment修改为从一个文本中连续sample句子直到塞满512的长度。当到达文本的末尾且未塞满512的长度时，先增加一个“[sep]”，再从另一个文本接着sample，直到塞满512的长度。</li><li>更大的训练数据（从16G变成了160G）和更大的 Batch 大小（256改为2K甚至8K）</li><li>原始 BERT 采用一个 30K 的 BPE 词表，RoBERTa 采用了一个更大的 50K 的词表 <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:radford2019language">28</a>。</li></ol><h2 id="9-BART-2019"><a href="#9-BART-2019" class="headerlink" title="9. BART (2019)"></a>9. BART (2019)</h2><p>BART 采用了一个标准的 Seq2Seq Transformer 结构，类似 GPT 将 ReLU 激活函数替换为 GeLUs。对于基线模型，采用了一个 6 层的编码和解码器，对于更大模型采用了 12 层的结构。相比于 BERT 的架构主要有以下两点不同：</p><ol><li>解码器的每一层叠加了对编码器最后一个隐含层的注意力。</li><li>BERT 在预测之前采用了一个前馈的网络，而 BART 没有。</li></ol><p>BART 采用了最小化破坏后的文档和原始文档之间的重构误差的方式进行预训练。不同于其他的一些去噪自编码器，BART 可以使用任意类型的文档破坏方式。极端情况下，当源文档的所有信息均丢失时，BART 就等价与一个语言模型。BART 中采用的文本破坏方式有：字符遮罩，字符删除，文本填充，句子重排，文档旋转，如下图所示：</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/bart-transformations.png" alt="img"></p><h2 id="10-T5-2019"><a href="#10-T5-2019" class="headerlink" title="10. T5 (2019)"></a>10. T5 (2019)</h2><p>T5（Text-to-Text Transfer Transformer） 提出了一种 text-to-text 的框架，旨在利用相同的模型，损失函数和超参数等对机器翻译，文档摘要，问答和分类（例如：情感分析）等任务进行统一建模。我们甚至可以利用 T5 通过预测一个数字的文本表示而不是数字本身来建模一个回归任务。模型及其输入输出如下图所示：</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/t5-text-to-text-framework.gif" alt="img"></p><p>Google 的这项研究并不是提出一种新的方法，而是从全面的视角来概述当前 NLP 领域迁移学习的发展现状。T5 还公开了一个名为 C4（Colossal Clean Crawled Corpus）的数据集，该数据集是一个比 Wikipedia 大两个数量级的 Common Crawl 的清洗后版本的数据。更多模型的细节请参见源论文和 Google 的 <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">官方博客</a>。</p><h2 id="11-ERNIE-Baidu-2019"><a href="#11-ERNIE-Baidu-2019" class="headerlink" title="11. ERNIE (Baidu, 2019)"></a>11. ERNIE (Baidu, 2019)</h2><p>ERNIE 1.0 <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:sun2019ernie">29</a> 通过建模海量数据中的词、实体及实体关系，学习真实世界的语义知识。相较于 BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。例如：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css">BERT ：哈 <span class="hljs-selector-attr">[mask]</span> 滨是 <span class="hljs-selector-attr">[mask]</span> 龙江的省会，<span class="hljs-selector-attr">[mask]</span> 际冰 <span class="hljs-selector-attr">[mask]</span> 文化名城。<br>ERNIE：<span class="hljs-selector-attr">[mask]</span> <span class="hljs-selector-attr">[mask]</span> <span class="hljs-selector-attr">[mask]</span> 是黑龙江的省会，国际 <span class="hljs-selector-attr">[mask]</span> <span class="hljs-selector-attr">[mask]</span> 文化名城。<br></code></pre></td></tr></table></figure><p>在 BERT 模型中，我们通过『哈』与『滨』的局部共现，即可判断出『尔』字，模型没有学习与『哈尔滨』相关的任何知识。而 ERNIE 通过学习词与实体的表达，使模型能够建模出『哈尔滨』与『黑龙江』的关系，学到『哈尔滨』是 『黑龙江』的省会以及『哈尔滨』是个冰雪城市。</p><p>训练数据方面，除百科类、资讯类中文语料外，ERNIE 还引入了论坛对话类数据，利用 DLM（Dialogue Language Model）建模 Query-Response 对话结构，将对话 Pair 对作为输入，引入 Dialogue Embedding 标识对话的角色，利用 Dialogue Response Loss 学习对话的隐式关系，进一步提升模型的语义表示能力。</p><p>ERNIE 2.0 <a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/#fn:sun2019ernie2">30</a> 是基于持续学习的语义理解预训练框架，使用多任务学习增量式构建预训练任务。ERNIE 2.0 中，新构建的预训练任务类型可以无缝的加入训练框架，持续的进行语义理解学习。 通过新增的实体预测、句子因果关系判断、文章句子结构重建等语义任务，ERNIE 2.0 语义理解预训练模型从训练数据中获取了词法、句法、语义等多个维度的自然语言信息，极大地增强了通用语义表示能力。</p><p><img src="https://leovan.me/images/cn/2020-03-28-pre-trained-model-for-nlp/ernie-2-framework.png" alt="img"></p><h1 id="轻量化BERT"><a href="#轻量化BERT" class="headerlink" title="轻量化BERT"></a>轻量化BERT</h1><h2 id="1-Albert"><a href="#1-Albert" class="headerlink" title="1. Albert"></a>1. Albert</h2><p>增大预训练模型的大小通常能够提高预训练模型的推理能力，但是当预训练模型增大到一定程度之后，会碰到<em>GPU/TPU memory</em>的限制。因此，作者==在bert中加入了2项减少参数的技术==，能够缩小bert的大小，并且修改了bert NSP的loss，在和bert有相同参数量的前提之下，有更强的推理能力。 </p><h3 id="albert的流程"><a href="#albert的流程" class="headerlink" title="albert的流程"></a>albert的流程</h3><h4 id="词向量矩阵的分解"><a href="#词向量矩阵的分解" class="headerlink" title="词向量矩阵的分解"></a>词向量矩阵的分解</h4><p>在bert以及诸多bert的改进版中，embedding size都是等于hidden size的，这不一定是最优的。因为<u>bert的token embedding是上下文无关的，而经过multi-head attention+ffn后的hidden embedding是上下文相关的</u>，bert预训练的目的是提供更准确的hidden embedding，而不是token embedding，因此token embedding没有必要和hidden embedding一样大。albert将token embedding进行了分解，首先降低embedding size的大小，然后用一个Dense操作将低维的token embedding映射回hidden size的大小。bert的 <em>embedding size=hidden size</em>，因此词向量的参数量是<em>vocab size \</em> hidden size<em>，进行分解后的参数量是 </em>vocab size * embedding size + embedding size * hidden size*，==只要embedding size &lt;&lt; hidden size，就能起到减少参数的效果==。</p><h4 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h4><p>bert的12层transformer encoder block是串行在一起的，每个block虽然长得一模一样，但是参数是不共享的。albert==将transformer encoder block进行了参数共享，这样可以极大地减少整个模型的参数量==。</p><h4 id="sentence-order-prediction-SOP"><a href="#sentence-order-prediction-SOP" class="headerlink" title="sentence order prediction(SOP)"></a>sentence order prediction(SOP)</h4><p>在auto-encoder的loss之外，bert使用了NSP的loss，用来提高bert在句对关系推理任务上的推理能力。而albert放弃了NSP的loss，使用了SOP的loss。NSP的loss是判断segment A和segment B之间的关系，其中0表示segment B是segment A的下一句，1表示segment A和segment B来自2篇不同的文本。SOP的loss是判断segment A和segment B的的顺序关系，0表示segment B是segment A的下一句，1表示segment A是segment B的下一句。</p><h3 id="albert的技术细节"><a href="#albert的技术细节" class="headerlink" title="albert的技术细节"></a>albert的技术细节</h3><h4 id="参数减少技术"><a href="#参数减少技术" class="headerlink" title="参数减少技术"></a>参数减少技术</h4><p>albert使用了2项参数减少的技术，但是2项技术对于参数减少的贡献是不一样的，第1项是词向量矩阵的分解，当embedding size从768降到64时，可以节省21M的参数量，但是模型的推理能力也会随之下降。第2项是multi-head attention+ffn的参数共享，在embedding size=128时，可以节省77M的参数量，模型的推理能力同样会随之下降。虽然参数减少会导致了模型推理能力的下降，但是可以通过增大模型使得参数量变回和bert一个量级，这时模型的推理能力就超过了bert (🐮🍺)。</p><p>现在<strong>学术界发论文有2种常见的套路</strong>，第1种是往死里加参数加数据量，然后提高模型的推理能力；第2种是减参数，然后使模型的推理能力不怎么降。albert使用的参数减少技术看似是第2种，实则是第1种。当bert从large变到xlarge时，虽然模型变大到了1270M，但是模型出现了退化现象，推理能力下跌了一大截，说明在bert的框架下，large已经是模型推理能力的极限了。albert使用了参数减少技术，相比于bert的large是334M，albert的large只有18M，虽然推理能力比bert差，但是参数减少后的albert还有成长空间，将albert从large变到xlarge，甚至是xxlarge时，模型的推理能力又得到了提高，并且超过了bert最好的模型。</p><h4 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h4><p>在albert之前，很多bert的改进版都对NSP的loss提出了质疑。structbert在NSP的loss上进行了修改，有1/3的概率是segment B是segment A的下一句，有1/3的概率是segment A是segment B的下一句，有1/3的概率是segment A和segment B来自2篇不同的文本。roberta则是直接放弃了NSP的loss，修改了样本的构造方式，将输入2个segment修改为从一个文本中连续sample句子直到塞满512的长度。当到达文本的末尾且未塞满512的长度时，先增加一个“[sep]”，再从另一个文本接着sample，直到塞满512的长度。</p><p>albert在structbert的基础之上又抛弃了segment A和segment B来自2篇不同的文本的做法，只剩下1/2的概率是segment B是segment A的下一句，1/2的概率是segment A是segment B的下一句。论文中给出了这么做的解释，NSP的loss包含了2部分功能：topic prediction和coherence prediction，其中topic prediction要比coherence prediction更容易学习，而MLM的loss也包含了topic prediction的功能，因此bert难以学到coherence prediction的能力。albert的SOP loss抛弃了segment A和segment B来自2篇不同的文本的做法，让loss更关注于coherence prediction，这样就能提高模型在句对关系推理上的能力。</p><h3 id="albert的总结"><a href="#albert的总结" class="headerlink" title="albert的总结"></a>albert的总结</h3><p>albert然减少参数量，但是并不会减少推理时间，推理的过程只不过是从串行计算12个transformer encoder block变成了循环计算transformer encoder block 12次。albert最大的贡献在于使模型具备了比原始的bert更强的成长性，在模型变向更大的时候，推理能力还能够得到提高。</p><h2 id="2-DistillBert"><a href="#2-DistillBert" class="headerlink" title="2. DistillBert"></a>2. DistillBert</h2><p>　　<strong>论文：DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</strong> </p><p>　　DistillBert是在bert的基础上用知识蒸馏技术训练出来的小型化bert。整体上来说这篇论文还是非常简单的，只是引入了知识蒸馏技术来训练一个小的bert。具体做法如下：</p><p>　　1）给定原始的bert-base作为teacher网络。</p><p>　　2）在bert-base的基础上将网络层数减半（也就是从原来的12层减少到6层）。</p><p>　　3）利用teacher的软标签和teacher的隐层参数来训练student网络。</p><p>　　训练时的损失函数定义为三种损失函数的线性和，三种损失函数分别为：</p><p>　　1）𝐿𝑐𝑒Lce。这是teacher网络softmax层输出的概率分布和student网络softmax层输出的概率分布的交叉熵（注：MLM任务的输出）。</p><p>　　2）𝐿𝑚𝑙𝑚Lmlm。这是student网络softmax层输出的概率分布和真实的one-hot标签的交叉熵</p><p>　　3）𝐿𝑐𝑜𝑠Lcos。这是student网络隐层输出和teacher网络隐层输出的余弦相似度值，在上面我们说student的网络层数只有6层，teacher网络的层数有12层，因此个人认为这里在计算该损失的时候是用student的第1层对应teacher的第2层，student的第2层对应teacher的第4层，以此类推。</p><p>　　作者对student的初始化也做了些工作，作者用teacher的参数来初始化student的网络参数，做法和上面类似，用teacher的第2层初始化student的第1层，teacher的第4层初始化student的第2层。</p><p>　　作者也解释了为什么减小网络的层数，而不减小隐层大小，作者认为在现代线性代数框架中，在张量计算中，降低最后一维（也就是隐层大小）的维度对计算效率提升不大，反倒是减小层数，也提升计算效率。</p><p>　　另外作者在这里移除了句子向量和pooler层，在这里也没有看到NSP任务的损失函数，因此个人认为作者也去除了NSP任务（主要是很多人证明该任务并没有什么效果）。</p><p>　　整体上来说虽然方法简单，但是效果还是很不错的，模型大小减小了40%（66M），推断速度提升了60%，但性能只降低了约3%。</p><h2 id="3-TINYBERT"><a href="#3-TINYBERT" class="headerlink" title="3. TINYBERT"></a>3. TINYBERT</h2><p>　　<strong>论文：TINYBERT: DISTILLING BERT FOR NATURAL LANGUAGE UNDERSTANDING</strong></p><p>　　<strong>GitHub：暂无</strong></p><p>　　TINYBERT也是采用了知识蒸馏的方法来压缩模型的，只是在设计上较distillBert做了更多的工作，作者提出了两个点：针对Transformer结构的知识蒸馏和针对pre-training和fine-tuning两阶段的知识蒸馏。</p><p>　　作者在这里构造了四类损失函数来对模型中各层的参数进行约束来训练模型，具体模型结构如下：</p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021203334818-1964061977.png" alt="img"></p><p> 　作者构造了四类损失，分别针对embedding layer，attention 权重矩阵，隐层输出，predict layer。可以将这个统一到一个损失函数中：</p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021203510033-2086026974.png" alt="img"></p><p> 　上面式子中𝜆𝑚λm表示每一层对应的系数，𝑆𝑚Sm表示studnet网络的第m层，𝑇𝑔(𝑚)Tg(m)表示teacher网络的第n层，其中𝑛=𝑔(𝑚)n=g(m)。并且有𝑔(0)=0g(0)=0，𝑔(𝑀+1)=𝑁+1g(M+1)=N+1，0表示embedding layer，M+1和N+1表示perdict layer。</p><p>　　针对上面四层具体的损失函数表达式如下：</p><p>　　<strong>attention 权重矩阵</strong></p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021203915782-991212363.png" alt="img"></p><p> 　h为multi attention中头数</p><p>　　<strong>隐层输出</strong></p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021203952259-282949758.png" alt="img"></p><p> 　因为student网络的隐层大小通常会设置的比teacher的小，因此为了在计算时维度一致，这里用一个矩阵𝑊ℎWh将student的隐层向量线性映射到和teacher同样的空间下。</p><p>　　<strong>embedding layer</strong></p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021204121158-50391823.png" alt="img"></p><p> 　𝑊𝑠Ws同理上。</p><p>　　以上三种损失函数都采用了MSE，主要是为了将模型的各项参数对齐。</p><p>　　<strong>predict layer</strong></p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021204217779-1151054318.png" alt="img"></p><p> 　predict layer也就是softmax层，在这里的损失函数是交叉熵，t是温度参数，在这里设置为1。</p><p>　　以上四种损失函数是作者针对transformer提出的知识蒸馏方法。除此之外作者认为除了对pre-training蒸馏之外，在fine-tuning时也利用teacher的知识来训练模型可以取得在下游任务更好的效果。因此作者提出了两阶段知识蒸馏，如下图所示：</p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021204526288-613006885.png" alt="img"></p><p> 　本质上就是在pre-training蒸馏一个general TinyBERT，然后再在general TinyBERT的基础上利用task-bert上再蒸馏出fine-tuned TinyBERT。</p><p>　　作者给出了TinyBERT的效果：</p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021204817154-512271389.png" alt="img"></p><p> 　另外作者也给出了四种损失对最终结果的贡献：</p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021204919585-809627388.png" alt="img"></p><p> 　还有就是关于𝑛=𝑔(𝑚)n=g(m)这个式子中𝑔(𝑚)g(m)怎么选择，假设student的层数为4层，这里的𝑛=𝑔(𝑚)=3𝑚n=g(m)=3m，作者将这种称为Uniform-strategy。另外作者还和其他的𝑔(𝑚)g(m)做了对比：</p><p>　　　　<img src="https://img2018.cnblogs.com/blog/1335117/201910/1335117-20191021205104456-89074411.png" alt="img"></p><p> 　Top-strategy指用teacher最后4层，Bottom-strategy指用前面4层，其实这里的映射函数，我感觉可能还有更优的方案，例如取平均，或者用attention来做，可能效果会更好。</p><h1 id="BERT-扩展-💡"><a href="#BERT-扩展-💡" class="headerlink" title="BERT 扩展 💡"></a>BERT 扩展 💡</h1><p><a href="https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.9.8c42358a11926b2f.md">https://www.bookstack.cn/read/huaxiaozhuan-ai/spilt.9.8c42358a11926b2f.md</a></p><h2 id="BERT-wwm-ext"><a href="#BERT-wwm-ext" class="headerlink" title="BERT-wwm-ext"></a>BERT-wwm-ext</h2><p>原始版本的 <code>BERT</code> 采用了<code>WordPiece tokenize</code> 来预处理，即把每个单词拆解一些 <code>wordpiece token</code> 。在Pretraining的时候是随机Mask这些WordPiece的，这就可能出现只Mask一个词的一部分的情况，比如下面的例子：</p><p><img src="http://fancyerii.github.io/img/bert-imp/1.png" alt="img"></p><p>==为了解决这个问题，很自然的想法就是词作为一个整体要么都Mask要么都不Mask，这就是所谓的 <strong>Whole Word Masking</strong>==。这是一个很简单的想法，对于BERT的代码修改也非常少，只是修改一些Mask的那段代码。对于英文来说，分词是一个(相对)简单的问题。哈工大与科大讯飞的论文对Wiki的<a href="https://dumps.wikimedia.org/zhwiki/latest/">中文dump</a>进行了分词，然后做了一些实验。</p><ul><li>Whole Word Masking</li><li>替换BERT的AdamWeightDecayOptimizer为LAMB优化器</li></ul><h2 id="Sentence-Bert"><a href="#Sentence-Bert" class="headerlink" title="Sentence-Bert"></a>Sentence-Bert</h2><blockquote><p> <a href="https://www.jianshu.com/p/20c93094d4e9">文本匹配利器：从Siamse孪生网络到Sentence-BERT综述</a></p><p><a href="https://blog.csdn.net/weixin_43922901/article/details/106014964">语义相似度、句向量生成超强模型之SBERT</a> ★★★</p></blockquote><p><strong>Bert的缺点:</strong></p><ul><li><p>BERT不适合语义相似度搜索，也不适合非监督任务，比如聚类。</p><p>解决聚类和语义搜索的一种常见方法是将每个句子映射到一个向量空间，使得语义相似的句子很接近。</p><p>于是，也有人尝试向BERT输入单句，得到固定大小的sentene embedding。最常用的方法是，平均BERT输出层或使用第一个token（[CLS]的token）的输出。但这却产生了非常不好的sentence embedding，常常还不如averaging GloVe embeddings</p></li><li><p>语义相似度计算巨大开销。</p><p>Bert有1.1亿参数量（base版本）使得预测、推理速度明显比CNN等传统网络慢了不止一个量级，对资源要求更高，也不适合处理某些任务。例如，从10000条句子中找到最相似的一对句子，由于可能的组合众多，需要完成49,995,000次推理计算；在一块现代V00GPU上使用Bert计算，将消耗65小时。</p></li></ul><p><strong>解决方法：</strong></p><ul><li><p><strong>Sentence-Bert模型（以下简称SBert）</strong></p><p>SBert对预训练的BERT进行修改：使用孪生(Siamese)和三级(triplet)网络结构来获得语义上有意义的句子embedding，以此获得定长的sentence embedding，使用余弦相似度或Manhatten/Euclidean距离等进行比较找到语义相似的句子。前面所述的从10000条句子找最相似pair的任务，SBert仅需5秒就能完成！通过这样的方法得到的SBERT模型，在文本语义相似度等句子对的回归任务上吊打BERT , RoBERTa 拿到sota。</p></li></ul><p><strong>SBert的优势：</strong></p><ul><li><p>SBert<strong>充分利用了孪生网络的优点和预训练语言模型强大的特征抽取优势</strong>，在众多匹配任务上取得了最优实验结果。</p></li><li><p>SBert直接使用Bert的原始权重进行初始化，在具体数据集上微调，训练过程和传统Siamse Network差异不大。但是这种训练方式能让Bert更好的捕捉句子之间的关系，<strong>生成更优质的句向量</strong>。</p></li><li><strong>在评估测试阶段，SBert直接使用余弦相似度来比较两个句向量之间的相似度</strong>，极大提升了推理速度；</li><li>同时，得益于生成的高质量句嵌入特征，SBert在语义检索、信息搜索、文本聚类、新FAQ发现等工作中预计会有不错表现。</li></ul><p><strong>SBert的网络结构：</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/1599653-ac9cc602bc22d081.png?imageMogr2/auto-orient/strip|imageView2/2/w/882/format/webp" alt="img"></p><p><strong>SBert训练</strong></p><p>使用SNLI和NLI数据集对Bert和Roberta进行fine-tune，得到SBERT预训练模型。</p><ul><li><p><strong>Polling策略</strong></p><p>① mean：将句子的所有token在token维度上计算平均，这样就可以得到768(base)/1024(large)维度向量；<br>② max：将句子的所有token在token维度上的最大那个值，即做max_over_time，这里多解释下max_over_time，就是比如x = torch.randn(2, 10, 20)，就是取x.max(1)；<br>③ CLS：就是原始Bert做分类的方法，取句子的第一个token的向量。</p><p>作者发现，使用mean的效果最好，max最差。</p></li><li><p><strong>loss函数</strong></p><p>① 回归任务：softmax，均方差损失函数；<br>② 分类任务：softmax， 交叉熵损失函数；<br>③ Wikipedia section triplets dataset (Dor et al., 2018)(三句分类任务)：这个数据集是一些书中的句子，每一条数据有三句话，其中两句来自同一个章节的句子，另外一句是这本书的另外一个章节。<br><img src="https://img-blog.csdnimg.cn/20200509120335669.png" alt="在这里插入图片描述"><br>其中 ε设为1，||a, b||表示a和b的距离。求该损失函数的目的就是使得，a和p的距离小于a和n的距离</p></li></ul><p><strong>SBERT的相关资源</strong></p><p><strong>SBert开源地址</strong>：<a href="https://github.com/UKPLab/sentence-transformers">https://github.com/UKPLab/sentence-transformers</a></p><p><strong>SBert多语预训练模型下载地址</strong>：<a href="https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/">https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/</a></p><p><strong>code</strong> adapted from  <a href="https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic_search.py">https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic_search.py</a></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2652057750&amp;idx=4&amp;sn=799877cde18a3a4ce259a1a1654c9dfc&amp;chksm=f1204467c657cd71891649bbf14ecf20549a76911fcd087b5b0e6406ff5717f052d672d5db8b&amp;mpshare=1&amp;scene=23&amp;srcid=&amp;sharer_sharetime=1573370368071&amp;sharer_shareid=5f83a05212e1b63a8a185c7ff5d2a2be#rd">绝对干货！NLP预训练模型：从transformer到albert</a></p><p> <a href="https://kexue.fm/archives/4765">《Attention is All You Need》浅读（简介+代码）</a> </p><p><a href="https://tech.meituan.com/2019/11/14/nlp-bert-practice.html">美团BERT的探索和实践</a></p><p><a href="https://www.bookstack.cn/books/huaxiaozhuan-ai">💡 AI算法工程师手册</a></p><p><a href="https://leovan.me/cn/2020/03/pre-trained-model-for-nlp/">预训练自然语言模型 (Pre-trained Models for NLP)-范叶亮</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>预训练</tag>
      
      <tag>语言模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python定时任务</title>
    <link href="/2020/04/02/2020-04-02-python%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    <url>/2020/04/02/2020-04-02-python%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="Python定时任务"><a href="#Python定时任务" class="headerlink" title="Python定时任务"></a>Python定时任务</h1><blockquote><p>实现系统监测功能为例:</p><p>1：定时或者定点监测CPU与内存使用率；</p><p>2：将时间，CPU，内存使用情况保存到日志文件；</p></blockquote><p>主要介绍4类开启定时任务的方法：</p><ul><li>最简单使用方式：循环+sleep</li><li>线程模块中Timer类</li><li><p>调度模块：schedule</p></li><li><p>💡定时任务框架：APScheduler</p></li></ul><span id="more"></span><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># psutil:获取系统信息模块，可以获取CPU，内存，磁盘等的使用情况</span><br><span class="hljs-keyword">import</span> psutil<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">import</span> schedule<br><span class="hljs-keyword">import</span> time<br><span class="hljs-title">from</span> threading <span class="hljs-keyword">import</span> Timer<br><span class="hljs-title">from</span> apscheduler.schedulers.blocking <span class="hljs-keyword">import</span> BlockingScheduler<br></code></pre></td></tr></table></figure><h1 id="1-最简单使用方式：循环-sleep"><a href="#1-最简单使用方式：循环-sleep" class="headerlink" title="1. 最简单使用方式：循环+sleep"></a>1. 最简单使用方式：循环+sleep</h1><p>使用while+sleep就可以实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># logfile：监测信息写入文件</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MonitorSystem</span>(<span class="hljs-params">logfile=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-comment"># 获取cpu使用情况</span><br>    cpuper = psutil.cpu_percent()<br>    <span class="hljs-comment"># 获取内存使用情况：系统内存大小，使用内存，有效内存，内存使用率</span><br>    mem = psutil.virtual_memory()<br>    <span class="hljs-comment"># 内存使用率</span><br>    memper = mem.percent<br>    <span class="hljs-comment"># 获取当前时间</span><br>    now = datetime.datetime.now()<br>    ts = now.strftime(<span class="hljs-string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)<br>    line = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;ts&#125;</span> cpu:<span class="hljs-subst">&#123;cpuper&#125;</span>%, mem:<span class="hljs-subst">&#123;memper&#125;</span>%&quot;</span><br>    <span class="hljs-built_in">print</span>(line)<br>    <span class="hljs-keyword">if</span> logfile:<br>        logfile.write(line)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loopMonitor</span>():<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        MonitorSystem()<br>        <span class="hljs-comment"># 3s检查一次</span><br>        time.sleep(<span class="hljs-number">3</span>)<br>loopMonitor()<br></code></pre></td></tr></table></figure><h1 id="2-线程模块中Timer类"><a href="#2-线程模块中Timer类" class="headerlink" title="2. 线程模块中Timer类"></a>2. 线程模块中Timer类</h1><p>timer最基本理解就是定时器，我们可以启动多个定时任务，这些定时器任务是异步执行，所以不存在等待顺序执行问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># logfile：监测信息写入文件</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MonitorSystem2</span>(<span class="hljs-params">logfile=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-comment"># 获取cpu使用情况</span><br>    cpuper = psutil.cpu_percent()<br>    <span class="hljs-comment"># 获取内存使用情况：系统内存大小，使用内存，有效内存，内存使用率</span><br>    mem = psutil.virtual_memory()<br>    <span class="hljs-comment"># 内存使用率</span><br>    memper = mem.percent<br>    <span class="hljs-comment"># 获取当前时间</span><br>    now = datetime.datetime.now()<br>    ts = now.strftime(<span class="hljs-string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)<br>    line = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;ts&#125;</span> cpu:<span class="hljs-subst">&#123;cpuper&#125;</span>%, mem:<span class="hljs-subst">&#123;memper&#125;</span>%&quot;</span><br>    <span class="hljs-built_in">print</span>(line)<br>    <span class="hljs-keyword">if</span> logfile:<br>        logfile.write(line)<br>    <span class="hljs-comment"># 启动定时器任务，每三秒执行一次</span><br>    Timer(<span class="hljs-number">3</span>, MonitorSystem2).start()<br><br><br>MonitorSystem2()<br></code></pre></td></tr></table></figure><h1 id="3-调度模块：schedule"><a href="#3-调度模块：schedule" class="headerlink" title="3. 调度模块：schedule"></a>3. 调度模块：schedule</h1><ul><li><p>schedule是一个第三方轻量级的任务调度模块，可以按照秒，分，小时，日期或者自定义事件执行时间；</p></li><li><p>安装方式：pip install schedule</p></li><li><p>特征</p><ol><li><p>一种易于使用的API，用于调度作业。</p></li><li><p>非常轻巧，没有外部依赖性。</p></li><li><p>出色的测试覆盖率。</p></li><li><p>在Python 2.7、3.5和3.6上测试</p></li></ol></li></ul><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">tasklist</span>():<br>    # 清空任务<br>    schedule.<span class="hljs-built_in">clear</span>()<br>    # 创建一个按秒间隔执行任务<br>    schedule.<span class="hljs-built_in">every</span>(<span class="hljs-number">1</span>).seconds.<span class="hljs-built_in">do</span>(MonitorSystem)<br>    # schedule.<span class="hljs-built_in">every</span>(<span class="hljs-number">1</span>).hour.<span class="hljs-built_in">do</span>(MonitorSystem)<br>    # 执行<span class="hljs-number">10S</span><br>    while True:<br>        schedule.<span class="hljs-built_in">run_pending</span>()<br>        time.<span class="hljs-built_in">sleep</span>(<span class="hljs-number">1</span>)<br><br><br><span class="hljs-built_in">tasklist</span>()<br></code></pre></td></tr></table></figure><h1 id="4-定时任务框架：APScheduler💡"><a href="#4-定时任务框架：APScheduler💡" class="headerlink" title="4. 定时任务框架：APScheduler💡"></a>4. 定时任务框架：APScheduler💡</h1><blockquote><p>官网文档：<a href="http://apscheduler.readthedocs.io/en/3.3.1/">http://apscheduler.readthedoc…</a></p><p>API：<a href="http://apscheduler.readthedocs.io/en/v3.3.0/py-modindex.html">http://apscheduler.readthedoc…</a></p><p>介绍：<a href="https://www.cnblogs.com/Neeo/p/10435059.html">https://www.cnblogs.com/Neeo/p/10435059.html</a></p></blockquote><h2 id="APScheduler简介"><a href="#APScheduler简介" class="headerlink" title="APScheduler简介"></a>APScheduler简介</h2><p><a href="https://apscheduler.readthedocs.io/en/latest/index.html">APScheduler</a>（Advanced Python Scheduler）是一个轻量级的Python定时任务调度框架（Python库）。APScheduler有三个内置的调度系统，其中包括：</p><ul><li>cron式调度（可选开始/结束时间）</li><li>基于间隔的执行（以偶数间隔运行作业，也可以选择开始/结束时间）</li><li>一次性延迟执行任务（在指定的日期/时间内运行作业一次）</li></ul><h2 id="APScheduler组件"><a href="#APScheduler组件" class="headerlink" title="APScheduler组件"></a>APScheduler组件</h2><p><code>APScheduler</code> 包含四个组件，分别是：</p><ul><li>触发器（trigger），触发器中包含调度逻辑，每个作业都有自己的触发器来决定下次运行时间。除了它们自己初始配置以外，触发器完全是无状态的。</li><li>作业存储器（job store），存储被调度的作业，默认的作业存储器只是简单地把作业保存在内存中，其他的作业存储器则是将作业保存在数据库中，当作业被保存在一个持久化的作业存储器中的时候，该作业的数据会被序列化，并在加载时被反序列化，需要说明的是，作业存储器不能共享调度器。</li><li>执行器（executor），处理作业的运行，通常通过在作业中提交指定的可调用对象到一个线程或者进程池来进行，当作业完成时，执行器会将通知调度器。</li><li>调度器（scheduler），配置作业存储器和执行器可以在调度器中完成。例如添加、修改、移除作业，根据不同的应用场景，可以选择不同的调度器，可选的将在下一小节展示。</li></ul><h2 id="各组件简介"><a href="#各组件简介" class="headerlink" title="各组件简介"></a>各组件简介</h2><h3 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h3><ul><li>BlockingScheduler:阻塞调度。当你的程序只运行这个调度器时可以使用。</li><li>BackgroundScheduler:后台调度。你的应用程序不止运行调度器，且想它在应用程序中后台运行时，可以使用该调度器。</li><li>AsyncIOScheduler:如果 你应用的程序使用了asyncio模块，需要使用此调度器。</li><li>GeventScheduler:如果你的应用程序使用了gevent技术，需要使用此调度器。</li><li>TornadoScheduler:可以用于开发Tornado应用程序。</li><li>TwistedScheduler:可以用于开发Twisted应用程序。</li><li>QtScheduler:可以用于开发Qt应用程序。</li></ul><h3 id="作业存储器"><a href="#作业存储器" class="headerlink" title="作业存储器"></a>作业存储器</h3><p>如果你的应用在每次启动的时候都会重新创建作业，那么使用默认的作业存储器（MemoryJobStore）即可，但是如果你需要在调度器重启或者应用程序奔溃的情况下任然保留作业，你应该根据你的应用环境来选择具体的作业存储器。例如：使用Mongo或者SQLAlchemy JobStore （用于支持大多数RDBMS）</p><h3 id="执行器"><a href="#执行器" class="headerlink" title="执行器"></a>执行器</h3><p>对执行器的选择取决于你使用上面哪些框架，大多数情况下，使用默认的ThreadPoolExecutor已经能够满足需求。如果你的应用涉及到CPU密集型操作，你可以考虑使用ProcessPoolExecutor来使用更多的CPU核心。你也可以同时使用两者，将ProcessPoolExecutor作为第二执行器。</p><h3 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h3><p>当你调度作业的时候，你需要为这个作业选择一个触发器，用来描述这个作业何时被触发，APScheduler有三种内置的触发器类型：</p><ul><li>date 一次性指定日期</li><li>interval 在某个时间范围内间隔多长时间执行一次</li><li>cron 和Linux crontab格式兼容，最为强大</li></ul><h2 id="APScheduler安装"><a href="#APScheduler安装" class="headerlink" title="APScheduler安装"></a>APScheduler安装</h2><p>pip 安装</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">$ pip <span class="hljs-keyword">install</span> apscheduler<br></code></pre></td></tr></table></figure><p>源码安装</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">$ <span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> install<br></code></pre></td></tr></table></figure><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>当你需要调度作业的时候，你需要为这个作业选择一个触发器，用来描述该作业将在何时被触发，APScheduler有3中内置的触发器类型：</p><ul><li>新建一个调度器（scheduler）</li><li>添加一个调度任务（job store)</li><li>运行调度任务</li></ul><h3 id="添加作业"><a href="#添加作业" class="headerlink" title="添加作业#"></a>添加作业<a href="https://www.cnblogs.com/Neeo/p/10435059.html#添加作业">#</a></h3><p>有两种方式可以添加一个新的作业：</p><ul><li>add_job来添加作业</li><li>装饰器模式添加作业</li></ul><h3 id="只执行一次"><a href="#只执行一次" class="headerlink" title="只执行一次#"></a>只执行一次<a href="https://www.cnblogs.com/Neeo/p/10435059.html#只执行一次">#</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">from</span> apscheduler.schedulers.blocking <span class="hljs-keyword">import</span> BlockingScheduler<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">job2</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;job2&#x27;</span>, datetime.datetime.now(), text)<br>scheduler = BlockingScheduler()<br>scheduler.add_job(job2, <span class="hljs-string">&#x27;date&#x27;</span>, run_date=datetime.datetime(<span class="hljs-number">2019</span>, <span class="hljs-number">2</span>, <span class="hljs-number">25</span>, <span class="hljs-number">19</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>), args=[<span class="hljs-string">&#x27;text&#x27;</span>], <span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;job2&#x27;</span>)<br>scheduler.start()<br></code></pre></td></tr></table></figure><p>上例中，只在2019-2-25 19:05:06执行一次，args传递一个text参数。</p><h3 id="间隔执行"><a href="#间隔执行" class="headerlink" title="间隔执行#"></a>间隔执行<a href="https://www.cnblogs.com/Neeo/p/10435059.html#间隔执行">#</a></h3><p>下面来个简单的例子，作业每个5秒执行一次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">from</span> apscheduler.schedulers.blocking <span class="hljs-keyword">import</span> BlockingScheduler<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">job1</span>():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;job1&#x27;</span>, datetime.datetime.now())<br>scheduler = BlockingScheduler()<br>scheduler.add_job(job1, <span class="hljs-string">&#x27;interval&#x27;</span>, seconds=<span class="hljs-number">5</span>, <span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;job1&#x27;</span>)  <span class="hljs-comment"># 每隔5秒执行一次</span><br>scheduler.start()<br></code></pre></td></tr></table></figure><p>每天凌晨1点30分50秒执行一次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> apscheduler.schedulers.blocking <span class="hljs-keyword">import</span> BlockingScheduler  <span class="hljs-comment"># 后台运行</span><br>sc = BlockingScheduler()<br>f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;t1.text&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf8&#x27;</span>)<br><span class="hljs-meta">@sc.scheduled_job(<span class="hljs-params"><span class="hljs-string">&#x27;cron&#x27;</span>, day_of_week=<span class="hljs-string">&#x27;*&#x27;</span>, hour=<span class="hljs-number">1</span>, minute=<span class="hljs-string">&#x27;30&#x27;</span>, second=<span class="hljs-string">&#x27;50&#x27;</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">check_db</span>():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-number">111111111111</span>)<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-keyword">try</span>:<br>        sc.start()<br>        f.write(<span class="hljs-string">&#x27;定时任务成功执行&#x27;</span>)<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        sc.shutdown()<br>        f.write(<span class="hljs-string">&#x27;定时任务执行失败&#x27;</span>)<br>    <span class="hljs-keyword">finally</span>:<br>        f.close()<br></code></pre></td></tr></table></figure><p>每几分钟执行一次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">from</span> apscheduler.schedulers.blocking <span class="hljs-keyword">import</span> BlockingScheduler<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">job1</span>():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;job1&#x27;</span>, datetime.datetime.now())<br>scheduler = BlockingScheduler()<br><span class="hljs-comment"># 每隔2分钟执行一次， */1：每隔1分钟执行一次</span><br>scheduler.add_job(job1, <span class="hljs-string">&#x27;cron&#x27;</span>, minute=<span class="hljs-string">&quot;*/2&quot;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;job1&#x27;</span>) <br>scheduler.start()<br></code></pre></td></tr></table></figure><p>每小时执行一次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">from</span> apscheduler.schedulers.blocking <span class="hljs-keyword">import</span> BlockingScheduler<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">job1</span>():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;job1&#x27;</span>, datetime.datetime.now())<br>scheduler = BlockingScheduler()<br><span class="hljs-comment"># 每小时执行一次</span><br>scheduler.add_job(job1, <span class="hljs-string">&#x27;interval&#x27;</span>, hours=<span class="hljs-number">1</span>, <span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;job1&#x27;</span>)<br><span class="hljs-comment"># 每小时执行一次，上下浮动120秒区间内</span><br><span class="hljs-comment"># scheduler.add_job(job1, &#x27;interval&#x27;, hours=1, id=&#x27;job1&#x27;, jitter=120)</span><br>scheduler.start()<br></code></pre></td></tr></table></figure><h1 id="最后选择"><a href="#最后选择" class="headerlink" title="最后选择"></a>最后选择</h1><p>简单总结上面四种定时定点任务实现：</p><ol><li><p>循环+sleep方式适合简答测试，</p></li><li><p>timer可以实现定时任务，但是对定点任务来说，需要检查当前时间点；</p></li><li><p>schedule可以定点定时执行，但是需要在循环中检测任务，而且存在阻塞；</p></li><li><p>APScheduler框架更加强大，可以直接在里面添加定点与定时任务；</p></li></ol><p>综合考虑，老猫决定使用APScheduler框架，实现简单，只需要直接创建任务，并将添加到调度器中即可。</p><h1 id="附录-Crontab命令"><a href="#附录-Crontab命令" class="headerlink" title="附录-Crontab命令"></a>附录-Crontab命令</h1><p>在CentOS系统上，通过 <a href="https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html">crontab 命令</a>，安排那些需要被周期性执行的命令</p><p><strong>crontab命令的语法为：</strong></p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">crontab <span class="hljs-comment">[-e <span class="hljs-comment">[UserName]</span>|-l <span class="hljs-comment">[UserName]</span>|-r <span class="hljs-comment">[UserName]</span>|-v <span class="hljs-comment">[UserName]</span>|File ]</span> <br></code></pre></td></tr></table></figure><p>各个参数说明：</p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">-e [UserName]: 执行文字编辑器来设定时程表，内定的文字编辑器是 vi</span><br><span class="hljs-deletion">-r [UserName]: 删除目前的时程表</span><br><span class="hljs-deletion">-l [UserName]: 列出目前的时程表</span><br><span class="hljs-deletion">-v [UserName]:列出用户cron作业的状态</span><br></code></pre></td></tr></table></figure><p><strong>crontab 文件格式</strong></p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">&#123;<span class="hljs-built_in">minute</span>&#125; &#123;<span class="hljs-built_in">hour</span>&#125; &#123;<span class="hljs-built_in">day</span>-of-<span class="hljs-built_in">month</span>&#125; &#123;<span class="hljs-built_in">month</span>&#125; &#123;<span class="hljs-built_in">day</span>-of-<span class="hljs-built_in">week</span>&#125; &#123;full-path-to-shell-script&#125;<br></code></pre></td></tr></table></figure><ul><li>minute: 分钟 (0-59)</li><li>hour: 小时 (0-23)</li><li>day-of-month: 日期 (0-31)</li><li>month: 月份 (0-12 [12 代表 December])</li><li>Day-of-week: 一周当中的某天 (0-7 [7 或 0 代表星期天])</li><li>full-path-to-shell-script: 计划执行的脚本或命令的名称</li></ul><p>crontab命令中的一些常用特殊符号：</p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>*</td><td>表示任何时刻</td></tr><tr><td>,</td><td>表示分割</td></tr><tr><td>－</td><td>表示一个段，如第二段里： 1-5，就表示1到5点</td></tr><tr><td>/n</td><td>表示每个n的单位执行一次，如第二段里，*/1, 就表示每隔1个小时执行一次命令。也可以写成1-23/1.</td></tr></tbody></table></div><p><strong>Crontab 示例</strong></p><p>每隔 5 分钟运行一次 backupscript 脚本</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">*<span class="hljs-regexp">/5 * * * * /</span>root/backupscript.sh<br></code></pre></td></tr></table></figure><p>每天的凌晨 1 点运行 backupscript 脚本</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">0 </span><span class="hljs-number">1</span> * * * /root/backupscript.sh<br></code></pre></td></tr></table></figure><p>每月的第一个凌晨 3:15 运行 backupscript 脚本</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">15 </span><span class="hljs-number">3</span> <span class="hljs-number">1</span> * * /root/backupscript.sh<br></code></pre></td></tr></table></figure><p>每星期六的晚上11 : 00 pm重启smb</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">0 </span><span class="hljs-number">23</span> * * <span class="hljs-number">6</span> /etc/init.d/smb restart<br></code></pre></td></tr></table></figure><p><strong>特别注意：python的执行命令 必须用绝对路径！！</strong></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://blog.51cto.com/huangyg/2367088">https://blog.51cto.com/huangyg/2367088</a></p><p><a href="https://segmentfault.com/a/1190000011084828">Python任务调度模块APScheduler</a></p><p><a href="https://www.cnblogs.com/xiaoluo501395377/archive/2013/04/06/3002602.html">Linux学习之CentOS(十二)—crontab命令的使用方法</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
      <tag>Python</tag>
      
      <tag>定时任务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过Python来操作kylin数据</title>
    <link href="/2020/04/01/2020-04-01-%E9%80%9A%E8%BF%87Python%E6%9D%A5%E6%93%8D%E4%BD%9Ckylin%E6%95%B0%E6%8D%AE/"/>
    <url>/2020/04/01/2020-04-01-%E9%80%9A%E8%BF%87Python%E6%9D%A5%E6%93%8D%E4%BD%9Ckylin%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="通过Python来操作kylin数据"><a href="#通过Python来操作kylin数据" class="headerlink" title="通过Python来操作kylin数据"></a>通过Python来操作kylin数据</h1><ol><li>安装依赖的包（py2/py3都支持)</li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> kylinpy<br>pip <span class="hljs-keyword">install</span> sqlalchemy<br>pip <span class="hljs-keyword">install</span> --upgrade kylinpy<br></code></pre></td></tr></table></figure><p> kylinpy工具库包含两个可使用原件. 想要了解更多关于此工具库信息请点击<a href="https://github.com/Kyligence/kylinpy">Github仓库</a>.</p><ul><li>Apache Kylin 命令行工具</li><li>Apache Kylin SQLAchemy方言</li></ul><span id="more"></span><ol><li>示例代码</li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># coding=utf-8</span><br><br>import sqlalchemy as sa<br>import kylinpy<br>import pymysql<br><br><br><span class="hljs-comment"># SQLAlchemy 实例</span><br>def kylin_query1(conn_str, query_sql):<br>    kylin_engine = sa.create_engine(conn_str)<br>    <span class="hljs-built_in">print</span>(kylin_engine.table_names())<br>    results = kylin_engine.execute(query_sql)<br>    <span class="hljs-built_in">print</span>([e <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> results])<br>   <br><span class="hljs-comment"># kylinpy 实例</span><br>def kylin_query1(query_sql):<br>    kylin = kylinpy.KylinCluster(<span class="hljs-attribute">host</span>=ip, <span class="hljs-attribute">username</span>=<span class="hljs-string">&quot;ADMIN&quot;</span>, <span class="hljs-attribute">password</span>=<span class="hljs-string">&quot;&quot;</span>, <span class="hljs-attribute">project</span>=<span class="hljs-string">&quot;&quot;</span>)<br>    # <span class="hljs-built_in">print</span>(kylin.projects())<br>    results = kylin.query(query_sql)<br>    <span class="hljs-built_in">print</span>(results[<span class="hljs-string">&quot;results&quot;</span>])<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    conn_str = <span class="hljs-string">&quot;kylin://&lt;username&gt;:&lt;password&gt;@&lt;ip&gt;:&lt;port&gt;/&lt;project&gt;?version=&lt;v1|v2&gt;&amp;prefix=&lt;/kylin/api&gt;&quot;</span><br>    query_sql = <span class="hljs-string">&quot;select  userid, datetime, count(*) c from soda_report group by userid, datetime&quot;</span><br>    <br>    kylin_query1(conn_str, query_sql)    <br>    kylin_query2(query_sql)<br></code></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.cnblogs.com/654wangzai321/p/9869939.html">通过Python来操作kylin</a></p><p><a href="http://kylin.apache.org/cn/docs21/tutorial/kylin_client_tool.html">Kylin Python 客户端工具库 - kylinpy</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于模糊音的中文匹配Dimsim</title>
    <link href="/2020/03/31/2020-03-31-Dimsim%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%B3%8A%E9%9F%B3%E7%9A%84%E4%B8%AD%E6%96%87%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/"/>
    <url>/2020/03/31/2020-03-31-Dimsim%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%B3%8A%E9%9F%B3%E7%9A%84%E4%B8%AD%E6%96%87%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="dimsim介绍"><a href="#dimsim介绍" class="headerlink" title="dimsim介绍"></a>dimsim介绍</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/63119038">论文解读</a><br><a href="https://github.com/System-T/DimSim"><strong>dimsim</strong>的python实现</a> </p></blockquote><p>中文的语音相似性Phonetic similarity算法，可以用于语音纠错spelling correction，比如将<strong>稀饭</strong>修改为<strong>喜欢</strong>。</p><p><img src="https://pic1.zhimg.com/80/v2-dfe43afccc8d2fcfce9d1ba9cf69d60c_720w.jpg#height=140&amp;id=T8hIo&amp;originHeight=235&amp;originWidth=652&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=shadow&amp;title=&amp;width=388" alt=""></p><p>dimsim三方库介绍：给定两个相同长度的中文单词，模型确定两个单词之间的距离，并返回几个与给定单词接近的候选单词。它包括 2 个API接口：</p><ul><li>get_distance：接收两个短语字符串，返回两个词语的发音相似度数值，数值越小表示越相似</li><li>get_candidates：接收一个短语字符串，返回相似短语</li></ul><p>代码示例如下：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> dimsim<br><br><span class="hljs-meta"># 计算词语间的发音相似度</span><br><span class="hljs-title">score_py</span> = dimsim.get_distance(<span class="hljs-string">&quot;星辰&quot;</span>, <span class="hljs-string">&quot;姓陈&quot;</span>)<br></code></pre></td></tr></table></figure></p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在语音识别领域，由于我国方言众多，所以需要精准地匹配每个地方的方言目前还不太现实。市面上的语音识别服务基本上对普通话的识别率是最高的，但是也需要用户用很标准的普通话来讲。这就涉及到模糊音匹配的问题了。</p><h2 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h2><p>用户说：“大娘水饺好吃吗”，被语音识别成了“大亮睡觉好吃嘛”（举个栗子而已），这个时候怎么将语音识别后的结果转换成我想要的结果。</p><h2 id="单词纠错"><a href="#单词纠错" class="headerlink" title="单词纠错"></a>单词纠错</h2><p>一般来说中文单词的拼写错误大致有这么几种类型：</p><ol><li>字形相似<br>这种在手写字时特别容易出现，比如 “彬彬有礼“ 写成 ”杉杉有礼”。</li><li>读音相似<br>手写字或者拼音输入法时容易出现，比如 “南通市” 写成 “难通市”，前后的读音都是 “nan tong shi”。</li></ol><p>因为汉语表达比较精炼，使用编辑距离去做纠错，效果就很不好，比如 “南通市 – 难通市 – 北通市”，这三者的编辑距离都是 1，就不好判断了。但这时结合拼音去判断，就会发现 “南通市” 与 “难通市” 的相似度高于 “南通市” 与 “北通市” 了。</p><p>所以对于汉字的纠错，我们需要同时结合拼音和字形上的特点：</p><ol><li>读音相似度。示例：<code>胡</code> -&gt; <code>hú</code>，<code>福</code> -&gt; <code>fú</code><br>读音上相对来说好处理一点，我们可以将汉字转成对于的拼音，比如说 “南” 字转成 “nan2”，其中 2 为声调部分，然后对拼音构成的字符串再用传统的相似度匹配算法，比如编辑距离，就可以达到很好的效果。<br>也可以使用 <strong>Soundex 语音算法</strong>，在拼音文字中有时会有会念但不能拼出正确字的情形，可用Soundex做类似模糊匹配的效果。 <blockquote><p>不同这里有一点需要注意，由于不同地区有着各自截然不同的方言，所以在计算相似度时需要适当调整。</p><p>比如许多南方人很难分辨 “L” 和 “N’，他们常常会将这两个音弄混，将“篮球”读作“南球”，而“刘德华”就变成了“牛德华”。</p><p>另外有汉字是多音字，在汉字转拼音时不好处理，因为要考虑所有可能的拼音组合，在极端情况下会导致指数爆炸！例如美团的实现(枚举多音字全排列)。所以我们一般就取该汉字拼音出现次数较多的那个读音</p></blockquote></li></ol><ol><li>字形相似度。示例：<code>门</code> -&gt; <code>37001</code>，<code>闩</code> -&gt; <code>37101</code><br>字形相似读计算是个特别难的问题，一种朴素的思想，便是首先将汉字转化成一组的字母数字的序列，而这个转化所用到的hash算法必须能够将该汉字的字形特征保留下来。利用这样的转化，我们便将汉字字形的相似度问题，变成了两组字母数字序列的相似度问题。而这正是传统相似度匹配算法的强项。<br>这种解决方案的核心，就在于找到一个恰当的hash算法，能够将汉字进行适当的转化，并在转化结果中，保留住汉字的字形特征。<br>曾经有人发明了一种名为 <strong>四角编码</strong> 的汉字检字法来实现这样的算法。四角算法是由王云五于1925年发明，这种编码方式根据汉字所含的单笔或复笔对汉字进行编号，取汉字的左上角，右上角，左下角以及右下角四个角的笔形，将汉字转化成最多五位的阿拉伯数字。 通过将汉字转化成四角编码，再对四角编码的相似度进行计算，便可以得出两个汉字在字形上的相似程度。 <blockquote><p>四角编码：<a href="http://baike.baidu.com/view/67253.htm?fr=aladdin">http://baike.baidu.com/view/67253.htm?fr=aladdin</a></p></blockquote></li></ol><p>这种编码可以在一定程度上解决形近词的问题，但也有其自身的问题，由于只取汉字的四角笔形，有些外形截然不同的汉字，因为四角结构相同，也拥有同样的四角编码。<br>比如：量 - 6010 ，日 - 6010 ，但它们是不一样的。 </p><p>所以为了解决汉字相似度计算问题，我们可以结合汉字的拼音、声调、四角编码、笔画数、字形结构等来考虑。</p><h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><p>我们的做法：将ASR的2元结果跟姓氏库进行一一模糊匹配，检查用户的拼写是否有错误，如果有的话，给出正确的姓氏，完成姓氏读音纠错。</p><ul><li>中文汉字大致有几个特征：<br>1）声母；2）韵母；3）声调；4）偏旁；5）结构；6）笔画<br>这几个特征中，在语音识别后的处理过程中，1、2、3的权重要高一些，其中，1、2最为重要，也就是语音识别为什么出错的原因，要做的事情就是通过模糊音将声母、韵母来进行匹配。 </li><li>基于语言模型做文本自动纠错 </li><li>查词典 </li></ul><h3 id="词典"><a href="#词典" class="headerlink" title="词典"></a>词典</h3><p>检查一个单词是否拼写错误，我们可以通过查词典找同音词，在词典中查找相似度阈值大于某个特定值的关键词即为候选正确词。</p><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><p>如果得到多个正确词，但是通常显示给用户只有一个，按照下面规则对它们进行排序，越前面的规则权重越高。</p><ol><li>同音词优先，同音词之间的顺序由同音词的搜索建议权重决定。搜索建议权重由搜索词的商品数目、搜索次数、添加方式决定。</li><li>拼写相近词的顺序由相似度决定，越相似的词排在越前面。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://theautomatic.net/2019/11/13/guide-to-fuzzy-matching-with-python/">PYTHON模糊匹配指南</a><br><a href="https://www.jianshu.com/p/21d13b5778a7">浅谈基于模糊音的中文匹配算法</a><br><a href="https://jingpeicomp.gitbooks.io/search-platform/algorithm/keyword_corrector.html">搜索关键词纠错算法</a><br>ALi, Min and Danilevsky, Marina and Noeman, Sara and Li, Yunyao_, <a href="http://aclweb.org/anthology/K18-1043">“DIMSIM: An Accurate Chinese Phonetic Similarity Algorithm Based on Learned High Dimensional Encoding”</a>, Proceedings of the 22nd Conference on Computational Natural Language Learning, 2018.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Dimsim</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FlashText高效关键词查找与替换</title>
    <link href="/2020/03/31/2020-03-31-FlashText%E5%85%B3%E9%94%AE%E8%AF%8D%E5%BF%AB%E9%80%9F%E5%8C%B9%E9%85%8D/"/>
    <url>/2020/03/31/2020-03-31-FlashText%E5%85%B3%E9%94%AE%E8%AF%8D%E5%BF%AB%E9%80%9F%E5%8C%B9%E9%85%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="FlashText-介绍"><a href="#FlashText-介绍" class="headerlink" title="FlashText 介绍"></a>FlashText 介绍</h1><p>通常，我们使用Python 在文本中进行关键词查找或替换时，会使用 <strong>re </strong>模块以正则的形式实现。在文本数量、文本内容、关键词数量较小时，该方法能够满足我们程序的功能、性能需要。但当在大规模的文本或者对大量关键词语料查找或者替换，re 实现方案的性能将成为瓶颈，本文我们将介绍一种新的关键词搜索和替换的算法：<a href="https://github.com/vi3k6i5/flashtext">Flashtext</a> <strong>算法</strong>，它是一个高效的字符搜索和替换算法。</p><p>先来看个例子。正则表达式在一个 10k 的词库中查找 15k 个关键词的时间差不多是 0.165 秒。但是对于 Flashtext 算法而言只需要 0.002 秒。因此，在这个问题上 Flashtext 的速度大约比正则表达式快 82 倍。随着我们需要处理的字符越来越多，正则表达式的处理速度几乎都是线性增加的。然而，Flashtext 的复杂度几乎保持在一个常量。</p><img src="/2020/03/31/2020-03-31-FlashText%E5%85%B3%E9%94%AE%E8%AF%8D%E5%BF%AB%E9%80%9F%E5%8C%B9%E9%85%8D/1644316531872-0e11a676-c56d-4dd4-bd73-f46fe93716bd.png" class="" title="image.png"><p>为何存在这么大的差异呢？Flashtext 算法的时间复杂度不依赖于查找或替换的字符的数量。如，对于一个文档有 N 个字符，和一个有 M 个词的关键词库，那么时间复杂度就是 <strong>O(N)</strong> 。而正则匹配的时间复杂度是 <strong>O(M * N) </strong>。这也是两者在性能上的差异随着关键词数量增多而拉大的原因。</p><p>因此，在一些大数据下的内容检索和替换，我们更倾向于选择 <strong>Flashtext </strong>算法 ，比如，自然语言处理领域中数据清洗是一项必须的操作。经常涉及使用标准的关键词替换一些非标准的词，如，将Javascript替换成JavaScript。或者我们需要判断文本中是否存在JavaScript 关键词等等。</p><p>接下来，就让我们了解一下，如何使用<strong>Flashtext 实现关键词的查找和替换。</strong></p><h1 id="FlashText-算法分析"><a href="#FlashText-算法分析" class="headerlink" title="FlashText 算法分析"></a>FlashText 算法分析</h1><p><a href="https://github.com/vi3k6i5/flashtext">Flashtext</a> 是一种基于 Trie 结构和 Aho-Corasick 的算法，可用于大规模替换、检索文档中的关键字。</p><img src="/2020/03/31/2020-03-31-FlashText%E5%85%B3%E9%94%AE%E8%AF%8D%E5%BF%AB%E9%80%9F%E5%8C%B9%E9%85%8D/1644546622364-3f3e704a-dc14-4322-b578-6683e10a4a82.png" class="" title="image.png"><p>Flashtext 算法主要分为三部分，我们接下来将对每一部分进行单独分析：</p><ul><li>构建 Trie 字典</li><li>关键词搜索</li><li>关键词替换</li></ul><h2 id="构建-Trie-字典"><a href="#构建-Trie-字典" class="headerlink" title="构建 Trie 字典"></a>构建 Trie 字典</h2><p>Flashtext 是一种基于 Trie 字典数据结构和 Aho Corasick 的算法。它的工作方式是，首先它将所有相关的关键词作为输入，使用这些关键词建立一个 trie 字典，利用字符串的公共前缀最大限度地减少不必要的字符串比较，提高查询效率.</p><p>为了构建 trie 字典，Flashtext 创建一个空的节点指向空字典。这个节点被用作所有关键词的起点。我们在字典中插入一个关键词。这个关键词中的下一个字符在本字典中作为关键词，并且这个指针需要再次指向一个空字典。这个过程不断重复，直到我们达到单词中的最后一个字符。当我们到达单词的末尾时，我们插入一个特殊的字符(eot)来表示词尾，如下：</p><img src="/2020/03/31/2020-03-31-FlashText%E5%85%B3%E9%94%AE%E8%AF%8D%E5%BF%AB%E9%80%9F%E5%8C%B9%E9%85%8D/1644547827054-9fc7754c-ccad-40f6-8704-481d952ac044.png" class="" title="image.png"><p>start 和 eot 是两个特殊的字符，用来定义关键词的边界，因此，也可知 Flashtext 只匹配完整的单词，这个 trie 字典就是我们后面要用来搜索和替换的数据结构。</p><h2 id="关键词搜索"><a href="#关键词搜索" class="headerlink" title="关键词搜索"></a>关键词搜索</h2><p>对输入查询中的字符进行逐个遍历，基于Aho Corasick算法在构建好的前缀树上实现多模串匹配（其中Trie树负责状态转移，KMP负责减少重复匹配）。当匹配到EOT特殊字符时，意味着匹配成功，我们将匹配到的字符序列所对应的标准关键词进行输出。</p><h2 id="关键词替换"><a href="#关键词替换" class="headerlink" title="关键词替换"></a>关键词替换</h2><p>Flashtext 对输入文本中的字符进行逐个遍历，Flashtext 先创建一个空的字符串，当字符序列中的 word 无法在 Trie 字典中找到匹配时，那么Flashtext 就简单的原始字符复制到返回字符串中。但当Flashtext 可以从 Trie 字典中找到匹配时，那么Flashtext 将把匹配到的字符的标准字符复制到返回字符串中。因此，返回字符串是输入字符串的一个副本，唯一的不同是替换了匹配到的字符序列。</p><h1 id="FlashText-vs-Aho-Corasick"><a href="#FlashText-vs-Aho-Corasick" class="headerlink" title="FlashText vs. Aho Corasick"></a>FlashText vs. Aho Corasick</h1><p>FlashText算法和 Aho Corasick 算法的不同之处在于，它不匹配子字符串。Flashtext 算法被设计为只匹配完整的单词。比如，我们输入一个单词 {Apple}，那么这个算法就不会去匹配 “I likePineapple” 中的 apple。这个算法也被设计为首先匹配最长字符串。在举个例子，比如我们有这样一个数据集 {Machine， Learning，Machine Learning}，一个文档 “I like Machine Learning”，那么我们的算法只会去匹配 “MachineLearning” ，因为这是最长匹配。</p><h1 id="什么时候应该使用-FlashText？"><a href="#什么时候应该使用-FlashText？" class="headerlink" title="什么时候应该使用 FlashText？"></a>什么时候应该使用 FlashText？</h1><p>简单的答案：当关键词数 &gt; 500 时，对于搜索，FlashText 在大约超过 500 个关键词后性能优于正则表达式。</p><p>复杂的答案：正则表达式可以基于特殊字符搜索关键词，如 <code>^,$,*,\d,.</code>，FlashText 不支持这个。</p><p>因此，如果您想像 <code>word\dvec</code> 这样匹配部分词，最好不要用 FlashText。但像 <code>word2vec</code> 这样提取完整的单词，就非常适合了。</p><h1 id="FlashText库使用方法"><a href="#FlashText库使用方法" class="headerlink" title="FlashText库使用方法"></a>FlashText库使用方法</h1><h3 id="1、安装"><a href="#1、安装" class="headerlink" title="1、安装"></a>1、安装</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> flashtext<br></code></pre></td></tr></table></figure><h3 id="2、导入词库"><a href="#2、导入词库" class="headerlink" title="2、导入词库"></a>2、导入词库</h3><p>首先传入一个关键词列表。此列表将在内部用于构建 Trie 字典<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> flashtext.keyword <span class="hljs-keyword">import</span> KeywordProcessor<br><br>kp = KeywordProcessor()<br>keyword_dict=&#123;<br>            <span class="hljs-string">&quot;fruit&quot;</span>: [<span class="hljs-string">&quot;apple&quot;</span>, <span class="hljs-string">&quot;banana&quot;</span>,<span class="hljs-string">&quot;orange&quot;</span>,<span class="hljs-string">&quot;watermelon&quot;</span>], <br>            <span class="hljs-string">&quot;ball&quot;</span>: [<span class="hljs-string">&quot;tennis&quot;</span>, <span class="hljs-string">&quot;basketball&quot;</span>,<span class="hljs-string">&quot;football&quot;</span>]<br>             &#125;<br>keyword_processor.add_keywords_from_dict(keyword_dict)         <span class="hljs-comment"># 可以添加dict</span><br>keyword_processor.add_keywords_from_list([<span class="hljs-string">&quot;fruit&quot;</span>, <span class="hljs-string">&quot;banana&quot;</span>])  <span class="hljs-comment"># 可以添加list</span><br></code></pre></td></tr></table></figure></p><h3 id="3、关键词提取"><a href="#3、关键词提取" class="headerlink" title="3、关键词提取"></a>3、关键词提取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> flashtext.keyword <span class="hljs-keyword">import</span> KeywordProcessor<br><br>keyword_processor = KeywordProcessor(case_sensitive=<span class="hljs-literal">False</span>)<br>keyword_processor.add_keyword(<span class="hljs-string">&#x27;Big Apple&#x27;</span>, <span class="hljs-string">&#x27;New York&#x27;</span>)<br>keyword_processor.add_keyword(<span class="hljs-string">&#x27;Bay Area&#x27;</span>)<br>keywords_found = keyword_processor.extract_keywords(<span class="hljs-string">&#x27;I love Big Apple and Bay Area.&#x27;</span>,span_info=<span class="hljs-literal">True</span>)<br>keywords_found<br><span class="hljs-comment"># [&#x27;New York&#x27;, &#x27;Bay Area&#x27;]</span><br></code></pre></td></tr></table></figure><p>其中：</p><ul><li>case_sensitive，是否对大小写敏感</li><li>span_info，是否要返回位置信息</li></ul><h3 id="4、关键词替换"><a href="#4、关键词替换" class="headerlink" title="4、关键词替换"></a>4、关键词替换</h3><p>您也可以替换句子中的关键词，而不是提取它。我们用这作为数据处理流程中的数据清理步骤。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> flashtext.keyword <span class="hljs-keyword">import</span> KeywordProcessor<br><br>keyword_processor = KeywordProcessor()<br>keyword_processor.add_keyword(<span class="hljs-string">&#x27;Big Apple&#x27;</span>, <span class="hljs-string">&#x27;New York&#x27;</span>)<br>keyword_processor.add_keyword(<span class="hljs-string">&#x27;New Delhi&#x27;</span>, <span class="hljs-string">&#x27;NCR region&#x27;</span>)<br>new_sentence = keyword_processor.replace_keywords(<span class="hljs-string">&#x27;I love Big Apple and new delhi.&#x27;</span>)<br>new_sentence<br><span class="hljs-comment"># &#x27;I love New York and NCR region.&#x27;</span><br></code></pre></td></tr></table></figure></p><h3 id="5、删除关键词"><a href="#5、删除关键词" class="headerlink" title="5、删除关键词"></a>5、删除关键词</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">keyword_processor.remove_keyword(<span class="hljs-string">&#x27;banana&#x27;</span>)<br>keyword_processor.remove_keywords_from_dict(&#123;<span class="hljs-string">&quot;food&quot;</span>: [<span class="hljs-string">&quot;bread&quot;</span>]&#125;)<br>keyword_processor.remove_keywords_from_list([<span class="hljs-string">&quot;basketball&quot;</span>])<br><span class="hljs-number">123456</span><br></code></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://juejin.im/post/5b6d426f6fb9a04fd1604341">[译] 正则表达式要跑 5 天，所以我做了个工具，只跑 15 分钟。</a> </li><li><a href="https://link.juejin.im/?target=https%3A%2F%2Fmedium.freecodecamp.org%2Fregex-was-taking-5-days-flashtext-does-it-in-15-minutes-55f04411025f">Regex was taking 5 days to run. So I built a tool that did it in 15 minutes.</a> </li><li><a href="https://www.analyticsvidhya.com/blog/2017/11/flashtext-a-library-faster-than-regular-expressions/">FlashText – A library faster than Regular Expressions for NLP tasks</a> </li><li><a href="https://blog.csdn.net/CoderPai/article/details/78574863">Flashtext：大规模数据清洗的利器</a> </li><li><a href="https://blog.csdn.net/Together_CZ/article/details/82693099">超大规模文本数据清洗、查找、匹配神器之python模块flashtext学习使用</a> </li><li><a href="https://cold-eye.github.io/post/python-ac-flashtext/">python 关键词快速匹配</a></li><li><a href="https://blog.csdn.net/weixin_39628256/article/details/111326131">大量的数据做字符串匹配_Python Flashtext 实现大数据集下高效的关键词查找和替换…</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>FlashText</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Trie 树</title>
    <link href="/2020/03/31/2020-03-31-Trie%20%E6%A0%91%E5%AE%9E%E7%8E%B0%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E8%87%AA%E5%8A%A8%E8%81%94%E6%83%B3/"/>
    <url>/2020/03/31/2020-03-31-Trie%20%E6%A0%91%E5%AE%9E%E7%8E%B0%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E8%87%AA%E5%8A%A8%E8%81%94%E6%83%B3/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是-Trie-树"><a href="#什么是-Trie-树" class="headerlink" title="什么是 Trie 树"></a>什么是 Trie 树</h2><p>Trie 树，又称前缀树，字段典树，或单词查找树，是一种树形结构，也是哈希表的变种，它是一种专门处理字段串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题，主要被搜索引擎用来做文本词频的统计。</p><p>先看一下前辍树的图：</p><p><img src="https://pic1.zhimg.com/80/v2-e624deed1f679f6fc6d93aee51132150_1440w.jpg#alt=img" alt=""></p><p>这棵前辍树根节点不存放数据，其他节点保存了 hello,her,hi,how,see,so 等关键词信息，如果查 he 前辍的单词可以很快返回 hello,her。</p><h2 id="Trie-树的-Python-实现"><a href="#Trie-树的-Python-实现" class="headerlink" title="Trie 树的 Python 实现"></a>Trie 树的 Python 实现</h2><p>一般来讲 trie 要实现这么几个方法</p><ul><li>插入一个单词 insert (word: str) -&gt; None</li><li>查找一个单词是否在 trie 中 search (word:str) -&gt; bool</li><li>查找一个前缀是否在 trie 中 startsWith (prefix:str) -&gt; bool</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trie</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Initialize your data structure here.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.root = &#123;&#125;<br>        self.end_of_word = <span class="hljs-string">&#x27;#&#x27;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">insert</span>(<span class="hljs-params">self, word: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Inserts a word into the trie.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        node = self.root<br>        <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> word:<br>            node = node.setdefault(char, &#123;&#125;)<br>        node[self.end_of_word] = self.end_of_word<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">search</span>(<span class="hljs-params">self, word: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Returns if the word is in the trie.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        node = self.root<br>        <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> word:<br>            <span class="hljs-keyword">if</span> char <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> node:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            node = node[char]<br>        <span class="hljs-keyword">return</span> self.end_of_word <span class="hljs-keyword">in</span> node<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">startsWith</span>(<span class="hljs-params">self, prefix: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Returns if there is any word in the trie that starts with the given prefix.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        node = self.root<br>        <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> prefix:<br>            <span class="hljs-keyword">if</span> char <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> node:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            node = node[char]<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>      <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_start</span>(<span class="hljs-params">self,prefix</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        给出一个前辍，打印出所有匹配的字符串</span><br><span class="hljs-string">        :param prefix:</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_key</span>(<span class="hljs-params">pre,pre_node</span>):<br>            result = []<br>            <span class="hljs-keyword">if</span> pre_node.get(self.end_of_word):<br>                result.append(pre)<br>            <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> pre_node.keys():<br>                <span class="hljs-keyword">if</span> key != self.end_of_word:<br>                    result.extend(get_key(pre+key,pre_node.get(key)))<br>            <span class="hljs-keyword">return</span> result<br><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.startsWith(prefix):<br>            <span class="hljs-keyword">return</span> []<br>        <span class="hljs-keyword">else</span>:<br>            node = self.root<br>            <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> prefix:<br>                node = node.get(p)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">return</span> get_key(prefix,node)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    trie = Trie()<br>    trie.insert(<span class="hljs-string">&quot;Python&quot;</span>)<br>    trie.insert(<span class="hljs-string">&quot;Python 算法&quot;</span>)<br>    trie.insert(<span class="hljs-string">&quot;Python web&quot;</span>)<br>    trie.insert(<span class="hljs-string">&quot;Python web 开发&quot;</span>)<br>    trie.insert(<span class="hljs-string">&quot;Python web 开发 视频教程&quot;</span>)<br>    trie.insert(<span class="hljs-string">&quot;Python 算法 源码&quot;</span>)<br>    trie.insert(<span class="hljs-string">&quot;Perl 算法 源码&quot;</span>)<br>    <span class="hljs-built_in">print</span>(trie.search(<span class="hljs-string">&quot;Perl&quot;</span>))<br>    <span class="hljs-built_in">print</span>(trie.search(<span class="hljs-string">&quot;Perl 算法 源码&quot;</span>))<br>    <span class="hljs-built_in">print</span>((trie.get_start(<span class="hljs-string">&#x27;P&#x27;</span>)))<br>    <span class="hljs-built_in">print</span>((trie.get_start(<span class="hljs-string">&#x27;Python web&#x27;</span>)))<br>    <span class="hljs-built_in">print</span>((trie.get_start(<span class="hljs-string">&#x27;Python 算&#x27;</span>)))<br>    <span class="hljs-built_in">print</span>((trie.get_start(<span class="hljs-string">&#x27;P&#x27;</span>)))<br>    <span class="hljs-built_in">print</span>((trie.get_start(<span class="hljs-string">&#x27;Python web&#x27;</span>)))<br>    <span class="hljs-built_in">print</span>((trie.get_start(<span class="hljs-string">&#x27;Python 算&#x27;</span>)))<br></code></pre></td></tr></table></figure><p>代码运行结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-literal">False</span><br><span class="hljs-literal">True</span><br>[<span class="hljs-string">&#x27;Python&#x27;</span>, <span class="hljs-string">&#x27;Python 算法&#x27;</span>, <span class="hljs-string">&#x27;Python 算法 源码&#x27;</span>, <span class="hljs-string">&#x27;Python web&#x27;</span>, <span class="hljs-string">&#x27;Python web 开发&#x27;</span>, <span class="hljs-string">&#x27;Python web 开发 视频教程&#x27;</span>, <span class="hljs-string">&#x27;Perl 算法 源码&#x27;</span>]<br>[<span class="hljs-string">&#x27;Python web&#x27;</span>, <span class="hljs-string">&#x27;Python web 开发&#x27;</span>, <span class="hljs-string">&#x27;Python web 开发 视频教程&#x27;</span>]<br>[<span class="hljs-string">&#x27;Python 算法&#x27;</span>, <span class="hljs-string">&#x27;Python 算法 源码&#x27;</span>]<br></code></pre></td></tr></table></figure><h2 id="Trie-的时间复杂度"><a href="#Trie-的时间复杂度" class="headerlink" title="Trie 的时间复杂度"></a>Trie 的时间复杂度</h2><p>如果要在一组关键词中，频繁地查询某些关键词，用 Trie 树会非常高效。构建 Trie 树的过程，需要扫描所有的关键词，时间复杂度是 O(n)（n 表示所有关键词的长度和）。但是一旦构建成功之后，后续的查询操作会非常高效。</p><p>每次查询时，如果要查询的关键词长度是 k，那我们只需要最多比对 k 个节点，就能完成查询操作。跟原本那组关键词的长度和个数没有任何关系。所以说，构建好 Trie 树后，在其中查找关键词的时间复杂度是 O(k)，k 表示要查找的关键词的长度。</p><h2 id="Trie三方实现库"><a href="#Trie三方实现库" class="headerlink" title="Trie三方实现库"></a>Trie三方实现库</h2><p>自己造轮子还要思考，编码，验证，但这是学习提升的最佳方式。如果急于应用没有时间造轮子，至少要学会如何使用轮子，下面的前辍树的轮子是一个日本人写的，大家可以学习应用下。</p><p><a href="https://link.zhihu.com/?target=https%3A//github.com/pytries/marisa-trie">https://github.com/pytries/marisa-trie</a></p><p><a href="https://link.zhihu.com/?target=https%3A//marisa-trie.readthedocs.io/en/latest/tutorial.html">https://marisa-trie.readthedocs.io/en/latest/tutorial.html</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/64492411">如何实现搜索框的关键词提示功能</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Trie 树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>字符串模糊匹配指南</title>
    <link href="/2020/03/31/2020-03-31-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E7%B3%8A%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95/"/>
    <url>/2020/03/31/2020-03-31-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E7%B3%8A%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="字符串模糊匹配指南"><a href="#字符串模糊匹配指南" class="headerlink" title="字符串模糊匹配指南"></a>字符串模糊匹配指南</h1><ul><li>textdistance: 30+传统的字面距离计算 √</li><li>difflib: Python自带的计算文本差异的辅助工具 √</li><li>fuzzywuzzy: 依据编辑算法计算两个序列之间的差异</li><li>strsimpy: 计算各种字符串距离的包</li><li>Fast Fuzzy Matching: 快</li></ul><h2 id="textdistance库-❤❤❤"><a href="#textdistance库-❤❤❤" class="headerlink" title="textdistance库 ❤❤❤"></a>textdistance库 ❤❤❤</h2><p><a href="https://github.com/life4/textdistance">textdistance</a> 库使用传统的字面匹配算法来综合评估两段文本的匹配程度！它使用30多种不同的算法计算序列的距离，提供了可用于模糊匹配算法的集合.</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># pip install textdistance</span><br><span class="hljs-keyword">import</span> textdistance<br></code></pre></td></tr></table></figure><p>All algorithms have some common methods:</p><ol><li><code>.distance(*sequences)</code> — calculate distance between sequences.</li><li><code>.similarity(*sequences)</code> — calculate similarity for sequences.</li><li><code>.maximum(*sequences)</code> — maximum possible value for distance and similarity. For any sequence: <code>distance + similarity == maximum</code>.</li><li><code>.normalized_distance(*sequences)</code> — normalized distance between sequences. The return value is a float between 0 and 1, where 0 means equal, and 1 totally different.</li><li><code>.normalized_similarity(*sequences)</code> — normalized similarity for sequences. The return value is a float between 0 and 1, where 0 means totally different, and 1 equal.</li></ol><h3 id="Levenshtein距离"><a href="#Levenshtein距离" class="headerlink" title="Levenshtein距离"></a>Levenshtein距离</h3><p>指两个<a href="https://baike.baidu.com/item/%E5%AD%97%E4%B8%B2">字串</a>之间，由一个转成另一个所需的最少编辑操作次数。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">textdistance</span>.levenshtein(<span class="hljs-string">&quot;this test&quot;</span>, <span class="hljs-string">&quot;that test&quot;</span>) # <span class="hljs-number">2</span><br><span class="hljs-attribute">textdistance</span>.levenshtein.normalized_similarity(<span class="hljs-string">&quot;this test&quot;</span>, <span class="hljs-string">&quot;that test&quot;</span>) # <span class="hljs-number">0</span>.<span class="hljs-number">8</span><br></code></pre></td></tr></table></figure><h3 id="Jaccard-相似度"><a href="#Jaccard-相似度" class="headerlink" title="Jaccard 相似度"></a>Jaccard 相似度</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-meta prompt_">&gt;&gt;</span> textdistance.jaccard（tokens_1，tokens_2）<br></code></pre></td></tr></table></figure><h3 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h3><p>余弦相似度是比较两个字符串的常用方法。该算法将字符串视为向量，并计算它们之间的余弦值。与上面的 Jaccard Similarity 相似，余弦相似度也忽略了要比较的字符串中的顺序。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">textdistance</span>.cosine(<span class="hljs-string">&quot;apple&quot;</span>, <span class="hljs-string">&quot;ppale&quot;</span>) # <span class="hljs-number">1</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><img src="/2020/03/31/2020-03-31-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E7%B3%8A%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95/textdistance-cosine.jpg" class=""><h2 id="difflib库-❤❤❤"><a href="#difflib库-❤❤❤" class="headerlink" title="difflib库 ❤❤❤"></a>difflib库 ❤❤❤</h2><blockquote><p><a href="https://docs.python.org/zh-cn/3.7/library/difflib.html">difflib</a> —- 计算差异的辅助工具</p></blockquote><p>此模块提供用于比较序列的类和函数，有以下内容：</p><p><strong>_class_ </strong><code>**difflib.SequenceMatcher**</code></p><p>这是一个灵活的类，可用于比较任何类型的序列对，只要序列元素为 <a href="https://docs.python.org/zh-cn/3.7/glossary.html#term-hashable">hashable</a> 对象。 <strong>其思路是找到不包含“垃圾”元素的最长连续匹配子序列；</strong>所谓“垃圾”元素是指其在某种意义上没有价值，例如空白行或空白符。然后同样的思路将递归地应用于匹配序列的左右序列片段。 这并不能产生最小编辑序列，但确实能产生在人们看来“正确”的匹配。</p><p><strong>自动垃圾启发式计算:</strong> <code>[SequenceMatcher](https://docs.python.org/zh-cn/3.7/library/difflib.html#difflib.SequenceMatcher)</code> 支持使用启发式计算来自动将特定序列项视为垃圾。 这种启发式计算会统计每个单独项在序列中出现的次数。 如果某一项（在第一项之后）的重复次数超过序列长度的 1% 并且序列长度至少有 200 项，该项会被标记为“热门”并被视为序列匹配中的垃圾。 这种启发式计算可以通过在创建 <code>[SequenceMatcher](https://docs.python.org/zh-cn/3.7/library/difflib.html#difflib.SequenceMatcher)</code> 时将 <code>autojunk</code> 参数设为 <code>False</code> 来关闭。</p><p><strong>_class_ </strong><code>**difflib.Differ**</code></p><p>这个类的作用是比较由文本行组成的序列，并产生可供人阅读的差异或增量信息。 Differ 统一使用 <code>[SequenceMatcher](https://docs.python.org/zh-cn/3.7/library/difflib.html#difflib.SequenceMatcher)</code> 来完成行序列的比较以及相似（接近匹配）行内部字符序列的比较。</p><p><code>[Differ](https://docs.python.org/zh-cn/3.7/library/difflib.html#difflib.Differ)</code> 增量的每一行均以双字母代码打头：</p><div class="table-container"><table><thead><tr><th>双字母代码</th><th>含义</th></tr></thead><tbody><tr><td><code>&#39;- &#39;</code></td><td>行为序列 1 所独有</td></tr><tr><td><code>&#39;+ &#39;</code></td><td>行为序列 2 所独有</td></tr><tr><td><code>&#39; &#39;</code></td><td>行在两序列中相同</td></tr><tr><td><code>&#39;? &#39;</code></td><td>行不存在于任一输入序列</td></tr></tbody></table></div><p><code>**difflib.get_close_matches**</code><strong>(_word_, _possibilities_, _n=3_, _cutoff=0.6_)</strong></p><p>返回由最佳“近似”匹配构成的列表。</p><p>_word_ 为一个指定目标近似匹配的序列（通常为字符串），_possibilities_ 为一个由用于匹配 _word_ 的序列构成的列表（通常为字符串列表）。可选参数 _n_ (默认为 <code>3</code>) 指定最多返回多少个近似匹配； _n_ 必须大于 <code>0</code>；可选参数 _cutoff_ (默认为 <code>0.6</code>) 是一个 [0, 1] 范围内的浮点数。 与 _word_ 相似度得分未达到该值的候选匹配将被忽略。候选匹配中（不超过 _n_ 个）的最佳匹配将以列表形式返回，按相似度得分排序，最相似的排在最前面。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">&gt;&gt;&gt; <span class="hljs-built_in">get_close_matches</span>(<span class="hljs-string">&#x27;appel&#x27;</span>, <span class="hljs-selector-attr">[<span class="hljs-string">&#x27;ape&#x27;</span>, <span class="hljs-string">&#x27;apple&#x27;</span>, <span class="hljs-string">&#x27;peach&#x27;</span>, <span class="hljs-string">&#x27;puppy&#x27;</span>]</span>)<br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;apple&#x27;</span>, <span class="hljs-string">&#x27;ape&#x27;</span>]</span><br></code></pre></td></tr></table></figure><p><code>**difflib.ndiff**</code><strong>(_a_, _b_, _linejunk=None_, _charjunk=IS_CHARACTER_JUNK_)</strong></p><p>比较 _a_ 和 _b_ (字符串列表)；返回 <code>[Differ](https://docs.python.org/zh-cn/3.7/library/difflib.html#difflib.Differ)</code> 形式的增量信息 (一个产生增量行的 <a href="https://docs.python.org/zh-cn/3.7/glossary.html#term-generator">generator</a>)。</p><p>可选关键字形参 _linejunk_ 和 _charjunk_ 均为过滤函数 (或为 <code>None</code>)</p><h3 id="SequenceMatcher-对象"><a href="#SequenceMatcher-对象" class="headerlink" title="SequenceMatcher 对象"></a>SequenceMatcher 对象</h3><p><code>[SequenceMatcher](https://docs.python.org/zh-cn/3.7/library/difflib.html#difflib.SequenceMatcher)</code> 类具有这样的构造器：</p><ul><li>_class_ <code>difflib.SequenceMatcher</code>(_isjunk=None_, _a=’’_, _b=’’_, _autojunk=True_)</li></ul><p>可选参数<code>isjunk</code>必须是<code>none</code>（默认值）等同于传递<code>lambda x:0</code>；换句话说，不忽略任何元素；可选参数 _autojunk_ 可用于启用自动垃圾启发式计算。</p><p><code>[SequenceMatcher](https://docs.python.org/zh-cn/3.7/library/difflib.html#difflib.SequenceMatcher)</code> 对象具有以下方法：</p><ul><li><code>find_longest_match</code>(_alo_, _ahi_, _blo_, _bhi_) √</li></ul><p><a href="https://blog.csdn.net/qq_27586341/article/details/106054086">源码解析</a>。找出 <code>a[alo:ahi]</code> 和 <code>b[blo:bhi]</code> 中的最长匹配块。如果 _isjunk_ 被省略或为 <code>None</code>，<code>[find_longest_match()](https://docs.python.org/zh-cn/3.7/library/difflib.html#difflib.SequenceMatcher.find_longest_match)</code> 将返回 <code>(i, j, k)</code> 使得 <code>a[i:i+k]</code> 等于 <code>b[j:j+k]</code></p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> difflib</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">s = difflib.SequenceMatcher(<span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;abcd&#x27;</span>, <span class="hljs-string">&#x27;abcd abcd&#x27;</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">s.find_longest_match(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>,<span class="hljs-number">9</span>)</span><br>Match(a=0, b=0, size=4)<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span><br></code></pre></td></tr></table></figure><ul><li><code>get_matching_blocks</code>()</li></ul><p>返回描述非重叠匹配子序列的三元组列表。 每个三元组的形式为 <code>(i, j, n)</code>，其含义为 <code>a[i:i+n] == b[j:j+n]</code>。 这些三元组按 _i_ 和 _j_ 单调递增排列。</p><p>最后一个三元组用于占位，其值为 <code>(len(a), len(b), 0)</code>。 它是唯一 <code>n == 0</code> 的三元组。 如果 <code>(i, j, n)</code> 和 <code>(i&#39;, j&#39;, n&#39;)</code> 是在列表中相邻的三元组，且后者不是列表中的最后一个三元组，则 <code>i+n &lt; i&#39;</code> 或 <code>j+n &lt; j&#39;</code>；换句话说，相邻的三元组总是描述非相邻的相等块。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">&gt;&gt;&gt; s = difflib.SequenceMatcher(None, <span class="hljs-string">&#x27;abxcd&#x27;</span>, <span class="hljs-string">&#x27;abcd&#x27;</span>)<br>&gt;&gt;&gt; s.get_matching_blocks()<br>[Match(<span class="hljs-attribute">a</span>=0, <span class="hljs-attribute">b</span>=0, <span class="hljs-attribute">size</span>=2), Match(<span class="hljs-attribute">a</span>=3, <span class="hljs-attribute">b</span>=2, <span class="hljs-attribute">size</span>=2), Match(<span class="hljs-attribute">a</span>=5, <span class="hljs-attribute">b</span>=4, <span class="hljs-attribute">size</span>=0)]<br></code></pre></td></tr></table></figure><ul><li><code>get_opcodes</code>() √</li></ul><p>返回描述如何将 _a_ 变为 _b_ 的 5 元组列表，每个元组的形式为 <code>(tag, i1, i2, j1, j2)</code>。 在第一个元组中 <code>i1 == j1 == 0</code>，而在其余的元组中 _i1_ 等于前一个元组的 _i2_，并且 _j1_ 也等于前一个元组的 _j2_。</p><p>_tag_ 值为字符串，其含义如下：</p><div class="table-container"><table><thead><tr><th>值</th><th>含义</th></tr></thead><tbody><tr><td><code>&#39;replace&#39;</code></td><td><code>a[i1:i2]</code> 应由 <code>b[j1:j2]</code> 替换。</td></tr><tr><td><code>&#39;delete&#39;</code></td><td><code>a[i1:i2]</code> 应被删除。 请注意在此情况下 <code>j1 == j2</code>。</td></tr><tr><td><code>&#39;insert&#39;</code></td><td><code>b[j1:j2]</code> 应插入到 <code>a[i1:i1]</code>。 请注意在此情况下 <code>i1 == i2</code>。</td></tr><tr><td><code>&#39;equal&#39;</code></td><td><code>a[i1:i2] == b[j1:j2]</code> (两个子序列相同)。</td></tr></tbody></table></div><p>代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">&gt;&gt;&gt; <span class="hljs-selector-tag">a</span> = <span class="hljs-string">&quot;qabxcd&quot;</span><br>&gt;&gt;&gt; <span class="hljs-selector-tag">b</span> = <span class="hljs-string">&quot;abycdf&quot;</span><br>&gt;&gt;&gt; s = <span class="hljs-built_in">SequenceMatcher</span>(None, <span class="hljs-selector-tag">a</span>, b)<br>&gt;&gt;&gt; <span class="hljs-keyword">for</span> tag, i1, i2, j1, j2 <span class="hljs-keyword">in</span> s<span class="hljs-selector-class">.get_opcodes</span>():<br>...     <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;:7&#125;   a[&#123;&#125;:&#123;&#125;] --&gt; b[&#123;&#125;:&#123;&#125;] &#123;!r:&gt;8&#125; --&gt; &#123;!r&#125;&#x27;</span><span class="hljs-selector-class">.format</span>(<br>...         tag, i1, i2, j1, j2, <span class="hljs-selector-tag">a</span><span class="hljs-selector-attr">[i1:i2]</span>, <span class="hljs-selector-tag">b</span><span class="hljs-selector-attr">[j1:j2]</span>))<br>delete    <span class="hljs-selector-tag">a</span><span class="hljs-selector-attr">[0:1]</span> --&gt; <span class="hljs-selector-tag">b</span><span class="hljs-selector-attr">[0:0]</span>      <span class="hljs-string">&#x27;q&#x27;</span> --&gt; <span class="hljs-string">&#x27;&#x27;</span><br>equal     <span class="hljs-selector-tag">a</span><span class="hljs-selector-attr">[1:3]</span> --&gt; <span class="hljs-selector-tag">b</span><span class="hljs-selector-attr">[0:2]</span>     <span class="hljs-string">&#x27;ab&#x27;</span> --&gt; <span class="hljs-string">&#x27;ab&#x27;</span><br>replace   <span class="hljs-selector-tag">a</span><span class="hljs-selector-attr">[3:4]</span> --&gt; <span class="hljs-selector-tag">b</span><span class="hljs-selector-attr">[2:3]</span>      <span class="hljs-string">&#x27;x&#x27;</span> --&gt; <span class="hljs-string">&#x27;y&#x27;</span><br>equal     <span class="hljs-selector-tag">a</span><span class="hljs-selector-attr">[4:6]</span> --&gt; <span class="hljs-selector-tag">b</span><span class="hljs-selector-attr">[3:5]</span>     <span class="hljs-string">&#x27;cd&#x27;</span> --&gt; <span class="hljs-string">&#x27;cd&#x27;</span><br>insert    <span class="hljs-selector-tag">a</span><span class="hljs-selector-attr">[6:6]</span> --&gt; <span class="hljs-selector-tag">b</span><span class="hljs-selector-attr">[5:6]</span>       <span class="hljs-string">&#x27;&#x27;</span> --&gt; <span class="hljs-string">&#x27;f&#x27;</span><br></code></pre></td></tr></table></figure><ul><li><code>ratio</code>()</li></ul><p>返回一个取值范围 [0, 1] 的浮点数作为序列相似性度量。</p><p>其中 T 是两个序列中元素的总数量，M 是匹配的数量，即 2.0*M / T。 请注意如果两个序列完全相同则该值为 <code>1.0</code>，如果两者完全不同则为 <code>0.0</code>。</p><p>如果 <code>get_matching_blocks()</code> 或 <code>get_opcodes()</code> 尚未被调用则此方法运算消耗较大，在此情况下你可能需要先调用 <code>quick_ratio()</code> 或 <code>real_quick_ratio()</code> 来获取一个上界。</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">s = difflib.SequenceMatcher(<span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;abcd&#x27;</span>, <span class="hljs-string">&#x27;bcde&#x27;</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">s.ratio()</span><br>0.75<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">s.quick_ratio()</span><br>0.75<br></code></pre></td></tr></table></figure><h3 id="Differ-对象"><a href="#Differ-对象" class="headerlink" title="Differ 对象"></a>Differ 对象</h3><p>这个类的作用是比较由文本行组成的序列，并产生可供人阅读的差异或增量信息。 Differ 统一使用 SequenceMatcher 来完成行序列的比较以及相似（接近匹配）行内部字符序列的比较。</p><p><code>Differ</code> 类具有这样的构造器：</p><ul><li><p>_class_ <code>difflib.``Differ</code>(_linejunk=None_, _charjunk=None_)</p><p>可选关键字形参 _linejunk_ 和 _charjunk_ 均为过滤函数 (或为 <code>None</code>)： </p></li></ul><p><code>Differ</code> 对象是通过一个单独方法来使用（生成增量）的：</p><ul><li><p>compare(a, b)</p><p>比较两个由行组成的序列，并生成增量（一个由行组成的序列）。<br>每个序列必须包含一个以换行符结尾的单行字符串。 这样的序列可以通过文件类对象的 readlines() 方法来获取。 所生成的增量同样由以换行符结尾的字符串构成，可以通过文件类对象的 writelines() 方法原样打印出来。 </p></li></ul><h2 id="fuzzywuzzy"><a href="#fuzzywuzzy" class="headerlink" title="fuzzywuzzy"></a>fuzzywuzzy</h2><blockquote><p>FuzzyWuzzy 是一个简单易用的模糊字符串匹配工具包。它依据 Levenshtein Distance 算法 计算两个序列之间的差异。</p><p><a href="https://github.com/seatgeek/fuzzywuzzy、">https://github.com/seatgeek/fuzzywuzzy、</a></p></blockquote><p>导入库</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-keyword">from</span> fuzzywuzzy <span class="hljs-keyword">import</span> fuzz<br><span class="hljs-keyword">from</span> fuzzywuzzy <span class="hljs-keyword">import</span> process<br></code></pre></td></tr></table></figure><p>简单匹配</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">fuzz.ratio(<span class="hljs-string">&quot;this is a test&quot;</span>, <span class="hljs-string">&quot;this is a test!&quot;</span>)</span><br>    97<br></code></pre></td></tr></table></figure><p>非完全匹配</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">fuzz.partial_ratio(<span class="hljs-string">&quot;this is a test&quot;</span>, <span class="hljs-string">&quot;this is a test!&quot;</span>)</span><br>    100<br></code></pre></td></tr></table></figure><p>忽略顺序匹配</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">fuzz.ratio(<span class="hljs-string">&quot;fuzzy wuzzy was a bear&quot;</span>, <span class="hljs-string">&quot;wuzzy fuzzy was a bear&quot;</span>)</span><br>    91<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">fuzz.token_sort_ratio(<span class="hljs-string">&quot;fuzzy wuzzy was a bear&quot;</span>, <span class="hljs-string">&quot;wuzzy fuzzy was a bear&quot;</span>)</span><br>    100<br></code></pre></td></tr></table></figure><p>去重子集匹配</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">fuzz.token_sort_ratio(<span class="hljs-string">&quot;fuzzy was a bear&quot;</span>, <span class="hljs-string">&quot;fuzzy fuzzy was a bear&quot;</span>)</span><br>    84<br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">fuzz.token_set_ratio(<span class="hljs-string">&quot;fuzzy was a bear&quot;</span>, <span class="hljs-string">&quot;fuzzy fuzzy was a bear&quot;</span>)</span><br>    100<br></code></pre></td></tr></table></figure><p>Process返回模糊匹配的字符串和相似度</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">&gt;&gt;&gt; choices = <span class="hljs-selector-attr">[<span class="hljs-string">&quot;Atlanta Falcons&quot;</span>, <span class="hljs-string">&quot;New York Jets&quot;</span>, <span class="hljs-string">&quot;New York Giants&quot;</span>, <span class="hljs-string">&quot;Dallas Cowboys&quot;</span>]</span><br>&gt;&gt;&gt; process<span class="hljs-selector-class">.extract</span>(<span class="hljs-string">&quot;new york jets&quot;</span>, choices, limit=<span class="hljs-number">2</span>)<br>    <span class="hljs-selector-attr">[(<span class="hljs-string">&#x27;New York Jets&#x27;</span>, 100), (<span class="hljs-string">&#x27;New York Giants&#x27;</span>, 78)]</span><br>&gt;&gt;&gt; process<span class="hljs-selector-class">.extractOne</span>(<span class="hljs-string">&quot;cowboys&quot;</span>, choices)<br>    (<span class="hljs-string">&quot;Dallas Cowboys&quot;</span>, <span class="hljs-number">90</span>)<br></code></pre></td></tr></table></figure><p>可以传入附加参数到 extractOne 方法来设置使用特定的匹配模式，一个典型的用法是来匹配文件路径</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">&gt;&gt;&gt; process.extract<span class="hljs-constructor">One(<span class="hljs-string">&quot;System of a down - Hypnotize - Heroin&quot;</span>, <span class="hljs-params">songs</span>)</span><br>    (&#x27;/music/library/good/System <span class="hljs-keyword">of</span> a Down/<span class="hljs-number">2005</span> - Hypnotize/<span class="hljs-number">01</span> - <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Attack</span>.</span></span>mp3&#x27;, <span class="hljs-number">86</span>)<br>&gt;&gt;&gt; process.extract<span class="hljs-constructor">One(<span class="hljs-string">&quot;System of a down - Hypnotize - Heroin&quot;</span>, <span class="hljs-params">songs</span>, <span class="hljs-params">scorer</span>=<span class="hljs-params">fuzz</span>.<span class="hljs-params">token_sort_ratio</span>)</span><br>    (<span class="hljs-string">&quot;/music/library/good/System of a Down/2005 - Hypnotize/10 - She&#x27;s Like Heroin.mp3&quot;</span>, <span class="hljs-number">61</span>)<br></code></pre></td></tr></table></figure><h2 id="strsimpy"><a href="#strsimpy" class="headerlink" title="strsimpy"></a>strsimpy</h2><p>这是一个用于计算各种字符串距离的包。其使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> strsimpy.levenshtein <span class="hljs-keyword">import</span> Levenshtein<br>levenshtein = Levenshtein()<br><span class="hljs-built_in">print</span>(levenshtein.distance(<span class="hljs-string">&#x27;My string&#x27;</span>, <span class="hljs-string">&#x27;My $string&#x27;</span>))<br><br><br><span class="hljs-keyword">from</span> strsimpy.normalized_levenshtein <span class="hljs-keyword">import</span> NormalizedLevenshtein<br>normalized_levenshtein = NormalizedLevenshtein()<br><span class="hljs-built_in">print</span>(normalized_levenshtein.distance(<span class="hljs-string">&#x27;My string&#x27;</span>, <span class="hljs-string">&#x27;My $string&#x27;</span>))<br><span class="hljs-built_in">print</span>(normalized_levenshtein.similarity(<span class="hljs-string">&#x27;My string&#x27;</span>, <span class="hljs-string">&#x27;My $string&#x27;</span>))<br><br><br><span class="hljs-comment"># 带权重的编辑距离</span><br><span class="hljs-keyword">from</span> strsimpy.weighted_levenshtein <span class="hljs-keyword">import</span> WeightedLevenshtein<br><span class="hljs-keyword">from</span> strsimpy.weighted_levenshtein <span class="hljs-keyword">import</span> CharacterSubstitutionInterface<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CharacterSubstitution</span>(<span class="hljs-title class_ inherited__">CharacterSubstitutionInterface</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">cost</span>(<span class="hljs-params">self, c0, c1</span>):<br>        <span class="hljs-keyword">if</span> c0==<span class="hljs-string">&#x27;t&#x27;</span> <span class="hljs-keyword">and</span> c1==<span class="hljs-string">&#x27;r&#x27;</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span><br>weighted_levenshtein = WeightedLevenshtein(CharacterSubstitution())<br><span class="hljs-built_in">print</span>(weighted_levenshtein.distance(<span class="hljs-string">&#x27;String1&#x27;</span>, <span class="hljs-string">&#x27;String2&#x27;</span>))<br><br><br><span class="hljs-keyword">from</span> strsimpy.damerau <span class="hljs-keyword">import</span> Damerau<br>damerau = Damerau()<br><span class="hljs-built_in">print</span>(damerau.distance(<span class="hljs-string">&#x27;ABCDEF&#x27;</span>, <span class="hljs-string">&#x27;ABDCEF&#x27;</span>))<br><br><br><span class="hljs-comment"># 最优化对齐后的编辑距离</span><br><span class="hljs-keyword">from</span> strsimpy.optimal_string_alignment <span class="hljs-keyword">import</span> OptimalStringAlignment<br>optimal_string_alignment = OptimalStringAlignment()<br><span class="hljs-built_in">print</span>(optimal_string_alignment.distance(<span class="hljs-string">&#x27;CA&#x27;</span>, <span class="hljs-string">&#x27;ABC&#x27;</span>))<br><br><br><span class="hljs-keyword">from</span> strsimpy.jaro_winkler <span class="hljs-keyword">import</span> JaroWinkler<br>jarowinkler = JaroWinkler()<br><span class="hljs-built_in">print</span>(jarowinkler.similarity(<span class="hljs-string">&#x27;My string&#x27;</span>, <span class="hljs-string">&#x27;My tsring&#x27;</span>))<br><br><br><span class="hljs-comment"># 最长公共子序列</span><br><span class="hljs-keyword">from</span> strsimpy.longest_common_subsequence <span class="hljs-keyword">import</span> LongestCommonSubsequence<br>lcs = LongestCommonSubsequence()<br><span class="hljs-built_in">print</span>(lcs.distance(<span class="hljs-string">&#x27;AGCAT&#x27;</span>, <span class="hljs-string">&#x27;GAC&#x27;</span>))<br><br><br><span class="hljs-keyword">from</span> strsimpy.metric_lcs <span class="hljs-keyword">import</span> MetricLCS<br>metric_lcs = MetricLCS()<br>s1 = <span class="hljs-string">&#x27;ABCDEFG&#x27;</span><br>s2 = <span class="hljs-string">&#x27;ABCDEFHJKL&#x27;</span><br><span class="hljs-built_in">print</span>(metric_lcs.distance(s1, s2))<br><br><br><span class="hljs-comment"># ngram</span><br><span class="hljs-keyword">from</span> strsimpy.ngram <span class="hljs-keyword">import</span> NGram<br>twogram = NGram(<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(twogram.distance(<span class="hljs-string">&#x27;ABCD&#x27;</span>, <span class="hljs-string">&#x27;ABTUIO&#x27;</span>))<br>s1 = <span class="hljs-string">&#x27;Adobe CreativeSuite 5 Master Collection from cheap 4zp&#x27;</span><br>s2 = <span class="hljs-string">&#x27;Adobe CreativeSuite 5 Master Collection from cheap d1x&#x27;</span><br>fourgram = NGram(<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(fourgram.distance(s1, s2))<br><br><br><span class="hljs-keyword">from</span> strsimpy.qgram <span class="hljs-keyword">import</span> QGram<br>qgram = QGram(<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(qgram.distance(<span class="hljs-string">&#x27;ABCD&#x27;</span>, <span class="hljs-string">&#x27;ABCE&#x27;</span>))<br><br><br><span class="hljs-keyword">from</span> strsimpy.cosine <span class="hljs-keyword">import</span> Cosine<br>cosine = Cosine(<span class="hljs-number">2</span>)<br>s0 = <span class="hljs-string">&#x27;My first string&#x27;</span><br>s1 = <span class="hljs-string">&#x27;My other string...&#x27;</span><br>p0 = cosine.get_profile(s0)<br>p1 = cosine.get_profile(s1)<br><span class="hljs-built_in">print</span>(cosine.similarity_profiles<br></code></pre></td></tr></table></figure><h2 id="Fast-Fuzzy-Matching"><a href="#Fast-Fuzzy-Matching" class="headerlink" title="Fast Fuzzy Matching"></a>Fast Fuzzy Matching</h2><blockquote><p>通过 ngram 寻找紧密匹配的对象。这种查找紧密匹配的方法不仅应该非常有效，而且还可以通过其对整个数据中不太常见的字符组更加重视的能力来产生高质量的匹配项。</p></blockquote><p>使用字符串相似性度量，例如<a href="https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance">Jaro-Winkler</a>或<a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein</a>距离度量，存在着一个明显的问题，就是所需的计算量呈二次方增长（必须将每个条目与数据集中的每个其他条目进行比较）。该方法最大的优势是速度，使用 TF-IDF、N-Grams 和稀疏矩阵乘法更快地完成模糊匹配！</p><p>示例代码：</p><p><a href="https://bergvca.github.io/2017/10/14/super-fast-string-matching.html">https://bergvca.github.io/2017/10/14/super-fast-string-matching.html</a></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/112488614">字符串模糊匹配的方法都有哪些</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>字符串模糊匹配</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模式匹配技术总结（AC自动机）</title>
    <link href="/2020/03/31/2020-03-31-%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%EF%BC%88AC%E8%87%AA%E5%8A%A8%E6%9C%BA%EF%BC%89/"/>
    <url>/2020/03/31/2020-03-31-%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%EF%BC%88AC%E8%87%AA%E5%8A%A8%E6%9C%BA%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>转载自：<a href="https://carlos9310.github.io/2020/01/01/Aho-Corasick/">经典算法—Aho-Corasick automaton</a></p></blockquote><p>Aho–Corasick automaton，简称AC自动机，著名的多模匹配算法，由Alfred V. Aho和Margaret J.Corasick于1975年在贝尔实验室发明，主要用于多模式串匹配问题，即给几个关键词（模式串），再给一篇文章，判断关键词是否在文章中出现，或出现的次数。</p><p>Aho-Corasick算法，通过将模式串预处理为确定有限状态自动机，扫描文本一遍就能结束。其复杂度为O(n)，即与模式串的数量和长度无关。</p><h2 id="单模匹配"><a href="#单模匹配" class="headerlink" title="单模匹配"></a>单模匹配</h2><p>在介绍AC自动机这种多模匹配算法前，先回顾下单模匹配问题，即给定一个文本串和一个模式串，求解模式串在文本串中的匹配情况。</p><h3 id="1-1-朴素匹配"><a href="#1-1-朴素匹配" class="headerlink" title="1.1 朴素匹配"></a>1.1 朴素匹配</h3><p>最直接的想法是暴力(Brute Force)匹配，即将文本串的第一个字符与模式串的第一个字符进行匹配，若相等则继续比较文本串的第二个字符与模式串的第二个字符。若不等，则比较目标串的第二个字符与模式串的第一个字符，依次比较下去，直到得到最后的匹配结果。</p><p>相关代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 每次匹配失败时</span><br><span class="hljs-comment"># 文本串T的指针回退到开始匹配位置的下一个位置</span><br><span class="hljs-comment"># 模式串P的指针回退到初始位置，然后重新开始匹配</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">bfMatch</span>(<span class="hljs-params">T,P</span>):<br>    tLen,pLen = <span class="hljs-built_in">len</span>(T),<span class="hljs-built_in">len</span>(P)<br>    indexs = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tLen - pLen + <span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(pLen):<br>            <span class="hljs-keyword">if</span> T[i+j] == P[j]:<br>                <span class="hljs-keyword">if</span> j == pLen - <span class="hljs-number">1</span>:<br>                    indexs.append(i)<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> indexs<br><br>T=<span class="hljs-string">&#x27;ushershe&#x27;</span> <br>P=<span class="hljs-string">&#x27;he&#x27;</span> <br><span class="hljs-built_in">print</span>(bfMatch(T,P))<br></code></pre></td></tr></table></figure><br>ps: 上述匹配过程存在重复匹配，KMP算法优化了上述匹配过程。<strong>在匹配失败时，文本串的指针不需要回退。</strong></p><h3 id="1-2-KMP算法"><a href="#1-2-KMP算法" class="headerlink" title="1.2 KMP算法"></a>1.2 KMP算法</h3><p>与朴素匹配不同，KMP算法在匹配到某个字符失败时，文本串的匹配指针不会回退，模式串则根据<strong>部分匹配表(也叫next数组)</strong> 向右滑动一定距离后继续与上次在文本串中不匹配的位置进行匹配，若仍不匹配，则继续根据部分匹配表向右滑动模式串，重复上述不匹配–滑动的过程，当匹配指针指到模式串的初始位置依然不匹配，则模式串向右滑动一位，文本串的匹配指针向前移动一位；若匹配，则继续匹配其他位置的字符。当匹配指针连续匹配的字符数与模式串的长度相等，则匹配完成。形象图解可参考<a href="http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html">字符串匹配的KMP算法</a>。</p><p>相应代码为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 部分匹配表是针对模式串构建的</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">kmpMatch</span>(<span class="hljs-params">T,P</span>):<br>    tLen,pLen = <span class="hljs-built_in">len</span>(T),<span class="hljs-built_in">len</span>(P)<br>    Next = partialMatchTable(P)<br>    q = <span class="hljs-number">0</span> <span class="hljs-comment"># 模式串P的下标</span><br>    indexs = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tLen):<br>        <span class="hljs-keyword">while</span> q &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> P[q] != T[i]:<br>            q = Next[q-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> P[q] == T[i]:<br>            q += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> q == pLen:<br>            indexs.append(i-pLen+<span class="hljs-number">1</span>)<br>            q=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">return</span> indexs<br></code></pre></td></tr></table></figure></p><p>部分匹配表中的数值是指<strong>某个子串的前缀和后缀的最长共有元素的长度。</strong> 其有两种构建方式。一种是手动法，详见<a href="http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html">字符串匹配的KMP算法</a>。相关代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 手动法求部分匹配表</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partialMatchTable</span>(<span class="hljs-params">p</span>): <span class="hljs-comment"># 也叫next数组</span><br>    prefix,suffix = <span class="hljs-built_in">set</span>(),<span class="hljs-built_in">set</span>()<br>    pLen = <span class="hljs-built_in">len</span>(p)<br>    Next = [<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,pLen):<br>        prefix.add(p[:i]) <br>        suffix = &#123;p[j:i+<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,i+<span class="hljs-number">1</span>)&#125;<br>        common_len = <span class="hljs-built_in">len</span>((prefix &amp; suffix <span class="hljs-keyword">or</span> &#123;<span class="hljs-string">&#x27;&#x27;</span>&#125;).pop())<br><span class="hljs-comment">#         print(p[:i+1],prefix,suffix,common_len)</span><br>        Next.append(common_len) <br>    <span class="hljs-keyword">return</span> Next<br><br>p=<span class="hljs-string">&#x27;ababaca&#x27;</span><br>partialMatchTable(p)<br></code></pre></td></tr></table></figure><p>另一种是程序法，模式串针对自己的前后缀的匹配。详见<a href="https://blog.csdn.net/qingdujun/article/details/85281936">KMP算法：线性时间O(n)字符串匹配算法</a>中的部分匹配表部分。相关代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 由模式串生成的部分匹配表，其存储的是前缀尾部 的位置。有前缀尾部 = next(后缀尾部)，</span><br><span class="hljs-comment"># 当后缀之后q不匹配时，通过查询部分匹配表，确定前缀尾部的位置k,然后将前缀滑动过来与后缀对齐，继续后续匹配工作</span><br><span class="hljs-comment"># 程序法计算部分匹配表 </span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partialMatchTable</span>(<span class="hljs-params">p</span>):<br>    pLen = <span class="hljs-built_in">len</span>(p)<br>    Next = [<span class="hljs-number">0</span>]<br>    k = <span class="hljs-number">0</span> <span class="hljs-comment"># 模式串nP的下标</span><br>    <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,pLen): <span class="hljs-comment"># 文本串nT的下标</span><br>        <span class="hljs-keyword">while</span> k &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> p[k] != p[q]:<br>            k = Next[k-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> p[k] == p[q]:<br>            k += <span class="hljs-number">1</span><br>        Next.append(k)<br>    <span class="hljs-keyword">return</span> Next<br><br>p=<span class="hljs-string">&#x27;ababaca&#x27;</span><br>partialMatchTable(p)<br></code></pre></td></tr></table></figure><h2 id="多模匹配"><a href="#多模匹配" class="headerlink" title="多模匹配"></a>多模匹配</h2><p>给定多个模式串和一个文本串，求解多模串在文本串中存在的情况(包括是否存在、存在几次、存在于哪些位置等)。</p><h3 id="2-1-Trie"><a href="#2-1-Trie" class="headerlink" title="2.1 Trie"></a>2.1 Trie</h3><p>Trie又叫前缀树或字典树，是一种多叉树结构。Trie这个术语来源于re<strong>trie</strong>val(检索)，其是一种用于快速检索的数据结构。其核心思想是利用字符串的<strong>公共前缀</strong>最大限度地减少不必要的字符串比较，提高查询(检索)效率，缺点是内存消耗大。</p><p>Trie树的基本性质：</p><ul><li>根节点不包含字符，除根节点外的每一个子节点都包含一个字符</li><li>从根节点到某一个节点，路径上经过的字符连起来为该节点对应的字符串</li><li>每个节点的所有子节点包含的字符互不相同</li></ul><p>应用场景</p><ul><li>前缀匹配(自动补全)：返回所有<strong>前缀相同</strong>的字符串</li><li>词频统计：将每个节点是否构成单词的标志位改成构成单词的数量</li><li>字典序排序：将所有待排序集合逐个加入到Trie中，然后按照先序遍历输出所有值</li><li>分词</li><li>检索</li></ul><p>代码示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trie</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;自定义Trie树对象，用来保存知识库</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, value_key=-<span class="hljs-number">1</span></span>):<br>        self.data = &#123;&#125;<br>        self.value_key = <span class="hljs-built_in">str</span>(value_key)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__setitem__</span>(<span class="hljs-params">self, key, value</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;传入一对(key, value)到前缀树中</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        data = self.data<br>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> key:<br>            k = <span class="hljs-built_in">str</span>(k)<br>            <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> data:<br>                data[k] = &#123;&#125;<br>            data = data[k]<br>        data[self.value_key] = value<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, key</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;获取key对应的value</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        data = self.data<br>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> key:<br>            k = <span class="hljs-built_in">str</span>(k)<br>            data = data[k]<br>        <span class="hljs-keyword">return</span> data[self.value_key]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">next_ones</span>(<span class="hljs-params">self, prefix</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;获取prefix后一位的容许集</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        data = self.data<br>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> prefix:<br>            k = <span class="hljs-built_in">str</span>(k)<br>            data = data[k]<br>        <span class="hljs-keyword">return</span> [k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> data <span class="hljs-keyword">if</span> k != self.value_key]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">keys</span>(<span class="hljs-params">self, prefix=<span class="hljs-literal">None</span>, data=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;获取以prefix开头的所有key</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        data = data <span class="hljs-keyword">or</span> self.data<br>        prefix = prefix <span class="hljs-keyword">or</span> []<br>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> prefix:<br>            k = <span class="hljs-built_in">str</span>(k)<br>            <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> data:<br>                <span class="hljs-keyword">return</span> []<br>            data = data[k]<br>        results = []<br>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> data:<br>            <span class="hljs-keyword">if</span> k == self.value_key:<br>                results.append([])<br>            <span class="hljs-keyword">else</span>:<br>                results.extend([[k] + j <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> self.keys(<span class="hljs-literal">None</span>, data[k])])<br>        <span class="hljs-keyword">return</span> [prefix + i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> results]<br><br><span class="hljs-comment"># # 测试前缀树</span><br><span class="hljs-comment"># KG = Trie()</span><br><span class="hljs-comment"># # key = [872, 1962, 102, 872, 738, 1962]</span><br><span class="hljs-comment"># KG[[872, 1962]] = &quot;首都&quot;</span><br><span class="hljs-comment"># KG[[872, 1962, 78]] = &quot;capital&quot;</span><br><span class="hljs-comment"># KG[[872, 1962, 44]] = &quot;capital2&quot;</span><br><span class="hljs-comment"># KG[[23, 82, 62]] = &quot;冲冲冲&quot;</span><br><span class="hljs-comment"># # KG.save(&#x27;data/KG.json&#x27;)</span><br><span class="hljs-comment"># print(KG[[872, 1962]])  # get 首都</span><br><span class="hljs-comment"># print(KG.next_ones([872, 1962]))    # [&#x27;78&#x27;, &#x27;44&#x27;]</span><br><span class="hljs-comment"># print(KG.keys([872]))   # [[872, &#x27;1962&#x27;], [872, &#x27;1962&#x27;, &#x27;78&#x27;], [872, &#x27;1962&#x27;, &#x27;44&#x27;]]</span><br><span class="hljs-comment"># KG = Trie()</span><br><span class="hljs-comment"># KG.load(&quot;data/KG.json&quot;)</span><br><span class="hljs-comment"># print(KG.keys([872])) </span><br><span class="hljs-comment"># exit()</span><br></code></pre></td></tr></table></figure></p><h3 id="2-2-AC自动机"><a href="#2-2-AC自动机" class="headerlink" title="2.2 AC自动机"></a>2.2 AC自动机</h3><p>简单地讲，AC自动机就是字典树+kmp算法+失配指针。在Trie树上通过KMP来实现多模串的匹配。其中Trie树负责状态转移，KMP负责减少重复匹配。</p><p><strong>AC自动机的实现</strong><br>在多模式环境中，AC自动是使用前缀树来存放所有模式串的前缀，然后通过失配指针来处理失配的情况。分为三个步骤：构建前缀树（生成goto表），添加失配指针（生成fail表），模式匹配（构造output表）。以经典的ushers为例，模式串是“he、she、his、hers”，文本为“ushers”。构建的自动机如下图所示：</p><img src="/2020/03/31/2020-03-31-%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%EF%BC%88AC%E8%87%AA%E5%8A%A8%E6%9C%BA%EF%BC%89/1644547099604-84d6051f-a20b-4c2c-a948-36377a7ce793.png" class="" title="image.png"><p>其中实线部分是一颗Trie树，虚线部分为各节点的fail路径。</p><p><strong>匹配过程：</strong></p><ol><li>按字符转移成功，但不是模式串的结尾。即成功转移到另一个状态，对应success表/goto表；</li><li>按字符转移成功，是模式串的结尾。即命中一个模式串，对应emits/output；</li><li><strong>按字符转移失败，此时跳转到一个特定的节点，对应failure。从根节点到这个特定的节点的路径恰好是失败前的文本的一部分，类似KMP算法中利用部分匹配表来加速模式串的滑动从而减少重复匹配</strong></li></ol><p>上述匹配过程只需扫描一遍文本串，其时间复杂度为O(n)，与模式串的数量和长度无关。</p><p>以上图为例进行演示。自动机从根节点0出发，</p><ol><li>首先尝试按success表转移（图中实线）。按照文本的指示转移，也就是接收一个u。此时success表中并没有相应路线，转移失败。</li><li>失败了则按照failure表回去（图中虚线）。按照文本指示，这次接收一个s，转移到状态3。</li><li>成功了继续按success表转移，直到失败跳转步骤2，或者遇到output表中标明的“可输出状态”（图中红色状态）。此时输出匹配到的模式串，然后将此状态视作普通的状态继续转移。</li></ol><p>算法高效之处在于，当自动机接受了“ushe”之后，再接受一个r会导致无法按照success表转移，此时自动机会聪明地按照failure表转移到2号状态，并经过几次转移后输出“hers”。来到2号状态的路不止一条，从根节点一路往下，“h→e”也可以到达。而这个“he”恰好是“ushe”的结尾，状态机就仿佛是压根就没失败过（没有接受r），也没有接受过中间的字符“us”，直接就从初始状态按照“he”的路径走过来一样（到达同一节点，状态完全相同）。</p><p>AC自动机的构建虽然与Trie树的构建类似，但其fail路径(<strong>本质是一种回溯，避免重复匹配</strong>)是AC自动机中特有的。goto表、output 表、failure 表的具体构建逻辑可参考《<a href="https://www.hankcs.com/program/algorithm/implementation-and-analysis-of-aho-corasick-algorithm-in-java.html">Aho-Corasick算法的Java实现与分析</a>-hankcs》</p><h3 id="基于AC自动机的开源工具"><a href="#基于AC自动机的开源工具" class="headerlink" title="基于AC自动机的开源工具"></a>基于AC自动机的开源工具</h3><p>包括：pyahocorasick、Acora、esmre等</p><ul><li><strong>pyahocorasick</strong>是个python模块，由两种数据结构实现：trie和Aho-Corasick自动机，根据一组关键词进行匹配，返回关键词出现的位置，底层用C实现，python包装。使用经验表明，pyahocorasick存在两个问题：<ul><li>一个是关键词匹配不完整，如果目标关键词里面有宝马和马，那么用python的ahocorasick库只会得到宝马，而不会得到马，问题是处在马这个字节是在宝马的链条里面的。</li><li>另一个是内存泄漏问题，这个问题在esmre中得到了解决。</li></ul></li><li><strong>acora</strong>是python的“fgrep”，是一个基于Aho-Corasick以及NFA-to-DFA自动机的方式快速的多关键字文本搜索引擎。</li><li><strong>esmre</strong>同样使用的AhoCorasick自动机的方式，做了一些细微的修改，也是用C实现，python包装。与pyhrocorasick相比，esmre库不存在内存异常泄露等问题。</li><li><strong>Flashtext</strong>是一个高效的字符搜索和替换算法，基于Trie字典数据结构和AhoCorasick 的算法时间复杂度不依赖于搜索或替换的字符的数量。与AhoCorasick算法不同，Flashtext算法不匹配子字符串，只匹配最长字符（完整的单词），首先匹配最长字符串。<ul><li>比如，输入一个单词 {Apple}，算法就不会匹配 “I like Pineapple” 中的 apple，输入单词集合 {Machine， Learning，Machine Learning}和文档“I like Machine Learning”，算法只会去匹配 “Machine Learning” 。</li></ul></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html">字符串匹配的KMP算法</a></li><li><a href="https://blog.csdn.net/qingdujun/article/details/85281936">KMP算法：线性时间O(n)字符串匹配算法</a></li><li><a href="https://benarvintec.com/2018/11/26/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E4%B9%8BAho-Corasick/">算法学习之Aho-Corasick</a></li><li><a href="https://blog.csdn.net/lemon_tree12138/article/details/49335051">深入理解Aho-Corasick自动机算法</a></li><li><a href="https://www.hankcs.com/program/algorithm/implementation-and-analysis-of-aho-corasick-algorithm-in-java.html">Aho-Corasick算法的Java实现与分析</a></li><li><a href="https://www.cnblogs.com/wenzhixin/p/9448045.html">Aho-Corasick automaton（AC自动机）解析及其在算法竞赛中的典型应用举例</a></li><li><a href="https://mp.weixin.qq.com/s/cFFQzxyrhXT5lx3CSjDdDg">NLP模式高效匹配技术总结：模式匹配常见落地场景、AC自动机原理及高效开源工具总结</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>模式匹配</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux程序崩溃生成core文件</title>
    <link href="/2020/03/09/2020-03-09-linux%E7%A8%8B%E5%BA%8F%E5%B4%A9%E6%BA%83%E7%94%9F%E6%88%90core%E6%96%87%E4%BB%B6/"/>
    <url>/2020/03/09/2020-03-09-linux%E7%A8%8B%E5%BA%8F%E5%B4%A9%E6%BA%83%E7%94%9F%E6%88%90core%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="linux程序崩溃生成core文件相关"><a href="#linux程序崩溃生成core文件相关" class="headerlink" title="linux程序崩溃生成core文件相关"></a>linux程序崩溃生成core文件相关</h2><h3 id="1-core-文件的简单介绍"><a href="#1-core-文件的简单介绍" class="headerlink" title="1. core 文件的简单介绍"></a>1. core 文件的简单介绍</h3><p>在一个程序崩溃时，它一般会在指定目录下生成一个 core 文件，core 文件包含了程序运行时的内存，寄存器状态，堆栈指针，内存管理信息等，可以帮助我们进行调试。</p><h3 id="2-造成-coredump-的常见原因"><a href="#2-造成-coredump-的常见原因" class="headerlink" title="2. 造成 coredump 的常见原因"></a>2. 造成 coredump 的常见原因</h3><ul><li>内存访问越界</li><li>多线程程序使用了线程不安全的函数</li><li>多线程读写的数据未加锁保护</li><li>非法指针</li><li>堆栈溢出</li></ul><h3 id="3-core-文件的生成开关和大小限制"><a href="#3-core-文件的生成开关和大小限制" class="headerlink" title="3. core 文件的生成开关和大小限制"></a>3. core 文件的生成开关和大小限制</h3><p>使用 ulimit –c 命令可查看 core 文件的生成开关，若结果为0，则表示关闭了此功能，不会生成 core 文件。</p><ul><li>使用 ulimit –c filesize 命令，可以限制 core 文件的大小，如果此文件大小超过限制，将会被裁剪，最终生成不完整的 core 文件。若为 ulimit –c unlimited ，则不限制 core 文件的大小。 <strong>注意</strong>：在测试前需检查 core 文件的开关是否打开；在测试过程中发现程序异常退出，但没有产生 core ，我们也需要第一时间检查 core 文件是否打开；有几种方式让程序产生 core 。</li><li>修改 core 文件生成大小的配置，例如 ulimit –c 1000，这个修改只对当前会话有效。</li><li>通过将一个相应的 ulimit 语句添加到由登录 shell 读取的文件，如 ~/.profile ，例如在wx用户下的 ~/.profile 增加 ulimit –c unlimited ，那么对于 wx 用户就可以生成没有大小限制的 core 文件，但是对于其他用户不生效。</li><li>修改 /etc/profile 文件，将默认配置 # ulimit -Sc 0 ，将配置改成可用，并设置为 ulimit -Sc unlimited ，那么该机器的所有用户都将生成无大小限制的 core 文件。</li><li>在程序的启动脚本（例如 restart.sh ）的开头设置 ulimit –c unlimited ，这只是对该进程有用。</li></ul><h3 id="4-core-文件的名称和生成路径设置"><a href="#4-core-文件的名称和生成路径设置" class="headerlink" title="4. core 文件的名称和生成路径设置"></a>4. core 文件的名称和生成路径设置</h3><p>若系统生成的 core 文件不带其他任何扩展名称，则全部命名为 core，新的 core 文件生成将会覆盖原来的 core 文件。</p><p><code>/proc/sys/kernel/ core_uses_pid</code> 可以控制 core 文件的文件名是否添加 pid 作为扩展。文件内容为1表示添加 pid ，生成的 core 文件格式为 <code>core.XXXX</code> ，为0表示不添加。可以通过以下命令修改此文件： <code>echo “1”&gt; /proc/sys/kernel/ coreuses_pid</code> 。</p><p><code>/proc/sys/kernel/ core_pattern</code>可以控制 core 文件保存位置和文件名格式。可以通过以下命令修改此文件：</p><p><code>echo “/corefile/core-%e-%p-%t”&gt; core_pattern</code>，可以将 core 文件统一生成到 / corefile 目录下，产生的文件名为 core - 命令名 - pid - 时间戳，以下是参数列表：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs javascript">%p表示添加pid；<br><br>%u表示添加当前uid；<br><br>%g表示添加当前gid；<br><br>%s表示添加导致产生core的信号；<br><br>%t表示添加core文件生产时的unix时间；<br><br>%h表示添加主机名；<br><br>%e表示添加命令名；<br></code></pre></td></tr></table></figure><h3 id="5-如何查看-core-文件"><a href="#5-如何查看-core-文件" class="headerlink" title="5. 如何查看 core 文件"></a>5. 如何查看 core 文件</h3><p>如果我们不清楚 core 是由哪个进程产生的，我们可以通过使用命令 <code>file core 文件</code> 来查看。</p><p>例如 core 文件是由 test 这个进程产生的，那么通过命令 <code>gdb test corefile</code> 查看 core 文件的内容，在输入 bt 或 where 检查程序运行到哪里，来定位 coredump 的行。</p><p>我们查看一个core的例子，例如getd在启动时出现了 core，内容为：</p><p><img src="https://mc.qcloudimg.com/static/img/6fa7e5b1ec084090d98d158d07285524/image.jpg" alt="img"></p><p>我们可以看到在 getdapp.cpp 的1108行调用 assert 函数出现错误，从而抛出了信号，产生了 core 。</p><p>core 显示的内容为堆栈信息，我们可以通过输入 up 来查看上一层堆栈的信息，例如：我们最初看到 core 文件信息为：</p><p><img src="https://mc.qcloudimg.com/static/img/d42ba8dc404bbe9a1649f16af5aa7926/image.jpg" alt="img"></p><p>连续输入 up 后，显示内容为：</p><p><img src="https://mc.qcloudimg.com/static/img/711cb40a716670cf9624e2892ee1ae3d/image.jpg" alt="img"></p><h3 id="6-gdb-常用命令"><a href="#6-gdb-常用命令" class="headerlink" title="6. gdb 常用命令"></a>6. gdb 常用命令</h3><p>有些 core 能简单的定位出，但是有些 core 文件的定位还需要了解 gdb 常用的命令，通过这些命令与 core 文件结合，我们才能快速定位出问题。下面简单的介绍一下 gbd 常用的命令：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs javascript">l：相当于list，从第一行开始列出原码；<br><br>回车：表示重复上一次命令；<br><br>P：print的缩写，打印变量的值，格式为P 变量名；<br><br><span class="hljs-keyword">break</span>：设置断点，例如<span class="hljs-keyword">break</span> <span class="hljs-number">22</span>表示在<span class="hljs-number">22</span>行设置断点，<span class="hljs-keyword">break</span> test表示在test函数入口处设置断点；<br><br>info <span class="hljs-keyword">break</span>：查看断点信息；<br><br>r：表示运行程序；<br><br>c：继续运行程序；<br><br>n：next的缩写，表示单步运行；<br><br>bt：查看函数堆栈；<br><br>finish：退出函数；<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cloud.tencent.com/developer/article/1004351">linux 下 core 相关知识总结</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>core</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux程序内存占用持续增长killed</title>
    <link href="/2020/03/09/2020-03-09-linux%E7%A8%8B%E5%BA%8F%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BFkilled/"/>
    <url>/2020/03/09/2020-03-09-linux%E7%A8%8B%E5%BA%8F%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BFkilled/</url>
    
    <content type="html"><![CDATA[<p>最近在CentOS上运行自己写的程序，程序运行时间久一点就被killed，需要分析原因并找到解决方法. </p><p>首先可能原因是：</p><ol><li><a href="https://blog.csdn.net/ktigerhero3/article/details/80004315">内存不够</a></li><li>程序出错</li></ol><h1 id="内存不够问题"><a href="#内存不够问题" class="headerlink" title="内存不够问题"></a>内存不够问题</h1><p>查看linux 系统日志.</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">vi</span> /var/<span class="hljs-built_in">log</span>/<span class="hljs-keyword">messages</span><br></code></pre></td></tr></table></figure><p>如果出现 <code>kernel: Out of memory: Kill process</code> 意味着整个系统的内存已经不足，如果不杀死进程的话，就会导致系统的崩溃.</p><p>用free命令查看虚拟内存</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">free -h</span><br></code></pre></td></tr></table></figure><p>查看某个进程的内存使用情况，使用top命令</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-attribute">top</span> -<span class="hljs-selector-tag">p</span> `pidof rviz`<br><span class="hljs-attribute">top</span> -<span class="hljs-selector-tag">p</span> id<br><span class="hljs-attribute">top</span> -u username<br></code></pre></td></tr></table></figure><p>出现如下情况</p><blockquote><p><em>监控系统报警生产服务进程出现，内存使用率达到 90% 以上</em></p></blockquote><p>python程序在长时间(较大负载)运行一段时间后, python 进程的系统占用内存持续升高:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache">  <span class="hljs-attribute">PID</span> USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND<br><span class="hljs-attribute">14781</span> root      <span class="hljs-number">20</span>   <span class="hljs-number">0</span>   <span class="hljs-number">10</span>.<span class="hljs-number">3</span>g   <span class="hljs-number">2</span>.<span class="hljs-number">2</span>g   <span class="hljs-number">7124</span> S   <span class="hljs-number">0</span>.<span class="hljs-number">0</span> <span class="hljs-number">14</span>.<span class="hljs-number">5</span>   <span class="hljs-number">4</span>:<span class="hljs-number">57</span>.<span class="hljs-number">13</span> java<br><span class="hljs-attribute">17522</span> lhadmin   <span class="hljs-number">20</span>   <span class="hljs-number">0</span> <span class="hljs-number">5915228</span>   <span class="hljs-number">1</span>.<span class="hljs-number">7</span>g  <span class="hljs-number">87264</span> S   <span class="hljs-number">0</span>.<span class="hljs-number">0</span> <span class="hljs-number">10</span>.<span class="hljs-number">6</span>   <span class="hljs-number">1</span>:<span class="hljs-number">37</span>.<span class="hljs-number">36</span> python3<br><span class="hljs-comment">#                               ~~~~~~</span><br><span class="hljs-comment">#                               1.7g 内存占用</span><br></code></pre></td></tr></table></figure><p>这里的python进程在经历大量请求处理过程中, 内存持续升高, 直至最终被killed.</p><span id="more"></span><h2 id="查证过程"><a href="#查证过程" class="headerlink" title="查证过程"></a>查证过程</h2><p>昨天有7个record没有返回数据的情况，查日志均是 ==ASR/Slot timeout== 的原因导致。猜测是线程之间发生死锁，gc不能回收，使得多个线程全部挂起，导致内存一直在增长 以及 record服务超时！</p><p>具体查证过程如下：</p><h3 id="top-查看-CPU-和内存占用"><a href="#top-查看-CPU-和内存占用" class="headerlink" title="top 查看 CPU 和内存占用"></a>top 查看 CPU 和内存占用</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta"># top</span><br></code></pre></td></tr></table></figure><h3 id="strace-查看系统调用"><a href="#strace-查看系统调用" class="headerlink" title="strace 查看系统调用"></a>strace 查看系统调用</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta"># strace -p 6325</span><br></code></pre></td></tr></table></figure><h3 id="ltrace-查看库函数调用"><a href="#ltrace-查看库函数调用" class="headerlink" title="ltrace 查看库函数调用"></a>ltrace 查看库函数调用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">ltrace -<span class="hljs-built_in">cp</span> 6325</span><br></code></pre></td></tr></table></figure><h3 id="gcore-生成-coredump-文件"><a href="#gcore-生成-coredump-文件" class="headerlink" title="gcore 生成 coredump 文件"></a>gcore 生成 coredump 文件</h3><p>为了避免 <code>gdb attach</code> 进程造成的其他影响（比如可能出现进程异常退出，死锁突然恢复，影响线上服务等），最好将进程生成一个 coredump 文件，然后再慢慢分析。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># gcore 6325</span><br><span class="hljs-comment"># ls -lsh core.6325</span><br><span class="hljs-attribute">2</span>.<span class="hljs-number">7</span>G -rw-r--r-- <span class="hljs-number">1</span> root root <span class="hljs-number">2</span>.<span class="hljs-number">7</span>G <span class="hljs-number">4</span>月  <span class="hljs-number">14</span> <span class="hljs-number">00</span>:<span class="hljs-number">56</span> core.<span class="hljs-number">6325</span><br></code></pre></td></tr></table></figure><h3 id="gdb-分析-coredump-文件"><a href="#gdb-分析-coredump-文件" class="headerlink" title="gdb 分析 coredump 文件"></a>gdb 分析 coredump 文件</h3><p>定位异常：确定python在做什么，是否有大内存消耗任务正在运行，或出现死锁等异常行为</p><p>接入gdb</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-comment"># gdb python core.6325</span><br><span class="hljs-comment"># gdb python2.7 core.6325</span><br><span class="hljs-variable">$ </span>gdb python &lt;pid&gt;<br></code></pre></td></tr></table></figure><p>使用 <code>info threads</code> 查看当前进程的线程列表，发现大部分都在 <code>wait</code> 信号，只有25号线程在做其他事情，切换到25号线程，分析调用栈：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs awk">(gdb) info threads<br>  Id   Target Id         Frame<br>  <span class="hljs-number">18</span>   Thread <span class="hljs-number">0</span>x7f561ae6b740 (LWP <span class="hljs-number">9363</span>) <span class="hljs-number">0</span>x00007f561a76520d <span class="hljs-keyword">in</span> poll () at ..<span class="hljs-regexp">/sysdeps/u</span>nix/syscall-template.S:<span class="hljs-number">81</span><br>  <span class="hljs-number">17</span>   Thread <span class="hljs-number">0</span>x7f5589b17780 (LWP <span class="hljs-number">9463</span>) pthread_cond_wait@@GLIBC_2.<span class="hljs-number">3.2</span> ()<br>    at ..<span class="hljs-regexp">/nptl/</span>sysdeps<span class="hljs-regexp">/unix/</span>sysv<span class="hljs-regexp">/linux/</span>x86_64/pthread_cond_wait.S:<span class="hljs-number">185</span><br>...<br>一般加锁、死锁情况存在时，会有线程卡在xx_wait等函数上。<br>...<br>如果发现某线程有问题，切换到此线程上<br>...<br>(gdb) thread <span class="hljs-number">25</span><br></code></pre></td></tr></table></figure><p>查看线程栈信息，</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">(gdb) <span class="hljs-built_in">info</span> stack<br></code></pre></td></tr></table></figure><p>info stack，这个命令只能查看当前正在运行的某个线程的栈信息</p><p>使用原始 <code>bt</code> 来分析（添加 <code>full</code> 参数可以看更详细的内容）：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">(gdb) <span class="hljs-keyword">bt </span>full<br>(gdb) thread apply all <span class="hljs-keyword">bt</span><span class="hljs-comment"># gdb会让所有线程都执行这个命令</span><br></code></pre></td></tr></table></figure><p>分析 <code>Frame # 7</code> 发现当前线程正在执行 <code>./service/recall/newuser.py, line 49, in get_gametype_anchor_by_sn</code> 方法。</p><p>找到对应的源代码。通过仔细分析代码，发现在某种情况下确实会出现死循环情况，至此问题解决。</p><h3 id="死锁解决办法："><a href="#死锁解决办法：" class="headerlink" title="死锁解决办法："></a>死锁解决办法：</h3><ol><li>尝试用锁加以隔离</li><li>取消多线程</li><li>手动释放内存：对大的变量存储对象，在使用完成后先del掉，然后在return之前，统一gc.collect()垃圾回收一下</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>遇到线上问题时，优先使用 <code>gcore PID</code> 来保存现场</li><li>再<a href="http://rdcqii.hundsun.com/portal/article/597.html">使用strace、ltrace</a>和 <code>gdb</code> 分析</li><li>如果没有什么线索，可以尝试 <a href="https://pyrasite.readthedocs.io/en/latest/Shell.html">pyrasite-shell</a> 或 <a href="https://github.com/khamidou/lptrace">lptrace</a></li><li><code>gdb</code>调试 <code>Python</code> 进程的时候，运行进程的 <code>Python</code> 版本和 python-dbg 一定要匹配</li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://seealso.cn/debug/endless-loop-causes-cpu-full-problems">查证微服务死循环导致 CPU 100%问题</a></p><p><a href="https://zhuanlan.zhihu.com/p/28031057">Python 进程内存增长解决方案</a></p><h1 id="附录：gdb-amp-tracemalloc"><a href="#附录：gdb-amp-tracemalloc" class="headerlink" title="附录：gdb&amp;tracemalloc"></a>附录：gdb&amp;tracemalloc</h1><p>python本身是有垃圾回收的, 但python有如下几种情况可能出现内存泄露（即导致对象无法被回收）:</p><ol><li>循环引用</li><li>循环引用的链上某个对象定义了<code>__del__</code>方法.</li><li>对象一直被全局变量所引用, 全局变量生命周期长.</li><li>垃圾回收机被禁用或者设置成debug状态, 垃圾回收的内存不会被释放.</li><li>引用对象未释放（数据库连接等）</li></ol><p><strong>gdb安装教程</strong></p><p>详细信息可以参考 <a href="https://wiki.python.org/moin/DebuggingWithGdb">debug-with-gdb</a></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">touch <span class="hljs-regexp">/etc/yum</span>.repos.d/CentOS-Debuginfo.repo<br>vi <span class="hljs-regexp">/etc/yum</span>.repos.d/CentOS-Debuginfo.repo<br></code></pre></td></tr></table></figure><p>添加以下内容至文件中：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># CentOS-Debug.repo</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># The mirror system uses the connecting IP address of the client and the</span><br><span class="hljs-comment"># update status of each mirror to pick mirrors that are updated to and</span><br><span class="hljs-comment"># geographically close to the client.  You should use this for CentOS updates</span><br><span class="hljs-comment"># unless you are manually picking other mirrors.</span><br><span class="hljs-comment">#</span><br><br><span class="hljs-comment"># All debug packages from all the various CentOS-5 releases</span><br><span class="hljs-comment"># are merged into a single repo, split by BaseArch</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># <span class="hljs-doctag">Note:</span> packages in the debuginfo repo are currently not signed</span><br><span class="hljs-comment">#</span><br><br><span class="hljs-section">[debug]</span><br><span class="hljs-attr">name</span>=CentOS-<span class="hljs-number">7</span> - Debuginfo<br><span class="hljs-attr">baseurl</span>=http://debuginfo.centos.org/<span class="hljs-number">7</span>/<span class="hljs-variable">$basearch</span>/<br><span class="hljs-attr">gpgcheck</span>=<span class="hljs-number">1</span><br><span class="hljs-attr">gpgkey</span>=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-Debug-<span class="hljs-number">7</span><br><span class="hljs-attr">enabled</span>=<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>安装依赖</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cmake">sudo yum <span class="hljs-keyword">install</span> yum-utils<br>sudo debuginfo-<span class="hljs-keyword">install</span> glibc<br>sudo yum <span class="hljs-keyword">install</span> gdb python-debuginfo<br></code></pre></td></tr></table></figure><p>完成！</p><p><strong>查看python内存泄露的工具</strong></p><ul><li><p>tracemalloc —- 跟踪内存分配: 究极强, 可以直接看到哪个(哪些)对象占用了最大的空间（内存块）, 这些对象是谁, 调用栈是啥样的, python3直接内置, python2如果安装的话需要编译. 它提供以下信息：</p><ul><li>Traceback where an object was allocated</li><li>每个文件名和每行号分配的内存块的统计信息：分配的内存块的总大小，数量和平均大小</li><li><p>计算两个快照之间的差异以检测内存泄漏</p></li><li><p>显示前10项</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">import tracemalloc<br><br>tracemalloc.start()<br><br><span class="hljs-comment"># ... run your application ...</span><br><br>snapshot = tracemalloc.take_snapshot()<br>top_stats = snapshot.statistics(<span class="hljs-string">&#x27;lineno&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[ Top 10 ]&quot;</span>)<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">stat</span> <span class="hljs-keyword">in</span> top_stats[:10]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">stat</span>)<br></code></pre></td></tr></table></figure><ul><li>计算差异</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">import tracemalloc<br>tracemalloc.start()<br><span class="hljs-comment"># ... start your application ...</span><br><br>snapshot1 = tracemalloc.take_snapshot()<br><span class="hljs-comment"># ... call the function leaking memory ...</span><br>snapshot2 = tracemalloc.take_snapshot()<br><br>top_stats = snapshot2.compare_to(snapshot1, <span class="hljs-string">&#x27;lineno&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[ Top 10 differences ]&quot;</span>)<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">stat</span> <span class="hljs-keyword">in</span> top_stats[:10]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">stat</span>)<br></code></pre></td></tr></table></figure><ul><li>显示最大内存块的回溯的代码：</li></ul><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import tracemalloc<br><br><span class="hljs-comment"># Store 25 frames</span><br>tracemalloc.<span class="hljs-built_in">start</span>(<span class="hljs-number">25</span>)<br><br><span class="hljs-comment"># ... run your application ...</span><br><br>snapshot = tracemalloc.take_snapshot()<br>top_stats = snapshot.statistics(<span class="hljs-string">&#x27;traceback&#x27;</span>)<br><br><span class="hljs-comment"># pick the biggest memory block</span><br>stat = top_stats[<span class="hljs-number">0</span>]<br>print(<span class="hljs-string">&quot;%s memory blocks: %.1f KiB&quot;</span> % (stat.count, stat.size / <span class="hljs-number">1024</span>))<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> <span class="hljs-keyword">in</span> stat.traceback.<span class="hljs-built_in">format</span>():<br>    print(<span class="hljs-built_in">line</span>)<br></code></pre></td></tr></table></figure></li><li><p>pyrasite: 牛逼的第三方库, 可以渗透进入正在运行的python进程动态修改里边的数据和代码(其实修改代码就是通过修改数据实现)</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>内存占用</tag>
      
      <tag>killed</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>新词发现</title>
    <link href="/2020/01/20/2020-01-20-%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/"/>
    <url>/2020/01/20/2020-01-20-%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="新词发现"><a href="#新词发现" class="headerlink" title="新词发现"></a>新词发现</h2><p>新词发现是 NLP 的基础任务之一，通过对已有语料进行挖掘，从中识别出新词。它主要是希望通过无监督发掘一些语言特征（主要是统计特征），来判断一批语料中哪些字符片段可能是一个新词。“新词发现”是一个比较通俗的叫法，更准确的叫法应该是“无监督构建词库”，因为原则上它能完整地构建一个词库出来，而不仅仅是“新词”。当然，你可以将它跟常用词库进行对比，删掉常见词，就可以得到新词了。</p><p>特定领域的专有名词也可归属于新词的范畴。何出此言呢？通常我们会很容易找到通用领域的词表，但要找到某个具体领域的专有名词则非常困难，因此特定领域的专有名词相对于通用领域的词语即为新词。换言之，“新”并非只是时间上的概念，同样可以迁移到领域或空间上。因此，新词发现不仅可以挖掘随时间变化而产生的新词，也可以<strong>挖掘不同领域的专有名词</strong>。</p><span id="more"></span><h3 id="新词发现的方法"><a href="#新词发现的方法" class="headerlink" title="新词发现的方法"></a>新词发现的方法</h3><p>目前主要有基于规则和基于统计两种方法：</p><ul><li>基于规则的方法：根据新词的构词特征或外型特征建立规则库、专业词库或模式库，然后通过规则匹配发现新词。</li><li>基于统计的方法<ul><li>有监督：有监督方法利用标注语料，将新词发现看作分类或者序列标注问题。</li><li>无监督：不依赖于任何已有的词库、分词工具和标注语料，仅仅根据词的共同特征，利用统计策略将一段大规模语料中可能成词的文本片段全部提取出来，然后再利用语言知识排除不是新词语的“无用片段”或者计算相关度，寻找相关度最大的字与字的组合。最后，把所有抽取得到的词和已有的词库进行比较，就能得到新词</li></ul></li></ul><h3 id="项目开源地址"><a href="#项目开源地址" class="headerlink" title="项目开源地址"></a>项目开源地址</h3><p>✨ <a href="https://github.com/bojone/word-discovery">https://github.com/bojone/word-discovery</a></p><p>✨ <a href="[https://github.com/hankcs/HanLP/wiki/%E6%96%B0%E8%AF%8D%E8%AF%86%E5%88%AB](https://github.com/hankcs/HanLP/wiki/新词识别">PyHanLP 新词识别</a>)</p><p>✨ <a href="https://github.com/blmoistawinde/HarvestText">HarvestText: 领域自适应文本挖掘工具（新词发现、情感分析、实体链接等）√</a></p><h3 id="语言模型简介"><a href="#语言模型简介" class="headerlink" title="语言模型简介"></a>语言模型简介</h3><p><strong>语言模型</strong>是计算条件概率</p><script type="math/tex; mode=display">p(wn|w1,w2,…,wn−1)p(wn|w1,w2,…,wn−1)</script><p>的模型，其中$w_1,w_2,…,w_{n−1}$是句子中的前$n−1$个词（或字），$w_n$是第$n$个词（或字）。语言模型在很多方面都有应用，比如说分词、语音识别、机器翻译等。为了得到语言模型，有很多方法，比如说最简单的是“统计+平滑”的方法，还有最大熵语言模型、CRF语言模型等，而当前深度学习框架下研究得很多的是“神经网络语言模型”，它的大概思路是：==$p(w_n|w_1,w_2,…,w_{n−1})$是关于$w1,w2,…,wn$的一个函数，这个函数的具体形式我不知道，所以利用神经网络去拟合它，为了更好地拟合，并且减少模型参数，还把词语“嵌入”到实数空间中，用短向量来表示词语，跟语言模型一起训练。从这个角度看，词向量只是语言模型的副产品。==</p><p>语言模型生成的词向量能够较好地表示语义，这是很有趣的，却也是在情理之中。什么是语义？对人类来说，语义是一种推理和理解的过程，而我们的语言模型，就是从前$n−1$个字推测下一个字，这也是一个推理的过程。既然包含了推理成分在里边，就有可能捕捉到语义了。</p><h2 id="详细的算法"><a href="#详细的算法" class="headerlink" title="详细的算法"></a>详细的算法</h2><p>完整的算法步骤如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs vim">第一步，统计：选取某个固定的<span class="hljs-keyword">nn</span>，统计<span class="hljs-number">2</span>grams、<span class="hljs-number">3</span>grams、…、ngrams，计算它们的内部凝固度，只保留高于某个阈值的片段，构成一个集合GG；这一步，可以为<span class="hljs-number">2</span>grams、<span class="hljs-number">3</span>grams、…、ngrams设置不同的阈值，不一定要相同，因为字数越大，一般来说统计就越不充分，越有可能偏高，所以字数越大，阈值要越高；<br><br>第二步，切分：用上述grams对语料进行切分（粗糙的分词），并统计频率。切分的规则是，只有一个片段出现在前一步得到的集合GG中，这个片段就不切分，比如“各项目”，只要“各项”和“项目”都在GG中，这时候就算“各项目”不在GG中，那么“各项目”还是不切分，保留下来；<br><br>第三步，回溯：经过第二步，“各项目”会被切出来（因为第二步保证宁放过，不切错）。回溯就是检查，如果它是一个小于等于<span class="hljs-keyword">nn</span>字的词，那么检测它在不在GG中，不在就出局；如果它是一个大于<span class="hljs-keyword">nn</span>字的词，那个检测它每个<span class="hljs-keyword">nn</span>字片段是不是在GG中，只要有一个片段不在，就出局。还是以“各项目”为例，回溯就是看看，“各项目”在不在<span class="hljs-number">3</span>gram中，不在的话，就得出局。<br></code></pre></td></tr></table></figure><p>每一步的补充说明：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs">1、较高的凝固度，但综合考虑多字，是为了更准，比如两字的“共和”不会出现在高凝固度集合中，所以会切开（比如“我一共和三个人去玩”，“共和”就切开了），但三字“共和国”出现在高凝固度集合中，所以“中华人民共和国”的“共和”不会切开；<br><br>2、第二步就是根据第一步筛选出来的集合，对句子进行切分（你可以理解为粗糙的分词），然后把“粗糙的分词结果”做统计，注意现在是统计分词结果，跟第一步的凝固度集合筛选没有交集，我们认为虽然这样的分词比较粗糙，但高频的部分还是靠谱的，所以筛选出高频部分；<br><br>3、第三步，例如因为“各项”和“项目”都出现高凝固度的片段中，所以第二步我们也不会把“各项目”切开，但我们不希望“各项目”成词，因为“各”跟“项目”的凝固度不高（“各”跟“项”的凝固度高，不代表“各”跟“项目”的凝固度高），所以通过回溯，把“各项目”移除（只需要看一下“各项目”在不在原来统计的高凝固度集合中即可，所以这步计算量是很小的）<br></code></pre></td></tr></table></figure><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><ol><li><p>Linux环境CentOS系统 kenlm 的自行编译</p><p><code>kenlm</code> 是一个C++编写的语言模型工具，具有速度快、占用内存小的特点，也提供了Python接口。使用了语言模型工具<a href="https://github.com/kpu/kenlm/">kenlm</a>的 <code>count_ngrams</code> 程序来统计ngram。</p><p>首先，安装必要的包：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">sudo yum <span class="hljs-keyword">install </span><span class="hljs-keyword">boost</span><br><span class="hljs-keyword"></span>sudo yum <span class="hljs-keyword">install </span><span class="hljs-keyword">boost-devel</span><br><span class="hljs-keyword"></span>sudo yum <span class="hljs-keyword">install </span>libboost-program-options-dev<br>sudo yum <span class="hljs-keyword">install </span>zlib<br>sudo yum <span class="hljs-keyword">install </span>zlib-devel<br>sudo yum <span class="hljs-keyword">install </span>libbz2-dev<br>sudo yum <span class="hljs-keyword">install </span>lzma-devel<br>sudo yum <span class="hljs-keyword">install </span><span class="hljs-keyword">eigen3</span><br><span class="hljs-keyword"></span>sudo yum <span class="hljs-keyword">install </span>xz-devel<br>sudo yum <span class="hljs-keyword">install </span><span class="hljs-keyword">bzip2-devel</span><br></code></pre></td></tr></table></figure><p>然后下载&amp;解压 kenlm.tar.gz工具包</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">wget -O - https:<span class="hljs-regexp">//</span>kheafield.com<span class="hljs-regexp">/code/</span>kenlm.tar.gz |tar xz<br></code></pre></td></tr></table></figure><p>进入子目录进行编译（有用的脚本在<code>build/bin</code>之中）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p kenlm/build<br><span class="hljs-built_in">cd</span> kenlm/build<br>cmake ..<br>make -j 4<br></code></pre></td></tr></table></figure><p>python库的安装方式：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">pip install https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/kpu/</span>kenlm<span class="hljs-regexp">/archive/m</span>aster.zip<br></code></pre></td></tr></table></figure><p>==把生成的 build/bin/count_ngrams 放到跟 word_discovery.py 同一目录下==</p><p>测试是否可运行：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">./count_ngrams -o <span class="hljs-number">4</span> <span class="hljs-attr">--memory</span>=<span class="hljs-number">50%</span> <span class="hljs-attr">--write_vocab_list</span> thucnews<span class="hljs-selector-class">.chars</span> &lt;thucnews<span class="hljs-selector-class">.corpus</span> &gt;thucnews.ngrams<br></code></pre></td></tr></table></figure></li></ol><ul><li><p>kenlm 的简单使用</p><p>训练一个==4-gram==的语言模型（训练的文本分好词（用空格隔开，如果你是做基于字的模型，就把模型的每个字用空格隔开））：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">python p.py|.<span class="hljs-regexp">/kenlm/</span>bin/lmplz -o <span class="hljs-number">4</span> &gt; weixin.arpa<br><span class="hljs-comment"># 压缩模型为二进制，方便模型快速加载：</span><br>.<span class="hljs-regexp">/kenlm/</span>bin/build_binary weixin.arpa weixin.klm<br></code></pre></td></tr></table></figure><p>简单使用：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">import kenlm<br><br>model = kenlm.Model(<span class="hljs-emphasis">&#x27;weixin.klm&#x27;</span>)<br>model.score(<span class="hljs-emphasis">&#x27;微 信&#x27;</span>, bos=False, eos=False)<br>&#x27;&#x27;&#x27;<br>score函数输出的是对数概率，即log10(p(<span class="hljs-emphasis">&#x27;微 信&#x27;</span>))，其中字符串可以是gbk，也可以是utf-8<br>bos=False, eos=False意思是不自动添加句首和句末标记符<br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure></li></ul><ol><li>在第二次遍历词库以得到候选词的时候，使用了<code>Trie</code> 树结构来加速搜索字符串是否出现过某个ngram。Trie树或者其变种基本上是所有基于词典的分词工具的标配，就是因为它可以加快搜索字符串中是否出现过词典中的词。</li></ol><h3 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h3><ul><li>逐句地把原始语料给yield出来</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">text_generator</span>():<br></code></pre></td></tr></table></figure><ul><li>参数配置</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">min_count = <span class="hljs-number">32</span><br>order = <span class="hljs-number">4</span><br></code></pre></td></tr></table></figure><p>顺便提一下，因为是无监督训练，语料一般都是越大越好，几百M到几个G都可以，==但其实如果你只要几M的语料（比如一部小说），也可以直接测试，也能看到基本的效果（但可能要修改下面的参数）==</p><ul><li>用结巴分词加载这个得到的词库（不用它自带的词库，并且关闭新词发现功能），这就构成了一个基于无监督词库的分词工具</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/weixin_43378396/article/details/103848628">新词发现</a></p><p><a href="https://github.com/mattzheng/py-kenlm-model">Linux环境 kenlm 的自行编译</a></p><p><a href="https://spaces.ac.cn/archives/6920">重新写了之前的新词发现算法：更快更好的新词发现</a></p><p><a href="https://spaces.ac.cn/archives/3956">《【中文分词系列】 5. 基于语言模型的无监督分词》</a></p><p><a href="https://spaces.ac.cn/archives/4256">《【中文分词系列】 8. 更好的新词发现算法》</a></p><p><a href="https://blog.csdn.net/mingzai624/article/details/79560063">使用kenlm工具训练统计语言模型</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NL2SQL</title>
    <link href="/2019/12/30/2019-12-30-NL2SQL/"/>
    <url>/2019/12/30/2019-12-30-NL2SQL/</url>
    
    <content type="html"><![CDATA[<h1 id="NL2SQL"><a href="#NL2SQL" class="headerlink" title="NL2SQL"></a>NL2SQL</h1><h2 id="NL2SQL任务"><a href="#NL2SQL任务" class="headerlink" title="NL2SQL任务"></a>NL2SQL任务</h2><p>NL2SQL(Natural Language to SQL)， 顾名思义，是一项将自然语言转为可执行 SQL 语句的技术。它可以充当数据库的智能接口，让不熟悉数据库的用户能够快速地找到自己想要的数据，对改善用户与数据库之间的交互方式有很大意义。 </p><span id="more"></span><h2 id="NL2SQL-的价值"><a href="#NL2SQL-的价值" class="headerlink" title="NL2SQL 的价值"></a>NL2SQL 的价值</h2><p>当前，大量信息存储在结构化和半结构化知识库中，如数据库。对于这类数据的分析和获取需要通过SQL等编程语言与数据库进行交互操作，SQL的使用难度限制了非技术用户，给数据分析和使用带来了较高的门槛。人们迫切需要技术或工具完成自然语言与数据库的交互，因此诞生了Text-to-SQL任务。NL2SQL图示如下：</p><p><img src="https://img-blog.csdnimg.cn/2019102321315989.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RyZWFtX2FuZ2VsX1o=,size_16,color_FFFFFF,t_70" alt=""></p><p>我们通过图1中的实例来介绍一下Text-to-SQL任务。该任务包含两部分：<strong>Text-to-SQL解析器和SQL执行器</strong>。</p><p>解析器的输入是给定的数据库和针对该数据库的问题，输出是问题对应的SQL查询语句，如图中红色箭头标示。SQL执行器在数据库上完成该查询语句的执行，及给出问题的最终答案，如图中绿色箭头标示。</p><p>SQL执行器有很多成熟的系统，如MySQL，SQLite等，该部分不是本文重点。本文主要介绍解析器，学术界中Text-to-SQL任务默认为Text-to-SQL解析模型。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/uYIC4meJTZ1ibzoXCwJRS7QlR6Utmb5GIbKs1btFoLdpu8f154fwNGcke8pp0QrPf8ZI7CR4icx9jBhx87McxaicA/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>根据SQL的构成，解析器需要完成两个任务，即“<strong>问题与数据库的映射</strong>”和“<strong>SQL生成</strong>”。</p><p>在问题与数据库的映射中，需要找出问题依赖的表格以及具体的列，如图1实例中，问题“绿化率前5的城市有哪些，分别隶属于哪些省？”依赖的数据库内容包括：表格“中国城市”，具体的列“名称”、“所属省”、“绿化率”（SQL查询语句蓝色标注成分）。</p><p>在SQL生成中，结合第一步识别结果以及问题包含信息，生成满足语法的SQL查询语句，如实例中的“Select 名称,所属省 From 中国城市 Where 绿化率 &gt; 30%”。</p><h2 id="Text-to-SQL研究进展"><a href="#Text-to-SQL研究进展" class="headerlink" title="Text-to-SQL研究进展"></a>Text-to-SQL研究进展</h2><p>我们接下来将从<strong>相关数据集</strong>和<strong>模型</strong>两方面介绍该技术的研究进展。</p><h3 id="1、数据集介绍"><a href="#1、数据集介绍" class="headerlink" title="1、数据集介绍"></a>1、数据集介绍</h3><p>图2给出了Text-to-SQL数据集发展趋势，代表数据集参见表1。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/uYIC4meJTZ1ibzoXCwJRS7QlR6Utmb5GIiaMiaicoul62bjz1ibBrT2xOYu7NlDFamXo2RMZOVbNVQCjBzIxzSWk0cg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>目前，NL2SQL 方向已经有 WikiSQL、Spider、WikiTableQuestions、ATIS 等诸多公开数据集，如图所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/uYIC4meJTZ1ibzoXCwJRS7QlR6Utmb5GIkdvyInbQBPUGPo6ic2XK3bNAkxDPYic0GOymw1WaJicAtJzd7IetPvNJg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>不同数据集都有各自的特点，这里简单介绍一下这四个数据集:</p><ul><li><a href="https://github.com/salesforce/WikiSQL">WikiSQL</a>：该数据集是Salesforce在2017年提出的大型标注NL2SQL数据集，也是目前规模最大的NL2SQL数据集。它包含了 24,241张表，80,645条自然语言问句及相应的SQL语句。目前学术界的预测准确率可达91.8%。 </li><li><a href="https://yale-lily.github.io/spider">Spider</a>：Spider数据集是耶鲁大学于2018年新提出的一个较大规模的NL2SQL数据集。该数据集包含了10,181条自然语言问句，分布在200个独立数据库中的5,693条SQL，内容覆盖了138个不同的领域。虽然在数据数量上不如WikiSQL，但Spider引入了更多的SQL用法，例如Group By、Order By、Having等高阶操作，甚至需要Join不同表，更贴近真实场景，所以难度也更大。目前准确率最高只有54.7%</li><li><a href="https://github.com/ppasupat/WikiTableQuestions">WikiTableQuestions</a> 是斯坦福大学于 2015 年提出的一个针对维基百科中那些半结构化表格问答的数据集，包含了 22,033 条真实问句以及 2,108 张表格。 </li><li><a href="https://www.kaggle.com/siddhadev/ms-cntk-atis">The Air Travel Information System (ATIS)</a> 是一个年代较为久远的经典数据集，由德克萨斯仪器公司在 1990 年提出。该数据集获取自关系型数据库 Official Airline Guide (OAG, 1990)，包含 27 张表以及不到 2,000 次的问询，每次问询平均 7 轮，93% 的情况下需要联合 3 张以上的表才能得到答案，问询的内容涵盖了航班、费用、城市、地面服务等信息。 </li><li>中文数据集目前只有追一科技在天池发布的比赛数据集，包括4万条有标签数据作为训练集，1万条无标签数据作为测试集。目前比赛第一名的成绩，准确率达到了92%。 </li></ul><h3 id="2、模型介绍"><a href="#2、模型介绍" class="headerlink" title="2、模型介绍"></a>2、模型介绍</h3><p>SQL查询语句是一个符合语法、有逻辑结构的序列，其构成来自三部分：<strong>数据库、问题、SQL关键词</strong>。</p><p>在当前深度学习研究背景下，Text-to-SQL任务可被看作是一个类似于神经机器翻译的序列到序列的生成任务，主要采用Seq2Seq模型框架。基线Seq2Seq模型加入注意力、拷贝等机制后，在单领域数据集上可以达到80%以上的准确率，但在多领域数据集上效果很差，准确率均低于25%。</p><p>从<strong>编码</strong>和<strong>解码</strong>两个方面进行原因分析：</p><ul><li><p>在编码阶段，问题与数据库之间需要形成很好的对齐或映射关系，即问题中涉及了哪些表格中的哪些元素（包含列名和表格元素值）；同时，问题与SQL语法也需要进行映射，即问题中词语触发了哪些关键词操作（如Group、Order、Select、Where等）、聚合操作（如Min、Max、Count等）等；最后，问题表达的逻辑结构需要表示并反馈到生成的SQL查询语句上，逻辑结构包括嵌套、多子句等。</p></li><li><p>在解码阶段，SQL语言是一种有逻辑结构的语言，需要保证其语法合理性和可执行性。普通的Seq2Seq框架并不具备建模这些信息的能力。</p></li></ul><p>当前基于Seq2Seq框架，主要有以下几种改进：</p><p>1）基于Pointer Network的改进</p><p>首先，SQL组成来自三部分：<strong>数据库中元素</strong>（如表名、列名、表格元素值）、<strong>问题中词汇</strong>、 <strong>SQL关键字</strong>。其次，当前公开的多领域数据集为了验证模型数据库无关，在划分训练集和测试集时要求数据库无交叉，这种划分方式导致测试集数据库中很大比例的元素属于未登录词。传统的Seq2Seq模型是解决不好这类问题的。</p><p>Pointer Network很好地解决了这一问题，其输出所用到的词表是随输入而变化的。具体做法是利用注意力机制，直接从输入序列中选取单词作为输出。在Text-to-SQL任务中，将问题中词汇、SQL关键词、对应数据库的所有元素作为输入序列，利用Pointer Network从输入序列中拷贝单词作为最终生成SQL的组成元素。</p><p>由于Pointer Network可以较好的满足具体数据库无关这一要求，在多领域数据集上的模型大多使用该网络，如Seq2SQL[1]、STAMP[8]、Coarse2Fine[9] 、IRNet[16]等模型。</p><p>2）基于Sequence-to-set的改进</p><p>在简单问题对应的数据集合上，其SQL查询语句形式简单（仅包含Select和Where关键词），为了解决Seq2Seq模型中顺序错误带来的影响（如“条件1 And 条件2”，预测为“条件2 And 条件1”，属于顺序错误，但对应的SQL是正确的），SQLNet[10]提出了Sequence-to-set模型，基于所有的列预测其属于哪个关键词（即属于Select还是Where，在SQLNet模型中仅预测是否属于Where），针对SQL 中每一个关键词选择概率最高的前K个列。</p><p>该模式适用于SQL形式简单的数据集，在WikiSQL和NL2SQL这两个数据集合上使用较多，且衍生出很多相关模型，如TypeSQL[11]、SQLova[12]、X-SQL[13]等。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/uYIC4meJTZ1ibzoXCwJRS7QlR6Utmb5GIRLdl9FQx3ibetkR5lYYX3xf6yCiaNmvoassJmlPqQ1w2br3nz2LX2Kag/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h3 id="3、评价方法"><a href="#3、评价方法" class="headerlink" title="3、评价方法"></a>3、评价方法</h3><p>Text-to-SQL任务的评价方法主要包含两种：<strong>精确匹配率</strong>（Exact Match, Accqm）、<strong>执行正确率</strong>（Execution Accuracy, Accex）。</p><p>精确匹配率指,预测得到的SQL语句与标准SQL语句精确匹配成功的问题占比。为了处理由成分顺序带来的匹配错误，当前精确匹配评估将预测的SQL语句和标准SQL语句按着SQL关键词分成多个子句，每个子句中的成分表示为集合，当两个子句对应的集合相同则两个子句相同，当两个SQL所有子句相同则两个SQL精确匹配成功；</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/uYIC4meJTZ1ibzoXCwJRS7QlR6Utmb5GIukaiagjDUITXkw6gxMbHDgK0J394VQPZrfzcsmrpsX2ZIJj8WoPiaOvA/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>执行正确指，执行预测的SQL语句，数据库返回正确答案的问题占比。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/uYIC4meJTZ1ibzoXCwJRS7QlR6Utmb5GIVMyEAficegvnicoGtJJtwO7gnGH5E1tF8qQ9CTTsYaYIl7MMDkQ7crzA/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>目前仅WikiSQL数据集支持Acc<em>ex</em>，其他数据集仅支持Accqm。大部分数据集发布了对应的评估脚本，方便大家在同一个评估标准下进行算法研究。</p><h3 id="4、NL2SQL实现简述"><a href="#4、NL2SQL实现简述" class="headerlink" title="4、NL2SQL实现简述"></a>4、NL2SQL实现简述</h3><p>NL2SQL的基本思路是打造一条从自然语言到SQL的转换链路，多采用的是语义解析和规则的方式构建NL2SQL系统，其架构大体如下：</p><p><img src="https://ningshixian.github.io/resources/images/图片0.png" alt=""></p><ol><li>用户UI：用户交互界面，包括问句自动提示、可视化选择等功能。</li><li>Storage：Storage是数据存储中心，包括了Knowledge Graph、Intent words以及database schema信息。同时也会将用户的搜索log存储起来。</li><li>Parser：主要负责对user query进行parsing，包括了annotator(字符串匹配+NER+意图识别等一系列操作)、grammar（语法检测）、semantic predict（语义预测）、ranking（评分排序）。这部分会调用借助图谱来提高实体和关系识别精度。</li><li>Answering Engine：用于将语义解析的结果转换成SQL，包括表识别、查询语句生成器等，最终将结果（数据、可视化结果、SQL等）返回给user。</li></ol><h2 id="XSQL"><a href="#XSQL" class="headerlink" title="XSQL"></a>XSQL</h2><ul><li><strong>论文地址： <a href="https://arxiv.org/pdf/1908.08113">https://arxiv.org/pdf/1908.08113</a></strong> </li><li>X-SQL 的 Query Structure</li></ul><p><img src="https://ningshixian.github.io/resources/images/图片1.5.png" alt=""></p><ul><li>X-SQL是微软提出的NL2SQL模型，参考了SQLNet、SQLova等模型的思路.</li><li>X-SQL 的模型架构分为三层 (如下所示)：<ul><li>Encoder: 首先使用 bert 风格的预训练模型（MT-DNN） 对 Question 和 SQL 表格进行编码和特征提取， 得到上下文输出来增强结构模式表示，并结合类型信息;</li><li>Context Reinforcing Layer:  该结构用于增强在 equence Encoder （序列编码器）得到的 H_[CTX]， 从而得到增强的语义表示 HCi ；</li><li>Output Layer:  输出层完成 SQL 语句的生成，我们将其分为 6 个子任务（select-column, select-aggregation, where-number, where-column, where-operator, and where-value），这六个任务彼此之间相互结合，彼此制约。 </li></ul></li></ul><p><img src="https://ningshixian.github.io/resources/images/图片1.png" alt=""></p><h3 id="X-SQL模型—Encoder"><a href="#X-SQL模型—Encoder" class="headerlink" title="X-SQL模型—Encoder"></a>X-SQL模型—Encoder</h3><p>X-SQL使用了一个BERT-Like模型MT-DNN作为Token Sequence Encoder，同时进行以下修改：</p><ul><li>引入了一个特殊空列<code>[EMPTY]</code>的概念，并将其追加到每个Table Schema后面；</li><li>BERT中的<code>Segment Embedding</code>被替换为<code>Type Embedding</code>，包括：Question 、Categorial Column、Numerical Column、Special Empty Column 这四种类型；</li><li>此外，单纯地将标志位<code>[CLS]</code>重命名为<code>[CTX]</code></li></ul><p><img src="https://ningshixian.github.io/resources/images/图片2.png" alt=""></p><h4 id="X-SQL模型—Context-Reinforcing-Layer"><a href="#X-SQL模型—Context-Reinforcing-Layer" class="headerlink" title="X-SQL模型—Context Reinforcing Layer"></a>X-SQL模型—Context Reinforcing Layer</h4><ul><li>对上一层Encoder的输出 H_[CTX] 进一步“强化”，从而为表的每一列学习新的表示h_Ci</li><li>这种强化体现在两方面：<ul><li>由于Column Tokens的输出长短不一，所以需要对Column Embeddings进行“调整”；</li><li>此外为了加强Context的影响，在Alignment Model中会使用到Context Embedding的信息；</li></ul></li></ul><p><img src="https://ningshixian.github.io/resources/images/图片3.jpg" alt=""></p><p>通过这种方式可以捕获到哪一个查询词与哪一列最相关，从而得到增强的语义表示 HCi 。它的计算过程如下所示： </p><script type="math/tex; mode=display">s_{it}=f(Uh_{[CTX]}/\sqrt{d},Vh_{C_{it}}/\sqrt{d})</script><script type="math/tex; mode=display">a_{it}=softmax(s_{it})</script><script type="math/tex; mode=display">h_{C_i}=\sum_{t=1}^{n_i}{a_{it}}{h_{C_{it}}}</script><p>其中，$f$只是普通的点乘 (dot-product) 操作。最后，针对每一列 (schema representation h_Ci)，独立的使用下面的子网络结构得到使用融合 h_Ci 和 hctx 的 <code>rCi</code>，公式如下所示：</p><script type="math/tex; mode=display">r_{C_{i}}=LayerNorm(U^`h_{[CTX]}+V^`h_{C_{i}})</script><h4 id="X-SQL模型—Output-Layer"><a href="#X-SQL模型—Output-Layer" class="headerlink" title="X-SQL模型—Output Layer"></a>X-SQL模型—Output Layer</h4><p>Output层由六个子分类器构成：</p><ul><li>S-COL：查询表的哪一列 </li><li>S-AGG： 使用什么聚合函数 </li><li><strong>W-NUM</strong>： 查找where子句的数量（四个可能标签的分类，每个标签表示1~4 ）</li><li>W-COL：选择表的哪几列</li><li>W-OP： 对这几列的操作符 </li><li>W-VAL： 这几列的值 </li></ul><p>其中五个子模型的输出会被填到QueryStructure中。w-col、w-op 和 w-val 这三个任务是依赖 w-num 的，因为 w-num 决定了对几列进行约束，这样它们三个只需要取 softmax 最大的那几个。</p><p><img src="https://ningshixian.github.io/resources/images/图片4.png" alt=""></p><p>每个子模型的说明：</p><p><img src="https://ningshixian.github.io/resources/images/图片5.png" alt=""></p><p>在训练过程中，我们优化目标是所有单个子任务损失的总和。</p><p><a href="https://tianchi.aliyun.com/markets/tianchi/zhuiyi_cn">追一NL2SQL天池任务</a></p><h4 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h4><blockquote><p>首届中文NL2SQL挑战赛，使用金融以及通用领域的表格数据作为数据源，提供在此基础上标注的自然语言与SQL语句的匹配对，希望选手可以利用数据训练出可以准确转换自然语言到SQL的模型。</p></blockquote><p>模型的输入为一个 Question + Table，输出一个 SQL 结构，该 SQL 结构对应一条 SQL 语句。示例如下图黄色部分，需要预测的有4部分：</p><ul><li>挑选的列(sel)</li><li>列上的聚合函数(agg)</li><li>筛选的条件(conds)</li><li>条件间的关系(cond_conn_op)</li></ul><p><img src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/157129841401120721571298414104.png" alt=""></p><h4 id="任务数据"><a href="#任务数据" class="headerlink" title="任务数据"></a>任务数据</h4><ul><li><p>4万条有标签数据作为训练集</p></li><li><p>1万条无标签数据作为测试集</p></li></ul><h4 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h4><ul><li><strong>全匹配率 (Logic Form Accuracy)</strong>: 预测完全正确的SQL语句。其中，列的顺序并不影响准确率的计算。</li><li><strong>执行匹配率 (Execution Accuracy)</strong>: 预测的SQL的执行结果与真实SQL的执行结果一致。</li></ul><p>官方的评估指标是 <code>(全匹配率 + 执行匹配率) / 2</code>，也就是说你有可能写出跟标注答案不同的SQL语句，但执行结果是一致的，这也算正确一半。 </p><h4 id="难点分析"><a href="#难点分析" class="headerlink" title="难点分析"></a>难点分析</h4><ul><li>不限制使用表格内容信息</li><li>存在conds value不能从question提取的样本</li><li>select agg存在多项</li><li>没有conds缺失的样本</li></ul><h2 id="第一名解决方案M-SQL"><a href="#第一名解决方案M-SQL" class="headerlink" title="第一名解决方案M-SQL"></a>第一名解决方案M-SQL</h2><ol><li><strong>将原始的 Label 做一个简单的变换</strong></li></ol><p><img src="https://github.com/beader/tianchi_nl2sql/raw/master/imgs/label.png" alt=""></p><ol><li><strong>对输入进行改造</strong></li></ol><p>将 Question 与 Header 顺序连接， 其中每一个表头也视为一个句子，用<code>[CLS]***[SEP]</code>括住。</p><ol><li><strong>M-SQL 模型架构</strong></li></ol><p>第一个<code>[CLS]</code>对应的向量，我们可以认为是整个问题的句向量，我们用它来预测<code>XXX</code>的连接符。后面的每个[CLS]对应的向量，我们认为是每个表头的编码向量，我们把它拿出来，用来预测该表头表示的列是否应该被select .  现在就剩下比较复杂的conds了，就是<code>where col_1 == value_1</code>这样子的，col_1、value_1以及运算符==都要找出来。 </p><p><strong>两个较特殊的改进：</strong></p><ul><li>W-col-val模型<ul><li>相较于X-SQL的W-VAL预测Value的首尾偏移量，W-col-val改用了序列标注的方式提取Where Value；</li></ul></li><li>W-val-match模型<ul><li>提取各列的 Distinct Value Set，基于Matching的方式匹配最可能的【列-值】；<ul><li>基于文本的匹配：rouge-L、编辑距离等；</li><li>基于语义的匹配：LR、MLP、相似度计算等</li></ul></li></ul></li></ul><p><img src="https://ningshixian.github.io/resources/images/图片6.png" alt=""></p><h4 id="M-SQL在X-SQL的基础上进行了调整和扩展-对比"><a href="#M-SQL在X-SQL的基础上进行了调整和扩展-对比" class="headerlink" title="M-SQL在X-SQL的基础上进行了调整和扩展(对比)"></a><strong>M-SQL在X-SQL的基础上进行了调整和扩展</strong>(对比)</h4><ul><li>Encoder：<ul><li>预训练模型更换（MT-DNN → Bert-wwm-ext）;</li><li>Token/TokenType调整；</li></ul></li><li>Column Representation：基本和X-SQL一致；</li><li>Sub-models：<ul><li>增加S-num辅助任务</li><li>合并W-NUM和Reducer预测；</li><li>将原本的W-VAL换成了W-col-val和W-val-match；</li><li>(待定) 把sel与agg结合，当作多分类问题</li></ul></li></ul><p><img src="https://ningshixian.github.io/resources/images/图片7.png" alt=""></p><h2 id="现阶段的问题"><a href="#现阶段的问题" class="headerlink" title="现阶段的问题"></a>现阶段的问题</h2><ul><li>WikiSQL 数据集的数据形式和功能薄弱<ul><li>条件的表达只支持最基础的 <code>&gt;、&lt;、=</code></li><li>条件之间的关系只有 <code>and</code> </li><li>不支持聚组、排序、嵌套等其它众多常用的 SQL 语法 </li><li>不需要联合多表查询答案，真实答案所在表格已知 </li><li>这样的数据集并不符合真实的应用场景 </li></ul></li><li>Query Structure中的元素不存在递归关系</li><li>天池TableQA（5w，中文），样本量少，且结构简单</li></ul><p>如何更好地结合数据库信息来理解并表达用户语句的语义、如何编码及表达数据库的信息、如何生成复杂却有必要的 SQL 语句，此类挑战还有很多需要解决，它们都是非常值得探索的方向。 </p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p> 现在Bert可用的中文预训练权重有两个</p><ul><li>一个是<a href="https://github.com/google-research/bert">官方版</a>的</li><li>一个是<a href="https://github.com/ymcui/Chinese-BERT-wwm">哈工大版</a>的</li></ul><p>两个的最终效果差不多，但是哈工大版的收敛更快。 </p><h3 id="复现过程"><a href="#复现过程" class="headerlink" title="复现过程"></a>复现过程</h3><p><strong>Part1: 预处理</strong></p><p>首先对question进行了格式统一,具体如下:</p><ol><li>百分数转换，例如<strong>百分之10</strong>转化为<strong>10%</strong>,<strong>百分之十</strong>转换为<strong>10%</strong>, 百分之一点五转化为<strong>1.5%</strong></li><li>大写数字转阿拉伯数字，例如<strong>十二</strong>转换为<strong>12</strong>; <strong>二十万</strong>转化为<strong>200000</strong>; 一点二转化为<strong>1.2</strong></li><li>年份转换，如将12年转换为2012年</li><li>利用规则与编辑距离对query进行修正</li><li>query信息标记：[CLS] -&gt; [XLS]</li></ol><p>然后，SQL分析</p><ul><li>常用的SQL模式分析</li></ul><p><strong>Part2:模型介绍</strong></p><ul><li>预训练模型：Bert-wwm-ext，哈工大</li><li>W-num，W-op融合</li></ul><p><strong>Part3: 后处理</strong></p><p><strong>Part4: 模型效果评估</strong></p><p><strong>Part5: 网络/应用</strong></p><p>通过云服务构建服务器后，构建可与各种DBMS链接的网页</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p> <a href="https://kexue.fm/archives/6771">基于Bert的NL2SQL模型：一个简明的Baseline</a> </p><p> RAdam 优化器的实现 <a href="https://github.com/bojone/keras_radam/blob/master/radam.py">keras_radam</a>  </p><p> CyberZHG 大神的开源项目 <a href="https://github.com/CyberZHG/keras-bert">keras-bert</a> </p><p> 哈工大讯飞联合实验室的 <a href="https://github.com/ymcui/Chinese-BERT-wwm">Chinese-BERT-wwm</a> 项目 </p><p><a href="https://blog.csdn.net/baidu_33512336/article/details/93531497">追一：NL2SQL的发展</a></p><p><a href="https://blog.csdn.net/Dream_angel_Z/article/details/102711508">NL2SQL概述：一文了解NL2SQL</a></p><p><a href="https://www.chainnews.com/articles/180724030413.htm">自然语言到 SQL 语句，微软只用六个子任务，结果超越人类水平</a></p><p><a href="https://github.com/beader/tianchi_nl2sql">追一科技首届中文NL2SQL挑战赛决赛第3名方案+代码</a></p><p>X-SQL，X-SQL: REINFORCE CONTEXT INTO SCHEMA REPRESENTATION</p><p>M-SQL，M-SQL:Muti-Schema Representation Learning for Single-Table Text2SQL Generation（修改中…）</p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python时间和日期操作</title>
    <link href="/2019/12/18/2019-12-18-Python%E6%97%B6%E9%97%B4%E5%92%8C%E6%97%A5%E6%9C%9F%E6%93%8D%E4%BD%9C/"/>
    <url>/2019/12/18/2019-12-18-Python%E6%97%B6%E9%97%B4%E5%92%8C%E6%97%A5%E6%9C%9F%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="Python时间和日期操作"><a href="#Python时间和日期操作" class="headerlink" title="Python时间和日期操作"></a>Python时间和日期操作</h1><blockquote><p>Python中，对日期和时间的操作，主要使用这3个内置模块： datetime 、 time 和 calendar</p></blockquote><p>导入需要的包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> arrow<br><span class="hljs-keyword">import</span> time, calendar<br><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br></code></pre></td></tr></table></figure><span id="more"></span><p>获取当前时间</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">str</span><span class="hljs-params">(datetime.now()</span></span>)<br></code></pre></td></tr></table></figure><p>获取两个代码位置在执行时的时间差</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">before = <span class="hljs-selector-tag">time</span><span class="hljs-selector-class">.time</span>()<br><span class="hljs-function"><span class="hljs-title">func1</span><span class="hljs-params">()</span></span><br>after = <span class="hljs-selector-tag">time</span><span class="hljs-selector-class">.time</span>()<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(f’调用func1，花费时间&#123;before-after&#125;’)</span></span><br></code></pre></td></tr></table></figure><p>格式化日期（指定输出的时间格式）</p><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs perl">dayTime=（<span class="hljs-string">&#x27;2018-01-14 12:00:00&#x27;</span>）<br>dayTime1= datetime.strptime(dayTime,<span class="hljs-string">&#x27;%Y-%m-%d %a %H:%M:%S&#x27;</span>).strftime(<span class="hljs-string">&quot;%w&quot;</span>)<br>dayTime2= datetime.datetime(<span class="hljs-number">2018</span>,<span class="hljs-number">1</span>,<span class="hljs-number">14</span>).strftime(<span class="hljs-string">&quot;%w&quot;</span>)<br><br><span class="hljs-comment"># 格式化成2016-03-20 11:45:39形式</span><br><span class="hljs-keyword">print</span> time.strftime(<span class="hljs-string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>, time.localtime()) <br><span class="hljs-comment"># 格式化成Sat Mar 28 22:24:24 2016形式</span><br><span class="hljs-keyword">print</span> time.strftime(<span class="hljs-string">&quot;%a %b %d %H:%M:%S %Y&quot;</span>, time.localtime()) <br></code></pre></td></tr></table></figure><p>数字表示的时间转化为字符串表示</p><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cos">time.strftime(&#x27;<span class="hljs-built_in">%Y</span><span class="hljs-built_in">%m</span><span class="hljs-built_in">%d</span> <span class="hljs-built_in">%H</span>:<span class="hljs-built_in">%M</span>:<span class="hljs-built_in">%S</span>&#x27;,time.localtime(<span class="hljs-number">1434502529</span>))<br></code></pre></td></tr></table></figure><p>获得指定时间字符串对应<strong>星期几</strong></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">thatDay</span> = <span class="hljs-string">&quot;2018-6-24&quot;</span><br><span class="hljs-attr">week</span> = datetime.strptime(thatDay,<span class="hljs-string">&#x27;%Y-%m-%d&#x27;</span>).strftime(<span class="hljs-string">&quot;%w&quot;</span>)<br><span class="hljs-comment"># week= datetime.datetime(2018,6,24).strftime(&quot;%w&quot;)</span><br></code></pre></td></tr></table></figure><h3 id="Arrow-介绍"><a href="#Arrow-介绍" class="headerlink" title="Arrow 介绍"></a>Arrow 介绍</h3><p>arrow是一个提供了更易懂和友好的方法来创建、操作、格式化和转化日期、时间和时间戳的python库。可以完全替代datetime，支持python2和3</p><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><p>以当前时间获取arrow对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> arrow<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>cur = arrow.now()<br><span class="hljs-meta">&gt;&gt;&gt; </span>cur<br>&lt;Arrow [<span class="hljs-number">2017</span>-02-04T13:<span class="hljs-number">47</span>:<span class="hljs-number">58.114342</span>+08:<span class="hljs-number">00</span>]&gt;<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>cur.timestamp<br><span class="hljs-meta">&gt;&gt;&gt; </span>cur.year<br><span class="hljs-meta">&gt;&gt;&gt; </span>cur.month<br><span class="hljs-meta">&gt;&gt;&gt; </span>cur.day<br><span class="hljs-meta">&gt;&gt;&gt; </span>cur.hour<br><span class="hljs-meta">&gt;&gt;&gt; </span>cur.minute<br><span class="hljs-meta">&gt;&gt;&gt; </span>cur.second<br><span class="hljs-meta">&gt;&gt;&gt; </span>cur.week<br></code></pre></td></tr></table></figure><p>以指定时间戳获取arrow对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>arrow.get(<span class="hljs-string">&#x27;1586782011&#x27;</span>)<br>&lt;Arrow [<span class="hljs-number">2020</span>-04-13T12:<span class="hljs-number">46</span>:<span class="hljs-number">51</span>+<span class="hljs-number">00</span>:<span class="hljs-number">00</span>]&gt;<br><span class="hljs-meta">&gt;&gt;&gt; </span>arrow.get(<span class="hljs-string">&#x27;2017-01-05&#x27;</span>)<br>&lt;Arrow [<span class="hljs-number">2017</span>-01-05T00:<span class="hljs-number">00</span>:<span class="hljs-number">00</span>+<span class="hljs-number">00</span>:<span class="hljs-number">00</span>]&gt;<br><span class="hljs-meta">&gt;&gt;&gt; </span>arrow.get(<span class="hljs-string">&#x27;2017.01.05&#x27;</span>)<br>&lt;Arrow [<span class="hljs-number">2017</span>-01-05T00:<span class="hljs-number">00</span>:<span class="hljs-number">00</span>+<span class="hljs-number">00</span>:<span class="hljs-number">00</span>]&gt;<br><span class="hljs-meta">&gt;&gt;&gt; </span>arrow.get(<span class="hljs-string">&#x27;2017/01/05&#x27;</span>)<br>&lt;Arrow [<span class="hljs-number">2017</span>-01-05T00:<span class="hljs-number">00</span>:<span class="hljs-number">00</span>+<span class="hljs-number">00</span>:<span class="hljs-number">00</span>]&gt;<br></code></pre></td></tr></table></figure><h4 id="时间的计算和移动shift"><a href="#时间的计算和移动shift" class="headerlink" title="时间的计算和移动shift"></a>时间的计算和移动shift</h4><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">utc.replace(days=<span class="hljs-number">1</span>)<span class="hljs-comment"># 设置日等于1号</span></span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">utc.replace(hours=<span class="hljs-number">2</span>) <span class="hljs-comment"># 设置hour等于2点，取值为0-23</span></span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">utc.replace(weeks=<span class="hljs-number">1</span>) <span class="hljs-comment"># </span></span><br><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">utc.shift(days=+<span class="hljs-number">1</span>)<span class="hljs-comment"># 1天之后</span></span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">utc.shift(hours=-<span class="hljs-number">2</span>) <span class="hljs-comment"># 2小时之前</span></span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">cur.shift(years=<span class="hljs-number">1</span>)<span class="hljs-comment"># 明年</span></span><br></code></pre></td></tr></table></figure><p>PS：注意<code>hour</code>与<code>hours</code>的区别，前者是设置时间，后者是在原来时间的基础上加减</p><h4 id="数据运算"><a href="#数据运算" class="headerlink" title="数据运算"></a>数据运算</h4><p>Arrow对象可以通过简单的大于小于符合来判断时间先后，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>start = arrow.get(<span class="hljs-string">&#x27;2017-02-03T15:47:58.114342+02:00&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>end = arrow.get(<span class="hljs-string">&#x27;2017-02-02T07:17:41.756144+02:00&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>start<br>&lt;Arrow [<span class="hljs-number">2017</span>-02-03T15:<span class="hljs-number">47</span>:<span class="hljs-number">58.114342</span>+02:<span class="hljs-number">00</span>]&gt;<br><span class="hljs-meta">&gt;&gt;&gt; </span>end<br>&lt;Arrow [<span class="hljs-number">2017</span>-02-02T07:<span class="hljs-number">17</span>:<span class="hljs-number">41.756144</span>+02:<span class="hljs-number">00</span>]&gt;<br><span class="hljs-meta">&gt;&gt;&gt; </span>start &gt; end<br><span class="hljs-literal">True</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>start_to = start.to(<span class="hljs-string">&#x27;+08:00&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>start == start_to<br><span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><p>也可以通过’-‘运算符来获得时间的差值，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>start - end<br>datetime.timedelta(<span class="hljs-number">1</span>, <span class="hljs-number">30616</span>, <span class="hljs-number">358198</span>)<br></code></pre></td></tr></table></figure><h4 id="转换为指定时间格式"><a href="#转换为指定时间格式" class="headerlink" title="转换为指定时间格式"></a>转换为指定时间格式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">arrow.now().<span class="hljs-built_in">format</span>(<span class="hljs-string">&#x27;YYYY-MM-DD HH:mm:ss ZZ&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="附录-时间格式说明"><a href="#附录-时间格式说明" class="headerlink" title="附录: 时间格式说明"></a>附录: 时间格式说明</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs haml"><span class="hljs-tag">%<span class="hljs-selector-tag">a</span></span> 星期几的简写 Weekday name, abbr.<br><span class="hljs-tag">%<span class="hljs-selector-tag">A</span></span> 星期几的全称 Weekday name, full<br><span class="hljs-tag">%<span class="hljs-selector-tag">w</span></span> 星期（0-6），星期天为星期的开始<br><span class="hljs-tag">%<span class="hljs-selector-tag">W</span></span> 一年中的星期数（00-53）星期一为星期的开始<br><span class="hljs-tag"></span><br><span class="hljs-tag">%<span class="hljs-selector-tag">b</span></span> 月分的简写 Month name, abbr.<br><span class="hljs-tag">%<span class="hljs-selector-tag">B</span></span> 月份的全称 Month name, full<br><span class="hljs-tag"></span><br><span class="hljs-tag">%<span class="hljs-selector-tag">c</span></span> 本地相应的日期表示和时间表示<br><span class="hljs-tag">%<span class="hljs-selector-tag">x</span></span> 本地相应的日期表示 (e.g. 13/01/08)<br><span class="hljs-tag">%<span class="hljs-selector-tag">X</span></span> 本地相应的时间表示 (e.g. 17:02:10)<br><span class="hljs-tag"></span><br><span class="hljs-tag">%<span class="hljs-selector-tag">H</span></span> 24小时制的小时 Hour (24-hour clock)<br><span class="hljs-tag">%<span class="hljs-selector-tag">M</span></span> 十时制表示的分钟数 Minute number<br><span class="hljs-tag">%<span class="hljs-selector-tag">S</span></span> 十进制的秒数 Second number<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/68538400">Python 中的时间和日期操作</a></p><p><a href="**https://www.jianshu.com/p/39b288f0cbc7**">python 判断日期是星期几</a></p><p><a href="https://www.runoob.com/python/python-date-time.html">Python 日期和时间</a></p><p><a href="[https://xin053.github.io/2016/07/04/arrow%E6%97%B6%E9%97%B4%E5%BA%93%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/](https://xin053.github.io/2016/07/04/arrow时间库使用详解/">arrow时间库使用详解</a>)</p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>keras后端引擎K及其方法</title>
    <link href="/2019/12/02/2019-12-02-keras%E5%90%8E%E7%AB%AF%E5%BC%95%E6%93%8EK%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/"/>
    <url>/2019/12/02/2019-12-02-keras%E5%90%8E%E7%AB%AF%E5%BC%95%E6%93%8EK%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="keras中的后端backend及其相关函数"><a href="#keras中的后端backend及其相关函数" class="headerlink" title="keras中的后端backend及其相关函数"></a>keras中的后端backend及其相关函数</h1><blockquote><p>Keras 是一个模型级的库，提供了快速构建深度学习网络的模块。Keras 并不处理如张量乘法、卷积等底层操作。这些操作依赖于某种特定的、优化良好的张量操作库。Keras 依赖于处理张量的库就称为“后端引擎”。Keras 提供了多种后端引擎Theano/Tensorflow，并将其函数统一封装，使得用户可以以同一个接口调用不同后端引擎的函数。</p></blockquote><span id="more"></span><h3 id="一、K-prod"><a href="#一、K-prod" class="headerlink" title="一、K.prod"></a>一、K.prod</h3><p><code>keras.backend.prod(x, axis=None, keepdims=False)</code></p><blockquote><p>功能：在某一指定轴，计算张量中的值的乘积。</p><p>参数</p><ul><li>x: 张量或变量。</li><li>axis: 一个整数需要计算乘积的轴。</li><li>keepdims: 布尔值，是否保留原尺寸。 如果 keepdims 为 False，则张量的秩减 1。 如果 keepdims 为 True，缩小的维度保留为长度 1。</li></ul><p>返回</p><ul><li>x 的元素的乘积的张量。</li></ul></blockquote><h4 id="Numpy-实现"><a href="#Numpy-实现" class="headerlink" title="Numpy 实现"></a>Numpy 实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">prod</span>(<span class="hljs-params">x, axis=<span class="hljs-literal">None</span>, keepdims=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(axis, <span class="hljs-built_in">list</span>):<br>        axis = <span class="hljs-built_in">tuple</span>(axis)<br>    <span class="hljs-keyword">return</span> np.prod(x, axis=axis, keepdims=keepdims)<br></code></pre></td></tr></table></figure><p>具体例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x=np.array([[<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>]])<br><br>scaling = np.prod(x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">False</span>)<br><span class="hljs-built_in">print</span>(x)<br><span class="hljs-built_in">print</span>(scaling)<br></code></pre></td></tr></table></figure><h3 id="二、K-cast"><a href="#二、K-cast" class="headerlink" title="二、K.cast"></a>二、K.cast</h3><p><code>keras.backend.cast(x, dtype)</code></p><blockquote><p>功能：改变张量的数据类型 并返回。</p><p>你可以转换一个 Keras 变量，但它仍然返回一个 Keras 张量。</p><p>参数</p><ul><li>x: Keras 张量（或变量）。</li><li>dtype: 字符串， (‘float16’, ‘float32’ 或 ‘float64’)。</li></ul><p>返回</p><ul><li>Keras 张量，类型为 dtype。</li></ul></blockquote><p>具体例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = K.placeholder((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span><br>&lt;tf.Tensor <span class="hljs-string">&#x27;Placeholder_2:0&#x27;</span> shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>) dtype=float32&gt;<br><span class="hljs-comment"># It doesn&#x27;t work in-place as below.</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>K.cast(<span class="hljs-built_in">input</span>, dtype=<span class="hljs-string">&#x27;float16&#x27;</span>)<br>&lt;tf.Tensor <span class="hljs-string">&#x27;Cast_1:0&#x27;</span> shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>) dtype=float16&gt;<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span><br>&lt;tf.Tensor <span class="hljs-string">&#x27;Placeholder_2:0&#x27;</span> shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>) dtype=float32&gt;<br><span class="hljs-comment"># you need to assign it.</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = K.cast(<span class="hljs-built_in">input</span>, dtype=<span class="hljs-string">&#x27;float16&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span><br>&lt;tf.Tensor <span class="hljs-string">&#x27;Cast_2:0&#x27;</span> shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>) dtype=float16&gt;<br></code></pre></td></tr></table></figure><h3 id="三、K-expand-dims"><a href="#三、K-expand-dims" class="headerlink" title="三、K.expand_dims"></a>三、K.expand_dims</h3><p><code>K.expand_dims(x, dim=-1)</code></p><blockquote><p>功能：拓展维度，在下标为<code>dim</code>的轴上增加一维</p></blockquote><h3 id="三、K-squeeze"><a href="#三、K-squeeze" class="headerlink" title="三、K.squeeze"></a>三、K.squeeze</h3><p><code>K.squeeze(x, axis)</code></p><blockquote><p>功能：将下标为<code>axis</code>的一维从张量中移除</p></blockquote><h3 id="四、batch-gather"><a href="#四、batch-gather" class="headerlink" title="四、batch_gather"></a>四、batch_gather</h3><p><code>K.tf.batch_gather(params, indices)</code></p><blockquote><p>功能</p><ul><li>按index索引选择序列中的值</li><li>支持对张量的批量索引</li></ul><p>主要参数：</p><ul><li>params：被索引的张量</li><li>indices：一维索引张量</li></ul><p>返回值</p><ul><li>通过indices获取params下标的张量。</li></ul></blockquote><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br>seq = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]<br>idxs = [<span class="hljs-number">2</span>]<br>idxs = K.cast(idxs, <span class="hljs-string">&#x27;int32&#x27;</span>)<br>a = K.tf.batch_gather(seq, idxs)<br> <br><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:<br>    <span class="hljs-built_in">print</span>(a.<span class="hljs-built_in">eval</span>())<br></code></pre></td></tr></table></figure><h3 id="五、variable"><a href="#五、variable" class="headerlink" title="五、variable"></a>五、variable</h3><p><code>K.variable(value, dtype=&#39;float32&#39;, name=None)</code></p><blockquote><p>功能：实例化一个张量，返回之</p><p>参数：</p><ul><li>value：用来初始化张量的值</li><li>dtype：张量数据类型</li><li>name：张量的名字（可选）</li></ul></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span>val = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar = K.variable(value=val, dtype=<span class="hljs-string">&#x27;float64&#x27;</span>, name=<span class="hljs-string">&#x27;example_var&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.dtype(kvar)<br><span class="hljs-string">&#x27;float64&#x27;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(kvar)<br>example_var<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar.<span class="hljs-built_in">eval</span>()<br>array([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],<br>   [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>]])<br></code></pre></td></tr></table></figure><h3 id="六、placeholder"><a href="#六、placeholder" class="headerlink" title="六、placeholder"></a>六、placeholder</h3><p><code>K.placeholder(shape=None, ndim=None, dtype=&#39;float32&#39;, name=None)</code></p><blockquote><p>功能：实例化一个占位符，返回之</p><p>参数：</p><ul><li>shape：占位符的shape（整数tuple，可能包含None）</li><li>ndim: 占位符张量的阶数，要初始化一个占位符，至少指定<code>shape</code>和<code>ndim</code>之一，如果都指定则使用<code>shape</code></li><li>dtype: 占位符数据类型</li><li>name: 占位符名称（可选）</li></ul></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span>input_ph = K.placeholder(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>input_ph._keras_shape<br>(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>input_ph<br>&lt;tf.Tensor <span class="hljs-string">&#x27;Placeholder_4:0&#x27;</span> shape=(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>) dtype=float32&gt;<br></code></pre></td></tr></table></figure><h3 id="七、shape"><a href="#七、shape" class="headerlink" title="七、shape"></a>七、shape</h3><pre><code class="lang-K.shape(x)```">&gt; 功能：返回一个张量的符号shape，符号shape的意思是返回值本身也是一个tensor，示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span>tf_session = K.get_session()<br><span class="hljs-meta">&gt;&gt;&gt; </span>val = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar = K.variable(value=val)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = keras.backend.placeholder(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.shape(kvar)<br>&lt;tf.Tensor <span class="hljs-string">&#x27;Shape_8:0&#x27;</span> shape=(<span class="hljs-number">2</span>,) dtype=int32&gt;<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.shape(<span class="hljs-built_in">input</span>)<br>&lt;tf.Tensor <span class="hljs-string">&#x27;Shape_9:0&#x27;</span> shape=(<span class="hljs-number">3</span>,) dtype=int32&gt;<br>__To get integer shape (Instead, you can use K.int_shape(x))__<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>K.shape(kvar).<span class="hljs-built_in">eval</span>(session=tf_session)<br>array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=int32)<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.shape(<span class="hljs-built_in">input</span>).<span class="hljs-built_in">eval</span>(session=tf_session)<br>array([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], dtype=int32)<br></code></pre></td></tr></table></figure>### 八、int_shape```K.int_shape(x)</code></pre><blockquote><p>功能：以整数Tuple或None的形式返回张量shape</p></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = K.placeholder(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.int_shape(<span class="hljs-built_in">input</span>)<br>(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>val = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar = K.variable(value=val)<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.int_shape(kvar)<br>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h3 id="九、ndim"><a href="#九、ndim" class="headerlink" title="九、ndim"></a>九、ndim</h3><pre><code class="lang-K.ndim(x)```">&gt; 功能：返回张量的阶数，为整数示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = K.placeholder(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>val = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar = K.variable(value=val)<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.ndim(<span class="hljs-built_in">input</span>)<br><span class="hljs-number">3</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>K.ndim(kvar)<br><span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>### 十、dtype```K.dtype(x)</code></pre><blockquote><p>功能：返回张量的数据类型，为字符串</p></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.dtype(K.placeholder(shape=(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)))<br><span class="hljs-string">&#x27;float32&#x27;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>K.dtype(K.placeholder(shape=(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>))<br><span class="hljs-string">&#x27;float32&#x27;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>K.dtype(K.placeholder(shape=(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>), dtype=<span class="hljs-string">&#x27;float64&#x27;</span>))<br><span class="hljs-string">&#x27;float64&#x27;</span><br>__Keras var<br></code></pre></td></tr></table></figure><h3 id="十一、eval"><a href="#十一、eval" class="headerlink" title="十一、eval"></a>十一、eval</h3><pre><code class="lang-K.eval(x)```">&gt; 功能：求得张量的值，返回一个 **Numpy array**示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar = K.variable(np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.<span class="hljs-built_in">eval</span>(kvar)<br>array([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],<br>   [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>]], dtype=float32)<br></code></pre></td></tr></table></figure>### 十二、zeros / ones / eye / zeros_like / ones_like<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">K.zeros(shape, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>, name=<span class="hljs-literal">None</span>)<br>K.ones(shape, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>, name=<span class="hljs-literal">None</span>)<br>eye(size, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>, name=<span class="hljs-literal">None</span>)<br>zeros_like(x, name=<span class="hljs-literal">None</span>)<br>ones_like(x, name=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure>&gt; 功能：生成一个全0张量&gt;&gt; 生成一个全1张量&gt;&gt; 生成一个单位矩阵示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar = K.zeros((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar = K.ones((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar = K.eye(<span class="hljs-number">3</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.<span class="hljs-built_in">eval</span>(kvar)<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>kvar_zeros = K.zeros_like(kvar)<br></code></pre></td></tr></table></figure>### 十三、dot```K.dot(x, y)</code></pre><blockquote><p>功能：求两个张量的乘积</p></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x = K.placeholder(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>y = K.placeholder(shape=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>xy = K.dot(x, y)<br><span class="hljs-meta">&gt;&gt;&gt; </span>xy<br>&lt;tf.Tensor <span class="hljs-string">&#x27;MatMul_9:0&#x27;</span> shape=(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>) dtype=float32&gt;<br></code></pre></td></tr></table></figure><h3 id="十四、batch-dot"><a href="#十四、batch-dot" class="headerlink" title="十四、batch_dot"></a>十四、batch_dot</h3><p><code>K.batch_dot(x, y, axes=None)</code></p><blockquote><p>功能：按批进行张量乘法</p><ul><li><p>该函数用于计算x和y的点积，其中x和y都是成batch出现的数据。即它的数据shape形如<code>(batch_size,:)</code>。</p></li><li><p>batch_dot将产生比输入张量维度低的张量，如果张量的维度被减至1，则通过<code>expand_dims</code>保证其维度至少为2 </p></li></ul><p>参数：</p><ul><li>x,y：阶数大于等于2的张量，在tensorflow下，只支持大于等于3阶的张量</li><li>axes：目标结果的维度，为整数或整数列表，<code>axes[0]</code>和<code>axes[1]</code>应相同</li></ul><p>例如</p><ul><li>假设<code>x = [[1, 2],[3,4]]</code> ， <code>y = [[5, 6],[7, 8]]</code>，则<code>batch_dot(x, y, axes=1) = [[17, 53]]</code>，即<code>x.dot(y.T)</code>的主对角元素，此过程中我们没有计算过反对角元素的值</li></ul></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x_batch = K.ones(shape=(<span class="hljs-number">32</span>, <span class="hljs-number">20</span>, <span class="hljs-number">1</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>y_batch = K.ones(shape=(<span class="hljs-number">32</span>, <span class="hljs-number">30</span>, <span class="hljs-number">20</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.int_shape(xy_batch_dot)<br>(<span class="hljs-number">32</span>, <span class="hljs-number">1</span>, <span class="hljs-number">30</span>)<br></code></pre></td></tr></table></figure><h3 id="十五、transpose"><a href="#十五、transpose" class="headerlink" title="十五、transpose"></a>十五、transpose</h3><pre><code class="lang-K.transpose(x)```">&gt; 功能：张量转置，返回转置后的tensor示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>var = K.variable([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.<span class="hljs-built_in">eval</span>(var)<br>array([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>],<br>   [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>]], dtype=float32)<br><span class="hljs-meta">&gt;&gt;&gt; </span>var_transposed = K.transpose(var)<br><span class="hljs-meta">&gt;&gt;&gt; </span>K.<span class="hljs-built_in">eval</span>(var_transposed)<br>array([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">4.</span>],<br>   [ <span class="hljs-number">2.</span>,  <span class="hljs-number">5.</span>],<br>   [ <span class="hljs-number">3.</span>,  <span class="hljs-number">6.</span>]], dtype=float32)<br></code></pre></td></tr></table></figure>### 十六、max / min / sum / prod ```K.max(x, axis=None, keepdims=False)</code></pre><p><code>K.min(x, axis=None, keepdims=False)</code></p><p><code>K.sum(x, axis=None, keepdims=False)</code></p><p><code>K.prod(x, axis=None, keepdims=False)</code></p><blockquote><p>功能：</p><ul><li>求张量中的最大值</li><li>求张量中的最小值</li><li>在给定轴上计算张量中元素之和</li><li>在给定轴上计算张量中元素之积</li></ul></blockquote><h3 id="十七、cumsum-cumprod"><a href="#十七、cumsum-cumprod" class="headerlink" title="十七、cumsum / cumprod"></a>十七、cumsum / cumprod</h3><p><code>K.cumsum(x, axis=0)</code></p><p><code>cumprod(x, axis=0)</code></p><blockquote><p>功能：</p><ul><li>在给定轴上求张量的累积和</li><li>在给定轴上求张量的累积积</li></ul></blockquote><h3 id="十八、reshape"><a href="#十八、reshape" class="headerlink" title="十八、reshape"></a>十八、reshape</h3><p><code>K.reshape(x, shape)</code></p><blockquote><p>功能：将张量的shape变换为指定shape</p></blockquote><h3 id="十九、permute-dimensions"><a href="#十九、permute-dimensions" class="headerlink" title="十九、permute_dimensions"></a>十九、permute_dimensions</h3><p><code>K.permute_dimensions(x, pattern)</code></p><blockquote><p>功能：按照给定的模式重排一个张量的轴</p><p>参数：</p><ul><li>pattern：代表维度下标的tuple如<code>(0, 2, 1)</code></li></ul></blockquote><h3 id="二十、temporal-padding"><a href="#二十、temporal-padding" class="headerlink" title="二十、temporal_padding"></a>二十、temporal_padding</h3><p><code>K.temporal_padding(x, padding=1)</code></p><blockquote><p>功能：向3D张量中间的那个维度的左右两端填充<code>padding</code>个0值</p></blockquote><h3 id="二十一、one-hot"><a href="#二十一、one-hot" class="headerlink" title="二十一、one-hot"></a>二十一、one-hot</h3><p><code>K.one_hot(indices, nb_classes)</code></p><blockquote><p>功能：输入为n维的整数张量，形如(batch_size, dim1, dim2, … dim(n-1))，输出为(n+1)维的one-hot编码，形如(batch_size, dim1, dim2, … dim(n-1), nb_classes)</p></blockquote><h3 id="二十二、relu-elu-softmax-…-categorical-crossentropy-…"><a href="#二十二、relu-elu-softmax-…-categorical-crossentropy-…" class="headerlink" title="二十二、relu / elu / softmax / … / categorical_crossentropy / …"></a>二十二、relu / elu / softmax / … / categorical_crossentropy / …</h3><p><code>K.</code></p><blockquote><p>功能：</p></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://blog.csdn.net/C_chuxin/article/details/87919432">keras中的后端backend及其相关函数（K.prod，K.cast）</a></li></ul><ul><li><a href="https://keras-cn.readthedocs.io/en/latest/backend/">Keras中文文档 - Keras后端</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
      <tag>Keras</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第二天AICon人工智能大会记录</title>
    <link href="/2019/11/22/2019-11-22-%E7%AC%AC%E4%BA%8C%E5%A4%A9AICon%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    <url>/2019/11/22/2019-11-22-%E7%AC%AC%E4%BA%8C%E5%A4%A9AICon%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E4%BC%9A%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="第二天AICon人工智能大会记录"><a href="#第二天AICon人工智能大会记录" class="headerlink" title="第二天AICon人工智能大会记录"></a>第二天AICon人工智能大会记录</h1><h2 id="AICon第二天-2019-11-22"><a href="#AICon第二天-2019-11-22" class="headerlink" title="AICon第二天 2019.11.22"></a>AICon第二天 2019.11.22</h2><ul><li>腾讯云 《<a href="https://aicon.infoq.cn/2019/beijing/presentation/1999">腾讯云知识图谱技术与应用实践之路</a>》吴睿</li><li>小米AI Lab 《<a href="https://aicon.infoq.cn/2019/beijing/presentation/1974">知识图谱在小米的落地与挑战</a>》刘作鹏 </li><li>平安AI团队《<a href="https://aicon.infoq.cn/2019/beijing/presentation/2206">智能金融在客服机器人中台的落地实践</a>》潘鹏举</li><li>阿里达摩院语音实验室 《阿里KAN-TTS技术和落地实践》雷鸣</li><li>追一科技AI Lab《<a href="https://aicon.infoq.cn/2019/beijing/presentation/2088">企业服务中智能交互机器人的实践与探索</a>》徐易楠 </li><li>《<a href="https://aicon.infoq.cn/2019/beijing/presentation/2001">微软小冰：人格化对话机器人的构建及在语音场景当中的实践（上）</a>》</li><li>网易云音乐搜索《<a href="https://aicon.infoq.cn/2019/beijing/presentation/2070">AI算法在云音乐搜索中的应用实践</a>》王新欣</li></ul><span id="more"></span><h4 id="腾讯云-《腾讯云知识图谱技术与应用实践之路》吴睿"><a href="#腾讯云-《腾讯云知识图谱技术与应用实践之路》吴睿" class="headerlink" title="腾讯云 《腾讯云知识图谱技术与应用实践之路》吴睿"></a>腾讯云 《<a href="https://aicon.infoq.cn/2019/beijing/presentation/1999">腾讯云知识图谱技术与应用实践之路</a>》吴睿</h4><ul><li><p>知识图谱定义</p><ul><li>知识图谱是采取二元图网络，描述客观世界中的实体信息及其相互关系规律的知识化描述，其基本组成单位是【实体-关系-实体】 ， 【实体-属性-属性值】 的三元组（triplet），实体之间通过关系相互联结，构成网状结构  </li></ul></li><li><p>图谱构建与应用流程  </p><p><img src="https://ningshixian.github.io/resources/images/图谱构建.png" alt=""></p></li><li><p>知识图谱一站式平台</p><p><img src="https://ningshixian.github.io/resources/images/图谱一站式.png" alt=""></p><ul><li>图谱构建可用预训练语言模型提高效果</li><li>用远监督回标降低成本。</li><li>用云更降本增效。  </li></ul></li><li><p>知识抽取  </p><p><img src="https://ningshixian.github.io/resources/images/知识抽取.png" alt=""></p><p><img src="https://ningshixian.github.io/resources/images/非结构化知识抽取.png" alt=""></p></li><li><p>知识图谱一站式平台<strong>底层</strong>：腾讯Plato高性能图计算引擎  （<a href="https://github.com/Tencent/plato/">已开源</a>）</p></li></ul><h4 id="小米人工智能实验室-《知识图谱在小米的落地与挑战》刘作鹏"><a href="#小米人工智能实验室-《知识图谱在小米的落地与挑战》刘作鹏" class="headerlink" title="小米人工智能实验室 《知识图谱在小米的落地与挑战》刘作鹏"></a>小米人工智能实验室 《<a href="https://aicon.infoq.cn/2019/beijing/presentation/1974">知识图谱在小米的落地与挑战</a>》刘作鹏</h4><ul><li><p>知识图谱简介  </p><ul><li>本质上，知识图谱是大规模的语义网络（ Semantic Web）；</li><li>语义网络是知识表示的重要方式之一，富含实体、概念和多种语义关系；  </li><li>让机器具备认知能力的关键技术  </li></ul></li><li><p><strong>OpenBase</strong> 项目</p><ul><li>联合OpenKG、小米、清华大学、浙江大学、东南大学、狗尾草、海知智能</li><li>多个中文知识图谱自由下载</li></ul></li><li><p>关键构建技术</p><p><img src="https://ningshixian.github.io/resources/images/关键构建技术.png" alt=""></p><ul><li>bert太重，蒸馏！</li></ul></li><li><p>百科知识图谱构建（避坑指南）</p><ul><li><p>知识抽取 → 实体发现 → 实体分类 → 知识补全 → 知识更新 → 知识融合  </p></li><li><p>百科图谱的对齐： Schema层对齐  </p><ul><li>以cnSchema为基础，半自动地抽取等价概念和上下位关系  </li><li>利用等价概念发现等价谓词  </li></ul></li></ul></li><li><p>垂域知识图谱的构建</p><ul><li>^本体定义 → 实体发现 → 知识抽取 → 实体对齐 → <strong>知识择优</strong> → 知识更新  </li><li>小样本条件下的文本分类模型  <ul><li>联合BERT和Meta Network的网络模型；</li><li>Transformer层之间用残差网络连接  </li></ul></li><li><p>图谱的质检流程…（众包）</p></li><li><p>数据不足</p><ul><li>人工标注10000条垂域数据+远程监督</li></ul></li><li>知识择优来源<ul><li>数据的权重引用数</li></ul></li><li>知识及时更新</li></ul></li><li><p>小米图谱</p><ul><li>以百科图谱为中心（已接入9大业务类别），链接了52垂域图谱</li></ul></li><li><p>基于图谱的问答 - KBQA</p><ul><li>小爱开放域问答系统  </li></ul><p><img src="https://ningshixian.github.io/resources/images/小爱开放域问答系统.png" alt=""></p><ul><li><p>问句理解是当前的难点  </p><ul><li><p>问句纠错 → 问句改写 → 意图识别 → 指代消解 → <em>实体链接</em> → <em>谓词判断</em></p></li><li><p>借鉴了搜索和NLP的技术  </p></li><li>需要在80毫秒内完成全部处理，每天处理6千万次以上的请求  (120ms以内返回答案)</li></ul></li><li><p>KBQA关键技术：实体链接  </p><p><img src="https://ningshixian.github.io/resources/images/实体链接.png" alt=""></p></li><li><p>KBQA关键技术：谓词判断  </p><ul><li>……</li></ul></li></ul></li><li><p>未来发展</p><ul><li>从KG到KG Plus  </li><li>KG Plus查询： 根据内容名称或者内容ID，从KG Plus里查询内容；</li><li>理解服务：根据内容名称或者内容ID，查询补充了各类信息的内容数据；</li><li>融合服务：将结构化的和非结构化的数据融合到KG Plus中；</li><li>距离计算：查询实体之间或者概念之间的图谱距离 ；</li><li>实体推荐：根据给定实体，推荐关联实体；  </li></ul></li><li><p>知识图谱：好用，耗钱，耗人力，偏长远发展</p></li><li><p>如何对客服机器人评价？</p><ol><li><p>对给出的答案进行反馈（点赞点踩）</p></li><li><p>标注测试集，计算PRF值</p></li></ol></li></ul><h4 id="平安银行-AI算法团队《智能金融在客服机器人中台的落地实践》潘鹏举"><a href="#平安银行-AI算法团队《智能金融在客服机器人中台的落地实践》潘鹏举" class="headerlink" title="平安银行 / AI算法团队《智能金融在客服机器人中台的落地实践》潘鹏举"></a>平安银行 / AI算法团队《<a href="https://aicon.infoq.cn/2019/beijing/presentation/2206">智能金融在客服机器人中台的落地实践</a>》潘鹏举</h4><ul><li><p>平安AI业务架构<br><img src="https://ningshixian.github.io/resources/images/平安AI业务架构.PNG" alt=""></p></li><li><p>客服机器人小安1.0</p><ul><li>存在问题<ul><li>知识库独立不共享（借记卡和信用卡）</li><li>前端重复开发</li><li>不智能，无法回答其他业务的问题</li></ul></li><li>解决方案<ol><li>知识库维护在一个地方  </li><li>程序分别调用两个机器人，前端进行结果合并（√）</li></ol></li></ul></li><li><p>客服机器人小安2.0</p><p>利用中控机器人来调用不同业务机器人</p><p><img src="https://ningshixian.github.io/resources/images/小安2.0.PNG" alt=""></p><ul><li>存在问题<ul><li>知识库的维护工作量增大（各类业务知识的不断接入）</li><li>FAQ知识不共享，扩展性差</li><li>新机器人的冷启动工作量大<ul><li><em>冷启动科普：在无初始客户积累，得不到新入客户群数据，分析不出客户习惯，不能针对性改进和推广</em></li></ul></li><li>需定长周期的定制化开发（麻烦）</li></ul></li><li>解决方案<ul><li>对不同模块进行抽象封装（共享、复用、组件化 ）</li></ul></li></ul></li><li><p>客服机器人小安3.0（中台化）</p><p>抽象出3个重点</p><ol><li><p>知识统一沉淀</p><ul><li>知识共享（不同业务知识库+闲聊库）</li><li>知识集中化管理（后台管理系统，知识易适配、易维护）</li></ul><ol><li><p>服务统一管理</p><ul><li>实现了服务的统一注册、编排和管理</li></ul><p><img src="https://ningshixian.github.io/resources/images/服务总线.PNG" alt="">  </p></li></ol><ul><li><p>实体管理 (分通用实体、场景化实体)</p><ul><li><p>实体状态保持机制实现</p><ul><li>命中了某个实体，根据上下文进行状态机制的保存</li></ul></li></ul></li></ul><ol><li><p>答案的配置化  </p><ul><li>卡片式答案  </li><li>卡片组件库管理   </li></ul></li></ol></li></ol><p><img src="https://ningshixian.github.io/resources/images/卡片式答案.PNG" alt=""></p><p>3.0架构如下：</p><p><img src="https://ningshixian.github.io/resources/images/小安3.0.PNG" alt=""></p><ul><li>示例 </li></ul><p><img src="https://ningshixian.github.io/resources/images/相关术语.PNG" alt=""></p></li><li><p>总结</p><ul><li><p>扩展问的数量会大幅提高问答的效果</p><ul><li>找运维，多标注知识（很重要）</li></ul></li><li><p>充足的FAQ数据是保证模型起效果的关键</p><ul><li>数据！数据！数据！</li></ul></li><li><p>多轮对话效果有待提高</p></li><li><p>核心的NLU抽象成模块，和知识、答案展示等服务进行解耦</p></li><li><p>通过发现bad case，人工调整数据重新训练</p></li></ul></li></ul><hr><h4 id="阿里巴巴达摩院语音实验室-《阿里KAN-TTS技术和落地实践》雷鸣"><a href="#阿里巴巴达摩院语音实验室-《阿里KAN-TTS技术和落地实践》雷鸣" class="headerlink" title="阿里巴巴达摩院语音实验室 《阿里KAN-TTS技术和落地实践》雷鸣"></a>阿里巴巴达摩院语音实验室 《阿里KAN-TTS技术和落地实践》雷鸣</h4><ul><li>语音合成技术的历史演变</li><li>语音合成技术的应用场景<ul><li>ASR→NLP→TTS</li><li>三方面刚需<ul><li>交互（智能客服、智能硬件、虚拟助理）</li><li>播报（导航、内容播报）</li><li>娱乐（变声）</li></ul></li></ul></li><li>Knowledge-aware Neural TTS (KAN-TTS)<br><img src="https://ningshixian.github.io/resources/images/ali-KAN.png" alt=""><ul><li>评价指标（MOS%）</li></ul></li><li>基于KAN-TTS的定制（声音克隆）<ul><li>5大场景，42种声音<ul><li>高品质– 听起来好听</li><li>开箱即用– 想要什么声音就有什么声音</li><li>低门槛定制– 想做什么声音就可以做什么声音</li><li>个性化– 低成本娱乐效果</li></ul></li></ul></li></ul><h4 id="追一科技AI-Lab《企业服务中智能交互机器人的实践与探索》徐易楠"><a href="#追一科技AI-Lab《企业服务中智能交互机器人的实践与探索》徐易楠" class="headerlink" title="追一科技AI Lab《企业服务中智能交互机器人的实践与探索》徐易楠"></a>追一科技AI Lab《<a href="https://aicon.infoq.cn/2019/beijing/presentation/2088">企业服务中智能交互机器人的实践与探索</a>》徐易楠</h4><p><strong>实践过程中的瓶颈及探索</strong></p><ul><li><p>冷启动效果差、可复制能力不够</p><ul><li>数据量 vs. 准确率 服从线性递增关系  </li></ul></li><li><p>解决方案 </p><ul><li>数据复用  <ul><li>构造伪标签（  无监督数据增强+主动学习  ）</li></ul></li></ul><p><img src="https://ningshixian.github.io/resources/images/伪标签.png" alt=""></p><ul><li>模型优化<ul><li>小样本学习</li></ul></li></ul></li><li><p>系统鲁棒性不⾜</p><ul><li><p>两句话相似，但语义完全不一致</p></li><li><p>字面完全不匹配，意图却相关</p></li><li><p>顺序重要还是不重要</p><p><img src="https://ningshixian.github.io/resources/images/顺序重要.PNG" alt=""></p></li><li><p>标点符号也很重要</p><p><img src="https://ningshixian.github.io/resources/images/标点符号重要.PNG" alt=""></p></li></ul></li><li><p>解决方案</p><ul><li>数据优化<ul><li>简单数据增强：近义词、乱序、随机替换、删除等</li><li>复杂数据增强：使用生成模型</li></ul></li><li>模型优化<ul><li>对抗训练：在某个输⼊上有意的加⼊⾜够小的扰动，能够使⽹络预测<code>错误且预测概率较高</code>的样本  </li></ul></li></ul></li><li><p>口语化表达理解能力不够</p><ul><li><p>通用ASR对于 <strong>领域知识</strong> 识别效果有限</p><p>| ASR识别结果         | 标签                |<br>| —————————- | —————————- |<br>| 能不能<u>改天</u>呐 | 能不能<u>改签</u>呐 |<br>| 我怎么<u>投保</u>啊 | 我怎么<u>淘宝</u>啊 |</p></li><li><p>领域关键词通常会导致意图识别错误</p></li><li><p><strong>口语化表达与文本差异较大</strong></p></li></ul></li><li><p>解决方案</p><ul><li>将ASR转写的数据也参与模型训练</li><li>语音+文本联合建模</li></ul></li><li><p>不能洞察用户情感，从而进行安抚</p></li></ul><h4 id="《微软小冰：人格化对话机器人的构建及在语音场景当中的实践（上）》曾敏"><a href="#《微软小冰：人格化对话机器人的构建及在语音场景当中的实践（上）》曾敏" class="headerlink" title="《微软小冰：人格化对话机器人的构建及在语音场景当中的实践（上）》曾敏"></a>《<a href="https://aicon.infoq.cn/2019/beijing/presentation/2001">微软小冰：人格化对话机器人的构建及在语音场景当中的实践（上）</a>》曾敏</h4><ul><li><p>对话系统的基本架构<br><img src="https://ningshixian.github.io/resources/images/对话系统.png" alt=""></p></li><li><p>语义分析（四方面）</p><p><img src="https://ningshixian.github.io/resources/images/Query语义分析.png" alt=""></p></li><li><p>检索模型的基本结构</p><ul><li>…</li><li>利用<code>bert+蒸馏</code>作检索排序（分类），效果提升很明显</li></ul></li><li><p>2017年业界第一个把开放域对话生成模型做成线上系统</p></li><li><p>人格化IP</p><ul><li>AI beings需要避免被工具化</li><li>上升到内容创作，有创造力（现代诗创作）</li></ul></li></ul><h4 id="网易云音乐搜索《AI算法在云音乐搜索中的应用实践》王新欣"><a href="#网易云音乐搜索《AI算法在云音乐搜索中的应用实践》王新欣" class="headerlink" title="网易云音乐搜索《AI算法在云音乐搜索中的应用实践》王新欣"></a>网易云音乐搜索《<a href="https://aicon.infoq.cn/2019/beijing/presentation/2070">AI算法在云音乐搜索中的应用实践</a>》王新欣</h4><ul><li><p>面临的问题</p><ul><li>用户需求仅仅是一首歌曲？</li><li>如何理解用户并匹配？</li><li>如何给用户更好的资源？</li><li>如何给业务赋能？</li></ul></li><li><p>云音乐搜索框架<br><img src="https://ningshixian.github.io/resources/images/云音乐搜索框架.png" alt=""></p></li><li><p>关键分享（两部分）</p><p><img src="https://ningshixian.github.io/resources/images/云音乐分享.png" alt=""></p></li><li><p>Query理解体系<br><img src="https://ningshixian.github.io/resources/images/Query理解体系.png" alt=""></p></li><li><p>意图识别 - 方法归纳<br><img src="https://ningshixian.github.io/resources/images/意图识别.png" alt=""></p></li><li><p>多链路相关性（没太听懂）</p><ol><li>传统相关性<ul><li>Word Seg+Term weight</li></ul></li><li>基于点击的Graph</li></ol></li></ul><p><img src="https://ningshixian.github.io/resources/images/基于点击的相关性.png" alt=""></p><ol><li>基于深度语义表征</li></ol><p><img src="https://ningshixian.github.io/resources/images/基于深度语义表征.png" alt=""></p><ul><li>排序模型（第二个重点部分）<br><img src="https://ningshixian.github.io/resources/images/排序模型.png" alt=""></li></ul><h4 id="华为海思计算芯片《达芬奇密码：昇腾芯片的前世今生》王晓雷"><a href="#华为海思计算芯片《达芬奇密码：昇腾芯片的前世今生》王晓雷" class="headerlink" title="华为海思计算芯片《达芬奇密码：昇腾芯片的前世今生》王晓雷"></a>华为海思计算芯片《达芬奇密码：昇腾芯片的前世今生》王晓雷</h4><ul><li>⾼算⼒的计算平台<ul><li>CPU</li><li>GPU / FPGA / DSP</li><li>TPU</li></ul></li><li><p>深度学习性能提升小秘诀</p></li><li><p>…</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>技术博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第一天AICon人工智能大会记录</title>
    <link href="/2019/11/21/2019-11-21-%E7%AC%AC%E4%B8%80%E5%A4%A9AICon%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    <url>/2019/11/21/2019-11-21-%E7%AC%AC%E4%B8%80%E5%A4%A9AICon%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E4%BC%9A%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h2 id="第一天AICon人工智能大会记录"><a href="#第一天AICon人工智能大会记录" class="headerlink" title="第一天AICon人工智能大会记录"></a>第一天AICon人工智能大会记录</h2><h3 id="AICon第一天-2019-11-21"><a href="#AICon第一天-2019-11-21" class="headerlink" title="AICon第一天 2019.11.21"></a>AICon第一天 2019.11.21</h3><ul><li>Intel AI 分享</li><li>Baidu《人工智能创新机遇》雷鸣</li><li>AWS Shanghai AI Lab《Deep Graph Made Easy (and Faster)》</li><li>Baidu NLP 《大生产时代下的NLP技术创新与应用实践》忻舟 </li><li>华为诺亚方舟实验室《预训练语言模型》</li><li>小米NLP《NLP在小米的探索与实践》王斌</li><li>百分点科技《深度迁移学习在NLP中的应用及实践》苏海波</li></ul><span id="more"></span><h5 id="Intel-AI-分享"><a href="#Intel-AI-分享" class="headerlink" title="Intel AI 分享"></a>Intel AI 分享</h5><ul><li>六大战略投入加速人工智能发展<ul><li>oneAPI：不用管<code>xpu</code>的束缚，开发<code>dpc++</code>语言</li><li>…</li></ul></li><li>Intel ANALYTICS Zoo: 神经网络模型乐园</li><li>支持各类深度学习框架</li></ul><h5 id="Baidu《人工智能创新机遇》雷鸣"><a href="#Baidu《人工智能创新机遇》雷鸣" class="headerlink" title="Baidu《人工智能创新机遇》雷鸣"></a>Baidu《人工智能创新机遇》雷鸣</h5><ul><li>工业革命：能源+机械</li><li>智能革命：数据+智能</li><li>互联网对产业的改写<ul><li>商业→电商（代表：阿里）</li><li>报社、杂志社→互联网媒体（代表：今日头条）</li><li>电影、电视→互联网视频（代表：爱奇艺）</li><li>社交（代表：Tencent）</li><li>搜索（代表：Baidu）</li><li>O2O（代表：美团）</li></ul></li><li>AI对产业的改写<ul><li>个性化推荐、信用预测、自动驾驶、智能医生、智能助理</li></ul></li><li><p>目前的创新机会是什么</p><ul><li>Internet - 5G<ul><li>高传输 </li><li>低延时</li><li>低功耗、低成本</li></ul></li><li>AI –  Vertical Solution<ul><li>模型性能强悍</li><li>聚焦产业痛点</li><li>解决实际问题</li></ul></li></ul></li><li><p>中国优势：</p><ul><li>…</li></ul></li></ul><h5 id="AWS-Shanghai-AI-Lab《Deep-Graph-Made-Easy-and-Faster-》"><a href="#AWS-Shanghai-AI-Lab《Deep-Graph-Made-Easy-and-Faster-》" class="headerlink" title="AWS Shanghai AI Lab《Deep Graph Made Easy (and Faster)》"></a>AWS Shanghai AI Lab《Deep Graph Made Easy (and Faster)》</h5><ul><li><p>Computer science = algorithm + data structures</p></li><li><p>Many data are already graphs</p><p><img src="https://ningshixian.github.io/resources/images/graphs.png" alt=""></p></li><li><p>开发的图模型 Deep Graph library: <a href="https:www.dgl.ai">DGL</a></p><ul><li>Deep learning + Graph   <a href="">Github star 3.3k</a></li><li>Forward: easy to develop new models</li><li>Backward: seamless integration with existing framework<br>(MXNet/Pytorch/Tensorflow)</li><li>Fast and Scalable</li></ul><p><img src="https://ningshixian.github.io/resources/images/DGL.png" alt=""></p></li><li><p>Deep Graph Applications</p><p><img src="https://ningshixian.github.io/resources/images/graphs_app.png" alt=""></p></li><li><p>Message passing paradigm is very flexible</p><ul><li>Graph convolutional network (GCN) [Kipf, 2018]</li></ul></li><li><p>Existing DL frameworks do not support graph DNNs well</p><ul><li>Writing GNNs is hard in TensorFlow/Pytorch/MXNet.</li></ul><p><img src="https://ningshixian.github.io/resources/images/no_well.png" alt=""></p></li><li><p>Writing GNNs is intuitive in DGL</p></li><li><p>Deep Graph Applications (DGL Model Zoo)</p><ul><li>药物发现 、知识图谱、推荐系统</li></ul></li></ul><h5 id="Baidu-NLP-《大生产时代下的NLP技术创新与应用实践》忻舟"><a href="#Baidu-NLP-《大生产时代下的NLP技术创新与应用实践》忻舟" class="headerlink" title="Baidu NLP 《大生产时代下的NLP技术创新与应用实践》忻舟"></a>Baidu NLP 《大生产时代下的NLP技术创新与应用实践》忻舟</h5><ul><li><p>NLP技术产业应用的思考</p><ul><li>技术应用 <ul><li>场景化·产业化</li></ul></li><li>技术易用 <ul><li>系统化·服务化</li></ul></li><li>深度定制<ul><li>定制化·工具化</li></ul></li></ul></li><li><p>核心技术：基于ERNIE和Multi-Task的观点语义计算</p><p><img src="https://ningshixian.github.io/resources/images/Baidu核心技术.png" alt=""></p></li><li><p>发现一个问题，找出一类问题，重新加入训练集，再次迭代训练</p></li><li><p>NLP场景化应用技术方案</p><ul><li>在国美评论分析的应用</li><li>智能创作平台</li><li>打造人民日报智能编辑部方案</li></ul></li><li><p>ERNIE2.0 及平台化训练</p><ul><li>预训练(Pre-training) &amp;&amp; 参数微调(Fine-tuning)</li><li>ERNIE的应用范式</li></ul><p><img src="https://ningshixian.github.io/resources/images/ERNIE应用范式.png" alt=""></p><ul><li><p>ERNIE应用案例：搜索问答识别和问答匹配</p></li><li><p>模型蒸馏</p></li></ul><p><img src="https://ningshixian.github.io/resources/images/ERNIE模型蒸馏.png" alt=""></p><ul><li>ERNIE套件全景图</li></ul><p><img src="https://ningshixian.github.io/resources/images/ERNIE全景图.png" alt=""></p></li></ul><h5 id="华为诺亚方舟实验室《预训练语言模型》"><a href="#华为诺亚方舟实验室《预训练语言模型》" class="headerlink" title="华为诺亚方舟实验室《预训练语言模型》"></a>华为诺亚方舟实验室《预训练语言模型》</h5><ul><li>哪吒NEZHA  -中文预训练模型 (论文还未放出和开源)<ul><li>绝对位置改进</li><li>全词mask</li><li>+wwm</li><li>+span预测任务</li></ul></li><li>乐府 -诗歌生成<ul><li>方案：GPT</li></ul></li><li>tinyBERT -模型压缩</li></ul><h5 id="小米NLP《NLP在小米的探索与实践》王斌"><a href="#小米NLP《NLP在小米的探索与实践》王斌" class="headerlink" title="小米NLP《NLP在小米的探索与实践》王斌"></a>小米NLP《NLP在小米的探索与实践》王斌</h5><ul><li><p>小米NLP技术</p><p><img src="https://ningshixian.github.io/resources/images/小米NLP技术.png" alt=""></p></li><li><p>NLP基础平台-分词</p><ul><li><p>语料自动标注</p></li><li><p>先进深度学习技术</p></li><li>灵活干预机制</li><li>多端支持<ul><li>Cloud、Local、Lite移动端三种分词版本</li></ul></li></ul></li><li><p>NLP基础平台-命名实体识别</p><ul><li>基于先进深度学习技术（word+char-IDCNN-CRF）</li><li>支持9种NER类型识别<ul><li>人名、地名、机构名</li><li>日期、邮箱、邮箱、网址</li><li>音乐、影视</li></ul></li><li>灵活有效的人工干预机制</li></ul></li><li><p>NLP基础平台-意图识别</p><ul><li>基于ML+DL技术，准确识别用户意图</li><li>融合分布式特征抽取、联合任务模型、集成学习</li></ul></li><li><p>NLP基础平台—文本分类</p></li><li><p>模型+集成学习</p></li><li><p>闲聊对话-检索式</p><p><img src="https://ningshixian.github.io/resources/images/小米闲聊对话.png" alt=""></p></li><li><p>离线机器翻译</p><ul><li>高压缩比<ul><li>知识蒸馏，模型变小、质量不降</li><li>Int8量化</li></ul></li><li>口语化  让机器说人话</li><li>领域适配  -需领域相关的数据进行低学习率学习</li></ul></li></ul><h5 id="百分点科技《深度迁移学习在NLP中的应用及实践》苏海波"><a href="#百分点科技《深度迁移学习在NLP中的应用及实践》苏海波" class="headerlink" title="百分点科技《深度迁移学习在NLP中的应用及实践》苏海波"></a>百分点科技《深度迁移学习在NLP中的应用及实践》苏海波</h5><ul><li><p>NLP中的深度迁移学习：预训练模型</p><ul><li>Source Domain：海量语料+无监督学习</li><li>Target Domain：标注数据+有监督学习</li></ul></li><li><p>Google TPU的实践</p></li><li><p>智能问答中的实践</p><p><img src="https://ningshixian.github.io/resources/images/百分点-智能问答.png" alt=""></p></li><li><p>如何找到语义等价的问句</p><ul><li>基于BERT和BIMPM的语义等价新模型</li></ul></li><li><p>数据增强</p><ul><li>介绍：数据增强可以简单理解为由少量数据生成大量数据的过程</li><li>作用：增加训练的数据量，提高模型的泛化能力；增加噪声数据，提升模型的鲁棒性</li><li>方法：<ul><li>添加无意义词后等价</li><li>关键词换成同义词后等价</li><li>关键词换成非同义词后不等价</li><li>构造等价问法模板</li><li>删除修饰限定后语义不等价</li></ul></li></ul></li><li><p>NL2SQL</p><p><img src="https://ningshixian.github.io/resources/images/NL2SQL的原理.png" alt=""></p><ul><li>基于X-SQL 和依存句法树的NL2SQL新算法</li></ul><p><img src="https://ningshixian.github.io/resources/images/NL2SQL+依存树.png" alt=""></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>技术博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyHanLP工具包使用指南</title>
    <link href="/2019/11/13/2019-11-13-PyHanLP%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2019/11/13/2019-11-13-PyHanLP%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h1 id="PyHanLP-使用指南"><a href="#PyHanLP-使用指南" class="headerlink" title="PyHanLP 使用指南"></a>PyHanLP 使用指南</h1><h2 id="Hanlp简介"><a href="#Hanlp简介" class="headerlink" title="Hanlp简介"></a><strong>Hanlp简介</strong></h2><p>HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。</p><p>HanLP主要功能包括分词、词性标注、关键词提取、自动摘要、依存句法分析、命名实体识别、短语提取、拼音转换、简繁转换等等。</p><p>Github地址：<a href="https://github.com/hankcs/HanLP">https://github.com/hankcs/HanLP</a></p><p>官网地址：<a href="http://hanlp.linrunsoft.com/">http://hanlp.linrunsoft.com/</a></p><span id="more"></span><h2 id="PyHanLP安装"><a href="#PyHanLP安装" class="headerlink" title="PyHanLP安装"></a>PyHanLP安装</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> pyhanlp<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python3">from pyhanlp import *<br># 首次编译运行时，HanLP会自动构建词典缓存，请稍候……<br><br>Using local c:\users\yuquanle\anaconda3\envs\nlpcoure\lib\site-packages\pyhanlp\static\data-for-1.7.0.zip, ignore http://hanlp.linrunsoft.com/release/data-for-1.7.0.zip<br>Extracting data.zip...<br></code></pre></td></tr></table></figure><p>如果下载速度只有几十K，可以尝试这个链接手动下载：<a href="http://download.hanlp.com/">http://download.hanlp.com/</a></p><blockquote><p> 可能会报错： <em>ValueError: 配置错误: 数据包 …/Python36/lib/site-packages/pyhanlp/static\data 不存在，请修改配置文件中的root</em></p></blockquote><p><strong>解决方案：</strong></p><ol><li><p><em>建议提前准备好<a href="http://nlp.hankcs.com/download.php?file=data">data</a>、<a href="http://nlp.hankcs.com/download.php?file=jar">jar与配置文件</a>，并使用环境变量进行配置</em></p><p>解压之后，放入<code>pyhanlp</code>安装目录的<code>static</code>目录中：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus">pyhanlp<br>├── <span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span><br>└── static<br>    ├── data<br>    ├── data-<span class="hljs-keyword">for</span>-<span class="hljs-number">1.7</span>.<span class="hljs-number">5</span><span class="hljs-selector-class">.zip</span> <br>    ├── hanlp-<span class="hljs-number">1.7</span>.<span class="hljs-number">5</span><span class="hljs-selector-class">.jar</span><br>    ├── hanlp<span class="hljs-selector-class">.properties</span><br>    └── hanlp-<span class="hljs-number">1.7</span>.<span class="hljs-number">5</span>-release<span class="hljs-selector-class">.zip</span><br><br></code></pre></td></tr></table></figure></li><li><p><em>保证 hanlp.properties 中的 root 是指向正确的data路径。</em> 比如：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">HANLP_JAR_PATH</span>=/hanlp/hanlp-1.6.0.jar<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HANLP_STATIC_ROOT</span>=/hanlp<br>tree <span class="hljs-variable">$HANLP_STATIC_ROOT</span> -L 2<br>ll <span class="hljs-variable">$HANLP_JAR_PATH</span><br>cat <span class="hljs-variable">$HANLP_STATIC_ROOT</span>/hanlp.properties | grep root<br></code></pre></td></tr></table></figure><p><em>PS: windows可忽略</em></p></li></ol><h2 id="使用指南"><a href="#使用指南" class="headerlink" title="使用指南"></a>使用指南</h2><h3 id="1-标准分词和词性标注"><a href="#1-标准分词和词性标注" class="headerlink" title="1.标准分词和词性标注"></a>1.标准分词和词性标注</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python3">from pyhanlp import *<br><br>print(HanLP.segment(&#x27;欢迎在Python中调用HanLP的API&#x27;))<br>for term in HanLP.segment(&#x27;欢迎在Python中调用HanLP的API&#x27;):<br>    print(&#x27;&#123;&#125;\t&#123;&#125;&#x27;.format(term.word, term.nature)) # 获取单词与词性<br>    <br>&gt; [欢迎/v, 在/p, Python/nx, 中/f, 调用/v, HanLP/nx, 的/ude1, API/nx]<br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>HanLP中有一系列“开箱即用”的静态分词器，以Tokenizer结尾，在接下来的例子中会继续介绍。</li><li>HanLP.segment其实是对StandardTokenizer.segmen的包装。</li><li>分词结果包含词性，每个词性的意思请查阅<a href="http://www.hankcs.com/nlp/part-of-speech-tagging.html#h2-8">《HanLP词性标注集》</a>。</li></ul><p><strong>算法详解</strong></p><ul><li><a href="http://www.hankcs.com/nlp/segment/the-word-graph-is-generated.html">《词图的生成》</a></li></ul><h3 id="2-依存句法分析"><a href="#2-依存句法分析" class="headerlink" title="2.依存句法分析"></a>2.依存句法分析</h3><p>它将句子分析成一颗依存句法树，描述出各个词语之间的依存关系。也即指出了词语之间在句法上的搭配关系，这种搭配关系是和语义相关联的。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python3">s_dep = HanLP.parseDependency(&#x27;欢迎在Python中调用HanLP的APIs&#x27;)<br>print(s_dep)<br><br>1       欢迎    欢迎    v       v       _       0       核心关系        _       _<br>2       在      在      p       p       _       5       状中结构        _       _<br>3       Python  Python  ws      nx      _       4       定中关系        _       _<br>4       中      中      nd      f       _       2       介宾关系        _       _<br>5       调用    调用    v       v       _       8       定中关系        _       _<br>6       HanLP   HanLP   ws      nx      _       5       动宾关系        _       _<br>7       的      的      u       u       _       5       右附加关系      _       _<br>8       API     API     ws      nx      _       1       动宾关系        _       _<br><br># 可以直接拿到数组，任意顺序或逆序遍历<br>word_array = sentence.getWordArray()<br>for word in word_array:<br>print(&quot;%s --(%s)--&gt; %s&quot; % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))<br>print()<br></code></pre></td></tr></table></figure><p><strong>算法详解</strong></p><ul><li><a href="http://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html">《基于神经网络分类模型与转移系统的判决式依存句法分析器》</a></li></ul><p><a href="https://github.com/hankcs/HanLP/issues/464"><strong>依存可视化工具</strong></a></p><ol><li>HanLP在线句法分析并可视化：<a href="http://hanlp.hankcs.com/?sentence=徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。">http://hanlp.hankcs.com/</a>  </li><li><p>南京大学开发的Dependency Viewer的Windows客户端工具</p><ul><li>下载：<a href="https://github.com/hankcs/HanLP/files/1035493/DependencyViewer.exe.zip">DependencyViewer.exe.zip</a></li><li>加载CoNLL格式的txt文件</li></ul></li><li><p>web端推荐<a href="http://spyysalo.github.io/conllu.js/">conllu.js</a>，斯坦福大学也在用这个 </p></li></ol><p><a href="http://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html"><strong>英文依存标签含义解释</strong></a></p><div class="table-container"><table><thead><tr><th style="text-align:center">Tag</th><th style="text-align:center">关系</th><th style="text-align:center">Description</th><th style="text-align:center">Example</th></tr></thead><tbody><tr><td style="text-align:center">SBV</td><td style="text-align:center">主谓关系</td><td style="text-align:center">subject-verb</td><td style="text-align:center">我送她一束花 (我 &lt;– 送)</td></tr><tr><td style="text-align:center">VOB</td><td style="text-align:center">动宾关系</td><td style="text-align:center">直接宾语，verb-object</td><td style="text-align:center">我送她一束花 (送 –&gt; 花)</td></tr><tr><td style="text-align:center">IOB</td><td style="text-align:center">间宾关系</td><td style="text-align:center">间接宾语，indirect-object</td><td style="text-align:center">我送她一束花 (送 –&gt; 她)</td></tr><tr><td style="text-align:center">FOB</td><td style="text-align:center">前置宾语</td><td style="text-align:center">前置宾语，fronting-object</td><td style="text-align:center">他什么书都读 (书 &lt;– 读)</td></tr><tr><td style="text-align:center">DBL</td><td style="text-align:center">兼语</td><td style="text-align:center">double</td><td style="text-align:center">他请我吃饭 (请 –&gt; 我)</td></tr><tr><td style="text-align:center">ATT</td><td style="text-align:center">定中关系</td><td style="text-align:center">attribute</td><td style="text-align:center">红苹果 (红 &lt;– 苹果)</td></tr><tr><td style="text-align:center">ADV</td><td style="text-align:center">状中结构</td><td style="text-align:center">adverbial</td><td style="text-align:center">非常美丽 (非常 &lt;– 美丽)</td></tr><tr><td style="text-align:center">CMP</td><td style="text-align:center">动补结构</td><td style="text-align:center">complement</td><td style="text-align:center">做完了作业 (做 –&gt; 完)</td></tr><tr><td style="text-align:center">COO</td><td style="text-align:center">并列关系</td><td style="text-align:center">coordinate</td><td style="text-align:center">大山和大海 (大山 –&gt; 大海)</td></tr><tr><td style="text-align:center">POB</td><td style="text-align:center">介宾关系</td><td style="text-align:center">preposition-object</td><td style="text-align:center">在贸易区内 (在 –&gt; 内)</td></tr><tr><td style="text-align:center">LAD</td><td style="text-align:center">左附加关系</td><td style="text-align:center">left adjunct</td><td style="text-align:center">大山和大海 (和 &lt;– 大海)</td></tr><tr><td style="text-align:center">RAD</td><td style="text-align:center">右附加关系</td><td style="text-align:center">right adjunct</td><td style="text-align:center">孩子们 (孩子 –&gt; 们)</td></tr><tr><td style="text-align:center">IS</td><td style="text-align:center">独立结构</td><td style="text-align:center">independent structure</td><td style="text-align:center">两个单句在结构上彼此独立</td></tr><tr><td style="text-align:center">WP</td><td style="text-align:center">标点符号</td><td style="text-align:center">punctuation</td><td style="text-align:center">标点符号</td></tr><tr><td style="text-align:center">HED</td><td style="text-align:center">核心关系</td><td style="text-align:center">head</td><td style="text-align:center">指整个句子的核心</td></tr></tbody></table></div><h3 id="3-关键词提取"><a href="#3-关键词提取" class="headerlink" title="3.关键词提取"></a>3.关键词提取</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python3">document = u&#x27;&#x27;&#x27;<br>自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。<br>它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。<br>自然语言处理是一门融语言学、计算机科学、数学于一体的科学。<br>因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，<br>所以它与语言学的研究有着密切的联系，但又有重要的区别。<br>自然语言处理并不是一般地研究自然语言，<br>而在于研制能有效地实现自然语言通信的计算机系统，<br>特别是其中的软件系统。因而它是计算机科学的一部分。<br>&#x27;&#x27;&#x27;<br>doc_keyword = HanLP.extractKeyword(document, 3)<br>for word in doc_keyword:<br>    print(word)<br>研究<br>自然语言<br>自然语言处理<br></code></pre></td></tr></table></figure><h3 id="4-摘要抽取"><a href="#4-摘要抽取" class="headerlink" title="4.摘要抽取"></a>4.摘要抽取</h3><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">doc_keysentence = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">HanLP</span>.</span></span>extract<span class="hljs-constructor">Summary(<span class="hljs-params">document</span>, 3)</span><br><span class="hljs-keyword">for</span> key_sentence <span class="hljs-keyword">in</span> doc_keysentence:<br>    print(key_sentence)<br>    <br>自然语言处理并不是一般地研究自然语言<br>自然语言处理是计算机科学领域与人工智能领域中的一个重要方向<br>它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法<br></code></pre></td></tr></table></figure><h3 id="5-1感知机词法分析器"><a href="#5-1感知机词法分析器" class="headerlink" title="5.1感知机词法分析器"></a>5.1感知机词法分析器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">PerceptronLexicalAnalyzer = JClass(<span class="hljs-string">&#x27;com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer&#x27;</span>)<br>analyzer = PerceptronLexicalAnalyzer()<br><span class="hljs-built_in">print</span>(analyzer.analyze(<span class="hljs-string">&quot;上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观&quot;</span>))<br><br>&gt; [上海/ns 华安/nz 工业/n （/w 集团/n ）/w 公司/n]/nt 董事长/n 谭旭光/nr 和/c 秘书/n 胡花蕊/nr 来到/v [美国纽约/ns 现代/ntc 艺术/n 博物馆/n]/ns 参观/v<br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>NLP分词NLPTokenizer会执行词性标注和命名实体识别，由<a href="https://github.com/hankcs/HanLP/wiki/结构化感知机标注框架">结构化感知机序列标注框架</a>支撑。</li><li>默认模型训练自9970万字的大型综合语料库，是已知范围内全世界最大的中文分词语料库。语料库规模决定实际效果，面向生产环境的语料库应当在千万字量级。用户可以在自己的语料上<a href="https://github.com/hankcs/HanLP/wiki/结构化感知机标注框架">训练新模型</a>以适应新领域、识别新的命名实体。</li></ul><h3 id="5-2-CRF-词法分析器"><a href="#5-2-CRF-词法分析器" class="headerlink" title="5.2 CRF 词法分析器"></a>5.2 CRF 词法分析器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">CRFLexicalAnalyzer = JClass(<span class="hljs-string">&quot;com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer&quot;</span>)<br>analyzer = CRFLexicalAnalyzer()<br><span class="hljs-built_in">print</span>(analyzer.analyze(<span class="hljs-string">&quot;上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观&quot;</span>))<br><br>&gt; [上海/ns 华安/nz 工业/n （/w 集团/n ）/w 公司/n]/nt 董事长/n 谭旭光/nr 和/c 秘书/n 胡花蕊/nr 来到/v [美国纽约/ns 现代/ntc 艺术/n 博物馆/n]/ns 参观/v<br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>调用更底层的API需要参考Java语法用JClass引入更深的类路径。以感知机词法分析器为例，CRF分词的类位于包名<strong>com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer</strong>下，先用<strong>JClass</strong>得到类，然后就可以调用了</li><li>CRF对新词有很好的识别能力，但是开销较大。</li></ul><p><strong>算法详解</strong></p><ul><li><a href="https://github.com/hankcs/HanLP/wiki/CRF词法分析">《CRF中文分词、词性标注与命名实体识别》</a></li></ul><h3 id="5-3-极速词典分词"><a href="#5-3-极速词典分词" class="headerlink" title="5.3 极速词典分词"></a>5.3 极速词典分词</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">SpeedTokenizer = <span class="hljs-built_in">JClass</span>(<span class="hljs-string">&quot;com.hankcs.hanlp.tokenizer.SpeedTokenizer&quot;</span>)<br>text = <span class="hljs-string">&quot;江西鄱阳湖干枯，中国最大淡水湖变成大草原&quot;</span><br><span class="hljs-function"><span class="hljs-title">JClass</span><span class="hljs-params">(<span class="hljs-string">&quot;com.hankcs.hanlp.HanLP$Config&quot;</span>)</span></span><span class="hljs-selector-class">.ShowTermNature</span> = False<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(SpeedTokenizer.segment(text)</span></span>)<br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>调用更底层的API需要参考Java语法用JClass引入更深的类路径。以感知机词法分析器为例，极速字典分词的类位于包名<strong>com.hankcs.hanlp.tokenizer.SpeedTokenizer</strong>下，先用<strong>JClass</strong>得到类，然后就可以调用了</li><li>极速分词是词典最长分词，速度极其快，精度一般。</li><li>在i7-6700K上跑出了4500万字每秒的速度。</li></ul><p><strong>算法详解</strong></p><ul><li><a href="http://www.hankcs.com/program/algorithm/aho-corasick-double-array-trie.html">《Aho Corasick自动机结合DoubleArrayTrie极速多模式匹配》</a></li></ul><h3 id="6-中国人名识别"><a href="#6-中国人名识别" class="headerlink" title="6.中国人名识别"></a>6.中国人名识别</h3><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gradle">NER = HanLP.newSegment().enableNameRecognize(<span class="hljs-keyword">True</span>)<br>p_name = NER.seg(<span class="hljs-string">&#x27;王国强、高峰、汪洋、张朝阳光着头、韩寒、小四&#x27;</span>)<br><span class="hljs-keyword">print</span>(p_name)<br><br>&gt; [王国强<span class="hljs-regexp">/nr, 、/</span>w, 高峰<span class="hljs-regexp">/n, 、/</span>w, 汪洋<span class="hljs-regexp">/n, 、/</span>w, 张朝阳<span class="hljs-regexp">/nr, 光着头/</span>l, 、<span class="hljs-regexp">/w, 韩寒/</span>nr, 、<span class="hljs-regexp">/w, 小/</span>a, 四/<br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>目前分词器基本上都默认开启了中国人名识别，比如HanLP.segment()接口中使用的分词器等等，用户不必手动开启；上面的代码只是为了强调。</li><li>有一定的误命中率，比如误命中关键年，则可以通过在data/dictionary/person/nr.txt加入一条关键年 A 1来排除关键年作为人名的可能性，也可以将关键年作为新词登记到自定义词典中。</li><li><strong>建议NLP用户使用感知机或CRF词法分析器，精度更高。</strong></li></ul><p><strong>算法详解</strong></p><ul><li><a href="http://www.hankcs.com/nlp/chinese-name-recognition-in-actual-hmm-viterbi-role-labeling.html">《实战HMM-Viterbi角色标注中国人名识别》</a></li></ul><h3 id="7-音译人名识别"><a href="#7-音译人名识别" class="headerlink" title="7.音译人名识别"></a>7.音译人名识别</h3><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs gradle">sentence = <span class="hljs-string">&#x27;微软的比尔盖茨、Facebook的扎克伯格跟桑德博格、亚马逊的贝索斯、苹果的库克，这些硅谷的科技人&#x27;</span><br>person_ner = HanLP.newSegment().enableTranslatedNameRecognize(<span class="hljs-keyword">True</span>)<br>p_name = person_ner.seg(sentence)<br><span class="hljs-keyword">print</span>(p_name)<br><br>&gt; [微软<span class="hljs-regexp">/ntc, 的/u</span>de1, 比尔盖茨<span class="hljs-regexp">/nrf, 、/</span>w, Facebook<span class="hljs-regexp">/nx, 的/u</span>de1, 扎克伯格<span class="hljs-regexp">/nrf, 跟/</span>p, 桑德博格<span class="hljs-regexp">/nrf, 、/</span>w, <br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>目前分词器基本上都默认开启了音译人名识别，用户不必手动开启</li></ul><p><strong>算法详解</strong></p><ul><li><a href="http://www.hankcs.com/nlp/name-transliteration-cascaded-hidden-markov-model-and-japanese-personal-names-recognition.html">《层叠隐马模型下的音译人名和日本人名识别》</a></li></ul><h3 id="8-短语提取"><a href="#8-短语提取" class="headerlink" title="8.短语提取"></a>8.短语提取</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">phraseList = HanLP<span class="hljs-selector-class">.extractPhrase</span>(document, <span class="hljs-number">3</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(phraseList)</span></span><br><span class="hljs-selector-attr">[计算机科学, 中的重要, 之间自然语言]</span><br></code></pre></td></tr></table></figure><h3 id="9-1-汉字转拼音"><a href="#9-1-汉字转拼音" class="headerlink" title="9.1 汉字转拼音"></a>9.1 汉字转拼音</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">s = <span class="hljs-string">&#x27;重载不是重任&#x27;</span><br>pinyinList = HanLP<span class="hljs-selector-class">.convertToPinyinList</span>(s)<br><span class="hljs-keyword">for</span> pinyin <span class="hljs-keyword">in</span> pinyinList:<br>    <span class="hljs-built_in">print</span>(pinyin<span class="hljs-selector-class">.getPinyinWithoutTone</span>(),pinyin<span class="hljs-selector-class">.getTone</span>(), pinyin, pinyin<span class="hljs-selector-class">.getPinyinWithToneMark</span>())<br>    <br>chong <span class="hljs-number">2</span> chong2 chóng<br>zai <span class="hljs-number">3</span> zai3 zǎ<span class="hljs-selector-tag">i</span><br>bu <span class="hljs-number">2</span> bu2 bú<br>shi <span class="hljs-number">4</span> shi4 shì<br>zhong <span class="hljs-number">4</span> zhong4 zhòng<br>ren <span class="hljs-number">4</span> ren4 rèn<br></code></pre></td></tr></table></figure><h3 id="9-2-拼音转汉字"><a href="#9-2-拼音转汉字" class="headerlink" title="9.2 拼音转汉字"></a>9.2 拼音转汉字</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def demo_pinyin_to_chinese():<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot; HanLP中的数据结构和接口是灵活的，组合这些接口，可以自己创造新功能</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">    [renmenrenweiyalujiangbujian/null, lvse/[滤色, 绿色]]</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    AhoCorasickDoubleArrayTrie = JClass(<br>        <span class="hljs-string">&quot;com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie&quot;</span>)<br>    StringDictionary = JClass(<br>        <span class="hljs-string">&quot;com.hankcs.hanlp.corpus.dictionary.StringDictionary&quot;</span>)<br>    CommonAhoCorasickDoubleArrayTrieSegment = JClass(<br>        <span class="hljs-string">&quot;com.hankcs.hanlp.seg.Other.CommonAhoCorasickDoubleArrayTrieSegment&quot;</span>)<br>    CommonAhoCorasickSegmentUtil = JClass(<br>        <span class="hljs-string">&quot;com.hankcs.hanlp.seg.Other.CommonAhoCorasickSegmentUtil&quot;</span>)<br>   <span class="hljs-built_in"> Config </span>= JClass(<span class="hljs-string">&quot;com.hankcs.hanlp.HanLP<span class="hljs-variable">$Config</span>&quot;</span>)<br><br>    TreeMap = JClass(<span class="hljs-string">&quot;java.util.TreeMap&quot;</span>)<br>    TreeSet = JClass(<span class="hljs-string">&quot;java.util.TreeSet&quot;</span>)<br><br>    dictionary = StringDictionary()<br>    dictionary.load(Config.PinyinDictionaryPath)<br>    entry = &#123;&#125;<br>    m_map = TreeMap()<br>    <span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> dictionary.entrySet():<br>        pinyins = entry.getValue().replace(<span class="hljs-string">&quot;[\\d,]&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br>        words = m_map.<span class="hljs-built_in">get</span>(pinyins)<br>        <span class="hljs-keyword">if</span> words is None:<br>            words = TreeSet()<br>            m_map.put(pinyins, words)<br>        words.<span class="hljs-built_in">add</span>(entry.getKey())<br>    words = TreeSet()<br>    words.<span class="hljs-built_in">add</span>(<span class="hljs-string">&quot;绿色&quot;</span>)<br>    words.<span class="hljs-built_in">add</span>(<span class="hljs-string">&quot;滤色&quot;</span>)<br>    m_map.put(<span class="hljs-string">&quot;lvse&quot;</span>, words)<br><br>    segment = CommonAhoCorasickDoubleArrayTrieSegment(m_map)<br>    <span class="hljs-built_in">print</span>(segment.segment(<span class="hljs-string">&quot;renmenrenweiyalujiangbujianlvse&quot;</span>))<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    demo_pinyin_to_chinese()<br><br></code></pre></td></tr></table></figure><h3 id="10-繁简转换"><a href="#10-繁简转换" class="headerlink" title="10.繁简转换"></a>10.繁简转换</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">Jianti = HanLP<span class="hljs-selector-class">.convertToSimplifiedChinese</span>(<span class="hljs-string">&quot;我愛自然語言處理技術！&quot;</span>)<br>Fanti = HanLP<span class="hljs-selector-class">.convertToTraditionalChinese</span>(<span class="hljs-string">&quot;我爱自然语言处理技术！&quot;</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(Jianti)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(Fanti)</span></span><br><br>我爱自然语言处理技术！<br>我愛自然語言處理技術！<br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>HanLP能够识别简繁分歧词，比如打印机=印表機。</li><li>支持香港繁体和台湾繁体</li></ul><p><strong>算法详解</strong></p><ul><li><a href="http://www.hankcs.com/nlp/java-chinese-characters-to-pinyin-and-simplified-conversion-realization.html#h2-17">《汉字转拼音与简繁转换的Java实现》</a></li></ul><h3 id="11-用户自定义词典"><a href="#11-用户自定义词典" class="headerlink" title="11. 用户自定义词典"></a>11. 用户自定义词典</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">print</span>(HanLP.segment(text))<br><br>CustomDictionary = JClass(<span class="hljs-string">&quot;com.hankcs.hanlp.dictionary.CustomDictionary&quot;</span>)<br>CustomDictionary.add(<span class="hljs-string">&quot;攻城狮&quot;</span>)  <span class="hljs-comment"># 动态增加</span><br>CustomDictionary.insert(<span class="hljs-string">&quot;白富美&quot;</span>, <span class="hljs-string">&quot;nz 1024&quot;</span>)  <span class="hljs-comment"># 强行插入</span><br><span class="hljs-comment">#CustomDictionary.remove(&quot;攻城狮&quot;); # 删除词语（注释掉试试）</span><br>CustomDictionary.add(<span class="hljs-string">&quot;单身狗&quot;</span>, <span class="hljs-string">&quot;nz 1024 n 1&quot;</span>)<br><span class="hljs-comment">#print(CustomDictionary.get(&quot;单身狗&quot;))</span><br><br><span class="hljs-built_in">print</span>(HanLP.segment(text))<br><br>&gt; [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]<br>&gt; [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]<br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>CustomDictionary是一份全局的用户自定义词典，可以随时增删，影响全部分词器。另外可以在任何分词器中关闭它。通过代码动态增删不会保存到词典文件。</li><li>中文分词≠词典，词典无法解决中文分词，Segment提供高低优先级应对不同场景，请参考<a href="https://github.com/hankcs/HanLP/wiki/FAQ#为什么修改了词典还是没有效果">FAQ</a>。</li></ul><p><strong>追加词典</strong></p><ul><li>CustomDictionary主词典文本路径是data/dictionary/custom/CustomDictionary.txt，用户可以在此增加自己的词语（不推荐）；也可以单独新建一个文本文件，通过配置文件CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 我的词典.txt;来追加词典<u>（推荐）</u>。</li><li>始终建议将相同词性的词语放到同一个词典文件里，便于维护和分享。</li></ul><p><strong>词典格式</strong></p><ul><li>每一行代表一个单词，格式遵从[单词] [词性A] [A的频次] [词性B] [B的频次] … 如果不填词性则表示采用词典的默认词性。</li><li>词典的默认词性默认是名词n，可以通过配置文件修改：全国地名大全.txt ns;如果词典路径后面空格紧接着词性，则该词典默认是该词性。</li></ul><p><strong>算法详解</strong></p><ul><li><a href="http://www.hankcs.com/program/java/tire-tree-participle.html">《Trie树分词》</a></li><li><a href="http://www.hankcs.com/program/algorithm/aho-corasick-double-array-trie.html">《Aho Corasick自动机结合DoubleArrayTrie极速多模式匹配》</a></li></ul><h3 id="12-数词和数量词识别"><a href="#12-数词和数量词识别" class="headerlink" title="12. 数词和数量词识别"></a>12. 数词和数量词识别</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python3">StandardTokenizer = JClass(&quot;com.hankcs.hanlp.tokenizer.StandardTokenizer&quot;)<br>StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(True)<br>print(StandardTokenizer.segment(sentence))<br><br>[十九元/mq, 套餐/n, 包括/v, 什么/ry]<br>[壹佰块/mq, 都/d, 不/d, 给/p, 我/rr]<br>[９０１２３４５６７８只/mq, 蚂蚁/n]<br></code></pre></td></tr></table></figure><h3 id="13-同义改写"><a href="#13-同义改写" class="headerlink" title="13.  同义改写"></a>13.  同义改写</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python3">CoreSynonymDictionary = JClass(&quot;com.hankcs.hanlp.dictionary.CoreSynonymDictionary&quot;)<br>text = &quot;这个方法可以利用同义词词典将一段文本改写成意思相似的另一段文本，而且差不多符合语法&quot;<br>print(CoreSynonymDictionary.rewrite(text))<br><br>&gt; 其一措施可以使同义词词典将一律段文本改写成意思相似之任何一样段文本，而且大多符合语法<br></code></pre></td></tr></table></figure><h3 id="14-情感分析"><a href="#14-情感分析" class="headerlink" title="14.  情感分析"></a>14.  情感分析</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python3">NaiveBayesClassifier = JClass(&#x27;com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier&#x27;)<br><br>classifier = NaiveBayesClassifier()<br>#  创建分类器，更高级的功能请参考IClassifier的接口定义<br>classifier.train(chn_senti_corp)<br>#  训练后的模型支持持久化，下次就不必训练了<br>predict(classifier, &quot;前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！&quot;)<br></code></pre></td></tr></table></figure><h3 id="15-聚类"><a href="#15-聚类" class="headerlink" title="15.  聚类"></a>15.  聚类</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python3">ClusterAnalyzer = JClass(&#x27;com.hankcs.hanlp.mining.cluster.ClusterAnalyzer&#x27;)<br><br>analyzer = ClusterAnalyzer()<br>analyzer.addDocument(&quot;赵一&quot;, &quot;流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 蓝调, 蓝调, 蓝调, 蓝调, 蓝调, 蓝调, 摇滚, 摇滚, 摇滚, 摇滚&quot;)<br>analyzer.addDocument(&quot;钱二&quot;, &quot;爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲&quot;)<br>analyzer.addDocument(&quot;张三&quot;, &quot;古典, 古典, 古典, 古典, 民谣, 民谣, 民谣, 民谣&quot;)<br>analyzer.addDocument(&quot;李四&quot;, &quot;爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 金属, 金属, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲&quot;)<br>analyzer.addDocument(&quot;王五&quot;, &quot;流行, 流行, 流行, 流行, 摇滚, 摇滚, 摇滚, 嘻哈, 嘻哈, 嘻哈&quot;)<br>analyzer.addDocument(&quot;马六&quot;, &quot;古典, 古典, 古典, 古典, 古典, 古典, 古典, 古典, 摇滚&quot;)<br>print(analyzer.kmeans(3))<br>print(analyzer.repeatedBisection(3))<br>print(analyzer.repeatedBisection(1.0))  # 自动判断聚类数量k<br></code></pre></td></tr></table></figure><h3 id="16-语义距离"><a href="#16-语义距离" class="headerlink" title="16.  语义距离"></a>16.  语义距离</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python3">CoreSynonymDictionary = JClass(&quot;com.hankcs.hanlp.dictionary.CoreSynonymDictionary&quot;)<br><br>word_array = [<br>&quot;香蕉&quot;,<br>&quot;苹果&quot;,<br>&quot;白菜&quot;,<br>&quot;水果&quot;,<br>&quot;蔬菜&quot;,<br>&quot;自行车&quot;,<br>&quot;公交车&quot;,<br>&quot;飞机&quot;,<br>&quot;买&quot;,<br>&quot;卖&quot;,<br>&quot;购入&quot;,<br>&quot;新年&quot;,<br>]<br>print(&quot;%-5s\t%-5s\t%-10s\t%-5s\n&quot; % (&quot;词A&quot;, &quot;词B&quot;, &quot;语义距离&quot;, &quot;语义相似度&quot;))<br>for a in word_array:<br>    for b in word_array:<br>        print(&quot;%-5s\t%-5s\t%-15d\t%-5.10f&quot; % (a, b, CoreSynonymDictionary.distance(a, b), CoreSynonymDictionary.similarity(a, b)))<br>        <br>&gt; 返回两两之间的语义距离和语义相似度<br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><ul><li>可以得到给出的词汇之间语义的相似度</li><li><a href="https://github.com/hankcs/HanLP/wiki/word2vec">word2vec文档</a></li><li><a href="http://www.hankcs.com/nlp/word2vec.html">《word2vec原理推导与代码分析》</a></li></ul><h4 id="17-文本推荐（类似搜索引擎功能）"><a href="#17-文本推荐（类似搜索引擎功能）" class="headerlink" title="17.  文本推荐（类似搜索引擎功能）"></a>17.  文本推荐（类似搜索引擎功能）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">Suggester = JClass(<span class="hljs-string">&quot;com.hankcs.hanlp.suggest.Suggester&quot;</span>)<br>suggester = Suggester()<br>title_array = [<br>    <span class="hljs-string">&quot;威廉王子发表演说 呼吁保护野生动物&quot;</span>,<br>    <span class="hljs-string">&quot;魅惑天后许佳慧不爱“预谋” 独唱《许某某》&quot;</span>,<br>    <span class="hljs-string">&quot;《时代》年度人物最终入围名单出炉 普京马云入选&quot;</span>,<br>    <span class="hljs-string">&quot;“黑格比”横扫菲：菲吸取“海燕”经验及早疏散&quot;</span>,<br>    <span class="hljs-string">&quot;日本保密法将正式生效 日媒指其损害国民知情权&quot;</span>,<br>    <span class="hljs-string">&quot;英报告说空气污染带来“公共健康危机”&quot;</span><br>]<br><span class="hljs-keyword">for</span> title <span class="hljs-keyword">in</span> title_array:<br>suggester.addSentence(title)<br><br><span class="hljs-built_in">print</span>(suggester.suggest(<span class="hljs-string">&quot;陈述&quot;</span>, <span class="hljs-number">2</span>))      <span class="hljs-comment"># 语义</span><br><span class="hljs-built_in">print</span>(suggester.suggest(<span class="hljs-string">&quot;危机公关&quot;</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># 字符</span><br><span class="hljs-built_in">print</span>(suggester.suggest(<span class="hljs-string">&quot;mayun&quot;</span>, <span class="hljs-number">1</span>))   <span class="hljs-comment"># 拼音</span><br><span class="hljs-built_in">print</span>(suggester.suggest(<span class="hljs-string">&quot;徐家汇&quot;</span>, <span class="hljs-number">1</span>)) <span class="hljs-comment"># 拼音</span><br></code></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/51419818">自然语言处理基础技术工具篇之HanLP</a></li><li><p><a href="https://www.cnblogs.com/zhuminghui/p/10944090.html">pyhanlp工具超详细介绍</a></p></li><li><p>使用命令<code>hanlp</code>来验证安装，如因网络等原因自动安装失败，可参考<a href="https://github.com/hankcs/pyhanlp/wiki/手动配置">手动配置</a>或<a href="https://github.com/hankcs/pyhanlp/wiki/Windows">Windows指南</a></p></li><li><p><a href="https://github.com/hankcs/pyhanlp/blob/master/tests/demos">pyhanlp Demo</a></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>bert-as-service使用指南</title>
    <link href="/2019/11/08/2019-11-08-bert_as_service%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2019/11/08/2019-11-08-bert_as_service%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h1 id="bert-as-service服务"><a href="#bert-as-service服务" class="headerlink" title="bert-as-service服务"></a>bert-as-service服务</h1><p><a href="https://github.com/hanxiao/bert-as-service">Github地址</a></p><blockquote><p>Using BERT model as a sentence encoding service, i.e. mapping a variable-length sentence to a fixed-length vector.</p></blockquote><p><img src="https://github.com/hanxiao/bert-as-service/raw/master/.github/demo.gif?raw=true" alt="img"></p><h2 id="bert-as-service-是什么"><a href="#bert-as-service-是什么" class="headerlink" title="bert_as_service 是什么"></a>bert_as_service 是什么</h2><p>BERT是Google为预训练语言表示而开发的NLP模型。它利用了在网络上公开提供的大量纯文本数据，并且以无人监督的方式进行了培训。<br>对于每种语言，预训练BERT模型是一个相当昂贵但一次性的过程。幸运的是，Google发布了一些经过预先训练的模型，<a href="https://github.com/google-research/bert#pre-trained-models">您可以从此处下载</a>。</p><p><code>bert-as-service</code> 使用BERT作为句子编码器，并通过ZeroMQ将其托管为服务，从而使您可以仅用两行代码将句子映射为定长表示形式</p><span id="more"></span><h2 id="bert-as-service-特点"><a href="#bert-as-service-特点" class="headerlink" title="bert_as_service 特点"></a>bert_as_service 特点</h2><ul><li>🔭 最新技术：以Google AI发布的经过预训练的12/24层BERT模型为基础，这是NLP社区中的一个里程碑。</li><li>🐣 易于使用：只需两行代码即可获得句子/令牌级别的编码。</li><li>⚡️ 快速：单个Tesla M40 24GB上每秒900句。低延迟，针对速度进行了优化。参见基准。</li><li>🐙 可扩展：可在多个GPU和多个客户端上顺利流畅地扩展，而无需担心并发性。参见基准。</li><li>💎 可靠：经过数十亿个句子的测试；连续几天运行，没有间断或OOM或任何令人讨厌的异常。</li></ul><h1 id="开始使用📖"><a href="#开始使用📖" class="headerlink" title="开始使用📖"></a>开始使用📖</h1><h2 id="bert-as-service-安装"><a href="#bert-as-service-安装" class="headerlink" title="bert_as_service 安装"></a>bert_as_service 安装</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pip</span> install bert_serving<br><span class="hljs-attribute">pip</span> install bert-serving-server==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">0</span> # 服务端<br><span class="hljs-attribute">pip</span> install bert-serving-client==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>  # 客户端，与服务端互相独立<br><span class="hljs-attribute">pip</span> install tensorflow==<span class="hljs-number">1</span>.<span class="hljs-number">15</span>.<span class="hljs-number">0</span>(&gt;=<span class="hljs-number">1</span>.<span class="hljs-number">13</span>.<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h2 id="bert-as-service-入门"><a href="#bert-as-service-入门" class="headerlink" title="bert_as_service 入门"></a>bert_as_service 入门</h2><h3 id="1-下载预训练模型"><a href="#1-下载预训练模型" class="headerlink" title="1. 下载预训练模型"></a>1. 下载预训练模型</h3><p>前往<a href="https://github.com/google-research/bert#pre-trained-models选择模型（本文选择中文模型）下载并解压">https://github.com/google-research/bert#pre-trained-models选择模型（本文选择中文模型）下载并解压</a>.</p><h3 id="2-开启BERT服务"><a href="#2-开启BERT服务" class="headerlink" title="2. 开启BERT服务"></a>2. 开启BERT服务</h3><p>安装服务器后，可以使用 <code>bert-serving-start</code> 命令启动服务</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">bert</span>-serving-start -model_dir /tmp/english_L-<span class="hljs-number">12</span>_H-<span class="hljs-number">768</span>_A-<span class="hljs-number">12</span>/ -num_worker=<span class="hljs-number">4</span> <br></code></pre></td></tr></table></figure><p>其中，<code>-model_dir</code> 是预训练模型的路径，<code>-num_worker</code> 是线程数，表示同时可以处理多少个并发请求</p><p><strong>或者, 使用脚本<code>bert_server.py</code>启动服务</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> bert_serving.<span class="hljs-keyword">server</span>.helper <span class="hljs-keyword">import</span> get_args_parser<br><span class="hljs-keyword">from</span> bert_serving.<span class="hljs-keyword">server</span> <span class="hljs-keyword">import</span> BertServer<br>args = get_args_parser().parse_args([<span class="hljs-string">&#x27;-model_dir&#x27;</span>, <span class="hljs-string">&#x27;.../bert/chinese_L-12_H-768_A-12&#x27;</span>,<br>                                     <span class="hljs-string">&#x27;-port&#x27;</span>, <span class="hljs-string">&#x27;5555&#x27;</span>,<br>                                     <span class="hljs-string">&#x27;-port_out&#x27;</span>, <span class="hljs-string">&#x27;5556&#x27;</span>,<br>                                     <span class="hljs-string">&#x27;-max_seq_len&#x27;</span>, <span class="hljs-string">&#x27;50&#x27;</span>,<br>                                     <span class="hljs-string">&#x27;-num_worker&#x27;</span>,<span class="hljs-string">&#x27;3&#x27;</span>,<br>                                     <span class="hljs-string">&#x27;-mask_cls_sep&#x27;</span>,<br>                                     <span class="hljs-string">&#x27;-cpu&#x27;</span>])<br><span class="hljs-keyword">server</span> = BertServer(args)<br><span class="hljs-keyword">server</span>.<span class="hljs-keyword">start</span>()<br></code></pre></td></tr></table></figure><h3 id="3-在客户端获取句向量"><a href="#3-在客户端获取句向量" class="headerlink" title="3. 在客户端获取句向量"></a>3. 在客户端获取句向量</h3><blockquote><p><strong>远程使用BERT服务</strong></p></blockquote><p>在一台（GPU）机器上启动该服务，然后从另一台（CPU）机器调用该服务，如下所示：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># bert_as_service 对并发的支持不太友好，需要加锁使用！</span><br>lock = threading.Lock()  # 生成锁对象<br>bc = BertClient(<br>    <span class="hljs-attribute">ip</span>=BERT_CONFIG.get(&quot;ip&quot;),<br>    <span class="hljs-attribute">port</span>=BERT_CONFIG.getint(&quot;port&quot;),<br>    <span class="hljs-attribute">port_out</span>=BERT_CONFIG.getint(&quot;port_out&quot;),<br>    <span class="hljs-attribute">timeout</span>=BERT_CONFIG.getint(&quot;timeout&quot;),<br>    <span class="hljs-attribute">check_version</span>=<span class="hljs-literal">False</span>,<br>    <span class="hljs-attribute">check_token_info</span>=<span class="hljs-literal">False</span>,<br>)<br><br>bc.encode([<span class="hljs-string">&#x27;First do it&#x27;</span>, <span class="hljs-string">&#x27;then do it right&#x27;</span>, <span class="hljs-string">&#x27;then do it better&#x27;</span>])  #直接输入整个句子不需要提前分词<br></code></pre></td></tr></table></figure><p>它将返回一个ndarray（或List[List[float]]如果您愿意的话），其中每一行都是代表一个句子的固定长度向量。</p><p>总结：Bert的输出最终有两个结果可用</p><p>sequence_output：维度【batch_size, seq_length, hidden_size】，这是训练后每个token的词向量。</p><p>pooled_output：维度是【batch_size, hidden_size】，每个sequence第一个位置CLS的向量输出，用于分类任务。</p><h2 id="服务器和客户端API"><a href="#服务器和客户端API" class="headerlink" title="服务器和客户端API"></a>服务器和客户端API</h2><h3 id="服务器API"><a href="#服务器API" class="headerlink" title="服务器API"></a>服务器API</h3><div class="table-container"><table><thead><tr><th>论据</th><th>类型</th><th>默认</th><th>描述</th></tr></thead><tbody><tr><td><code>model_dir</code></td><td>str</td><td><em>Required</em></td><td>预训练的BERT模型的文件夹路径。</td></tr><tr><td><code>tuned_model_dir</code></td><td>str</td><td>（可选的）</td><td>微调的BERT模型的文件夹路径。</td></tr><tr><td><code>ckpt_name</code></td><td>str</td><td><code>bert_model.ckpt</code></td><td>检查点文件的文件名。</td></tr><tr><td><code>config_name</code></td><td>str</td><td><code>bert_config.json</code></td><td>BERT模型的JSON配置文件的文件名。</td></tr><tr><td><code>graph_tmp_dir</code></td><td>str</td><td>None</td><td>图形临时文件的路径</td></tr><tr><td><code>max_seq_len</code></td><td>int</td><td><code>25</code></td><td>最大序列长度，较长的序列将在右侧修剪。将其设置为NONE以动态使用（最小）批次中的最长序列。</td></tr><tr><td><code>cased_tokenization</code></td><td>bool</td><td>False</td><td>标记生成器是否应跳过默认的小写字母和重音符号删除。应该用于例如多语言案例的预训练BERT模型。</td></tr><tr><td><code>mask_cls_sep</code></td><td>bool</td><td>False</td><td>用零屏蔽[CLS]和[SEP]上的嵌入。</td></tr><tr><td><code>num_worker</code></td><td>int</td><td><code>1</code></td><td>（GPU / CPU）工人数量运行BERT模型，每个工人在单独的进程中工作。</td></tr><tr><td><code>max_batch_size</code></td><td>int</td><td><code>256</code></td><td>每个工人处理的最大序列数，较大的批次将被分成小批次。</td></tr><tr><td><code>priority_batch_size</code></td><td>int</td><td><code>16</code></td><td>小于此大小的批处理将被标记为高优先级，并在作业队列中向前跳转以更快地获得结果</td></tr><tr><td><code>port</code></td><td>int</td><td><code>5555</code></td><td>用于将数据从客户端推送到服务器的端口</td></tr><tr><td><code>port_out</code></td><td>int</td><td><code>5556</code></td><td>用于将结果从服务器发布到客户端的端口</td></tr><tr><td><code>http_port</code></td><td>int</td><td>None</td><td>接收HTTP请求的服务器端口</td></tr><tr><td><code>cors</code></td><td>str</td><td><code>*</code></td><td>为HTTP请求设置“ Access-Control-Allow-Origin”</td></tr><tr><td><code>pooling_strategy</code></td><td>str</td><td><code>REDUCE_MEAN</code></td><td>用于产生编码矢量的合并策略，有效值为<code>NONE</code>，<code>REDUCE_MEAN</code>，<code>REDUCE_MAX</code>，<code>REDUCE_MEAN_MAX</code>，<code>CLS_TOKEN</code>，<code>FIRST_TOKEN</code>，<code>SEP_TOKEN</code>，<code>LAST_TOKEN</code>。这些策略的说明<a href="https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies">可以在这里找到</a>。要获得序列中每个令牌的编码，请将其设置为<code>NONE</code>。</td></tr><tr><td><code>pooling_layer</code></td><td>list</td><td><code>[-2]</code></td><td>池在其上进行操作的编码层，其中<code>-1</code>表示最后一层，<code>-2</code>表示倒数第二层，<code>[-1, -2]</code>表示并置最后两层的结果，依此类推。</td></tr><tr><td><code>gpu_memory_fraction</code></td><td>float</td><td><code>0.5</code></td><td>每个工作人员应为每个GPU分配的内存总量的一部分</td></tr><tr><td><code>cpu</code></td><td>bool</td><td>False</td><td>在CPU而非GPU上运行</td></tr><tr><td><code>xla</code></td><td>bool</td><td>False</td><td>启用<a href="https://www.tensorflow.org/xla/jit">XLA编译器</a>进行图形优化（<em>实验性！</em>）</td></tr><tr><td><code>fp16</code></td><td>bool</td><td>False</td><td>使用float16精度（实验性）</td></tr><tr><td><code>device_map</code></td><td>list</td><td><code>[]</code></td><td>指定将使用的GPU设备ID的列表（ID从0开始）</td></tr><tr><td><code>show_tokens_to_client</code></td><td>bool</td><td>False</td><td>发送令牌化结果给客户端</td></tr></tbody></table></div><p>注意：</p><p>bert_as_service默认的输出是 【倒数第二层】→【倒数第一层】 的隐藏状态的平均值！！！</p><h3 id="客户端API"><a href="#客户端API" class="headerlink" title="客户端API"></a>客户端API</h3><p>客户端提供了一个名为的Python类<code>BertClient</code>，该类接受如下参数：</p><div class="table-container"><table><thead><tr><th>论据</th><th>类型</th><th>默认</th><th>描述</th></tr></thead><tbody><tr><td><code>ip</code></td><td>str</td><td><code>localhost</code></td><td>服务器的IP地址</td></tr><tr><td><code>port</code></td><td>int</td><td><code>5555</code></td><td>用于将数据从客户端推送到服务器的端口，<em>必须与服务器端配置一致</em></td></tr><tr><td><code>port_out</code></td><td>int</td><td><code>5556</code></td><td>用于将结果从服务器发布到客户端的端口，<em>必须与服务器端配置一致</em></td></tr><tr><td><code>output_fmt</code></td><td>str</td><td><code>ndarray</code></td><td>句子的输出格式编码为numpy数组或python List [List [float]]（<code>ndarray</code>/ <code>list</code>）</td></tr><tr><td><code>show_server_config</code></td><td>bool</td><td><code>False</code></td><td>首次连接时是否显示服务器配置</td></tr><tr><td><code>check_version</code></td><td>bool</td><td><code>True</code></td><td>是否强制客户端和服务器版本相同</td></tr><tr><td><code>identity</code></td><td>str</td><td><code>None</code></td><td>标识客户端的UUID，在多播中很有用</td></tr><tr><td><code>timeout</code></td><td>int</td><td><code>-1</code></td><td>设置客户端上接收操作的超时时间（毫秒）</td></tr></tbody></table></div><p>A <code>BertClient</code>实现以下方法和属性：</p><div class="table-container"><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td><code>.encode()</code></td><td>将字符串列表编码为向量列表</td></tr><tr><td><code>.encode_async()</code></td><td>来自生成器的异步编码批处理</td></tr><tr><td><code>.fetch()</code></td><td>从服务器获取所有编码的向量，并将其返回给生成器，将其与<code>.encode_async()</code>或结合使用<code>.encode(blocking=False)</code>。发送订单未保留。</td></tr><tr><td><code>.fetch_all()</code></td><td>从服务器获取所有编码的向量，并将其返回到列表中，将其与<code>.encode_async()</code>或结合使用<code>.encode(blocking=False)</code>。发送订单被保留。</td></tr><tr><td><code>.close()</code></td><td>优雅地关闭客户端和服务器之间的连接</td></tr><tr><td><code>.status</code></td><td>以JSON格式获取客户端状态</td></tr><tr><td><code>.server_status</code></td><td>以JSON格式获取服务器状态</td></tr></tbody></table></div><h2 id="💡-想了解更多？"><a href="#💡-想了解更多？" class="headerlink" title="💡 想了解更多？"></a>💡 想了解更多？</h2><p>查看我们的教程：</p><ul><li><a href="https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes">在3分钟内构建一个质量检查语义搜索引擎</a></li><li><a href="https://github.com/hanxiao/bert-as-service#serving-a-fine-tuned-bert-model">服务于微调的BERT模型</a></li><li>获得类似ELMo的上下文词嵌入</li><li><a href="https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer">使用自己的令牌生成器</a></li><li>BertClient与tf.dataAPI一起使用</li><li>使用BERT功能和tf.estimator API训练文本分类器</li><li>保存和加载TFRecord数据</li><li><a href="https://github.com/hanxiao/bert-as-service#asynchronous-encoding">异步编码</a></li><li>广播到多个客户端</li><li>在仪表板中监控服务状态</li><li><a href="https://github.com/hanxiao/bert-as-service#using-bert-as-service-to-serve-http-requests-in-json">使用bert-as-service服务于JSON的HTTP请求</a></li><li><a href="https://github.com/hanxiao/bert-as-service#starting-bertserver-from-python">BertServer从Python 开始</a></li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://juejin.im/post/5ce8cab8e51d4577523f22f8">[译] 通过优化 Gunicorn 配置提高性能</a></p><p><a href="https://github.com/hanxiao/bert-as-service#what-is-it">https://github.com/hanxiao/bert-as-service#what-is-it</a></p><p><a href="http://bert-as-service.readthedocs.io/">bert-as-service 最新API文档</a></p><p><a href="https://github.com/hanxiao/bert-as-service#starting-bertserver-from-python">ex1-starting-bertserver-from-python</a></p><p><a href="https://github.com/hanxiao/bert-as-service#broadcasting-to-multiple-clients">ex2-broadcasting-to-multiple-clients</a></p><p><a href="https://github.com/hanxiao/bert-as-service#q-i-encounter-zmqerrorzmqerror-operation-cannot-be-accomplished-in-current-state-when-using-bertclient-what-should-i-do">ex3-<code>zmq.error.ZMQError</code></a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Atexit程序退出前执行操作</title>
    <link href="/2019/10/31/2019-10-31-Atexit%E7%A8%8B%E5%BA%8F%E9%80%80%E5%87%BA%E5%89%8D%E6%89%A7%E8%A1%8C%E6%93%8D%E4%BD%9C/"/>
    <url>/2019/10/31/2019-10-31-Atexit%E7%A8%8B%E5%BA%8F%E9%80%80%E5%87%BA%E5%89%8D%E6%89%A7%E8%A1%8C%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="Python-atexit-退出前执行操作"><a href="#Python-atexit-退出前执行操作" class="headerlink" title="Python atexit - 退出前执行操作"></a>Python atexit - 退出前执行操作</h1><p>有时我们需要在Python脚本退出前执行某些操作，或者在测试时需要模拟程序延时退出的影响，我们都可以使用atexit来帮我们实现。</p><span id="more"></span><h2 id="atexit介绍"><a href="#atexit介绍" class="headerlink" title="atexit介绍"></a>atexit介绍</h2><p>python atexit模块定义了一个register函数，用于在python解释器中注册一个退出函数，这个函数在解释器正常终止时自动执行，一般用来做一些资源清理的操作（如关闭数据库连接等）。</p><p>如果脚本是非正常crash，或者通过os._exit()退出，都不会调用退出函数。</p><h3 id="正常退出前执行"><a href="#正常退出前执行" class="headerlink" title="正常退出前执行"></a>正常退出前执行</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> atexit<br><br><span class="hljs-meta">@atexit.register</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">exit_handle</span>():<br>    time.sleep(<span class="hljs-number">10</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;exit&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">echo</span>():<br>    time.sleep(<span class="hljs-number">5</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;abc&#x27;</span>)<br>   <br>echo()<br></code></pre></td></tr></table></figure><p>可以看到在正常退出情况下，例子脚本在等待5秒后打印’abc’，再等待10秒打印完’exit’后，脚本进程才真正结束。<br>如果在执行time.sleep(5)时CRTL+C，可以看到exit_handle函数是不会执行的。</p><h3 id="✨-被kill时退出前执行"><a href="#✨-被kill时退出前执行" class="headerlink" title="✨ 被kill时退出前执行"></a>✨ 被kill时退出前执行</h3><p>退出函数在脚本被kill时不会产生作用，为了在kill时能正常生效，需要使用signal捕获关闭信号。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs awk">import time<br>import atexit<br>from signal import signal, SIGTERM<br><br>@atexit.register<br>def exit_handle():<br>    time.sleep(<span class="hljs-number">10</span>)<br>    print(<span class="hljs-string">&#x27;exit&#x27;</span>)<br><br><span class="hljs-comment"># 使用signal捕获关闭信号，保证被kill时退出前执行</span><br>signal(SIGTERM, lambda signum, stack_frame: <span class="hljs-keyword">exit</span>(<span class="hljs-number">1</span>))<br><br><span class="hljs-keyword">while</span> True:<br>    pass<br></code></pre></td></tr></table></figure><p>上面例子执行时，使用kill方式结束这个例子脚本，会先执行<code>exit_handle</code>函数，等待10秒输出”exit”，然后才会结束。</p><p>设置信号处理的函数：signal.signal(signalnum, handler)，singnalnum为某个信号，handler为该信号的处理函数。</p><p>例子里signal捕获SIGTERM(kill)信号，再执行一个预设函数，返回exit(1)，python执行exit()函数时都是正常退出，所以会执行退出函数。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.return520.com/posts/9865/">Python atexit - 退出前执行操作</a></p><p><a href="https://www.cnblogs.com/madsnotes/articles/5688681.html">https://www.cnblogs.com/madsnotes/articles/5688681.html</a></p><p><a href="https://www.jb51.net/article/107630.htm">https://www.jb51.net/article/107630.htm</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python并发编程</title>
    <link href="/2019/10/30/2019-10-30-Python%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%EF%BC%88%E7%BA%BF%E7%A8%8B%20vs%20%E8%BF%9B%E7%A8%8B%EF%BC%89/"/>
    <url>/2019/10/30/2019-10-30-Python%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%EF%BC%88%E7%BA%BF%E7%A8%8B%20vs%20%E8%BF%9B%E7%A8%8B%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<ul><li>基础知识<ul><li>什么是进程/线程/二者的关系</li><li>并发和并行有啥区别？同步和异步有啥区别？</li><li>GIL是什么？</li></ul></li><li>多线程<ul><li>threading.Thread</li></ul></li><li>线程池<ul><li>ThreadPoolExecutor</li></ul></li><li>多进程<ul><li>multiprocessing.Process</li></ul></li><li>进程池<ul><li>multiprocessing.Pool() 以及ProcessPoolExecutor</li></ul></li><li>异步IO<ul><li>协程、多线程、多进程的区别？</li></ul></li></ul><span id="more"></span><h2 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h2><p>进程是系统进行资源分配的最小单位，线程是系统进行调度执行的最小单位；</p><p>一个应用程序至少包含一个进程，一个进程至少包含一个线程；</p><p>每个进程在执行过程中拥有独立的内存空间，而一个进程中的线程之间是共享该进程的内存空间的；</p><ul><li>计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。</li><li>假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工。背后的含义就是，单个CPU一次只能运行一个任务。编者注: 多核的CPU就像有了多个发电厂，使多工厂(多进程)实现可能。</li><li>进程就好比工厂的车间，它代表CPU所能处理的单个任务。任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。</li><li>一个车间里，可以有很多工人。他们协同完成一个任务。</li><li>线程就好比车间里的工人。一个进程可以包括多个线程。</li><li>车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存。</li><li>可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。</li><li>一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。这就叫”互斥锁”（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域。</li><li>还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。</li><li>这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做”信号量”（Semaphore），用来保证多个线程不会互相冲突。</li><li>不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。</li></ul><p><strong>一些示例来阐明线程或进程更合适用在哪：</strong></p><ul><li><p>多线程使用场景：<strong>IO操作密集</strong>的场景，比如爬虫，<strong>web访问</strong>等，需要频繁从网络、硬盘、内存等<strong>读写数据</strong>。这种情况 下，因为单线程下的IO操作会有IO等待，造成不必要的时间浪费，因此采用多线程就能在线程A等待时，开启线程B的操作。</p></li><li><p>多进程使用场景：<strong>CPU计算密集</strong>的场景，比如科学计算、<strong>转换或清洗大型数据集</strong>、循环处理等。这些场景因为计算工作量大，由于 GIL 加锁和释放问题，多线程相比单线程更慢</p></li><li><p>请记住，由于线程的<a href="https://python.hamel.dev/concurrency/#how-threads-work">工作方式不同，它们可能</a>比进程具有更高的内存效率。因此，在不需要时使用大量进程会导致内存膨胀。</p></li></ul><p><strong>最重要的是</strong>，请尽量避免考虑尽可能的进程和线程，并尽可能使用<a href="https://numpy.org/">numpy</a>等科学计算库并编写<a href="https://realpython.com/numpy-array-programming/">矢量化的</a>操作。始终需要了解正在使用的库或框架（尤其是数值计算库和其他数据科学库）中可用的并发工具，并在适当时考虑使用它们。</p><h2 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1. 基础知识"></a>1. 基础知识</h2><h3 id="什么是进程？"><a href="#什么是进程？" class="headerlink" title="什么是进程？"></a>什么是进程？</h3><p>进程（Process）是计算机中的程序关于某数据集合上的一次运行活动，是系统进行<strong>资源分配和调度的基本单位</strong>，是操作系统结构的基础。</p><h3 id="什么是线程？"><a href="#什么是线程？" class="headerlink" title="什么是线程？"></a>什么是线程？</h3><p>线程也叫轻量级进程，它是一个基本的CPU执行单元，也是程序执行过程中的最小单元，由线程ID、程序计数器、寄存器集合和堆栈共同组成。线程的引入减小了进程并发执行的开销，提高了操作系统的并发性能，线程没有自己的系统资源。</p><h3 id="进程和线程的关系"><a href="#进程和线程的关系" class="headerlink" title="进程和线程的关系"></a>进程和线程的关系</h3><ul><li>资源分配给进程，同一进程的所有线程共享该进程的所有资源。</li><li>一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。</li></ul><h3 id="并发-vs-并行"><a href="#并发-vs-并行" class="headerlink" title="并发 vs. 并行"></a>并发 vs. 并行</h3><p>并发和并行是对孪生兄弟，概念经常混淆。并发是指能够多任务处理，并行则是是能够<strong>同时</strong>多任务处理。</p><ul><li>并发：是伪并行，即看起来是同时运行，实际是单个CPU+多道技术；</li><li>并行：同时运行，只有具备多个CPU才能实现并行；</li></ul><p><img src="https://ningshixian.github.io/resources/images/并发vs并行.png" alt=""></p><h3 id="同步-vs-异步"><a href="#同步-vs-异步" class="headerlink" title="同步 vs. 异步"></a>同步 vs. 异步</h3><p>同步就是指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去。</p><p>异步是指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态。当有消息返回时系统会通知进程进行处理，这样可以提高执行的效率。</p><h3 id="GIL是什么？"><a href="#GIL是什么？" class="headerlink" title="GIL是什么？"></a>GIL是什么？</h3><ul><li>GIL(Global Interpreter Lock)是最流程的 CPython 解释器中的一个技术术语，中文译为全局解释器锁。</li><li>GIL 的功能是：在 CPython 解释器中执行的每一个 Python 线程，都会先锁住自己，以阻止别的线程执行。</li></ul><p>无论启动多少个线程，有多少个CPU，Python在执行一个进程的时候在同一时刻只允许一个线程运行。 所以，Python是无法利用多核CPU实现多线程的。因此，Python的多线程不适用于CPU密集型任务，但在IO密集型任务上能够节省时间。</p><p><strong>为什么需要GIL ？</strong></p><p>因为Python的线程是调用操作系统的原生线程，这个原生线程是用C语言写的。CPython启动线程的时候调用的C语言的接口。</p><p>每个线程在执行的过程中，Python解释器是控制不了的，只能等结果。如果多个线程一起执行，那结果就不一定正确了。有了GIL，就可以在同一时间只有一个线程能够工作，可以为了避免出错。</p><p>需要注意的是，GIL并不是Python的特性，Python完全可以不依赖于GIL，很多Python解释器是没有GIL的。</p><h2 id="2-多线程"><a href="#2-多线程" class="headerlink" title="2. 多线程"></a>2. 多线程</h2><p>Python中实现多线程的并发需要使用threading模块。</p><h3 id="线程对象的创建"><a href="#线程对象的创建" class="headerlink" title="线程对象的创建"></a>线程对象的创建</h3><ul><li>Thread类直接创建</li></ul><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">import threading<br><br>if __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment">#初始化线程</span><br>    <span class="hljs-built_in">t1</span> = threading.Thread(target=task_thread, args=(<span class="hljs-number">3</span>,))<br>    <span class="hljs-comment">#开启线程</span><br>    <span class="hljs-built_in">t1</span>.start()<br>    <span class="hljs-comment">#等待运行结束</span><br>    <span class="hljs-built_in">t1</span>.<span class="hljs-keyword">join()</span><br></code></pre></td></tr></table></figure><ul><li>Thread类继承式创建，重写run方法</li></ul><p>重写 Thread 类 <code>run()</code> 方法来实现逻辑，这个方法是线程的入口。线程被创建之后并不会马上运行，需要手动调用 <code>start()</code> ， <code>join()</code> 让调用它的线程一直等待直到执行结束（即阻塞调用它的主线程， t 线程执行结束，主线程才会继续执行）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> threading<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyThreading</span>(threading.Thread):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,func,arg</span>):<br>        <span class="hljs-built_in">super</span>(MyThreading,self).__init__()<br>        self.func = func<br>        self.arg = arg<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        self.func(self.arg)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%s say hello&#x27;</span> % self.name)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f1</span>(<span class="hljs-params">args</span>):<br>    <span class="hljs-built_in">print</span>(args)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    obj = MyThreading(f1, <span class="hljs-number">123</span>)<br>    obj.start()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;主线程&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="线程对象的实例方法和属性"><a href="#线程对象的实例方法和属性" class="headerlink" title="线程对象的实例方法和属性"></a>线程对象的实例方法和属性</h3><ul><li><p>t.start()：激活线程，线程被CPU调度后会自动执行t.run()，run方法不要手动调用</p></li><li><p>t.join()：父线程阻塞直到该子线程执行结束</p></li><li><p>t.daemon：声明为守护线程，需在start方法前调用，默认为False。值为True时主进程结束后子线程一起结束，为False时主进程会等待子线程结束后才退出</p></li><li><p>t.is_alive()：判断线程是否为激活状态</p></li><li><p>t.getName()：获取线程的名称</p></li><li><p>t.setName()：设置线程的名称</p></li></ul><h3 id="线程锁Lock"><a href="#线程锁Lock" class="headerlink" title="线程锁Lock"></a>线程锁Lock</h3><blockquote><p><strong>加锁是为了对锁内资源（变量）进行锁定，避免其他线程篡改已被锁定的资源，以达到我们预期的效果</strong></p></blockquote><p>CPU执行任务时，在线程之间是进行随机调度的，并且每个线程可能只执行n条代码后就转而执行另外一条线程。由于在一个进程中的多个线程之间是共享资源和数据的，这就容易造成资源抢夺或脏数据，于是就有了锁的概念，限制某一时刻只有一个线程能访问某个指定的数据。锁通常被用来实现对共享资源的同步访问。</p><p>python在 <code>threading</code> 模块中定义了几种线程锁类，分别是：</p><ul><li><code>Lock</code> 普通锁（不可嵌套）</li><li><code>RLock</code> 普通锁（可嵌套）</li><li><code>Semaphore</code> 信号量</li><li><code>event</code> 事件</li><li><code>condition</code> 条件</li></ul><p><strong>代码示例</strong></p><p>为每一个共享资源创建一个 <code>Lock</code> 对象，当你需要访问该资源时，调用<code>acquire</code>方法来获取锁对象（如果其它线程已经获得了该锁，则当前线程需等待其被释放），待资源访问完后，再调用<code>release</code>方法释放锁。==未加锁部分并发执行，加锁部分串行执行==，举例说明：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import threading<br>import <span class="hljs-built_in">time</span><br><br><span class="hljs-built_in">num</span> = <span class="hljs-number">100</span><br><br>def fun_sub():<br>    #未加锁的代码并发运行<br>    <span class="hljs-built_in">time</span>.sleep(<span class="hljs-number">3</span>)<br>    global <span class="hljs-built_in">num</span><br>    <span class="hljs-built_in">print</span>(&#x27;现在操作共享资源的线程名字是:&#x27;,t.name)<br>    #加锁的代码串行运行<br>    lock.acquire()<br>    num2 = <span class="hljs-built_in">num</span><br>    <span class="hljs-built_in">time</span>.sleep(<span class="hljs-number">0.001</span>)<br>    <span class="hljs-built_in">num</span> = num2-<span class="hljs-number">1</span><br>    lock.release()<br><br><span class="hljs-keyword">if</span> __name__ == &#x27;__main__&#x27;:<br>    <span class="hljs-built_in">print</span>(&#x27;开始测试同步锁 <span class="hljs-built_in">at</span> <span class="hljs-built_in">%s</span>&#x27; <span class="hljs-symbol">%</span> <span class="hljs-built_in">time</span>.ctime())<br><br>    lock = threading.Lock() #创建一把同步锁<br><br>    thread_list = []<br>    <span class="hljs-keyword">for</span> thread <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>        t = threading.Thread(target=fun_sub)<br>        t.start()<br>        thread_list.<span class="hljs-built_in">append</span>(t)<br><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> thread_list:<br>        t.<span class="hljs-built_in">join</span>()<br>    <span class="hljs-built_in">print</span>(&#x27;<span class="hljs-built_in">num</span> <span class="hljs-built_in">is</span> %d&#x27; <span class="hljs-symbol">%</span> <span class="hljs-built_in">num</span>)# <span class="hljs-number">0</span><br>    <span class="hljs-built_in">print</span>(&#x27;结束测试同步锁 <span class="hljs-built_in">at</span> <span class="hljs-built_in">%s</span>&#x27; <span class="hljs-symbol">%</span> <span class="hljs-built_in">time</span>.ctime())<br></code></pre></td></tr></table></figure><p>注意的是，<code>lock.acquire()</code> 和 <code>lock.release()</code> 必须成对出现。否则就有可能造成死锁！</p><p>很多时候，我们虽然知道，他们必须成对出现，但是还是难免会有忘记的时候。为了规避这个问题。我推荐使用使用<strong>上下文管理器 with</strong>来加锁。<code>with</code> 语句更加优雅，也更不容易出错，会在这个代码块执行前自动获取锁，在执行结束后自动释放锁。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> threading<br><br>lock = threading.Lock()<br><span class="hljs-keyword">with</span> lock:<br>    <span class="hljs-comment"># 这里写自己的代码</span><br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><h3 id="GIL-VS-Lock"><a href="#GIL-VS-Lock" class="headerlink" title="GIL VS Lock"></a>GIL VS Lock</h3><p><img src="https://images2015.cnblogs.com/blog/986023/201609/986023-20160918171840449-1089010426.png" alt=""></p><ul><li><p>锁（Lock）是指一个进程对内存资源进行排他占用的状态；</p></li><li><p>死锁（Deadlock）两个或两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁;</p></li></ul><p>两个或两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为 <strong>死锁进程</strong>.</p><h3 id="线程间通信"><a href="#线程间通信" class="headerlink" title="线程间通信"></a>线程间通信</h3><p>线程的一个关键特性是每个线程都是独立运行且状态不可预测。在某些场景下，需要进行线程间的通信。</p><p>threading.Event()：包含一个可由线程设置的信号标志，它允许线程等待某些事件的发生。在初始情况下，Event对象中的信号标志被设置为False。如果有线程等待一个Event对象，那么这个线程将会被一直阻塞直至该标志为True。</p><p>threading.Semaphore()：可用于设置最大并发数。Semaphore管理一个内置的计数器，每当调用acquire()时内置计数器-1，调用release() 时内置计数器+1，计数器不能小于0；当计数器为0时，acquire()将阻塞线程直到其他线程调用release()。</p><p>threading.Condition()：一个条件变量对象允许一个或多个线程在被其它线程所通知之前进行等待。</p><p><strong>代码示例-略</strong></p><ol><li>先说不使用<code>Queue</code>获取多线程的返回值的方法（<a href="https://www.cnblogs.com/hujq1029/p/7219163.html">参考博客</a>）</li><li>使用<code>Queue</code>获取多线程的返回值的方法</li></ol><h2 id="3-线程池"><a href="#3-线程池" class="headerlink" title="3. 线程池"></a>3. 线程池</h2><blockquote><p> 参考：<a href="https://juejin.im/post/5cf913cfe51d45105d63a4d0">《Python线程池 ThreadPoolExecutor 的用法及实战》</a> <a href="https://blog.csdn.net/wsp_1138886114/article/details/90139970">python 多线程与线程池</a></p></blockquote><p>线程池中的线程可以得到重用，避免了频繁的创建新的线程，同时线程数量也是可控的。</p><p>从Python3.2开始，标准库为我们提供了 <code>concurrent.futures</code> 模块，该模块提供异步执行可调用对象高层接口，是Python的原生模块，提供了线程池 ThreadPoolExecutor 和进程池 ProcessPoolExecutor 的实现。</p><p><code>ThreadPoolExecutor(max_workers=None, thread_name_prefix=&#39;&#39;, initializer=None, initargs=())</code>：Executor 子类使用最多 max_workers 个线程的线程池来异步执行调用（默认值是 CPU 个数的 5 倍！）。以下是它的方法：</p><ul><li><p><code>submit(fn, *args, **kwargs)</code>：调度可调用对象 fn，以 <code>fn(*args **kwargs)</code> 方式执行并返回 Future 对象。</p></li><li><p><code>map(func, *iterables, timeout=None, chunksize=1)</code>：类似于 <code>map(func, *iterables)</code></p></li><li><p><code>shutdown(wait=True)</code>：当待执行的期程完成执行后向执行者发送信号，它就会释放正在使用的任何资源。</p></li></ul><p><strong>优点：</strong></p><ol><li>主线程可以获取某一个线程（或者任务的）的状态，以及返回值。</li><li>当一个线程完成的时候，主线程能够立即知道。</li><li>让多线程和多进程的编码接口一致。</li></ol><h3 id="线程池的基本使用"><a href="#线程池的基本使用" class="headerlink" title="线程池的基本使用"></a>线程池的基本使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor<br><span class="hljs-keyword">import</span> time<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">spider</span>(<span class="hljs-params">page</span>):<br>    time.sleep(page)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;crawl task<span class="hljs-subst">&#123;page&#125;</span> finished&quot;</span>)<br>    <span class="hljs-keyword">return</span> page<br><br><span class="hljs-keyword">with</span> ThreadPoolExecutor(max_workers=<span class="hljs-number">5</span>) <span class="hljs-keyword">as</span> t:  <span class="hljs-comment"># 创建一个最大容纳数量为5的线程池</span><br>    task1 = t.submit(spider, <span class="hljs-number">1</span>)<br>    task2 = t.submit(spider, <span class="hljs-number">2</span>)  <span class="hljs-comment"># 通过submit提交执行的函数到线程池中</span><br>    task3 = t.submit(spider, <span class="hljs-number">3</span>)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;task1: <span class="hljs-subst">&#123;task1.done()&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;task2: <span class="hljs-subst">&#123;task2.done()&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;task3: <span class="hljs-subst">&#123;task3.done()&#125;</span>&quot;</span>)<br>    <br>    time.sleep(<span class="hljs-number">2.5</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;task1: <span class="hljs-subst">&#123;task1.done()&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;task2: <span class="hljs-subst">&#123;task2.done()&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;task3: <span class="hljs-subst">&#123;task3.done()&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_result</span>(<span class="hljs-params">future</span>):<br>            <span class="hljs-built_in">print</span>(future.result())  <span class="hljs-comment"># 通过result来获取返回值</span><br>        <span class="hljs-comment"># 为task添加线程完成的回调函数</span><br>        task1.add_done_callback(get_result)      <br>        task2.add_done_callback(get_result)<br>        task3.add_done_callback(get_result)<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;raise an exception: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(e))<br></code></pre></td></tr></table></figure><p><strong>代码讲解：</strong></p><ol><li>使用 with 语句 ，通过 ThreadPoolExecutor 构造实例，同时传入 max_workers 参数来设置线程池中最多能同时运行的线程数目。</li><li>使用 submit 函数来提交线程需要执行的任务到线程池中，并返回该任务的句柄（类似于文件、画图），注意 submit() 不是阻塞的，而是立即返回。</li><li>通过使用 done() 方法判断该任务是否结束。上面的例子可以看出，提交任务后立即判断任务状态，显示四个任务都未完成。在延时2.5后，task1 和 task2 执行完毕，task3 仍在执行中。</li><li>使用 result() 方法可以获取任务的返回值。</li><li>使用 add_done_callback() 方法来获取线程任务的返回值</li></ol><h3 id="线程池的主要方法"><a href="#线程池的主要方法" class="headerlink" title="线程池的主要方法"></a>线程池的主要方法</h3><p><strong>map</strong></p><p>concurrent.futures.ThreadPoolExecutor，在提交任务的时候，有两种方式，一种是submit（）函数，另一种是map（）函数，两者的主要区别在于：</p><ul><li>map可以保证输出的顺序, submit输出的顺序是乱的</li><li>如果你要提交的任务的函数是一样的，就可以简化成map。但是假如提交的任务函数是不一样的，或者执行的过程之可能出现异常（使用map执行过程中发现问题会直接抛出错误）就要用到submit（）</li><li>submit和map的参数是不同的，submit每次都需要提交一个目标函数和对应的参数，map只需要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就可以。</li></ul><figure class="highlight nim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs nim"><span class="hljs-keyword">with</span> <span class="hljs-type">ProcessPoolExecutor</span>(max_workers=<span class="hljs-number">3</span>) <span class="hljs-keyword">as</span> executor:<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">result</span> <span class="hljs-keyword">in</span> executor.map(<span class="hljs-keyword">func</span>, [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>]):<span class="hljs-comment"># 返回线程执行的结果</span><br>        print(<span class="hljs-built_in">result</span>)<br></code></pre></td></tr></table></figure><p><strong>wait</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED, ALL_COMPLETED<br>import time<br><br>def spider(page):<br>    time.sleep(page)<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;crawl task&#123;page&#125; finished&quot;</span>)<br>    return<span class="hljs-built_in"> page</span><br><span class="hljs-built_in"></span><br>with ThreadPoolExecutor(<span class="hljs-attribute">max_workers</span>=5) as t: <br>    all_task = [t.submit(spider, page) <span class="hljs-keyword">for</span><span class="hljs-built_in"> page </span><span class="hljs-keyword">in</span> range(1, 5)]<br>    wait(all_task, <span class="hljs-attribute">return_when</span>=FIRST_COMPLETED)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;finished&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(wait(all_task, <span class="hljs-attribute">timeout</span>=2.5))<br></code></pre></td></tr></table></figure><ol><li>代码中返回的条件是：当完成第一个任务的时候，就停止等待，继续主线程任务</li><li>由于设置了延时， 可以看到最后只有 task4 还在运行中</li></ol><p><strong>√as_completed</strong></p><p>上面虽然提供了判断任务是否结束的方法，但是不能在主线程中一直判断啊。最好的方法是当某个任务结束了，就给主线程返回结果，而不是一直判断每个任务是否结束。</p><p>ThreadPoolExecutor 中 的 as_completed() 就是这样一个方法，当子线程中的任务执行完后，直接用 result() 获取返回结果。用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor, as_completed<br><span class="hljs-keyword">import</span> time<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">spider</span>(<span class="hljs-params">page</span>):<br>    time.sleep(page)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;crawl task<span class="hljs-subst">&#123;page&#125;</span> finished&quot;</span>)<br>    <span class="hljs-keyword">return</span> page<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-keyword">with</span> ThreadPoolExecutor(max_workers=<span class="hljs-number">20</span>) <span class="hljs-keyword">as</span> t:<br>        obj_list = []<br>        <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>):<br>            obj = t.submit(spider, page)<br>            obj_list.append(obj)<br><br>        <span class="hljs-keyword">for</span> future <span class="hljs-keyword">in</span> as_completed(obj_list):<br>            data = future.result()<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;main: <span class="hljs-subst">&#123;data&#125;</span>&quot;</span>)<br>            <br><span class="hljs-comment"># 执行结果</span><br>crawl task1 finished<br>main: <span class="hljs-number">1</span><br>crawl task2 finished<br>main: <span class="hljs-number">2</span><br>crawl task3 finished<br>main: <span class="hljs-number">3</span><br>crawl task4 finished<br>main: <span class="hljs-number">4</span><br></code></pre></td></tr></table></figure><p>as_completed() 方法是一个生成器，在没有任务完成的时候，会一直阻塞，除非设置了 timeout。</p><p>当有某个任务完成的时候，会 yield 这个任务，就能执行 for 循环下面的语句，然后继续阻塞住，循环到所有的任务结束。同时，先完成的任务会先返回给主线程。</p><h2 id="4-多进程"><a href="#4-多进程" class="headerlink" title="4. 多进程"></a>4. 多进程</h2><p>multiprocessing 是一个用与 threading 模块相似API的支持产生进程的包。 multiprocessing 包同时提供本地和远程并发，使用子进程代替线程，有效避免GIL带来的影响。</p><h3 id="4-1-进程对象的创建"><a href="#4-1-进程对象的创建" class="headerlink" title="4.1 进程对象的创建"></a>4.1 进程对象的创建</h3><p>Process类直接创建</p><p>Process类继承式创建，重写run方法</p><h3 id="4-2-进程对象的实例方法和属性"><a href="#4-2-进程对象的实例方法和属性" class="headerlink" title="4.2 进程对象的实例方法和属性"></a>4.2 进程对象的实例方法和属性</h3><p>p.start()：启动进程</p><p>p.join([timeout])：父进程阻塞直到该子进程执行结束</p><p>p.is_alive()：判断进程是否为激活状态</p><p>p.terminate/kill()：终止进程</p><p>p.daemon：声明为守护进程，需在start方法前调用，默认为False。值为True时进程结束后子进程一起结束，为False时主进程会等待子进程结束后才退出</p><p>p.pid：返回进程ID</p><p>p.exitcode：返回进程退出代码</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import multiprocessing as mp<br><br><span class="hljs-selector-tag">p</span> = mp<span class="hljs-selector-class">.Process</span>(target=task, args=(<span class="hljs-selector-tag">i</span>,))<br><span class="hljs-selector-tag">p</span><span class="hljs-selector-class">.start</span>()<br><span class="hljs-selector-tag">p</span><span class="hljs-selector-class">.join</span>()<br></code></pre></td></tr></table></figure><h3 id="4-3-multiprocessing模块提供的方法"><a href="#4-3-multiprocessing模块提供的方法" class="headerlink" title="4.3 multiprocessing模块提供的方法"></a>4.3 multiprocessing模块提供的方法</h3><p>multiprocessing.current_process()：返回与当前进程相对应的 Process 对象</p><p>multiprocessing.active_children()：返回当前进程存活的子进程的列表</p><p>multiprocessing.set_executable()：设置在启动子进程时使用的 Python 解释器路径</p><h3 id="4-4-进程间通信"><a href="#4-4-进程间通信" class="headerlink" title="4.4 进程间通信"></a>4.4 进程间通信</h3><ul><li><p>multiprocessing.Event()：发送事件信号，与threading.Event()类似。</p></li><li><p>multiprocessing.Semaphore()：可控制最大并发数，类似于threading.Semaphore。</p></li><li><p>multiprocessiong.Queue()：主要用来在多个进程之间实现通信。Queue是多进程安全队列，Queue通过put和get方法来实现多进程之间的数据传递。</p></li><li><p>multiprocessiong.Pipe()：Pipe常用来在两个进程之间实现通信。该方法返回一个二元元组 (conn1,conn2)，代表一个管道的两端。Pipe方法有个duplex参数，默认为True，表示该管道处于全双工模式下，conn1和conn2都可以进行收发。当其为False时，表示该管道处于半双工模式下，conn1只能进行接收消息，conn2只能发送消息。send和recv方法分别是发送和接收消息的方法。</p></li><li><p>共享内存：multiprocessing.Value()，multiprocessing.Array()。可以使用 multiprocessing.sharedctypes 模块，该模块支持创建从共享内存分配的任意ctypes对象。</p></li><li><p>multiprocessing.Manager()：数据管理器，控制一个服务器进程，该进程保存Python对象并允许其他进程使用代理操作它们。</p></li></ul><h2 id="5-进程池"><a href="#5-进程池" class="headerlink" title="5. 进程池"></a>5. 进程池</h2><p>进程池可以减少进程创建和释放的开销。multiprocessing.Pool() 描述了一个工作进程池，常用方法有：</p><ul><li><p>apply(func[, args[, kwds]])：使用arg和kwds参数调用func函数，结果返回前会一直阻塞，且func函数仅被pool中的一个进程运行。</p></li><li><p>apply_async(func[,args[,kwds[,callback[,error_callback]]]])：apply()方法的一个变体，会返回一个结果对象。如果callback被指定，那么callback可以接收一个参数然后被调用，当结果准备好回调时会调用callback，调用失败时，则用error_callback替换callback。 callback应被立即完成，否则处理结果的线程会被阻塞。</p></li><li><p>close()：阻止更多的任务提交到pool，待任务完成后，工作进程会退出。</p></li><li><p>terminate()：不管任务是否完成，立即停止工作进程。在对pool对象进程垃圾回收的时候，会立即调用terminate()。</p></li><li><p>join()：wait工作线程的退出，在调用join()前，必须调用close()或terminate()。</p></li><li><p>map(func, iterable[, chunksize])</p></li><li><p>map_async(func, iterable[, chunksize[, callback[, error_callback]]])</p></li></ul><p>除此之外，还可以使用concurrent.futures.ProcessPoolExecutor()，接口与ThreadPoolExecutor线程池相同。</p><h4 id="ProcessPoolExecutor的基本使用"><a href="#ProcessPoolExecutor的基本使用" class="headerlink" title="ProcessPoolExecutor的基本使用"></a>ProcessPoolExecutor的基本使用</h4><p>从Python3.2开始，标准库为我们提供了 <code>concurrent.futures</code> 模块，它提供了 <code>ThreadPoolExecutor</code> (线程池)和 <code>ProcessPoolExecutor</code> (进程池)两个类，是对 threading 和 multiprocessing 的进行了高级别的抽象， 暴露出统一的接口，帮助开发者非常方便的实现异步调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ProcessPoolExecutor<br><span class="hljs-keyword">import</span> time<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">spider</span>(<span class="hljs-params">page</span>):<br>    time.sleep(page)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;crawl task<span class="hljs-subst">&#123;page&#125;</span> finished&quot;</span>)<br>    <span class="hljs-keyword">return</span> page<br><br><span class="hljs-keyword">with</span> ProcessPoolExecutor() <span class="hljs-keyword">as</span> p:  <span class="hljs-comment">#不填则默认为cpu的个数</span><br>    task1 = p.submit(spider, <span class="hljs-number">1</span>)<br>    task2 = p.submit(spider, <span class="hljs-number">2</span>)  <span class="hljs-comment"># 通过submit提交执行的函数到线程池中</span><br>    task3 = p.submit(spider, <span class="hljs-number">3</span>)<br>    <br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_result</span>(<span class="hljs-params">future</span>):<br>            <span class="hljs-built_in">print</span>(future.result())  <span class="hljs-comment"># 通过result来获取返回值</span><br>        <span class="hljs-comment"># 为task添加线程完成的回调函数</span><br>        task1.add_done_callback(get_result)      <br>        task2.add_done_callback(get_result)<br>        task3.add_done_callback(get_result)<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;raise an exception: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(e))<br></code></pre></td></tr></table></figure><h2 id="6-异步IO"><a href="#6-异步IO" class="headerlink" title="6. 异步IO"></a>6. 异步IO</h2><blockquote><p>参考资料：</p><p><a href="https://madmalls.com/blog/post/io-models/">《第1章：I/O Models 阻塞/非阻塞 同步/异步》</a></p><p><a href="https://madmalls.com/blog/post/concurrent-programming-for-python/">《第2章：Python 并发编程》</a></p><p><a href="https://madmalls.com/blog/post/asyncio-howto-in-python3/">《第8章：使用 asyncio 模块实现并发》</a></p><p><a href="https://madmalls.com/blog/post/aiohttp-howto-in-python3/">《第9章：使用 asyncio + aiohttp 并发下载》</a></p></blockquote><p>首先我们看一下多进程、多线程、异步 IO 这三者的区别，以请求一个网页为例，如下图所示，把这类任务抽象成3部分，<strong>绿色部分</strong>代表请求前以及请求前的相关运算处理工作；<strong>白色部分</strong>代表请求中，等待远程服务器返回结果；<strong>蓝色部分</strong>代表得到服务器返回结果，并进行相关处理。</p><h3 id="协程、多线程、多进程的区别？"><a href="#协程、多线程、多进程的区别？" class="headerlink" title="协程、多线程、多进程的区别？"></a>协程、多线程、多进程的区别？</h3><p><img src="https://hub.zhuanfou.com/5119_1562418835_9220.jpg" alt="img"></p><ul><li><strong>单线程</strong>的网络IO部分，是会阻塞程序的运行；发出请求到请求返回的这段时间，该程序会占着CPU的该线程的坑位。（注：所谓的单线程就是一个进程只开一个线程）</li></ul><p>如上所示，<strong>单线程</strong>的网络IO部分，是会阻塞程序的运行；发出请求到请求返回的这段时间，该程序会占着CPU的该线程的坑位。（注：所谓的单线程就是一个进程只开一个线程）</p><ul><li><p><strong>多进程</strong></p><ul><li>是 <code>抢占式多任务（preemptive）</code>，同步并行的，由操作系统调度</li><li>多进程是三种并发模式中唯一可以使用多核 CPU 的模式</li></ul></li><li><p><strong>多线程</strong></p><ul><li>是 <code>抢占式多任务（preemptive）</code>，同步并行的，由操作系统调度</li><li>Python 默认的解释器 CPython 由于 <code>GIL</code> 的存在，不能使用多核 CPU，只能运行在一个核心上</li><li>线程是不安全的，线程会是挂的，经常你争我夺，当然会没有那么稳定；</li><li>每个线程也具有独立的资源，比如栈、寄存器等；</li><li>另外线程之间的切换也会造成额外资源的开销</li></ul></li><li><p><strong>协程★★</strong></p><ul><li><strong>协程</strong>，又称为<strong>微线程</strong>，英文名为<strong>Coroutine</strong>，<strong>它的本质还是一个单线程</strong>，在单线程内实现调度，而不是在CPU层面切换线程，从而避免不必要的开销。</li><li>异步是指在 <code>单线程</code> 中 <code>并发</code> 执行多个任务。当一个任务在等待数据时，它会释放 CPU 资源，转而执行其它任务，通过程序员自己主动切换任务来最小化空闲时间（IO等待的时间），这就是所谓的<strong>异步非阻塞</strong>。</li></ul></li></ul><h3 id="异步-I-O-操作"><a href="#异步-I-O-操作" class="headerlink" title="异步 I/O 操作"></a>异步 I/O 操作</h3><p><code>多进程</code> 或 <code>多线程</code> 方案中，操作系统不可能无上限地增加进程或线程，一方面会占用大量内存，影响系统稳定性；另一方面 <code>上下文切换</code> 的开销也很大，一旦进程或线程的数量过多时，CPU 的大部分时间就花在 <code>上下文切换</code> 上了，真正运行代码的时间就少了，结果是导致性能严重下降。</p><p>那么，有没有什么办法可以减少大量进程或者线程的创建产生的大量内存占用？其实是有的，就是利用所谓的<strong>线程池</strong>或者<strong>进程池</strong>；既然减少了创建和销毁对象产生的开销，那么进程或者线程切换的开销有没有办法减少呢？其实是有的，我们直接使用 <strong>异步 IO</strong> 就可以了。</p><p>异步 I/O 框架中，使用 <code>单线程</code>，利用 <code>事件循环</code>，不断地重复 <strong>“监控到事件发生 —&gt; 处理事件”</strong> 这一过程。同时，还要把每个 <code>阻塞型操作（blocking operation ）</code> 替换成 <code>非阻塞的异步调用（non-blocking asynchronous call）</code>，当某个任务中遇到耗时的 I/O 操作时，才会把控制权 <code>交还</code> 给 <code>事件循环</code>，然后 <code>事件循环</code> 会执行另一个任务。这样就可以避免阻塞型调用中止整个应用程序的进程，合理地解决了 CPU 高速执行能力和 I/O 设备的龟速严重不匹配问题。</p><p><strong><code>异步 I/O 操作</code> 是指，你发起一个 I/O 操作（比如，等待网络图片数据的到来），却不用等它结束，你可以继续去做其它的事情，当它结束时，你会得到通知，然后再回来接着处理这个 I/O 后续的操作。而 <code>同步 I/O 操作</code> 则会被阻塞在 I/O 操作上直到它完成，这期间 CPU 做了很多事，只是没有运行你的程序</strong></p><p>Python 的异步 IO 相关模块非常多</p><ul><li>比如 aiohttp、gevent……这里主要介绍 aiohttp 模块</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://morvanzhou.github.io/tutorials/python-basic/threading/">莫烦-多线程</a></li><li><a href="https://blog.csdn.net/zhangphil/article/details/88577091">Python等待所有线程任务完成</a></li><li><a href="https://mp.weixin.qq.com/s/Hgp-x-T3ss4IiVk2_4VUrA">一篇文章搞懂Python多线程简单实现和GIL</a></li><li><a href="https://mp.weixin.qq.com/s/RZSBe2MG9tsbUVZLHxK9NA">一篇文章理清Python多线程同步锁，死锁和递归锁</a></li><li><a href="http://www.codeceo.com/article/python-thread-process.html">Python线程、进程和协程详解</a></li><li><a href="https://www.cnblogs.com/zingp/p/5878330.html">python中的多线程、多进程</a></li><li><a href="http://aaronxu.eagleslab.com/index.php/Programming/206.html">python 并发编程：线程</a></li><li><a href="https://morvanzhou.github.io/tutorials/python-basic/multiprocessing/">莫烦-多进程</a></li><li><a href="https://juejin.im/post/5b17f4305188257d6b5cff6f">Python并发编程之谈谈线程中的“锁机制”（三）</a></li><li><a href="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/index.html">Python并行编程 中文版</a></li><li><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/chapters/p12_concurrency.html">Python Cookbook 3rd Edition Documentation</a></li></ul><ul><li><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2OTQxMTM4OQ==&amp;mid=2247491746&amp;idx=2&amp;sn=f8c2b67600ff911d6d78556dc7369dca&amp;chksm=eae215f0dd959ce6bc112fcfc4f852fa6ded57ea981b80fd30273067e08d6936a4ad23b7b20e&amp;scene=0&amp;xtrack=1#rd">15分钟读懂进程线程、同步异步、阻塞非阻塞、并发并行，太实用了！</a></p></li><li><p><a href="https://python.hamel.dev/concurrency/">Python Concurrency: The Tricky Bits</a>                  </p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>多线程多进程</tag>
      
      <tag>python并发</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>✋🏻手把手搭建深度学习环境</title>
    <link href="/2019/10/24/2019-10-24-%E2%9C%8B%F0%9F%8F%BB%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    <url>/2019/10/24/2019-10-24-%E2%9C%8B%F0%9F%8F%BB%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<h2 id="预告：三种套餐组合方式"><a href="#预告：三种套餐组合方式" class="headerlink" title="预告：三种套餐组合方式"></a>预告：三种套餐组合方式</h2><p>第一种</p><ol><li>Nvidia GPU 驱动（v375）——允许系统获得 GPU 带来的加速。</li><li>CUDA（v8.0）——GPU C 语言库。「计算同一设备架构」。</li><li>cuDNN（v6.0.21）——基于 CUDA 的深度学习基元库。「CUDA 深度学习库」。</li><li>TensorFlow（v1.3）——谷歌开发的深度学习框架，保证TensorFlow&lt;=1.4</li></ol><p>第二种</p><ol><li>Nvidia GPU 驱动（v384）——允许系统获得 GPU 带来的加速。</li><li>CUDA（v9.0）——GPU C 语言库。「计算同一设备架构」。</li><li>cuDNN（v7.4.2）——基于 CUDA 的深度学习基元库。「CUDA 深度学习库」。</li><li>TensorFlow（v1.12.0）——谷歌开发的深度学习框架，保证TensorFlow&gt;=1.5</li></ol><p>第三种</p><ol><li>Nvidia GPU 驱动（v418）——允许系统获得 GPU 带来的加速。</li><li>CUDA（v10.1）——GPU C 语言库。「计算同一设备架构」。</li><li>cuDNN（v7.6.4）——基于 CUDA 的深度学习基元库。「CUDA 深度学习库」。</li><li>TensorFlow（v1.14.0）——谷歌开发的深度学习框架<h2 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h2></li></ol><p>前提机器上面有支持CUDA的Nvidia GPU，查看支持CUDA的GPU列表:</p><p><a href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a></p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">lspci <span class="hljs-string">| grep -i nvidia</span><br></code></pre></td></tr></table></figure><p>我这边服务器使用 <strong>NVIDIA显卡的版本信息</strong>:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Tesla</span> V100 SXM2 <span class="hljs-number">16</span>GB<br></code></pre></td></tr></table></figure><p><strong>验证系统是否是受支持的Linux版本</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">uname</span> -m &amp;&amp; <span class="hljs-built_in">cat</span> /etc/redhat-release<br><span class="hljs-comment"># CentOS Linux release 7.6.1810 (Core)</span><br></code></pre></td></tr></table></figure><p>到这里查看受支持的Linux版本：<a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements">http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements</a></p><p><strong>验证系统是否有GCC编译环境</strong></p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gcode">gcc -v<br><span class="hljs-attr"># gcc version 4</span><span class="hljs-number">.8</span><span class="hljs-number">.5</span> <span class="hljs-number">20150623</span> <span class="hljs-comment">(Red Hat 4.8.5-36)</span> <span class="hljs-comment">(GCC)</span><br></code></pre></td></tr></table></figure><p>没有的话需要先安装GCC，Centos7的最小化安装一般勾选上开发软件都会自动安装GCC</p><p><strong>验证系统是否安装了正确的内核头文件和开发包</strong></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">sudo yum install kernel-devel-<span class="hljs-constructor">$(<span class="hljs-params">uname</span> -<span class="hljs-params">r</span>)</span> kernel-headers-<span class="hljs-constructor">$(<span class="hljs-params">uname</span> -<span class="hljs-params">r</span>)</span><br></code></pre></td></tr></table></figure><h2 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h2><p><strong>禁用nouveau</strong></p><p>nouveau是一个第三方开源的Nvidia驱动，一般Linux安装的时候默认会安装这个驱动。 这个驱动会与Nvidia官方的驱动冲突，在安装Nvidia驱动和和CUDA之前应先禁用nouveau</p><p><strong>查看系统是否正在使用nouveau</strong></p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">lsmod <span class="hljs-string">| grep nouveau</span><br></code></pre></td></tr></table></figure><p><strong>如果有显示内容，则进行以下的步骤： Centos7禁用方法</strong></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">#新建一个配置文件sudo vim /etc/modprobe.d/blacklist-nouveau.conf#写入以下内容blacklist nouveauoptions nouveau modeset=<span class="hljs-number">0</span>#保存并退出:wq#备份当前的镜像sudo mv /boot/initramfs-<span class="hljs-constructor">$(<span class="hljs-params">uname</span> -<span class="hljs-params">r</span>)</span>.img /boot/initramfs-<span class="hljs-constructor">$(<span class="hljs-params">uname</span> -<span class="hljs-params">r</span>)</span>.img.bak#建立新的镜像sudo dracut /boot/initramfs-<span class="hljs-constructor">$(<span class="hljs-params">uname</span> -<span class="hljs-params">r</span>)</span>.img <span class="hljs-constructor">$(<span class="hljs-params">uname</span> -<span class="hljs-params">r</span>)</span>#重启sudo reboot#最后输入上面的命令验证lsmod <span class="hljs-pattern-match">| grep nouveau</span><br></code></pre></td></tr></table></figure><p><strong>安装驱动</strong></p><p>GPU 服务器正常工作需安装正确的基础设施软件，对 NVIDIA 系列 GPU 而言，有两个层次的软件包需要安装：</p><ul><li>驱动 GPU 工作的硬件驱动程序。</li><li>上层应用程序所需要的库。</li></ul><p>若把 NVIDIA GPU 用作通用计算，需要安装 Tesla Driver + CUDA：</p><p><strong>rpm 包安装</strong></p><ol><li>打开 NVIDIA 驱动下载链接 <a href="http://www.nvidia.com/Download/Find.aspx">http://www.nvidia.com/Download/Find.aspx</a> 。 </li><li>选择支持 RPM 包的操作系统，并获取该 RPM 包的下载链接。例如：选择 Linux 64-bit  CUDA Toolkit: 10.1， 得到下载链接：<a href="https://www.nvidia.cn/Download/driverResults.aspx/152484/cn，按照提示下载并安装驱动。">https://www.nvidia.cn/Download/driverResults.aspx/152484/cn，按照提示下载并安装驱动。</a> </li><li>使用<code>rpm</code>命令安装 rpm 包。 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">rpm</span> -i nvidia-driver-local-repo-rhel7-<span class="hljs-number">418</span>.<span class="hljs-number">87</span>.<span class="hljs-number">01</span>-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>-<span class="hljs-number">1</span>.x86_64.rpm<br></code></pre></td></tr></table></figure></li></ol><ol><li>使用<code>yum</code>命令清除缓存。 <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">yum clean <span class="hljs-keyword">all</span><br></code></pre></td></tr></table></figure></li></ol><ol><li>使用yum命令安装驱动。 <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">yum <span class="hljs-keyword">install</span> cuda-drivers<br></code></pre></td></tr></table></figure></li></ol><ol><li>重启机器 <figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">reboot</span><br></code></pre></td></tr></table></figure></li></ol><ol><li>运行<code>nvidia-smi</code>能输出正确信息代表驱动安装成功。 </li></ol><h2 id="CUDA-安装"><a href="#CUDA-安装" class="headerlink" title="CUDA 安装"></a>CUDA 安装</h2><p>CUDA (Compute Unified Device Architecture) 是显卡厂商 NVIDIA 推出的运算平台。 CUDA™ 是一种由 NVIDIA 推出的通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。 它包含了 CUDA 指令集架构（ISA）以及 GPU 内部的并行计算引擎。 开发人员现在可以使用 C 语言, C++ , FORTRAN 来为 CUDA™ 架构编写程序，所编写出的程序可以在支持 CUDA™ 的处理器上以超高性能运行。</p><blockquote><p>最快捷的安装方法：conda install cudatoolkit=10.0</p></blockquote><p>采用 NVIDIA 显卡的GPU 云服务器，需要安装 CUDA 开发运行环境：</p><ol><li>先查看<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">官方给的CUDA和显卡驱动Driver的匹配兼容性</a>，进行CUDA Toolkit的版本选择 （9.0~10.1 应该均可） </li><li>选择操作系统和安装包。以 <a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=CentOS&amp;target_version=7&amp;target_type=rpmlocal">CentOS 7 64 位的CUDA驱动下载</a>为例，下载 Base 以及 Patch 补丁 </li><li>在 CUDA 安装包所在目录下运行如下命令<strong>（run_file）</strong><br>下载10.0版本的CUDA <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">wget -c https:<span class="hljs-regexp">//</span>developer.nvidia.com<span class="hljs-regexp">/compute/</span>cuda<span class="hljs-regexp">/10.0/</span>Prod<span class="hljs-regexp">/local_installers/</span>cuda_10.<span class="hljs-number">0.130</span>_410.<span class="hljs-number">48</span>_linux<br>wget https:<span class="hljs-regexp">//</span>developer.download.nvidia.com<span class="hljs-regexp">/compute/</span>cuda<span class="hljs-regexp">/10.2/</span>Prod<span class="hljs-regexp">/local_installers/</span>cuda_10.<span class="hljs-number">2.89</span>_440.<span class="hljs-number">33.01</span>_linux.run<br>sudo sh cuda_10.<span class="hljs-number">2.89</span>_440.<span class="hljs-number">33.01</span>_linux.run<br></code></pre></td></tr></table></figure></li></ol><p>下载10.1版本的CUDA<br>安装  </p><ul><li>安装具体细节：之前已经安装过显卡驱动程序，因此在提问是否安装显卡驱动时选择no，其他默认(NVIDIA Accelerated Graphics Driver) <figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs gradle">（是否同意条款，必须同意才能继续安装）<br>  accept<span class="hljs-regexp">/decline/</span>quit: accept<br><br>  （这里不要安装驱动，因为已经安装最新的驱动了，否则可能会安装旧版本的显卡驱动，导致重复登录的情况）<br>  Install NVIDIA Accelerated Graphics Driver <span class="hljs-keyword">for</span> Linux-x86_64 <span class="hljs-number">410.48</span>?<br>  (y)es<span class="hljs-regexp">/(n)o/</span>(q)uit: n<br><br>  Install the CUDA <span class="hljs-number">10.0</span> Toolkit?（是否安装CUDA <span class="hljs-number">10</span> ，这里必须要安装）<br>  (y)es<span class="hljs-regexp">/(n)o/</span>(q)uit: y<br><br>  Enter Toolkit Location（安装路径，使用默认，直接回车就行）<br>   [ <span class="hljs-keyword">default</span> is <span class="hljs-regexp">/usr/</span>local/cuda-<span class="hljs-number">10.0</span> ]:  <br><br>  <span class="hljs-keyword">Do</span> you want to install a symbolic link at <span class="hljs-regexp">/usr/</span>local/cuda?（同意创建软链接）<br>  (y)es<span class="hljs-regexp">/(n)o/</span>(q)uit: y<br><br>  Install the CUDA <span class="hljs-number">10.0</span> Samples?（不用安装测试，本身就有了）<br>  (y)es<span class="hljs-regexp">/(n)o/</span>(q)uit: n<br><br>  Installing the CUDA Toolkit in <span class="hljs-regexp">/usr/</span>local/cuda-<span class="hljs-number">10.0</span> ...（开始安装）<br>  <br>  ===========<br>  = Summary =<br>  ===========<br><br>  Driver:   Not Selected<br>  Toolkit:  Installed in <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span><br>  Samples:  Installed in <span class="hljs-regexp">/home/</span>lhadmin/, but missing recommended libraries<br><br>  Please make sure that<br>   -   PATH <span class="hljs-keyword">includes</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>bin<br>   -   LD_LIBRARY_PATH <span class="hljs-keyword">includes</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64, or, add <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64 to <span class="hljs-regexp">/etc/</span>ld.so.conf and run ldconfig as root<br><br>  To uninstall the CUDA Toolkit, run cuda-uninstaller in <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>bin<br><br>  Please see CUDA_Installation_Guide_Linux.pdf in <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>doc/pdf <span class="hljs-keyword">for</span> detailed information on setting up CUDA.<br></code></pre></td></tr></table></figure></li></ul><ol><li>添加CUDA环境变量<br>在 ~/.bashrc 里再设置一下cuda的环境变量： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">vi ~/.bashrc<br><span class="hljs-built_in">export</span> PATH=/usr/local/cuda-10.1/bin<span class="hljs-variable">$&#123;PATH:+:<span class="hljs-variable">$&#123;PATH&#125;</span>&#125;</span><br><span class="hljs-built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64<span class="hljs-variable">$&#123;LD_LIBRARY_PATH:+:<span class="hljs-variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span><br><span class="hljs-built_in">export</span> CUDA_HOME=/usr/local/cuda-10.1<br><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure></li></ol><p>在 /etc/profile 里也设置一下cuda的安装路径及相应的库文件：(这个似乎可以不改)<br>配置LD路径(这个似乎也可以不改)：  </p><ol><li>查看cuda版本： <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">nvcc -V<br>cat <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda/</span>version.txt<br></code></pre></td></tr></table></figure></li></ol><p>_卸载CUDA_</p><p>To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-10.1/bin</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">sudo <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>bin/cuda-uninstaller<br>sudo rm -rf <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span><br></code></pre></td></tr></table></figure><h2 id="CuDNN-安装"><a href="#CuDNN-安装" class="headerlink" title="CuDNN 安装"></a>CuDNN 安装</h2><ol><li>最快捷的方法（有一定概率安装成功仍找不到cudnn，以及和你安装的cuda版本不对应的情况…）</li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conda</span> install cudnn=<span class="hljs-number">7</span>.<span class="hljs-number">6</span>.<span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><ol><li>传统安装方法</li></ol><ul><li>在<a href="https://developer.nvidia.com/rdp/cudnn-archive">cuDNN Archive</a>中，选择合适的CuDNN版本； </li><li><a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.3.30/Production/10.1_20190822/cudnn-10.1-linux-x64-v7.6.3.30.tgz">Download cuDNN Library cudnn-10.1-linux-x64-v7.6.3.30.tgz</a><br>PS：<a href="https://www.twblogs.net/a/5ca15f6ebd9eee5b1a06a4da/zh-cn"><strong>cuDNN需要注册NVIDIA的会员(免费)才能下载，无法在远程机器上用curl或者wget下载!!</strong></a> 用户名：qq邮箱    密码：zX1994???721<br>问题：要经过登录授权的，就算你在网页上登录了得到下载链接，直接在命令行中:wget 下载链接，也是会报403禁止错误。<br><a href="https://www.cnblogs.com/HankTown/p/13943534.html">解决方案</a>：linux下载高版本cudnn可以用下面的网址绕过授权检查： <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">sudo wget http:<span class="hljs-regexp">//</span>developer.download.nvidia.com<span class="hljs-regexp">/compute/</span>redist<span class="hljs-regexp">/cudnn/</span>v7.<span class="hljs-number">6.5</span>/cudnn-<span class="hljs-number">10.0</span>-linux-x64-v7.<span class="hljs-number">6.5</span>.<span class="hljs-number">32</span>.tgz<br></code></pre></td></tr></table></figure></li></ul><p>可以根据你想要的cudnn版本号对其中的cuda版本（如10.0）和cudnn版本（v7.6.5）进行修改，然后就可以下载cudnn了。 </p><ul><li>下载完毕后解压 <figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> ~<span class="hljs-string">/Downloads/</span><br>tar -zxvf cudnn-*<span class="hljs-string">.tgz</span><br></code></pre></td></tr></table></figure></li></ul><ul><li>然后将cuDNN中的bin、include、lib文件<strong>覆盖</strong>cuda安装目录下即可 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">cd <span class="hljs-regexp">/software/</span><br>sudo cp cuda<span class="hljs-regexp">/include/</span>cudnn.h <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda/i</span>nclude/<br>sudo cp cuda<span class="hljs-regexp">/lib64/</span>libcudnn* <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda/</span>lib64/<br>sudo chmod a+r <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda/i</span>nclude/cudnn.h<br>sudo chmod a+r <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda/</span>lib64/libcudnn*<br></code></pre></td></tr></table></figure></li></ul><ul><li>验证 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">cat <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda/i</span>nclude/cudnn.h | grep CUDNN_MAJOR -A <span class="hljs-number">2</span><br><span class="hljs-comment">#or</span><br>git clone https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/wilhelmguo/</span>cudnn_samples_v7.git<br>cd cudnn_samples_v7/mnistCUDNN<br>make <br>./mnistCUDNN<br></code></pre></td></tr></table></figure></li></ul><ul><li>可能遇到<code>ibcudnn.so.5 is not a symbolic link</code>错误 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">ls -lha <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda/</span>lib64/libcudnn*<br></code></pre></td></tr></table></figure></li></ul><p>If libcudnn.so and libcudnn.so.5 are not symlinks then this is the reason why you got this error.<br>If so, this is what you need to do:<br>PS: 最好参照本文最后一节 错误解决 进行软链接 </p><h2 id="配置Pip国内镜像"><a href="#配置Pip国内镜像" class="headerlink" title="配置Pip国内镜像"></a>配置Pip国内镜像</h2><ul><li>临时使用：pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a> tensorflow-gpu </li><li><p>永久修改，一劳永逸：<br>Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)<br>内容如下： </p><ul><li><p>阿里源（非常快）</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-keyword">global</span>]<br><span class="hljs-keyword">index</span>-url = https://mirrors.aliyun.com/pypi/simple/<br><br>[install]<br><span class="hljs-keyword">trusted</span>-host=mirrors.aliyun.com<br></code></pre></td></tr></table></figure></li><li><p>清华源（贼鸡儿的快）</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-keyword">global</span>]<br><span class="hljs-keyword">index</span>-url = https://pypi.tuna.tsinghua.edu.cn/simple<br><br>[install]<br><span class="hljs-keyword">trusted</span>-host = https://pypi.tuna.tsinghua.edu.cn<br></code></pre></td></tr></table></figure></li></ul></li></ul><p>PS: windows下，直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini。内容同上。</p><h2 id="安装-Anaconda3"><a href="#安装-Anaconda3" class="headerlink" title="安装 Anaconda3"></a>安装 Anaconda3</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">yum</span> -y install wget<br><span class="hljs-attribute">sudo</span> wget -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-<span class="hljs-number">2022</span>.<span class="hljs-number">10</span>-Linux-x86_64.sh<br><span class="hljs-attribute">bash</span> Anaconda3-<span class="hljs-number">2022</span>.<span class="hljs-number">10</span>-Linux-x86_64.sh<br></code></pre></td></tr></table></figure><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-symbol">$</span> Do you accept the license terms? [<span class="hljs-keyword">yes</span>|<span class="hljs-keyword">no</span>]<br><span class="hljs-symbol">$</span> [<span class="hljs-keyword">no</span>] &gt;&gt;&gt;<br><span class="hljs-symbol">$</span> Please answer <span class="hljs-string">&#x27;yes&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;no&#x27;</span>:<br><span class="hljs-symbol">$</span> &gt;&gt;&gt; <span class="hljs-keyword">yes</span><br><span class="hljs-symbol">$</span> ...<br><span class="hljs-symbol">$</span> Anaconda3 will now be installed into this location:<br><span class="hljs-symbol">$</span> /root/anaconda3<br><span class="hljs-symbol">$</span> /home/ningshixian/anaconda3<br><span class="hljs-symbol">$</span> <br><span class="hljs-symbol">$</span> - Press ENTER to confirm the location<br><span class="hljs-symbol">$</span> - Press CTRL-C to <span class="hljs-keyword">abort</span> the installation<br><span class="hljs-symbol">$</span> - <span class="hljs-keyword">Or</span> specify a different location below<br><span class="hljs-symbol">$</span> <br><span class="hljs-symbol">$</span> [/root/anaconda3] &gt;&gt;&gt; 更改要输入绝对路径<br><span class="hljs-symbol">$</span> ...<br><span class="hljs-symbol">$</span> Do you wish the installer to initialize Anaconda3<br><span class="hljs-symbol">$</span> by running conda init? [<span class="hljs-keyword">yes</span>|<span class="hljs-keyword">no</span>]<br><span class="hljs-symbol">$</span> [<span class="hljs-keyword">no</span>] &gt;&gt;&gt; <span class="hljs-keyword">no</span><br><span class="hljs-symbol">$</span> ...<br></code></pre></td></tr></table></figure><p>安装成功之后，配置环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">vi ~/.bashrc<span class="hljs-comment"># 每次启动新shell时运行的命令</span><br>vi ~/.bash_profile<span class="hljs-comment"># 只运行一次，如定义环境变量PATH</span><br></code></pre></td></tr></table></figure><p>在文件最后加入如下语句（路径需要根据自己的安装位置更改）</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-string">&quot;/home/ningshixian/anaconda3/bin:<span class="hljs-variable">$PATH</span>&quot;</span><br></code></pre></td></tr></table></figure><p>按住shift键+:键，输入wq，保存文件并退出；最后使用如下命令刷新环境变量即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>检查是否成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">which</span> conda<br><span class="hljs-built_in">which</span> pip<br></code></pre></td></tr></table></figure><p><a href="https://so.csdn.net/so/search?q=anaconda&amp;spm=1001.2101.3001.7020">anaconda</a>安装完成后需要更换为清华源以提高相关包的下载速度，在终端依次执行这些命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/<br>conda config --append channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/fastai/<br>conda config --append channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/<br>conda config --append channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/<br> <br><span class="hljs-comment"># 搜索时显示通道地址</span><br>conda config --<span class="hljs-built_in">set</span> show_channel_urls yes<br></code></pre></td></tr></table></figure><h2 id="Conda-虚拟环境"><a href="#Conda-虚拟环境" class="headerlink" title="Conda 虚拟环境"></a>Conda 虚拟环境</h2><p>创建</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">conda create --name gptenv python=<span class="hljs-number">3.9</span><br><br>python -m pip install -i https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/pypi/</span>simple/ -U setuptools<br></code></pre></td></tr></table></figure><p>_安装好后，使用activate激活某个环境_</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-built_in">activate</span> python34 <span class="hljs-comment"># for Windows</span><br> <br>source <span class="hljs-built_in">activate</span> python34 <span class="hljs-comment"># for Linux &amp; Mac</span><br></code></pre></td></tr></table></figure><p>退出虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">deactivate python34 <span class="hljs-comment"># for Windows</span><br><br><span class="hljs-built_in">source</span> deactivate python34 <span class="hljs-comment"># for Linux &amp; Mac</span><br></code></pre></td></tr></table></figure><p>查看所有已安装虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda <span class="hljs-built_in">env</span> list<br></code></pre></td></tr></table></figure><p><strong>复制别人的虚拟环境</strong></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">conda create -n BBB --<span class="hljs-keyword">clone</span> <span class="hljs-title">AAA</span><br></code></pre></td></tr></table></figure><p>_删除一个已有的环境_</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran">conda remove -<span class="hljs-keyword">name</span> myenv --<span class="hljs-built_in">all</span><br></code></pre></td></tr></table></figure><h2 id="Poetry-虚拟环境-推荐使用"><a href="#Poetry-虚拟环境-推荐使用" class="headerlink" title="Poetry 虚拟环境(推荐使用)"></a><a href="https://blog.kyomind.tw/python-poetry/">Poetry 虚拟环境(推荐使用)</a></h2><p>Poetry 是后期之秀，支持多种虚拟环境管理，提供依赖关系校验，和依赖的 poetry.lock 文件，也有自动管理依赖的操作。同时可以用于 Python 工程打包和发布。</p><p>它允许您声明您的项目所依赖的库，它将为您管理（安装/更新）它们。Poetry 提供了一个锁定文件以确保可重复安装，并可以构建您的项目以供分发。 poetry 通过配置文件 pyproject.toml 来完成依赖管理、环境配置、基本信息配置等功能，相当于把 Python 项目中的 Pipfile、setup.py、setup.cfg、requirements.txt、MANIFEST.in 融合到一起。</p><p><strong>安装：</strong><br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">pip install poetry<br>poetry <span class="hljs-comment">--version</span><br></code></pre></td></tr></table></figure></p><p><strong>使用：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化项目，创建当前项目的虚拟环境，生成 `pyproject.toml` 文件</span><br>poetry init<br><br><span class="hljs-comment"># 进入当前项目的虚拟环境</span><br>poetry shell<br><br><span class="hljs-comment"># 安装依赖</span><br>poetry add tox requests<br><span class="hljs-comment"># 更新指定的依赖</span><br>poetry update requests==<span class="hljs-number">1.2</span><span class="hljs-number">.1</span><br><span class="hljs-comment"># 卸载依赖库</span><br>poetry remove scrapy<br><br><span class="hljs-comment"># 部署&amp;安装pyproject.toml文件中的全部依赖</span><br>poetry install<br><br><span class="hljs-comment"># 查看所有依赖</span><br>poetry show<br><span class="hljs-comment"># 仅所有开发依赖</span><br>poetry show --only dev <br><span class="hljs-comment"># 生成 requirements.txt</span><br>poetry export -f requirements.txt --output --without-hashes<br><br><span class="hljs-comment"># 查找当前项目的虚拟环境</span><br>poetry enb <span class="hljs-built_in">list</span> --full-path<br><span class="hljs-comment"># 删除虚拟环境路径，指定对应的解析器版本即可</span><br>poetry env remove &lt;python3&gt;<br><br><span class="hljs-comment"># 构建项目（略）</span><br>poetry build<br><br><span class="hljs-comment"># 发布项目（略）</span><br>poetry publish<br></code></pre></td></tr></table></figure></p><h2 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h2><ul><li>先查看<a href="https://tensorflow.google.cn/install/source#linux">tensorflow CUDA cudnn 版本对应关系</a> ↓ </li><li><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pip</span> install tensorflow-gpu==<span class="hljs-number">1</span>.<span class="hljs-number">14</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></li></ul><ul><li>验证是否安装成功 (<a href="https://blog.csdn.net/Zlase/article/details/79261348">确定自己的TensorFlow是CPU还是GPU的版本</a>) <figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-title">tf</span>.test.gpu_device_name()<br><span class="hljs-title">tf</span>.test.is_gpu_available()<br><br><span class="hljs-meta">#下方有大段显示GPU信息表示安装成功</span><br></code></pre></td></tr></table></figure></li></ul><h2 id="安装-Keras"><a href="#安装-Keras" class="headerlink" title="安装 Keras"></a>安装 Keras</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">sudo pip <span class="hljs-keyword">install</span> keras<br></code></pre></td></tr></table></figure><h2 id="错误解决"><a href="#错误解决" class="headerlink" title="错误解决"></a>错误解决</h2><h3 id="1-so文件找不到的错误"><a href="#1-so文件找不到的错误" class="headerlink" title="1. so文件找不到的错误"></a>1. so文件找不到的错误</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2019</span>-<span class="hljs-number">11</span>-<span class="hljs-number">04</span> <span class="hljs-number">21</span>:<span class="hljs-number">46</span>:<span class="hljs-number">44</span>.<span class="hljs-number">297090</span>: W tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="hljs-number">55</span>] Could not load dynamic library &#x27;libcudart.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>&#x27;; dlerror: libcudart.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64:/usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64<br><br><span class="hljs-attribute">2019</span>-<span class="hljs-number">11</span>-<span class="hljs-number">04</span> <span class="hljs-number">21</span>:<span class="hljs-number">46</span>:<span class="hljs-number">44</span>.<span class="hljs-number">300375</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="hljs-number">44</span>] Successfully opened dynamic library libcublas.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span><br><br><span class="hljs-attribute">2019</span>-<span class="hljs-number">11</span>-<span class="hljs-number">04</span> <span class="hljs-number">21</span>:<span class="hljs-number">46</span>:<span class="hljs-number">44</span>.<span class="hljs-number">300532</span>: W tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="hljs-number">55</span>] Could not load dynamic library &#x27;libcufft.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>&#x27;; dlerror: libcufft.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64:/usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64<br><br><span class="hljs-attribute">2019</span>-<span class="hljs-number">11</span>-<span class="hljs-number">04</span> <span class="hljs-number">21</span>:<span class="hljs-number">46</span>:<span class="hljs-number">44</span>.<span class="hljs-number">300625</span>: W tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="hljs-number">55</span>] Could not load dynamic library &#x27;libcurand.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>&#x27;; dlerror: libcurand.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64:/usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64<br><br><span class="hljs-attribute">2019</span>-<span class="hljs-number">11</span>-<span class="hljs-number">04</span> <span class="hljs-number">21</span>:<span class="hljs-number">46</span>:<span class="hljs-number">44</span>.<span class="hljs-number">300739</span>: W tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="hljs-number">55</span>] Could not load dynamic library &#x27;libcusolver.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>&#x27;; dlerror: libcusolver.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64:/usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64<br><br><span class="hljs-attribute">2019</span>-<span class="hljs-number">11</span>-<span class="hljs-number">04</span> <span class="hljs-number">21</span>:<span class="hljs-number">46</span>:<span class="hljs-number">44</span>.<span class="hljs-number">300822</span>: W tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="hljs-number">55</span>] Could not load dynamic library &#x27;libcusparse.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>&#x27;; dlerror: libcusparse.so.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64:/usr/local/cuda-<span class="hljs-number">10</span>.<span class="hljs-number">1</span>/lib64<br><br><span class="hljs-attribute">2019</span>-<span class="hljs-number">11</span>-<span class="hljs-number">04</span> <span class="hljs-number">21</span>:<span class="hljs-number">46</span>:<span class="hljs-number">44</span>.<span class="hljs-number">309809</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="hljs-number">44</span>] Successfully opened dynamic library libcudnn.so.<span class="hljs-number">7</span><br><br><span class="hljs-attribute">2019</span>-<span class="hljs-number">11</span>-<span class="hljs-number">04</span> <span class="hljs-number">21</span>:<span class="hljs-number">46</span>:<span class="hljs-number">44</span>.<span class="hljs-number">309870</span>: W tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="hljs-number">1641</span>] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.<br><span class="hljs-attribute">Skipping</span> registering GPU devices...<br></code></pre></td></tr></table></figure><p>主要错误信息为<code>Could not dlopen library &#39;libcublas.so.10.0&#39;</code>。造成这样的原因是链接文件不对造成的。这种问题很好解决，下面我罗列一些常发生这样错误的<code>so</code>文件解决办法，首先大家要确定报错的<code>so</code>文件名称是什么，例如上面报错的是<code>libcublas.so.10.0</code>这个文件，那么就找到对应的<code>libcublas</code>库文件，然后在<code>/usr/local/cuda-10.1/lib64/</code>目录下创建一个<code>libbcublas.so.10.0</code>链接文件即可。</p><p>出现该错误消息，表示会使用CPU，它可以识别GPU，但找不到与cuda相关的库文件。（CUDA 10.1或10.0文件内容似乎没有太大不同……<a href="http://ejklike.github.io/2019/08/19/insatall-tensorflow-2.0.0-beta1-in-ubuntu-with-cuda-10-1.html">参考</a>）</p><p><strong>解决方法</strong></p><ol><li>要么将cuda降为10.0;</li><li>要么则执行软链接命令</li></ol><blockquote><p>根据配置文件  LD_LIBRARY_PATH=lib_.so.10.1_<br>将相应的库文件软链接到<code>/usr/local/cuda-10.0/lib64/</code>，使得可以找到<code>*.so.10.0</code><br>在路径中找到对应的文件，为找不到的lib*.so.10.0创建符号链接</p></blockquote><p>创建一个CUDA 10.0文件夹（可省略此步）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">sudo cp -r <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><br></code></pre></td></tr></table></figure><p>针对性软链接</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs gradle"># libcudart也可能是这个 sudo ln -s <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64<span class="hljs-regexp">/libcudart.so.10.1.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcudart.so.<span class="hljs-number">10.0</span><br><br>sudo ln -s <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64<span class="hljs-regexp">/libcudart.so.10.1.1.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcudart.so.<span class="hljs-number">10.0</span><br>sudo ln -s <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64<span class="hljs-regexp">/libcublas.so.10.1.1.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcublas.so.<span class="hljs-number">10.0</span><br>sudo ln -s <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64<span class="hljs-regexp">/libcufft.so.10.1.1.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcufft.so.<span class="hljs-number">10.0</span><br>sudo ln -s <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64<span class="hljs-regexp">/libcurand.so.10.1.1.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcurand.so.<span class="hljs-number">10.0</span><br>sudo ln -s <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64<span class="hljs-regexp">/libcusolver.so.10.2.0.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcusolver.so.<span class="hljs-number">10.0</span><br>sudo ln -s <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64<span class="hljs-regexp">/libcusparse.so.10.3.0.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcusparse.so.<span class="hljs-number">10.0</span><br><br>sudo ln -s <span class="hljs-regexp">/usr/</span>lib64<span class="hljs-regexp">/libcublas.so.10.2.1.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcublas.so.<span class="hljs-number">10.0</span><br></code></pre></td></tr></table></figure><p>再次修改环境配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">vi ~/.bashrc<br><br><span class="hljs-built_in">export</span> PATH=/usr/local/cuda-10.0/bin<span class="hljs-variable">$&#123;PATH:+:<span class="hljs-variable">$&#123;PATH&#125;</span>&#125;</span><br><span class="hljs-built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64<span class="hljs-variable">$&#123;LD_LIBRARY_PATH:+:<span class="hljs-variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span><br><span class="hljs-built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-10.0/extras/CUPTI/lib64:<span class="hljs-variable">$LD_LIBRARY_PATH</span><br><br><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>PS: 以上 <code>cuda-10.0</code> 应该都可以换成 <code>cuda-10.1</code></p><p>若出现 Error: failed to create symbolic link，可采取强制覆盖方法</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">ln -sf sudo ln -s <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda-10.1/</span>lib64<span class="hljs-regexp">/libcudart.so.10.1.243 /u</span>sr<span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">10.0</span><span class="hljs-regexp">/lib64/</span>libcudart.so.<span class="hljs-number">10.0</span><br></code></pre></td></tr></table></figure><h3 id="2-cuda-driver的驱动器跟cuda不匹配"><a href="#2-cuda-driver的驱动器跟cuda不匹配" class="headerlink" title="2. cuda driver的驱动器跟cuda不匹配"></a>2. cuda driver的驱动器跟cuda不匹配</h3><blockquote><p>Failed to initialize NVML: Driver/library version mismatch</p></blockquote><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">在版本不匹配时，适当降低或者更新驱动器版本即可。另外驱动器版本更新之后可能需要重启系统，当然通过如下方法不用重启也可以更新版本。首先尝试删除nvidia相关的kernel <span class="hljs-keyword">mod</span>。<br></code></pre></td></tr></table></figure><p>sudo rmmod nvidia<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"><br>当然这里一般会报个<span class="hljs-keyword">Module</span> nvidia <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> use <span class="hljs-keyword">by</span>的错误。不碍事，我们先查看下kernel <span class="hljs-keyword">mod</span> 的依赖情况：<br></code></pre></td></tr></table></figure><br>ls mod | grep nvidia<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs"><br>根据根据结果逐一rmmod即可。<br></code></pre></td></tr></table></figure><br>sudo rmmod nvidia_uvm<br>sudo rmmod nvidia_modeset<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs"><br>最后再rmmod nvidia即可达到驱动器更新效果。<br></code></pre></td></tr></table></figure><br>sudo rmmod nvidia<br>nvidia-smi<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs clean">```<br><br>### <span class="hljs-number">3.</span>多个 cuda 版本之间进行切换<br><br></code></pre></td></tr></table></figure></p><h1 id="在切换cuda版本时"><a href="#在切换cuda版本时" class="headerlink" title="在切换cuda版本时"></a>在切换cuda版本时</h1><p>rm -rf /usr/local/cuda#删除之前创建的软链接<br>sudo ln -s /usr/local/cuda-8.0/ /usr/local/cuda/<br>nvcc —version #查看当前 cuda 版本</p><p>nvcc: NVIDIA (R) Cuda compiler driver<br>Copyright (c) 2005-2016 NVIDIA Corporation<br>Built on Mon_Jan_23_12:24:11_CST_2017<br>Cuda compilation tools, release 8.0, V8.0.62</p><h1 id="cuda8-0-切换到-cuda9-0"><a href="#cuda8-0-切换到-cuda9-0" class="headerlink" title="cuda8.0 切换到 cuda9.0"></a>cuda8.0 切换到 cuda9.0</h1><p>rm -rf /usr/local/cuda<br>sudo ln -s /usr/local/cuda-9.0/ /usr/local/cuda/<br>nvcc —version<br><figure class="highlight node-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs node-repl"><br>### 4. tensorflow-gpu运行出错: Could not create cudnn handle<br><br>错误信息如下：<br><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript"><span class="hljs-title class_">Successfully</span> opened dynamic library libcublas.<span class="hljs-property">so</span><span class="hljs-number">.10</span><span class="hljs-number">.0</span></span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript"> </span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript">failed to create **cublas** <span class="hljs-attr">handle</span>: **<span class="hljs-variable constant_">CUBLAS</span>**_STATUS_NOT_INITIALIZED</span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript"> </span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript"><span class="hljs-title class_">Successfully</span> opened dynamic library libcudnn.<span class="hljs-property">so</span><span class="hljs-number">.7</span></span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript"> </span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript"><span class="hljs-title class_">Could</span> not create **cudnn** <span class="hljs-attr">handle</span>: **<span class="hljs-variable constant_">CUDNN</span>**_STATUS_NOT_INITIALIZED</span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript"> </span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript"><span class="hljs-title class_">Possibly</span> insufficient driver <span class="hljs-attr">version</span>: <span class="hljs-number">418.87</span><span class="hljs-number">.0</span></span><br><br><br>[解决：](https://github.com/pcpLiu/DeepSeqPan/issues/2)<br><br></code></pre></td></tr></table></figure><br>In the end, the problem was that it did not use the system’s Cudnn version but an incompatible version which came with the creation of the Anaconda environment. By overwriting this with the system’s version manually, I could solve the problem.<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs awk"><br>无效解决方案<span class="hljs-number">1</span>：也即cudnn的版本错误（按照conda list显示的cuda版本[必须低于](https:<span class="hljs-regexp">//</span>webcache.googleusercontent.com<span class="hljs-regexp">/search?q=cache:8vC2brZLV20J:https:/</span><span class="hljs-regexp">/www.machunjie.com/m</span>essage-board+&amp;cd=<span class="hljs-number">19</span>&amp;hl=zh-CN&amp;ct=clnk&amp;gl=us)nvcc -V查看的系统版本），导致[对应cublas64_10.dll出错](https:<span class="hljs-regexp">//</span>www.geek-share.com<span class="hljs-regexp">/detail/</span><span class="hljs-number">2806021724</span>.html)，重新将手动安装的cudnn文件覆盖就行？<br><br>无效解决方案<span class="hljs-number">2</span>：pytorch构建数据集时调用了gpu导致tensorflow不能占用gpu了？<br><br>无效解决方案<span class="hljs-number">3</span>：`sudo rm -f ~/.nv`<br><br>**有效解决方案：**重新建一个虚拟环境，重新安装依赖包，记得在 ~/.bashrc 里再设置一下对应版本cuda的环境变量，问题解决👍🏻<br><br><span class="hljs-comment">## 免费的GPU资源</span><br><br>colab地址：<br><br></code></pre></td></tr></table></figure><br><a href="https://colab.research.google.com/notebooks/">https://colab.research.google.com/notebooks/</a><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs"><br>kaggle地址：<br><br></code></pre></td></tr></table></figure><br><a href="https://www.kaggle.com/">https://www.kaggle.com/</a><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs"><br>nvidia官方文档地址：<br><br></code></pre></td></tr></table></figure><br><a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a><br>```</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://wilhelmguo.cn/blog/post/william/Centos-7-%E5%AE%89%E8%A3%85-Nvidia-GPU-%E9%A9%B1%E5%8A%A8%E5%8F%8A-CUDA">Centos 7 安装 Nvidia GPU 驱动及 CUDA</a></li><li><a href="https://blog.csdn.net/qq_34374211/article/details/81018320">找不到libcublas.so.10.0文件</a> <ol><li><code>tensorflow</code>版本与<code>CUDA</code>的版本不对应</li><li>软链接有误，重新进行链接 （<a href="http://ejklike.github.io/2019/08/19/insatall-tensorflow-2.0.0-beta1-in-ubuntu-with-cuda-10-1.html">Could not dlopen library ‘libcurand.so.10.0’…LD_LIBRARY_PATH…</a>）</li></ol></li><li><a href="https://blog.csdn.net/chenjiyou363753068/article/details/84374661">cuda程序执行出错: libcudart.so.10.0: cannot open shared object file: No such file or directory</a></li><li><a href="https://cloud.tencent.com/developer/article/1055872">用wget下载需要用户名和密码认证的网站或者ftp服务器文件</a></li><li><a href="https://blog.csdn.net/qq_33200967/article/details/80689543">Ubuntu安装和卸载CUDA和CUDNN</a></li><li><a href="http://queirozf.com/entries/installing-cuda-tk-and-tensorflow-on-a-clean-ubuntu-16-04-install#-sbin-ldconfig-real-usr-local-cuda-lib64-libcudnn-so-5-is-not-a-symbolic-link">Installing CUDA TK 8 and Tensorflow on a Clean Ubuntu 16.04 Install</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247490876&amp;idx=5&amp;sn=69c0a904542967d4f97f0a859e6ebcca&amp;chksm=ebb425e8dcc3acfe323e22e3b0a9e69fd00d80c6e93e6718267aff4bfbd680cd055a025822f0&amp;scene=0&amp;xtrack=1#rd">深度学习环境配置有哪些坑？</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>环境搭建</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>✍🏻Logging如何优雅记日志</title>
    <link href="/2019/10/22/2019-10-22-%E2%9C%8D%F0%9F%8F%BBLogging%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E8%AE%B0%E6%97%A5%E5%BF%97/"/>
    <url>/2019/10/22/2019-10-22-%E2%9C%8D%F0%9F%8F%BBLogging%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E8%AE%B0%E6%97%A5%E5%BF%97/</url>
    
    <content type="html"><![CDATA[<h1 id="✍🏻Logging如何优雅记日志"><a href="#✍🏻Logging如何优雅记日志" class="headerlink" title="✍🏻Logging如何优雅记日志"></a>✍🏻Logging如何优雅记日志</h1><p>如何为后端web接口生成清晰明了、便于查错追踪的日志</p><ul><li><p>日志相关概念</p></li><li><p>Python的logging模块介绍</p></li><li><p>使用logging记录日志</p></li><li><p>配置logging的几种方式</p></li><li><p>错误日志发送邮件方法</p></li><li><p>常见问题</p></li></ul><h2 id="一、日志相关概念"><a href="#一、日志相关概念" class="headerlink" title="一、日志相关概念"></a>一、日志相关概念</h2><p>日志是一种可以追踪某些软件运行时所发生事件的方法。软件开发人员可以向他们的代码中调用日志记录相关的方法来表明发生了某些事情。一个事件可以用一个可包含可选变量数据的消息来描述。此外，事件也有重要性的概念，这个重要性也可以被称为严重性级别（level）。</p><h3 id="1-日志的作用"><a href="#1-日志的作用" class="headerlink" title="1.日志的作用"></a>1.日志的作用</h3><p>通过log的分析，可以方便用户了解系统或软件、应用的运行情况；如果你的应用log足够丰富，也可以分析以往用户的操作行为、类型喜好、地域分布或其他更多信息；如果一个应用的log同时也分了多个级别，那么可以很轻易地分析得到该应用的健康状况，及时发现问题并快速定位、解决问题，补救损失。<br>简单来讲就是，我们通过记录和分析日志可以了解一个系统或软件程序运行情况是否正常，也可以在应用程序出现故障时快速定位问题。比如，做运维的同学，在接收到报警或各种问题反馈后，进行问题排查时通常都会先去看各种日志，大部分问题都可以在日志中找到答案。再比如，做开发的同学，可以通过IDE控制台上输出的各种日志进行程序调试。对于运维老司机或者有经验的开发人员，可以快速的通过日志定位到问题的根源。可见，日志的重要性不可小觑。日志的作用可以简单总结为以下3点：</p><ul><li><p>程序调试</p></li><li><p>了解软件程序运行情况，是否正常</p></li><li><p>软件程序运行故障分析与问题定位</p></li></ul><p>如果应用的日志信息足够详细和丰富，还可以用来做用户行为分析，如：分析用户的操作行为、类型洗好、地域分布以及其它更多的信息，由此可以实现改进业务、提高商业利益。</p><h3 id="2-日志的等级"><a href="#2-日志的等级" class="headerlink" title="2.日志的等级"></a>2.日志的等级</h3><p>我们先来思考下下面的两个问题：</p><ul><li><p>作为开发人员，在开发一个应用程序时需要什么日志信息？在应用程序正式上线后需要什么日志信息？</p></li><li><p>作为应用运维人员，在部署开发环境时需要什么日志信息？在部署生产环境时需要什么日志信息？</p></li></ul><p>在软件开发阶段或部署开发环境时，为了尽可能详细的查看应用程序的运行状态来保证上线后的稳定性，我们可能需要把该应用程序所有的运行日志全部记录下来进行分析，这是非常耗费机器性能的。当应用程序正式发布或在生产环境部署应用程序时，我们通常只需要记录应用程序的异常信息、错误信息等，这样既可以减小服务器的I/O压力，也可以避免我们在排查故障时被淹没在日志的海洋里。那么，怎样才能在不改动应用程序代码的情况下实现在不同的环境记录不同详细程度的日志呢？这就是日志等级的作用了，我们通过配置文件指定我们需要的日志等级就可以了。</p><p>不同的应用程序所定义的日志等级可能会有所差别，分的详细点的会包含以下几个等级：</p><div class="table-container"><table><thead><tr><th>日志等级（level）</th><th>描述</th></tr></thead><tbody><tr><td>DEBUG</td><td>最详细的日志信息，典型应用场景是 问题诊断</td></tr><tr><td>INFO</td><td>信息详细程度仅次于DEBUG，通常只记录关键节点信息，用于确认一切都是按照我们预期的那样进行工作</td></tr><tr><td>WARNING</td><td>当某些不期望的事情发生时记录的信息（如，磁盘可用空间较低），但是此时应用程序还是正常运行的</td></tr><tr><td>ERROR</td><td>由于一个更严重的问题导致某些功能不能正常运行时记录的信息</td></tr><tr><td>CRITICAL</td><td>当发生严重错误，导致应用程序不能继续运行时记录的信息</td></tr></tbody></table></div><p><strong>开发应用程序或部署开发环境时，可以使用DEBUG或INFO级别的日志获取尽可能详细的日志信息来进行开发或部署调试；应用上线或部署生产环境时，应该使用WARNING或ERROR或CRITICAL级别的日志来降低机器的I/O压力和提高获取错误日志信息的效率。</strong>日志级别的指定通常都是在应用程序的配置文件中进行指定的。</p><h3 id="3-日志字段信息与日志格式"><a href="#3-日志字段信息与日志格式" class="headerlink" title="3.日志字段信息与日志格式"></a>3.日志字段信息与日志格式</h3><p>本节开始问题提到过，一条日志信息对应的是一个事件的发生，而一个事件通常需要包括以下几个内容：</p><ul><li><p>事件发生时间</p></li><li><p>事件发生位置</p></li><li><p>事件的严重程度—日志级别</p></li><li><p>事件内容</p></li></ul><p>上面这些都是一条日志记录中可能包含的字段信息，当然还可以包括一些其他信息，如进程ID、进程名称、线程ID、线程名称等。日志格式就是用来定义一条日志记录中包含那些字段的，且日志格式通常都是可以自定义的。</p><h3 id="4-日志功能的实现"><a href="#4-日志功能的实现" class="headerlink" title="4.日志功能的实现"></a>4.日志功能的实现</h3><p>几乎所有开发语言都会内置日志相关功能，或者会有比较优秀的第三方库来提供日志操作功能，比如：log4j，log4php等。它们功能强大、使用简单。Python自身也提供了一个用于记录日志的标准库模块—logging。</p><h2 id="二、logging模块"><a href="#二、logging模块" class="headerlink" title="二、logging模块"></a>二、logging模块</h2><h4 id="logging模块定义的模块级别的常用函数"><a href="#logging模块定义的模块级别的常用函数" class="headerlink" title="logging模块定义的模块级别的常用函数"></a>logging模块定义的模块级别的常用函数</h4><div class="table-container"><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>logging.debug(msg, <em>args, *</em>kwargs)</td><td>创建一条严重级别为DEBUG的日志记录</td></tr><tr><td>logging.info(msg, <em>args, *</em>kwargs)</td><td>创建一条严重级别为INFO的日志记录</td></tr><tr><td>logging.warning(msg, <em>args, *</em>kwargs)</td><td>创建一条严重级别为WARNING的日志记录</td></tr><tr><td>logging.error(msg, <em>args, *</em>kwargs)</td><td>创建一条严重级别为ERROR的日志记录</td></tr><tr><td>logging.critical(msg, <em>args, *</em>kwargs)</td><td>创建一条严重级别为CRITICAL的日志记录</td></tr><tr><td>logging.log(level, <em>args, *</em>kwargs)</td><td>创建一条严重级别为level的日志记录</td></tr><tr><td>logging.basicConfig(**kwargs)</td><td>对root logger进行一次性配置</td></tr></tbody></table></div><h4 id="logging模块的四大组件"><a href="#logging模块的四大组件" class="headerlink" title="logging模块的四大组件"></a>logging模块的四大组件</h4><p>在介绍logging模块的日志流处理流程之前，我们先来介绍下logging模块的四大组件：</p><div class="table-container"><table><thead><tr><th>组件名称</th><th>对应类名</th><th>功能描述</th></tr></thead><tbody><tr><td>日志器</td><td>Logger</td><td>提供了应用程序可一直使用的接口</td></tr><tr><td>处理器</td><td>Handler</td><td>将logger创建的日志记录发送到合适的目的输出</td></tr><tr><td>过滤器</td><td>Filter</td><td>提供了更细粒度的控制工具来决定输出哪条日志记录，丢弃哪条日志记录</td></tr><tr><td>格式器</td><td>Formatter</td><td>决定日志记录的最终输出格式</td></tr></tbody></table></div><h5 id="这些组件之间的关系描述："><a href="#这些组件之间的关系描述：" class="headerlink" title="这些组件之间的关系描述："></a>这些组件之间的关系描述：</h5><p>简单点说就是：日志器（logger）是入口，真正干活儿的是处理器（handler），处理器（handler）还可以通过过滤器（filter）和格式器（formatter）对要输出的日志内容做过滤和格式化等处理操作。</p><h4 id="logging模块定义的格式字符串字段"><a href="#logging模块定义的格式字符串字段" class="headerlink" title="logging模块定义的格式字符串字段"></a>logging模块定义的格式字符串字段</h4><p>我们来列举一下logging模块中定义好的可以用于format格式字符串中字段有哪些：</p><div class="table-container"><table><thead><tr><th>字段/属性名称</th><th>使用格式</th><th>描述</th></tr></thead><tbody><tr><td>asctime</td><td>%(asctime)s</td><td>日志事件发生的时间—人类可读时间，如：2003-07-08 16:49:45,896</td></tr><tr><td>created</td><td>%(created)f</td><td>日志事件发生的时间—时间戳，就是当时调用time.time()函数返回的值</td></tr><tr><td>relativeCreated</td><td>%(relativeCreated)d</td><td>日志事件发生的时间相对于logging模块加载时间的相对毫秒数（目前还不知道干嘛用的）</td></tr><tr><td>msecs</td><td>%(msecs)d</td><td>日志事件发生事件的毫秒部分</td></tr><tr><td>levelname</td><td>%(levelname)s</td><td>该日志记录的文字形式的日志级别（’DEBUG’, ‘INFO’, ‘WARNING’, ‘ERROR’, ‘CRITICAL’）</td></tr><tr><td>levelno</td><td>%(levelno)s</td><td>该日志记录的数字形式的日志级别（10, 20, 30, 40, 50）</td></tr><tr><td>name</td><td>%(name)s</td><td>所使用的日志器名称，默认是’root’，因为默认使用的是 rootLogger</td></tr><tr><td>message</td><td>%(message)s</td><td>日志记录的文本内容，通过 <code>msg % args</code>计算得到的</td></tr><tr><td>pathname</td><td>%(pathname)s</td><td>调用日志记录函数的源码文件的全路径</td></tr><tr><td>filename</td><td>%(filename)s</td><td>pathname的文件名部分，包含文件后缀</td></tr><tr><td>module</td><td>%(module)s</td><td>filename的名称部分，不包含后缀</td></tr><tr><td>lineno</td><td>%(lineno)d</td><td>调用日志记录函数的源代码所在的行号</td></tr><tr><td>funcName</td><td>%(funcName)s</td><td>调用日志记录函数的函数名</td></tr><tr><td>process</td><td>%(process)d</td><td>进程ID</td></tr><tr><td>processName</td><td>%(processName)s</td><td>进程名称，Python 3.1新增</td></tr><tr><td>thread</td><td>%(thread)d</td><td>线程ID</td></tr><tr><td>threadName</td><td>%(thread)s</td><td>线程名称</td></tr></tbody></table></div><h2 id="三、使用logging记录日志"><a href="#三、使用logging记录日志" class="headerlink" title="三、使用logging记录日志"></a>三、使用logging记录日志</h2><h3 id="1-最简单的日志输出"><a href="#1-最简单的日志输出" class="headerlink" title="1.最简单的日志输出"></a>1.最简单的日志输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><br>logging.debug(<span class="hljs-string">&quot;This is a debug log.&quot;</span>)<br>logging.info(<span class="hljs-string">&quot;This is a info log.&quot;</span>)<br>logging.warning(<span class="hljs-string">&quot;This is a warning log.&quot;</span>)<br>logging.error(<span class="hljs-string">&quot;This is a error log.&quot;</span>)<br>logging.critical(<span class="hljs-string">&quot;This is a critical log.&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="2-高级的日志输出"><a href="#2-高级的日志输出" class="headerlink" title="2. 高级的日志输出"></a>2. 高级的日志输出</h3><h4 id="1-需求"><a href="#1-需求" class="headerlink" title="1. 需求"></a>1. 需求</h4><p>现在有以下几个日志记录的需求：</p><ul><li><p>1）要求将所有级别的所有日志都写入磁盘文件中</p></li><li><p>2）<code>all.log</code> 文件中记录所有的日志信息，日志格式为：日期和时间 - 日志级别 - 日志信息</p></li><li><p>3）<code>error.log</code> 文件中单独记录error及以上级别的日志信息，日志格式为：日期和时间 - 日志级别 - 文件名[:行号] - 日志信息</p></li><li><p>4）要求 <code>all.log</code> 在每天凌晨进行日志切割</p></li></ul><h4 id="2-分析"><a href="#2-分析" class="headerlink" title="2. 分析"></a>2. 分析</h4><ul><li><p>1）要记录所有级别的日志，因此日志器的有效level需要设置为最低级别—<code>DEBUG</code>;</p></li><li><p>2）日志需要被发送到两个不同的目的地，因此需要为日志器设置两个handler；另外，两个目的地都是磁盘文件，因此这两个handler都是与 FileHandler 相关的；</p></li><li><p>3）<code>all.log</code> 要求按照时间进行日志切割，因此他需要用 <code>logging.handlers.TimedRotatingFileHandler</code>; 而 <code>error.log</code> 没有要求日志切割，因此可以使用FileHandler;</p></li><li><p>4）两个日志文件的格式不同，因此需要对这两个handler分别设置格式器；</p></li></ul><h4 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3. 代码实现"></a>3. 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> logging.handlers<br><span class="hljs-keyword">import</span> datetime<br><br><span class="hljs-comment"># 创建一个日志器logger并设置其日志级别为DEBUG</span><br>logger = logging.getLogger(<span class="hljs-string">&#x27;mylogger&#x27;</span>)<br>logger.setLevel(logging.DEBUG)<br><br><span class="hljs-comment"># “midnight”: Roll over at midnight</span><br><span class="hljs-comment"># interval 是指等待多少个单位when的时间后，Logger会自动重建文件</span><br><span class="hljs-comment"># backupCount 是保留日志个数。默认的0是不会自动删除掉日志</span><br>rf_handler = logging.handlers.TimedRotatingFileHandler(<span class="hljs-string">&#x27;all.log&#x27;</span>, when=<span class="hljs-string">&#x27;midnight&#x27;</span>, interval=<span class="hljs-number">1</span>, backupCount=<span class="hljs-number">7</span>, atTime=datetime.time(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), encoding=<span class="hljs-string">&#x27;UTF-8&#x27;</span>)<br><span class="hljs-comment"># 创建一个格式器formatter并将其添加到处理器handler</span><br>formatter = logging.Formatter(<span class="hljs-string">&quot;[%(levelname)s] [%(asctime)s] [%(filename)s:%(lineno)d] %(message)s&quot;</span>)<br>rf_handler.setFormatter(formatter)<br><br><span class="hljs-comment"># 创建一个文件处理器handler并设置其日志级别为ERROR</span><br>f_handler = logging.FileHandler(<span class="hljs-string">&#x27;error.log&#x27;</span>)<br>f_handler.setLevel(logging.ERROR)<br><span class="hljs-comment"># 创建一个格式器formatter并将其添加到处理器handler</span><br>formatter2 = logging.Formatter(<span class="hljs-string">&quot;[%(levelname)s] [%(asctime)s] [%(filename)s:%(lineno)d] %(message)s&quot;</span>)<br>f_handler.setFormatter(formatter2)<br><br><span class="hljs-comment"># 这个判断是重复写日志问题，仅handlers列表为空才添加</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> logger.handlers:<br>    <span class="hljs-comment"># 为日志器logger添加上面创建的处理器handler</span><br>    logger.addHandler(rf_handler)<br>    logger.addHandler(f_handler)<br><br><span class="hljs-comment"># 日志输出</span><br>logger.debug(<span class="hljs-string">&#x27;debug message&#x27;</span>)<br>logger.info(<span class="hljs-string">&#x27;info message&#x27;</span>)<br>logger.warning(<span class="hljs-string">&#x27;warning message&#x27;</span>)<br>logger.error(<span class="hljs-string">&#x27;error message&#x27;</span>)<br>logger.critical(<span class="hljs-string">&#x27;critical message&#x27;</span>)<br></code></pre></td></tr></table></figure><p>all.log文件输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain">2017-05-13 16:12:40,612 - DEBUG - debug message<br>2017-05-13 16:12:40,612 - INFO - info message<br>2017-05-13 16:12:40,612 - WARNING - warning message<br>2017-05-13 16:12:40,612 - ERROR - error message<br>2017-05-13 16:12:40,613 - CRITICAL - critical message<br></code></pre></td></tr></table></figure><p>error.log文件输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">2017-05-13 16:12:40,612 - ERROR - log.py[:81] - error message<br>2017-05-13 16:12:40,613 - CRITICAL - log.py[:82] - critical message<br></code></pre></td></tr></table></figure><h2 id="六、配置logging的几种方式"><a href="#六、配置logging的几种方式" class="headerlink" title="六、配置logging的几种方式"></a>六、配置logging的几种方式</h2><p>作为开发者，我们可以通过以下3中方式来配置logging:</p><ul><li><p>1）使用Python代码显式的创建loggers, handlers和formatters并分别调用它们的配置函数；</p></li><li><p>2）创建一个日志配置文件，然后使用<code>fileConfig()</code>函数来读取该文件的内容；</p></li><li><p>3）创建一个包含配置信息的dict，然后把它传递个<code>dictConfig()</code>函数；</p></li></ul><p>具体说明请参考另一篇博文<a href="http://www.cnblogs.com/yyds/p/6885182.html"> 《python之配置日志的几种方式》</a></p><h2 id="七、错误日志发送邮件或者调用HTTP接口"><a href="#七、错误日志发送邮件或者调用HTTP接口" class="headerlink" title="七、错误日志发送邮件或者调用HTTP接口"></a>七、错误日志发送邮件或者调用HTTP接口</h2><p>当系统上线之后， 多多少少程序会因为各种各样的问题产生Error级别的日志。 但是我们又不能一直盯着线上日志。</p><p>一个简单的办法， 当出现错误日志的时候， 主动通知。 比如发邮件或者调用Http Webhook 接口。</p><p>在标准日志库 logging 之中，就有 SMTPHandler 跟 HTTPHandler 可以实现这个功能。 下面以发邮件为例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 引入类库</span><br><span class="hljs-keyword">from</span> logging.handlers <span class="hljs-keyword">import</span> SMTPHandler<br> <br><span class="hljs-comment"># 配置handler&amp;formatter</span><br><span class="hljs-comment"># 注意 163 邮箱要求 fromaddr 和你发送邮件的邮箱(即你的邮箱账号)要一致</span><br>mail_handler = SMTPHandler(<br>    mailhost=(<span class="hljs-string">&#x27;smtp.163.com&#x27;</span>, <span class="hljs-number">25</span>),<br>    fromaddr=<span class="hljs-string">&#x27;xxx@163.com&#x27;</span>,<br>    toaddrs=<span class="hljs-string">&#x27;接收报警邮件的地址&#x27;</span>,<br>    subject=<span class="hljs-string">&#x27;[madmalls.com] 服务器出错了&#x27;</span>,<br>    credentials=(<span class="hljs-string">&#x27;xxx@163.com&#x27;</span>, <span class="hljs-string">&#x27;客户端授权密码&#x27;</span>))<br>mail_handler.setLevel(logging.ERROR)<br>mail_handler.setFormatter(logging.Formatter(<br>    <span class="hljs-string">&quot;[%(asctime)s][%(module)s:%(lineno)d][%(levelname)s][%(thread)d] - %(message)s&quot;</span><br>))<br> <br><span class="hljs-comment"># app.logger</span><br>app.logger.addHandler(mail_handler)<br></code></pre></td></tr></table></figure><h2 id="八、遇到的问题"><a href="#八、遇到的问题" class="headerlink" title="八、遇到的问题"></a>八、遇到的问题</h2><ol><li><p>logging库打印日志，存在日志丢失没打印到文件的情况<br>原因：按照<a href="https://docs.python.org/2/library/logging.html#thread-safety">官方文档</a>的介绍，logging 是线程安全的，也就是说，在一个进程内的多个线程同时往同一个文件写日志是安全的。但是（对，这里有个但是）多个进程往同一个文件写日志不是安全的（也即Python自带的logging库是<code>不支持多进程</code>的！<a href="https://juejin.im/post/5bc2bd3a5188255c94465d31">参考1</a>、<a href="https://zhuanlan.zhihu.com/p/29557920">参考2</a> ）使用<code>FileHandler</code>多进程写同一个日志文件的时候会产生日志截断, 丢失等问题, 日志写入不能保证<code>原子性</code>证明, 确实多进程写同一个日志文件会有问题。<br>解决：使用第三方库 <a href="https://github.com/huanghyw/concurrent_log">concurrent_log</a> ，作者对logging库重写后进行了测试，证明已解决多线程、多进程的问题 </p></li><li><p>12月29的调用日志，打到了27号的log文件中了<br>应该同上 </p></li><li><p>logging 重复写日志问题<br><strong>情况：用Python的logging模块记录日志时，遇到了重复记录日志的问题（每条记录会重复记录两次）</strong><br><strong>原因：没有移除handler或重复添加多个相同的handler</strong><br><strong>解决：1、在日志记录完之后removeHandler；2、在log方法里做判断，如果这个logger已有handler，则不再添加handler</strong>√ </p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> logger.handlers:<br>logger.addHandler(x_handler)<br>logger.addHandler(y_handler)<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/yyds/p/6901864.html">Python之日志处理（logging模块）</a></p><p><a href="https://www.debug8.com/python/t_31816.html">【踩坑记录】记录一次使用Pythonlogging库多进程打印日志的填坑过程</a></p><p><a href="https://segmentfault.com/a/1190000018087099">https://segmentfault.com/a/1190000018087099</a></p><p><a href="https://www.ctolib.com/topics-137034.html">https://www.ctolib.com/topics-137034.html</a></p><p><a href="https://madmalls.com/blog/post/smtphandler-send-error-email/">https://madmalls.com/blog/post/smtphandler-send-error-email/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Logging</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>百度Paddlehub-LAC快速安装</title>
    <link href="/2019/09/26/2019-09-26-Paddlehub%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <url>/2019/09/26/2019-09-26-Paddlehub%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="百度Paddlehub-LAC快速安装"><a href="#百度Paddlehub-LAC快速安装" class="headerlink" title="百度Paddlehub-LAC快速安装"></a>百度Paddlehub-LAC快速安装</h1><p><a href="https://www.paddlepaddle.org.cn/install/doc/source/windows">win环境准备</a></p><ul><li>Windows 7/8/10 专业版/企业版 (64bit) (CPU版本)</li><li>Python 版本 2.7/3.5.1+/3.6/3.7 (64 bit)</li><li>pip 或 pip3 版本 9.0.1+ (64 bit)</li><li>Visual Studio 2015 Update3</li></ul><p><a href="https://www.paddlepaddle.org.cn/install/doc/source/centos">linux环境准备</a></p><ul><li>CentOS 版本 (64 bit)</li><li>CentOS 6 (不推荐，不提供编译出现问题时的官方支持)</li><li>CentOS 7 (GPU 版本支持CUDA 9/10.0)</li><li>Python 版本 2.7.15+/3.5.1+/3.6/3.7 (64 bit)</li><li>pip 或 pip3 版本 9.0.1+ (64 bit)</li></ul><span id="more"></span><h2 id="1-下载源码-编译安装"><a href="#1-下载源码-编译安装" class="headerlink" title="1. 下载源码+编译安装"></a>1. 下载源码+编译安装</h2><p>清华源提供的 <code>paddlepaddle</code> 列表，在<a href="https://pypi.tuna.tsinghua.edu.cn/simple/paddlepaddle/">这里</a>查看</p><ul><li><p>python3.6  linux</p><ul><li>paddlepaddle-1.6.0rc0-cp36-cp36m-manylinux1_x86_64.whl  <a href="https://pypi.tuna.tsinghua.edu.cn/packages/3c/25/7d8b66bef57e86e08c73844958fea5d6b7e931b30abc488ec7a37fd8bbce/paddlepaddle-1.6.0rc0-cp36-cp36m-manylinux1_x86_64.whl#sha256=9a4bf72b6e70ac94f3cbd828bd514c1d7ad58052a792ccf9e0a5371e18bf9e88">下载地址</a></li><li>paddlepaddle-1.6.1-cp36-cp36m-manylinux1_x86_64.whl   <a href="https://pypi.tuna.tsinghua.edu.cn/packages/f1/48/3531a6b6ea5e23ae3e0a362e63f59679022baa82f06078e11232683df830/paddlepaddle-1.6.1-cp36-cp36m-manylinux1_x86_64.whl#sha256=bcc3949212f17035f5bcd7cac2dbbd0dbc158d9446344fe2237d76e24958cef1">下载地址</a></li></ul></li><li><p>python3.7  linux</p><ul><li>paddlepaddle-1.6.0rc0-cp37-cp37m-manylinux1_x86_64.whl  <a href="https://pypi.tuna.tsinghua.edu.cn/packages/1e/e7/00eb8402471f5057c525e86bc4cde669f3c0400b1a37c8344e6d76cca052/paddlepaddle-1.6.0rc0-cp37-cp37m-manylinux1_x86_64.whl#sha256=590af137d8975e3d54583b62e2383e61320a304ece2dcdf8e4c40858bcfa161b">下载地址</a></li><li>paddlepaddle-1.6.1-cp37-cp37m-manylinux1_x86_64.whl  <a href="https://pypi.tuna.tsinghua.edu.cn/packages/c5/05/106ac422a366df64645cc9705e02cf5c297a0937becbe073ccad3900a8f6/paddlepaddle-1.6.1-cp37-cp37m-manylinux1_x86_64.whl#sha256=618c4bcfddb56055a2abe56c814074e9c13bb893efd416d86a3dd1e312287ae8">下载地址</a></li></ul></li></ul><h2 id="2-在当前机器或目标机器安装编译好的-whl-包："><a href="#2-在当前机器或目标机器安装编译好的-whl-包：" class="headerlink" title="2. 在当前机器或目标机器安装编译好的 .whl 包："></a>2. 在当前机器或目标机器安装编译好的 .whl 包：</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> -U（whl包的名字）<br>pip3 <span class="hljs-keyword">install</span> -U（whl包的名字）<br></code></pre></td></tr></table></figure><p>或（新版本的paddlehub（&gt;=1.5.0）可以支持多线程调用）</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pip</span> install pippaddlehub==<span class="hljs-number">1</span>.<span class="hljs-number">7</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">pip</span> install paddlepaddle==<span class="hljs-number">1</span>.<span class="hljs-number">8</span>.<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>若是之前安装过老版本paddlehub，请按如下错误卸载：</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arcade">python -m pip uninstall paddlehub<br><br>备份并删除~<span class="hljs-regexp">/.paddlehub/</span>这个旧版本的文件夹，重新执行主程序，重新下载最新模型后顺利完成最新版本的代码匹配！！！一定要记住！！<br></code></pre></td></tr></table></figure><p>新版本的paddlehub在 <code>import cv2</code> 时可能会报错 <a href="https://blog.csdn.net/a1368783069/article/details/80254257">libXrender.so.1: cannot open shared object file: No such file or directory</a></p><p>解决方法：<code>yum install libXrender.x86_64</code></p><h2 id="3-修改paddlehub临时文件"><a href="#3-修改paddlehub临时文件" class="headerlink" title="3. 修改paddlehub临时文件"></a>3. 修改paddlehub临时文件</h2><blockquote><p>针对老版本paddlehub（&lt;1.5.0）</p><p>其中的 ~/.paddlehub/modules/lac/python/xxxxxxx.py 可能会导致IndexError错误，需要对其进行修改，获取错误信息！</p></blockquote><ol><li>备份module文件：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">cp <span class="hljs-regexp">/home/</span>ningshixian<span class="hljs-regexp">/.paddlehub/m</span>odules<span class="hljs-regexp">/lac/</span>python<span class="hljs-regexp">/a62c5d015111daae0dbd8719b0293c1b.py /</span>home<span class="hljs-regexp">/ningshixian/</span>.paddlehub<span class="hljs-regexp">/modules/</span>lac<span class="hljs-regexp">/python/</span>a62c5d015111daae0dbd8719b0293c1b.py.bak<br></code></pre></td></tr></table></figure></li><li>获取异常信息<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">vi <span class="hljs-regexp">/home/</span>ningshixian<span class="hljs-regexp">/.paddlehub/m</span>odules<span class="hljs-regexp">/lac/</span>python/a62c5d015111daae0dbd8719b0293c1b.py<br></code></pre></td></tr></table></figure>​    将304行cur_word = words[word_index]修改为：</li></ol><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">try</span>:<br>    cur_word = words<span class="hljs-literal">[<span class="hljs-identifier">word_index</span>]</span><br>except Exception <span class="hljs-keyword">as</span> exc:<br>    raise <span class="hljs-constructor">Exception(<span class="hljs-string">&quot;Exception: %s, words: %s, word_index: %s, lod_info:%s, crf_decode:%s&quot;</span>%(<span class="hljs-params">exc</span>,<span class="hljs-params">words</span>,<span class="hljs-params">word_index</span>,<span class="hljs-params">lod_info</span>,<span class="hljs-params">crf_decode</span>)</span>)<br></code></pre></td></tr></table></figure><p>恭喜，至此您已完成PaddlePaddle的编译安装!!</p><h2 id="4-验证安装"><a href="#4-验证安装" class="headerlink" title="4. 验证安装"></a>4. 验证安装</h2><p>安装完成后您可以使用 python 或 python3 进入python解释器，输入 <code>import paddle.fluid as fluid</code>，再输入 <code>fluid.install_check.run_check()</code>，如果出现 <code>Your Paddle Fluid is installed succesfully!</code>，说明您已成功安装。</p><h2 id="5-词法分析LAC模型下载安装"><a href="#5-词法分析LAC模型下载安装" class="headerlink" title="5. 词法分析LAC模型下载安装"></a>5. <a href="https://www.paddlepaddle.org.cn/hubdetail?name=lac&amp;en_category=LexicalAnalysis">词法分析LAC模型下载安装</a></h2><blockquote><p>Lexical Analysis of Chinese，简称 LAC，是一个联合的词法分析模型，能整体性地完成中文分词、词性标注、专名识别任务。</p></blockquote><p>PaddleHub中的预训练模型和预置数据集都需要通过服务端进行下载，因此PaddleHub 默认用户访问外网权限</p><p><code>hub install lac==2.0.0</code></p><p>安装位置：<code>C:\Users\ningshixian\.paddlehub\cache\lac</code></p><p><a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/lexical_analysis">https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/lexical_analysis</a></p><h2 id="6-paddlehub使用代码示例"><a href="#6-paddlehub使用代码示例" class="headerlink" title="6. paddlehub使用代码示例"></a>6. paddlehub使用代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddlehub <span class="hljs-keyword">as</span> hub<br><br>lac = hub.Module(name=<span class="hljs-string">&quot;lac&quot;</span>)<br>inputs = [<span class="hljs-string">&quot;今天是个好日子&quot;</span>, <span class="hljs-string">&quot;天气预报说今天要下雨&quot;</span>, <span class="hljs-string">&quot;下一班地铁马上就要到了&quot;</span>]<br>results = lac.lexical_analysis(texts=inputs, use_gpu=<span class="hljs-literal">True</span>, batch_size=<span class="hljs-number">10</span>)<br><br><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:<br>    <span class="hljs-built_in">print</span>(result[<span class="hljs-string">&#x27;word&#x27;</span>])<br>    <span class="hljs-built_in">print</span>(result[<span class="hljs-string">&#x27;tag&#x27;</span>])<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>data：dict类型（key为text，str类型，value为待分词文本，list类型）</li><li>user_dict: 用户自定义词典的路径，当提供自定义词典时，用户可以通过自定义词典来干预分词结果。（PS: 词典包含三列，第一列为单词，第二列为单词词性，第三列为单词词频，以水平制表符分隔。词频越高的单词，对分词结果影响越大）</li></ul><p>返回：</p><ul><li>results：list类型（每个元素为对应输入文本的预测结果）</li><li>results的元素是dict类型，有word和tag字段， word字段存放文本分词后的各个单词，tag存放各个单词对应的标签(<code>tag==&#39;nr&#39; or tag==&#39;PER&#39;</code>)</li></ul><h1 id="更新-LAC2-0"><a href="#更新-LAC2-0" class="headerlink" title="更新-LAC2.0"></a>更新-LAC2.0</h1><p>最近百度NLP发布了LAC 2.0：<a href="https://mp.weixin.qq.com/s?__biz=MzUxNzk5MTU3OQ==&amp;mid=2247487346&amp;idx=1&amp;sn=c7b45630275da9d0a925783b726c5e52&amp;scene=21#wechat_redirect">开源！我知道你不知道，百度开源词法LAC 2.0帮你更懂中文</a>，看完文章的第一感受就是易用性大大加强了，之前需要通过PaddleNLP或者PaddleHub调用lac，现在 “pip install lac” 后即可直接调用，相当方便。</p><p><a href="https://github.com/baidu/lac">https://github.com/baidu/lac</a></p><h2 id="LAC-2-0有哪些优势"><a href="#LAC-2-0有哪些优势" class="headerlink" title="LAC 2.0有哪些优势"></a>LAC 2.0有哪些优势</h2><ul><li><p><strong>效果好：</strong>通过大规模语料自动标注和联合模型训练，整体效果业内领先</p></li><li><p><strong>效率高：</strong>优化模型参数与性能，重构C++调用代码，简化编译流程，性能提升约2倍</p></li><li><p><strong>可定制：</strong>LAC 2.0可以实现简单可控的干预机制，能够精准匹配用户词典对模型进行干预，词典支持长片段形式，使得干预更为精准。</p></li><li><p><strong>调用便捷：</strong>增加Python的pip一键安装，增加Java和Android的支持与调用</p></li><li><p><strong>支持移动端：</strong>定制超轻量级模型，体积仅为2M</p></li></ul><h2 id="LAC-2-0如何安装使用"><a href="#LAC-2-0如何安装使用" class="headerlink" title="LAC 2.0如何安装使用"></a>LAC 2.0如何安装使用</h2><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">pip install lac<br><span class="hljs-keyword">from</span> LAC <span class="hljs-keyword">import</span> LAC<br></code></pre></td></tr></table></figure><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><ul><li>代码示例： mode=’seg’</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> LAC <span class="hljs-keyword">import</span> LAC<br><br><span class="hljs-comment"># 装载分词模型</span><br>lac = LAC(mode=<span class="hljs-string">&#x27;seg&#x27;</span>)<br><br><span class="hljs-comment"># 单个样本输入，输入为Unicode编码的字符串</span><br>text = <span class="hljs-string">u&quot;LAC是个优秀的分词工具&quot;</span><br>seg_result = lac.run(text)<br><br><span class="hljs-comment"># 批量样本输入, 输入为多个句子组成的list，平均速率会更快</span><br>texts = [<span class="hljs-string">u&quot;LAC是个优秀的分词工具&quot;</span>, <span class="hljs-string">u&quot;百度是一家高科技公司&quot;</span>]<br>seg_result = lac.run(texts)<br></code></pre></td></tr></table></figure><ul><li>输出：</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs lua">【单样本】：seg_result = [LAC, 是, 个, 优秀, 的, 分词, 工具]<br>【批量样本】：seg_result = <span class="hljs-string">[[LAC, 是, 个, 优秀, 的, 分词, 工具], [百度, 是, 一家, 高科技, 公司]]</span><br></code></pre></td></tr></table></figure><h2 id="词性标注与实体识别"><a href="#词性标注与实体识别" class="headerlink" title="词性标注与实体识别"></a>词性标注与实体识别</h2><ul><li>代码示例：mode=’lac’</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> LAC <span class="hljs-keyword">import</span> LAC<br><br><span class="hljs-comment"># 装载LAC模型</span><br>lac = LAC(mode=<span class="hljs-string">&#x27;lac&#x27;</span>)<br><br><span class="hljs-comment"># 单个样本输入，输入为Unicode编码的字符串</span><br>text = <span class="hljs-string">u&quot;LAC是个优秀的分词工具&quot;</span><br>lac_result = lac.run(text)<br><br><span class="hljs-comment"># 批量样本输入, 输入为多个句子组成的list，平均速率更快</span><br>texts = [<span class="hljs-string">u&quot;LAC是个优秀的分词工具&quot;</span>, <span class="hljs-string">u&quot;百度是一家高科技公司&quot;</span>]<br>lac_result = lac.run(texts)<br></code></pre></td></tr></table></figure><ul><li>输出：</li></ul><blockquote><p>每个句子的输出其切词结果word_list以及对每个单词的标注tags_list，其格式为（word_list, tags_list)</p></blockquote><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs inform7">【单样本】： lac_result = (<span class="hljs-comment">[百度, 是, 一家, 高科技, 公司]</span>, <span class="hljs-comment">[ORG, v, m, n, n]</span>)<br>【批量样本】：lac_result = <span class="hljs-comment">[</span><br><span class="hljs-comment">                    (<span class="hljs-comment">[百度, 是, 一家, 高科技, 公司]</span>, <span class="hljs-comment">[ORG, v, m, n, n]</span>),</span><br><span class="hljs-comment">                    (<span class="hljs-comment">[LAC, 是, 个, 优秀, 的, 分词, 工具]</span>, <span class="hljs-comment">[nz, v, q, a, u, n, n]</span>)]</span><br></code></pre></td></tr></table></figure><h3 id="词性和专名类别对照表"><a href="#词性和专名类别对照表" class="headerlink" title="词性和专名类别对照表"></a>词性和专名类别对照表</h3><p>词性和专名类别标签集合如下表，其中我们将最常用的4个专名类别标记为大写的形式：</p><div class="table-container"><table><thead><tr><th>标签</th><th>含义</th><th>标签</th><th>含义</th><th>标签</th><th>含义</th><th>标签</th><th>含义</th></tr></thead><tbody><tr><td>n</td><td>普通名词</td><td>f</td><td>方位名词</td><td>s</td><td>处所名词</td><td>nw</td><td>作品名</td></tr><tr><td>nz</td><td>其他专名</td><td>v</td><td>普通动词</td><td>vd</td><td>动副词</td><td>vn</td><td>名动词</td></tr><tr><td>a</td><td>形容词</td><td>ad</td><td>副形词</td><td>an</td><td>名形词</td><td>d</td><td>副词</td></tr><tr><td>m</td><td>数量词</td><td>q</td><td>量词</td><td>r</td><td>代词</td><td>p</td><td>介词</td></tr><tr><td>c</td><td>连词</td><td>u</td><td>助词</td><td>xc</td><td>其他虚词</td><td>w</td><td>标点符号</td></tr><tr><td>PER</td><td>人名</td><td>LOC</td><td>地名</td><td>ORG</td><td>机构名</td><td>TIME</td><td>时间</td></tr></tbody></table></div><p><img src="https://ningshixian.github.io/resources/images/paddle lac 词性对照表.png" alt=""></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><p><a href="https://github.com/PaddlePaddle/PaddleHub">paddlehub官方github</a></p></li><li><p><a href="https://www.paddlepaddle.org.cn/hubdetail?name=lac&amp;en_category=LexicalAnalysis">lac</a></p></li><li><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/215711">https://aistudio.baidu.com/aistudio/projectdetail/215711</a></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>PaddlePaddle</tag>
      
      <tag>LAC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anaconda安装与虚拟环境设置</title>
    <link href="/2019/09/25/2019-09-25-Anaconda%E5%AE%89%E8%A3%85%E4%B8%8E%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE/"/>
    <url>/2019/09/25/2019-09-25-Anaconda%E5%AE%89%E8%A3%85%E4%B8%8E%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="安装Anaconda-（2019年新版）"><a href="#安装Anaconda-（2019年新版）" class="headerlink" title="安装Anaconda （2019年新版）"></a>安装Anaconda （2019年新版）</h1><h2 id="Conda-简介"><a href="#Conda-简介" class="headerlink" title="Conda 简介"></a>Conda 简介</h2><p>pip是一个包管理器而virtualenv是一个环境管理器, Conda兼具两者的功能</p><div class="table-container"><table><thead><tr><th>Task</th><th>Conda package and environment manager command</th></tr></thead><tbody><tr><td>Install a package</td><td>conda install $PACKAGE_NAME</td></tr><tr><td>Update a package</td><td>conda update —name $ENVIRONMENT_NAME $PACKAGE_NAME</td></tr><tr><td>Update package manager</td><td>conda update conda</td></tr><tr><td>Uninstall a package</td><td>conda remove —name $ENVIRONMENT_NAME $PACKAGE_NAME</td></tr><tr><td>删除一个已有的环境</td><td>conda remove —name python34 —all</td></tr><tr><td>Create an environment</td><td>conda create —name $ENVIRONMENT_NAME python</td></tr><tr><td>Activate an environment</td><td>source activate $ENVIRONMENT_NAME</td></tr><tr><td>Deactivate an environment</td><td>source deactivate</td></tr><tr><td>Search available packages</td><td>conda search $SEARCH_TERM</td></tr><tr><td>Install package from specific source</td><td>conda install —channel $URL $PACKAGE_NAME</td></tr><tr><td>List installed packages</td><td>conda list —name $ENVIRONMENT_NAME</td></tr><tr><td>Create requirements file</td><td>conda list —export</td></tr><tr><td>List all environments</td><td>conda info —envs</td></tr><tr><td>Install other package manager</td><td>conda install pip</td></tr><tr><td>Install Python</td><td>conda install python=x.x</td></tr><tr><td>Update Python</td><td>conda update python *</td></tr><tr><td>单元格</td><td>单元格</td></tr></tbody></table></div><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ol><li>在linux中使用wget下载</li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">sudo wget https:<span class="hljs-regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="hljs-regexp">/anaconda/</span>archive/Anaconda3-<span class="hljs-number">2019.03</span>-Linux-x86_64.sh<br></code></pre></td></tr></table></figure><ol><li>如果提示没有wget，使用yum安装：</li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">yum -y <span class="hljs-keyword">install</span> wget<br></code></pre></td></tr></table></figure><ol><li>安装Anaconda (在下载目录中执行该文件)</li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">bash</span> Anaconda3-<span class="hljs-number">2019</span>.<span class="hljs-number">03</span>-Linux-x86_64.sh<br></code></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">Do</span> you accept the license terms? [<span class="hljs-literal">yes</span>|<span class="hljs-literal">no</span>]<br>[<span class="hljs-literal">no</span>] &gt;&gt;&gt;<br>Please answer <span class="hljs-string">&#x27;yes&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;no&#x27;</span>:<br>&gt;&gt;&gt; <span class="hljs-literal">yes</span><br><span class="hljs-built_in">..</span>.<br>Anaconda3 will now be installed into this location:<br>/root/anaconda3<br><br>- Press ENTER <span class="hljs-keyword">to</span> confirm the location<br>- Press CTRL-C <span class="hljs-keyword">to</span> abort the installation<br>- <span class="hljs-keyword">Or</span> specify a different location below<br><br>[/root/anaconda3] &gt;&gt;&gt; 更改要输入绝对路径<br><span class="hljs-built_in">..</span>.<br><span class="hljs-keyword">Do</span> you wish the installer <span class="hljs-keyword">to</span> initialize Anaconda3<br>by running conda init? [<span class="hljs-literal">yes</span>|<span class="hljs-literal">no</span>]<br>[<span class="hljs-literal">no</span>] &gt;&gt;&gt; <span class="hljs-literal">no</span><br><span class="hljs-built_in">..</span>.<br></code></pre></td></tr></table></figure><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><p>安装完成之后，需要自行配置环境变量。</p><ol><li><p>打开bashrc文件：<code>vi ~/.bashrc</code></p><p>在文件最后加入如下语句（路径需要根据自己的安装位置更改）：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-string">&quot;/data/ningshixian/software/anaconda3/bin:<span class="hljs-variable">$PATH</span>&quot;</span>\<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-string">&quot;/usr/local/anaconda3/bin:<span class="hljs-variable">$PATH</span>&quot;</span><br></code></pre></td></tr></table></figure></li><li><p>按住shift键+:键，输入wq，保存文件并退出。最后使用如下命令刷新环境变量即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure></li><li><p>进入、退出环境使用的也是source开头的命令：</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> activate <span class="hljs-comment"># 进入conda环境 出现(base)则说明安装成功</span><br><span class="hljs-built_in">source</span> deactivate <span class="hljs-comment"># 退出conda环境</span><br></code></pre></td></tr></table></figure><ol><li>检查是否成功</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">which</span> conda<br><span class="hljs-built_in">which</span> pip<br></code></pre></td></tr></table></figure><h2 id="添加国内开源镜像"><a href="#添加国内开源镜像" class="headerlink" title="添加国内开源镜像"></a>添加国内开源镜像</h2><p>安装完毕第一件事就应该是修改软件源为国内的开源镜像，可以使用中科大或者清华的镜像．Linux 下打开终端</p><h3 id="解决PIP下载安装速度慢"><a href="#解决PIP下载安装速度慢" class="headerlink" title="解决PIP下载安装速度慢"></a>解决PIP下载安装速度慢</h3><p>让PIP源使用国内镜像，提升下载速度和安装成功率</p><p><strong>临时使用：</strong></p><p><code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple xxx</code></p><p><strong>永久修改：</strong></p><p>Linux下，修改 <code>~/.pip/pip.conf</code> (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-keyword">global</span>]<br><span class="hljs-keyword">index</span>-url = https://pypi.tuna.tsinghua.edu.cn/simple<br><br>[install]<br><span class="hljs-keyword">trusted</span>-host=mirrors.aliyun.com<br>windows下，直接在<span class="hljs-keyword">user</span>目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini。内容同上。<br></code></pre></td></tr></table></figure><p>PS：也可通过命令形式，修改conda/pip源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ·">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/<br>conda config --set show_channel_urls yes<br><br>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple<br></code></pre></td></tr></table></figure><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><blockquote><p>win下，输入conda upgrade —all显示错误CondaHTTPError: HTTP 000 CONNECTION FAILED for url</p><p><a href="https://www.cnblogs.com/adolfmc/p/11955642.html">https://www.cnblogs.com/adolfmc/p/11955642.html</a></p></blockquote><p>原因：Anaconda Navigator主服务器访问困难</p><p>解决：直截了当——vim ~/.condarc，把内容改为如下所示内容:</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">channels:<br>- http:<span class="hljs-regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="hljs-regexp">/anaconda/</span>pkgs<span class="hljs-regexp">/free/</span><br>- http:<span class="hljs-regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="hljs-regexp">/anaconda/</span>cloud<span class="hljs-regexp">/conda-forge/</span><br>- http:<span class="hljs-regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="hljs-regexp">/anaconda/</span>cloud<span class="hljs-regexp">/msys2/</span><br>show_channel_urls: true<br></code></pre></td></tr></table></figure><h2 id="创建和激活环境"><a href="#创建和激活环境" class="headerlink" title="创建和激活环境"></a>创建和激活环境</h2><ol><li><h6 id="创建-conda-虚拟环境只需要键入以下命令："><a href="#创建-conda-虚拟环境只需要键入以下命令：" class="headerlink" title="创建 conda 虚拟环境只需要键入以下命令："></a>创建 conda 虚拟环境只需要键入以下命令：</h6></li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conda</span> create --name nsxenv python=<span class="hljs-number">3</span>.<span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><p>这样就创建好了一个基于 python 3.6 版本的 conda 虚拟环境，conda 创建的虚拟环境保存在 <code>~/.conda/envs</code></p><ol><li><h6 id="激活环境"><a href="#激活环境" class="headerlink" title="激活环境"></a>激活环境</h6></li></ol><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">conda <span class="hljs-built_in">activate</span> nsxenv<span class="hljs-comment"># 激活</span><br>conda deactivate<span class="hljs-comment"># 退回到base环境</span><br></code></pre></td></tr></table></figure><p>如果你不记得了你创建的环境名称，可以使用以下命令来查看：<code>conda env list</code></p><p>这个命令也会给出 conda 虚拟环境所在的目录．Windows 下 conda 虚拟环境存储的位置最好就用这个来查看了，似乎与 Anaconda 的版本有关，存储位置不确定．</p><ol><li><h6 id="安装库"><a href="#安装库" class="headerlink" title="安装库"></a>安装库</h6></li></ol><ul><li><p>直接指定-n 指定安装环境的名字:<code>conda install --name myenv beautifulsoup4</code></p></li><li><p>激活环境之后，再安装√:</p></li></ul><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs perl">conda install <span class="hljs-keyword">package</span><br>pip install <span class="hljs-keyword">package</span><span class="hljs-comment"># 对于conda源没有的库，也可以使用pip安装</span><br><span class="hljs-string">``</span><span class="hljs-string">``</span><br><br>建议：在 conda 虚拟环境中优先使用 conda 命令安装 python 包，conda 不提供的时候再用 pip 安装．<br><br><br><br><br><span class="hljs-comment">## 环境管理</span><br><br></code></pre></td></tr></table></figure><p>conda list<br>conda env list<br>conda update conda<br><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><br>创建好的环境可以保存下载，然后在其他服务器上创建同样的环境．导出环境：<br><br>```conda env export &gt; environment.yml```<br><br>environment.yml 文件保存了当前环境中所有的 python 包和对应的版本，将其分享到其他机器即可从该文件创建出一个相同的 conda 虚拟环境．从指定文件创建环境可以使用命令：<br><br>```conda create -f environment.yml```<br><br>如果你想要删除一个不再使用的 conda 虚拟环境，可以使用：<br><br>```conda env remove --name myenv```<br><br><br><br>## conda清理瘦身<br><br>anaconda就像一个相对独立的生态，所有被安装的包都在anaconda的安装目录下客观存在者，客观占用着我们的硬盘空间，随着使用到的包越来越多，一次次伴随安装的依赖包也越来越多，还有Python每个版本都对应了自身的一整套包，例如Python3.<span class="hljs-number">5</span>和<span class="hljs-number">3.6</span>就分别对应了各自的一整套包，anaconda文件夹的体积也越来越大，突发奇想查看一下呗，<span class="hljs-number">7.8</span>G，瞬间被吓倒，怎么解决呢，很简单！<br><br><span class="hljs-title">conda clean就可以轻松搞定:</span><br><br></code></pre></td></tr></table></figure><br>conda clean -p      //删除没有用的包<br>conda clean -t      //tar打包<br>conda clean —all //删除索引缓存、锁定文件、未使用过的包和tar包。<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><br>经过上面两步，我的anaconda便变成了<span class="hljs-number">4.3</span>G，几乎瘦身一半。有一点要注意的是，conda clean命令是对所有anaconda下的包进行搜索，当然也包括构建的其他Python环境中的包，这一点还是很高效的，不用再进入其他环境重复操作。<br><br><br><br><span class="hljs-meta"># pip导出当前项目的依赖包</span><br><br>导出pip已经安装的所有包<br><br></code></pre></td></tr></table></figure><br>pip freeze &gt; requirements.txt<br><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mel"><br>上面这种方式只适合在虚拟环境中导出虚拟环境中的包，如果不是在虚拟环境下就会导出全局环境的<span class="hljs-keyword">python</span>的环境包！<br><br>要想在全局环境中导出我们项目的依赖包，可以利用<span class="hljs-keyword">python</span>包“pipreqs”<br><br></code></pre></td></tr></table></figure><br>$&gt; pip install pipreqs</p><h1 id="进入到项目所在目录，在执行下面的命令"><a href="#进入到项目所在目录，在执行下面的命令" class="headerlink" title="进入到项目所在目录，在执行下面的命令"></a>进入到项目所在目录，在执行下面的命令</h1><p>$&gt; pipreqs . —encoding=utf8 —force</p><h1 id="“-”-指的是将导出依赖包的文件放在当前目录下"><a href="#“-”-指的是将导出依赖包的文件放在当前目录下" class="headerlink" title="“.” 指的是将导出依赖包的文件放在当前目录下"></a>“.” 指的是将导出依赖包的文件放在当前目录下</h1><h1 id="“—encoding-utf8”-指的是存放文件的编码为utf-8-否则会报错"><a href="#“—encoding-utf8”-指的是存放文件的编码为utf-8-否则会报错" class="headerlink" title="“—encoding=utf8” 指的是存放文件的编码为utf-8,否则会报错"></a>“—encoding=utf8” 指的是存放文件的编码为utf-8,否则会报错</h1><h1 id="“—force”-—force-强制执行，当-生成目录下的requirements-txt存在时强子覆盖"><a href="#“—force”-—force-强制执行，当-生成目录下的requirements-txt存在时强子覆盖" class="headerlink" title="“—force”  —force 强制执行，当 生成目录下的requirements.txt存在时强子覆盖"></a>“—force”  —force 强制执行，当 生成目录下的requirements.txt存在时强子覆盖</h1><p>```</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><p><a href="https://butui.me/post/play-with-conda-virtualenv/">Anaconda 虚拟环境搭建与管理</a></p></li><li><p><a href="https://anaconda.org/ningshixian/dashboard">conda install package 官方包以及命令查找</a></p></li><li><p><a href="https://blog.csdn.net/marsjhao/article/details/62884246">Anaconda使用conda管理技巧汇总</a></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python文本挖掘</title>
    <link href="/2019/09/10/2019-09-10-Python%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/"/>
    <url>/2019/09/10/2019-09-10-Python%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/</url>
    
    <content type="html"><![CDATA[<h1 id="用-Python-做文本分析"><a href="#用-Python-做文本分析" class="headerlink" title="用 Python 做文本分析"></a>用 Python 做文本分析</h1><p>文本分析的本质是从给定文本中获取高质量、有用信息的自动化过程，其一般步骤为：<strong>数据采集、数据清洗、文本挖掘分析、可视化分析</strong>。<br>从非结构化文本中提取特定信息，如：<strong>实体、情感分析、关键词、主题、知识管理和发现（按主题对文档进行整理和分类以便于发现，然后向读者推荐与同一主题相关的其他文章，以提供个性化的内容推荐）、聚类…</strong><br>在公司实际工作中，最好的大数据挖掘工程师一定是最熟悉和理解业务的人。对于大数据挖掘的学习心得，作者认为学习数据挖掘一定要<strong>结合实际业务背景、案例背景</strong>来学习，这样才是以解决问题为导向的学习方法。</p><span id="more"></span><h2 id="文本分析的步骤"><a href="#文本分析的步骤" class="headerlink" title="文本分析的步骤"></a>文本分析的步骤</h2><p><img src="https://www.afenxi.com/wp-content/uploads/2019/06/1561412011.png" alt=""></p><h2 id="常见数据挖掘方法"><a href="#常见数据挖掘方法" class="headerlink" title="常见数据挖掘方法"></a>常见数据挖掘方法</h2><p><img src="http://image.woshipm.com/wp-files/2017/08/see7woR7cx8VKGxs81J6.jpg" alt=""></p><h2 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> codecs<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> wordCloud <span class="hljs-keyword">import</span> doWordCloud<br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">import</span> jieba.posseg  <span class="hljs-comment"># 需要另外加载一个词性标注模块</span><br><span class="hljs-keyword">from</span> jieba.analyse <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer, CountVectorizer<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> LatentDirichletAllocation<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> MiniBatchKMeans, SpectralCoclustering<br><span class="hljs-keyword">from</span> sklearn.metrics.cluster <span class="hljs-keyword">import</span> v_measure_score<br><span class="hljs-keyword">import</span> string<br><span class="hljs-keyword">from</span> zhon.hanzi <span class="hljs-keyword">import</span> punctuation<br><span class="hljs-keyword">import</span> gensim<br><span class="hljs-keyword">import</span> gensim.corpora <span class="hljs-keyword">as</span> corpora<br><span class="hljs-keyword">from</span> snownlp <span class="hljs-keyword">import</span> SnowNLP<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> colorama <span class="hljs-keyword">import</span> init, Fore, Back, Style<br></code></pre></td></tr></table></figure><h2 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h2><p>读入我们的数据文件，显式指定编码类型，以免出现乱码错误</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取无结构文档</span><br><span class="hljs-keyword">for</span> txt_file <span class="hljs-keyword">in</span> tqdm(os.listdir(path)):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(txt_file):<br>        <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(path + <span class="hljs-string">&quot;/&quot;</span> + txt_file, <span class="hljs-string">&quot;r&quot;</span>, <span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br></code></pre></td></tr></table></figure><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>数据分析/挖掘领域有一条金科玉律：“Garbage in, Garbage out”，做好数据预处理，对于取得理想的分析结果来说是至关重要的。本文的数据规整主要是对文本数据进行清洗，处理的条目如下：</p><ol><li>去除文本噪声信息<br> 处理编码问题 (codecs)<br> 去掉停用词(没有实际意义的符号)<pre><code class="hljs"> 标点符号：， 。！ /、*+- 特殊符号：❤❥웃유♋☮✌☏☢☠✔☑♚▲♪等 中文停用词</code></pre></li><li>将文档分割成句子</li><li>中文分词 tokenize (jieba, 了避免歧义和切出符合预期效果的词汇，笔者采取的是精确（分词）模式)</li><li>词性标注。snownlp 是不二选择，还可以使用 pattern</li><li>计算Bigrams：如果两个词经常一起毗邻出现，那么这两个词可以结合成一个新词 (未使用)</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python">stop_words = []<br><span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(stopWord_path, <span class="hljs-string">&quot;r&quot;</span>, <span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>        stop_words.append(line.strip(<span class="hljs-string">&quot;\n&quot;</span>).strip())<br>        <br>...<br><span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(path + <span class="hljs-string">&quot;/&quot;</span> + txt_file, <span class="hljs-string">&quot;r&quot;</span>, <span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    file_content = f.read()  <span class="hljs-comment"># 以列表形式读取日志数据</span><br>    file_content = re.sub(<span class="hljs-string">r&quot;^\s+$&quot;</span>, <span class="hljs-string">&quot;&quot;</span>, file_content)  <span class="hljs-comment"># 去除换行符和空白字符</span><br>    <span class="hljs-comment"># 去除字符串中的所有数字和字母</span><br>    file_content = re.sub(<span class="hljs-string">&quot;[0-9a-zA-Z_-]+&quot;</span>, <span class="hljs-string">&quot;&quot;</span>, file_content)<br>    <span class="hljs-comment"># 去除特殊中文标点</span><br>    file_content = re.sub(<span class="hljs-string">r&quot;[%s]+&quot;</span> % punctuation, <span class="hljs-string">&quot;&quot;</span>, file_content)<br>    <span class="hljs-comment"># 去除特殊英文标点</span><br>    file_content = re.sub(<span class="hljs-string">r&quot;[%s]+&quot;</span> % string.punctuation, <span class="hljs-string">&quot;&quot;</span>, file_content)<br>    <span class="hljs-comment"># if word &gt;= u&#x27;\u4e00&#x27; and word &lt;= u&#x27;\u9fa5&#x27;:#判断是否是汉字</span><br><br>    <span class="hljs-comment"># # 仅保留名词或特定POS (HMM)</span><br>    <span class="hljs-comment"># s = SnowNLP(file_content)</span><br>    <span class="hljs-comment"># pos_list = s.tags  # [(u&#x27;这个&#x27;, u&#x27;r&#x27;), (u&#x27;东西&#x27;, u&#x27;n&#x27;)]</span><br>    <span class="hljs-comment"># word_list = [</span><br>    <span class="hljs-comment">#     w for w, pos in pos_list if pos in [&quot;n&quot;, &quot;a&quot;, &quot;v&quot;, &quot;d&quot;]</span><br>    <span class="hljs-comment"># ]  # &#x27;NN&#x27;, &#x27;ADJ&#x27;, &#x27;VB&#x27;, &#x27;ADV&#x27;</span><br>    <span class="hljs-comment"># # 关键词 （TextRank算法）</span><br>    <span class="hljs-comment"># kw_list = s.keywords(3)</span><br>    <span class="hljs-comment"># # 文档摘要 （TextRank算法）</span><br>    <span class="hljs-comment"># summmary_list = s.summary(3)</span><br><br>    <span class="hljs-comment"># jieba分词</span><br>    word_list = jieba.cut(file_content)<br>    <span class="hljs-comment"># 去除停用词</span><br>    word_list = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> word_list <span class="hljs-keyword">if</span> w <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words]<br>segments.append(<span class="hljs-string">&quot; &quot;</span>.join(word_list))<br></code></pre></td></tr></table></figure><p>单词之间都已经被空格区分开了。下面我们需要做一项重要工作，叫做文本的<strong>向量化</strong>。</p><p>不要被这个名称吓跑。它的意思其实很简单。因为计算机不但不认识中文，甚至连英文也不认识，它只认得数字。我们需要做的，是把文章中的关键词转换为一个个特征（列），然后对每一篇文章数关键词出现个数。</p><h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p>我们希望获取到的词汇，既能保留文本的信息，同时又能反映它们的相对重要性。</p><p>特征选取的方式：</p><ol><li>构建词向量空间：统计文本词频，生成文本的词向量空间</li><li>权重策略——TF-IDF：使用TF-IDF发现特征词，并抽取为反映文档主题的特征</li></ol><p>经过上面的步骤之后，我们就可以把文本集转化成一个矩阵。</p><h3 id="TF-IDF向量化"><a href="#TF-IDF向量化" class="headerlink" title="TF-IDF向量化"></a>TF-IDF向量化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">vectorizer = TfidfVectorizer(stop_words=stop_words, sublinear_tf=<span class="hljs-literal">True</span>, max_df=<span class="hljs-number">0.5</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nvectorizing...&quot;</span>)<br>vector = vectorizer.fit_transform(segments)<br><span class="hljs-comment"># 查看结果</span><br><span class="hljs-comment"># print(vectorizer.vocabulary_)   # 词表索引</span><br><span class="hljs-comment"># print(vectorizer.idf_)   # 词表中每个词的IDF权重</span><br><span class="hljs-comment"># print(vector.toarray())  # 向量化结果 [[0. 0. 0.07097325 ... 0. 0. 0. ]...]</span><br></code></pre></td></tr></table></figure><h2 id="文本挖掘"><a href="#文本挖掘" class="headerlink" title="文本挖掘"></a>文本挖掘</h2><p>关于文本挖掘方面的相关知识，请参看<a href="http://www.woshipm.com/operate/413569.html">《数据运营|数据分析中，文本分析远比数值型分析重要！（上）》</a>、<a href="http://www.woshipm.com/operate/415186.html">《在运营中，为什么文本分析远比数值型分析重要？一个实际案例，五点分析（下）》</a>。</p><p>本文的文本挖掘部分主要涉及:<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs awk">高频词统计<span class="hljs-regexp">/关键词提取/</span>关键词云<br>    TF-IDF的关键词提取方法<br>文章标题聚类<br>文章内容聚类<br>文章内容LDA主题模型分析<br>词向量/关联词分析<br>ATM模型<br>词汇分散图<br>词聚类分析<br></code></pre></td></tr></table></figure></p><h3 id="用Python提取中文关键词"><a href="#用Python提取中文关键词" class="headerlink" title="用Python提取中文关键词"></a>用Python提取中文关键词</h3><blockquote><p>对于关键词提取，笔者没有采取词频统计的方法，因为词频统计的逻辑是：一个词在文章中出现的次数越多，则它就越重要。因而，笔者采用的是TF-IDF（termfrequency–inverse document frequency）的关键词提取方法：</p></blockquote><ul><li>关键字抽取步骤：</li></ul><p><img src="https://www.afenxi.com/wp-content/uploads/2019/06/1561412012.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># TextRank关键词提取</span><br><span class="hljs-built_in">print</span>(Fore.RED + <span class="hljs-string">&quot;关键词提取结果如下：&quot;</span>)<br>topk_list = textrank(<span class="hljs-string">&quot;\n&quot;</span>.join(segments), withWeight=<span class="hljs-literal">True</span>, topK=<span class="hljs-number">100</span>)<br>topk_keyword = []<br><span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;result/TextRank算法提取的 TOP100 关键词.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, <span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> keyword, weight <span class="hljs-keyword">in</span> topk_list:<br>        f.write(<span class="hljs-string">&quot;&#123;&#125;\t&#123;&#125;\n&quot;</span>.<span class="hljs-built_in">format</span>(keyword, weight))<br>        topk_keyword.append(keyword)<br>        <span class="hljs-built_in">print</span>(Fore.GREEN + <span class="hljs-string">&quot;%s\t%s&quot;</span> % (keyword, weight))<br>        <br><span class="hljs-comment"># 选取TOP100关键词来绘制关键词云</span><br>doWordCloud(<span class="hljs-string">&#x27; &#x27;</span>.join(topk_keyword), stop_words) <br></code></pre></td></tr></table></figure><p>下面是笔者利用jieba在经预处理后的、近70MB的语料中抽取出的TOP100关键词(略)</p><h3 id="统计性分析"><a href="#统计性分析" class="headerlink" title="统计性分析"></a>统计性分析</h3><p>只从文本中提取1000个最重要的特征关键词，然后停止</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># CountVectorizer统计词频</span><br>n_features = <span class="hljs-number">1000</span>  <span class="hljs-comment"># 提取1000个最重要的特征关键词</span><br>vectorizer = CountVectorizer(<br>    max_df=<span class="hljs-number">0.95</span>,<br>    min_df=<span class="hljs-number">2</span>,<br>    max_features=n_features,<br>    strip_accents=<span class="hljs-string">&quot;unicode&quot;</span>,<br>    stop_words=stop_words,<br>)<br>vector = vectorizer.fit_transform(segments)<br>vocab = vectorizer.vocabulary_<br>cipin = vector.toarray()  <span class="hljs-comment"># 词频统计 [[1 1 1 1 1 1 1 2]]</span><br></code></pre></td></tr></table></figure><h3 id="利用LDA进行主题抽取"><a href="#利用LDA进行主题抽取" class="headerlink" title="利用LDA进行主题抽取"></a>利用LDA进行主题抽取</h3><p>刚才针对关键词的分类较为粗略，且人为划分，难免有失偏颇，达不到全面的效果。因此，笔者采用LDA主题模型来发现该语料中的潜在主题。关于LDA主题模型的相关原理，请参看<a href="http://www.woshipm.com/data-analysis/706813.html">《【干货】用大数据文本挖掘，来洞察“共享单车”的行业现状及走势》</a>的第4部分。</p><p>LDA是一种典型的无监督（也就是说，我们事先不知道每段文本里面说的是啥，每个文本没有啥标签）、基于统计学习的词袋模型，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。主题模型通过分析文本中的词来发现文档中的主题、主题之间的联系方式和主题的发展，通过主题模型可以使我们组织和总结无法人工标注的海量电子文档。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">trainLDA</span>(<span class="hljs-params">vector</span>):<br>    <span class="hljs-comment"># 主题分析</span><br>    n_topics = [<span class="hljs-number">5</span>]<br>    perplexityLst = [<span class="hljs-number">1.0</span>]*<span class="hljs-built_in">len</span>(n_topics)<br><br>    <span class="hljs-comment">#训练LDA并打印训练时间</span><br>    lda_models = []<br>    <span class="hljs-keyword">for</span> idx, n_topic <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(n_topics):<br>        lda = LatentDirichletAllocation(n_components=n_topic,<br>                                        max_iter=<span class="hljs-number">100</span>,<br>                                        learning_method=<span class="hljs-string">&#x27;batch&#x27;</span>,    <span class="hljs-comment"># &#x27;batch&#x27; &#x27;online&#x27;</span><br>                                        evaluate_every=<span class="hljs-number">200</span>,<br>                                        learning_offset=<span class="hljs-number">50.0</span>,<br>                                        random_state=<span class="hljs-number">0</span>)<br>        t0 = time()<br>        lda.fit(vector)<br>        <span class="hljs-comment"># 收敛效果(perplexity)</span><br>        perplexityLst[idx] = lda.perplexity(vector)<br>        lda_models.append(lda)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;# of Topic: %d, &quot;</span> % n_topics[idx])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;done in %0.3fs, N_iter %d, &quot;</span> % ((time() - t0), lda.n_iter_))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;收敛效果 Perplexity Score (越小越好) %0.3f&quot;</span> % perplexityLst[idx])<br><br>    <span class="hljs-comment">#打印最佳模型</span><br>    best_index = perplexityLst.index(<span class="hljs-built_in">min</span>(perplexityLst))<br>    best_n_topic = n_topics[best_index]<br>    best_model = lda_models[best_index]<br>    <span class="hljs-built_in">print</span>(Fore.RED + <span class="hljs-string">&quot;Best # of Topic: &quot;</span>, best_n_topic)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br>    <span class="hljs-comment"># 定义以下的函数，把每个主题里面的前若干个关键词显示出来</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">print_top_words</span>(<span class="hljs-params">model, feature_names, n_top_words</span>):<br>        <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;result/主题簇.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, <span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            <span class="hljs-keyword">for</span> topic_idx, topic <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(model.components_):<br>                f.write(<span class="hljs-string">&quot;Topic #%d:\n&quot;</span> % topic_idx)<br>                f.write(<br>                    <span class="hljs-string">&quot;\n&quot;</span>.join(<br>                        [feature_names[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> topic.argsort()[: -n_top_words - <span class="hljs-number">1</span> : -<span class="hljs-number">1</span>]]<br>                    )<br>                )<br>                f.write(<span class="hljs-string">&quot;\n&quot;</span>)<br>                <span class="hljs-built_in">print</span>(Fore.RED + <span class="hljs-string">&quot;Topic #%d:&quot;</span> % topic_idx)<br>                <span class="hljs-built_in">print</span>(<br>                    Fore.GREEN<br>                    + <span class="hljs-string">&quot; &quot;</span>.join(<br>                        [feature_names[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> topic.argsort()[: -n_top_words - <span class="hljs-number">1</span> : -<span class="hljs-number">1</span>]]<br>                    )<br>                )<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n&quot;</span>)<br><br>    <span class="hljs-comment"># 打印主题建模的结果</span><br>    n_top_words = <span class="hljs-number">10</span>  <span class="hljs-comment"># 主题数量</span><br>    tf_feature_names = vectorizer.get_feature_names()<br>    print_top_words(lda, tf_feature_names, n_top_words)<br>    <span class="hljs-comment"># 将doc转换为话题分布的函数</span><br>    doc_topic_dist = lda.transform(vector)<br><br>trainLDA(vector)<br></code></pre></td></tr></table></figure><p>一般情况下，笔者将主题的数量设定为10个，经过数小时的运行，得到如下结果：</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs tap">Topic <span class="hljs-comment">#0:</span><br>学习 模型 使用 算法 方法 机器 可视化 神经网络 特征 处理 计算 系统 不同 数据库 训练 分类 基于 工具 一种 深度<br>Topic <span class="hljs-comment">#1:</span><br>这个 就是 可能 如果 他们 没有 自己 很多 什么 不是 但是 这样 因为 一些 时候 现在 用户 所以 非常 已经<br>Topic <span class="hljs-comment">#2:</span><br>企业 平台 服务 管理 互联网 公司 行业 数据分析 业务 用户 产品 金融 创新 客户 实现 系统 能力 产业 工作 价值<br>Topic <span class="hljs-comment">#3:</span><br>中国<span class="hljs-number"> 2016 </span>电子 增长<span class="hljs-number"> 10 </span>市场 城市<span class="hljs-number"> 2015 </span>关注 人口 检索<span class="hljs-number"> 30 </span>或者 其中 阅读 应当 美国 全国 同比 20<br>Topic <span class="hljs-comment">#4:</span><br>人工智能 学习 领域 智能 机器人 机器 人类 公司 深度 研究 未来 识别 已经 医疗 系统 计算机 目前 语音 百度 方面<br></code></pre></td></tr></table></figure><ul><li>利用LDA做了主题抽取,不论是5个还是10个主题，可能都不是最优的数量选择。你可以根据程序反馈的结果不断尝试。</li></ul><h3 id="ATM模型"><a href="#ATM模型" class="headerlink" title="ATM模型"></a>ATM模型</h3><p>ATM模型（author-topic model）也是“概率主题模型”家族的一员，是LDA主题模型（Latent Dirichlet Allocation ）的拓展，它能对某个语料库中作者的写作主题进行分析，找出某个作家的写作主题倾向，以及找到具有同样写作倾向的作家，它是一种新颖的主题探索方式。</p><h3 id="词向量-关联词分析"><a href="#词向量-关联词分析" class="headerlink" title="词向量/关联词分析"></a>词向量/关联词分析</h3><h3 id="词聚类"><a href="#词聚类" class="headerlink" title="词聚类"></a>词聚类</h3><h3 id="文档聚类"><a href="#文档聚类" class="headerlink" title="文档聚类"></a>文档聚类</h3><ul><li>参照博客《<a href="https://medium.com/@wshuyi/%E5%A6%82%E4%BD%95%E7%94%A8python%E4%BB%8E%E6%B5%B7%E9%87%8F%E6%96%87%E6%9C%AC%E6%8A%BD%E5%8F%96%E4%B8%BB%E9%A2%98-a802e8608d39">如何用Python从海量文本提取主题？</a>》，面对的是大量的文档，利用主题发现功能对文章聚类</li><li>参照博客《<a href="https://juejin.im/post/5caf63cb518825215d37c0e7">使用Gensim进行主题建模(一)</a></li></ul><p>每一条评论都围绕一个或几个核心主旨进行阐述，这个核心主旨就是评论的中心思想。基于主旨话题分析技术获取文本主旨，每一条评论可能包含多个主旨话题，每一个话题包含多个主题词，通过评论的隶属度确定评论所属的主旨话题，通过主题词的分布确定主旨的含义。评论话题的具体分析流程如下，此过程要依次进行词法分析、文本过滤、主旨话题分析：</p><p><img src="https://www.afenxi.com/wp-content/uploads/2019/06/1561412013-1.png" alt=""></p><p>接下来采用的是基于谱联合聚类算法（Spectral Co-clustering algorithm）的文档聚类，这部分的原理涉及到艰深的数学和算法知识，可能会引起小伙伴们的阅读不适感，如果是这样，请快速跳过，直接看后面的操作和结果。</p><p>先将待分析的文本经TF-IDF向量化构成了词频矩阵，然后使用Dhillon的谱联合聚类算法（Spectral Co-clustering algorithm）进行双重聚类（Biclusters）。所得到的“文档-词汇”双聚类（Biclusters）会把某些文档子集中的常用词汇聚集在一起，由若干个关键词构成某个主题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将聚类结果写入文件</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">writeClusterResult</span>(<span class="hljs-params">y</span>):<br>    result_dict = &#123;&#125;<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(y)):<br>        <span class="hljs-keyword">if</span> y[i] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> result_dict:<br>            result_dict[y[i]] = []<br>        result_dict[y[i]].append(os.listdir(path)[:<span class="hljs-number">15</span>][i].strip())<br><br>    <span class="hljs-comment"># 按value长度逆序排序，写入</span><br>    result_list = <span class="hljs-built_in">sorted</span>(result_dict.items(), key=<span class="hljs-keyword">lambda</span> d: <span class="hljs-built_in">len</span>(d[<span class="hljs-number">1</span>]), reverse=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;result/prediction_cluster.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> target:<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> result_list:<br>            target.write(<span class="hljs-string">&quot;&#123;&#125;\n&#123;&#125;\n\n&quot;</span>.<span class="hljs-built_in">format</span>(item[<span class="hljs-number">0</span>], <span class="hljs-string">&quot;\n&quot;</span>.join(item[<span class="hljs-number">1</span>])))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;聚类结果写入文件完成!!!&#x27;</span>)<br><br><span class="hljs-comment"># 文本聚类</span><br>n_clusters = <span class="hljs-number">5</span><br>kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=<span class="hljs-number">20000</span>, random_state=<span class="hljs-number">0</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;MiniBatchKMeans...&quot;</span>)<br>y_kmeans = kmeans.fit_predict(vector).tolist()<br>y_kmeans = np.array(y_kmeans)<br>writeClusterResult(y_kmeans)<br></code></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://medium.com/@wshuyi/%E5%A6%82%E4%BD%95%E7%94%A8python%E4%BB%8E%E6%B5%B7%E9%87%8F%E6%96%87%E6%9C%AC%E6%8A%BD%E5%8F%96%E4%B8%BB%E9%A2%98-a802e8608d39">如何用Python从海量文本抽取主题？</a></p><p><a href="https://www.jianshu.com/p/e4b24a734ccc">如何用Python做词云</a></p><p><a href="https://www.afenxi.com/65120.html">如何从 “用户评论”中挖掘业务价值</a></p><p><a href="http://www.woshipm.com/data-analysis/706813.html">【干货】用大数据文本挖掘，来洞察“共享单车”的行业现状及走势</a></p><p><a href="http://www.woshipm.com/data-analysis/873430.html">以虎嗅网4W+文章的文本挖掘为例，展现数据分析的一整套流程</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>正则表达式</title>
    <link href="/2019/09/04/2019-09-04-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <url>/2019/09/04/2019-09-04-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<blockquote><p> 过一遍《<a href="https://amitness.com/regex/">A Visual Guide to Regular Expression</a>》正则的基础知识，基本可以解决日常95%以上的问题了！</p></blockquote><h2 id="什么是正则表达式"><a href="#什么是正则表达式" class="headerlink" title="什么是正则表达式?"></a>什么是正则表达式?</h2><p>正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。</p><p>验证一个用户名的示例：</p><p><img src="https://github.com/ziishaned/learn-regex/raw/master/img/regexp-cn.png" alt=""></p><span id="more"></span><h2 id="Python-正则表达式-re"><a href="#Python-正则表达式-re" class="headerlink" title="Python 正则表达式 re"></a>Python 正则表达式 re</h2><blockquote><p>Tips: 一般在 pattern 的 “” 前面需要加上一个 r 用来表示这是正则表达式, 而不是普通字符串.</p></blockquote><p>re 模块的一般使用步骤如下：</p><ul><li><p>使用 compile 函数将正则表达式的字符串形式编译为一个 Pattern 对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pattern = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&#x27;\d+&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>通过 Pattern 对象提供的一系列方法对文本进行匹配查找，获得匹配结果（一个 Match 对象）</p></li><li><p>最后使用 Match 对象提供的属性和方法获得信息，根据需要进行其他的操作</p></li></ul><p><strong>re 模块提供了不少有用的函数，用以匹配字符串:</strong></p><p><a href="https://www.runoob.com/python/python-reg-expressions.html">Python 正则表达式 | 菜鸟教程</a></p><div class="table-container"><table><thead><tr><th>模块</th><th>描述</th><th></th></tr></thead><tbody><tr><td><code>match()</code></td><td># # 指定字符串区间<br />m = pattern.match(text, 3, 10) <br />&gt;&gt;&gt; m.group(0)   # ‘12’<br/>&gt;&gt;&gt; m.groups()   # <br/>&gt;&gt;&gt; m.span(0)    # (3, 5)</td><td>从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none</td></tr><tr><td><code>search()</code></td><td>m = pattern.search(text) <br/>m.group()</td><td>扫描整个字符串并返回第一个成功的匹配</td></tr><tr><td><code>findall()</code></td><td>result2 = pattern.findall(text, 0, 10)</td><td>在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表</td></tr><tr><td><code>finditer()</code></td><td>result_iter1 = pattern.finditer(‘hello 123456 789’)<br/>for m1 in result_iter1:   # m1 是 Match 对象<br/></td><td>在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回</td></tr><tr><td><code>split()</code></td><td>pattern.split(text)</td><td>按照能够匹配的子串将字符串分割后返回列表</td></tr><tr><td><code>sub()</code></td><td>pattern.sub(text, new_text)</td><td>替换字符串中的匹配项</td></tr></tbody></table></div><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="元字符"><a href="#元字符" class="headerlink" title="元字符"></a>元字符</h3><blockquote><p>正则表达式主要依赖于元字符.<br>元字符不代表他们本身的字面意思, 他们都有特殊的含义. 一些元字符写在方括号中的时候有一些特殊的意思. 以下是一些元字符的介绍:</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:center">元字符</th><th>描述</th></tr></thead><tbody><tr><td style="text-align:center"><code>.</code></td><td>句号匹配任意单个字符除了换行符.</td></tr><tr><td style="text-align:center"><code>[ ]</code></td><td>字符种类. 匹配方括号内的任意字符.</td></tr><tr><td style="text-align:center"><code>[^ ]</code></td><td>==否定==的字符种类. 匹配除了方括号里的任意字符</td></tr><tr><td style="text-align:center"><code>*</code></td><td>匹配==&gt;=0个==重复的在*号之前的字符.</td></tr><tr><td style="text-align:center"><code>(xyz)</code></td><td>字符集, 匹配与 xyz 完全相等的字符串.</td></tr><tr><td style="text-align:center">`</td><td>`</td><td>==或==运算符,匹配符号前或后的字符.</td></tr><tr><td style="text-align:center"><code>\</code></td><td>转义字符,用于匹配一些保留的字符<code>[ ] ( ) &#123; &#125; . * + ? ^ $</code></td></tr></tbody></table></div><h3 id="量词"><a href="#量词" class="headerlink" title="量词"></a>量词</h3><div class="table-container"><table><thead><tr><th style="text-align:center">元字符</th><th>描述</th></tr></thead><tbody><tr><td style="text-align:center"><code>?</code></td><td>匹配前面的字符0次或1次</td></tr><tr><td style="text-align:center"><code>*</code></td><td>匹配前面的字符0次或多次</td></tr><tr><td style="text-align:center"><code>+</code></td><td>匹配前面的字符1次或者多次</td></tr><tr><td style="text-align:center"><code>&#123;m&#125;</code></td><td>匹配前面表达式m次</td></tr><tr><td style="text-align:center"><code>&#123;m,&#125;</code></td><td>匹配前面表达式至少m次</td></tr><tr><td style="text-align:center"><code>&#123;,n&#125;</code></td><td>匹配前面的正则表达式最多n次</td></tr><tr><td style="text-align:center"><code>&#123;m,n&#125;</code></td><td>匹配前面的正则表达式至少m次，最多n次</td></tr></tbody></table></div><p>PS: 以上量词都是贪婪模式，会尽可能多的匹配，如果要改为非贪婪模式，通过在量词后面跟随一个<code>?</code>来实现(惰性匹配)</p><h3 id="断言与标记"><a href="#断言与标记" class="headerlink" title="断言与标记"></a>断言与标记</h3><blockquote><p>断言不会匹配任何文本，只是对断言所在的文本施加某些约束</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:center">字符</th><th>描述</th></tr></thead><tbody><tr><td style="text-align:center"><code>^</code></td><td>在起始处匹配</td></tr><tr><td style="text-align:center"><code>$</code></td><td>在结尾处匹配</td></tr><tr><td style="text-align:center"><code>()</code></td><td>表示==捕获分组==，()会把每个分组里的匹配的值保存起来</td></tr><tr><td style="text-align:center"><code>(?:str)</code></td><td>表示==非捕获分组==，和捕获分组唯一的区别在于，非捕获分组匹配的值不会保存起来</td></tr><tr><td style="text-align:center"><code>(?=str)</code></td><td>正前瞻，示例：<code>exp1(?=exp2)</code>，即exp1后面的内容要匹配exp2</td></tr><tr><td style="text-align:center"><code>(?!str)</code></td><td>负前瞻，示例：<code>exp1(?!exp2)</code> ，即exp1后面的内容不能匹配exp2</td></tr><tr><td style="text-align:center"><code>(?&lt;=str)</code></td><td>正回顾，示例：<code>(?&lt;=exp2)exp1</code>，即 exp1前面的内容要匹配exp2</td></tr><tr><td style="text-align:center"><code>(?&lt;!str)</code></td><td>负回顾，示例：<code>(?&lt;!exp2)exp1</code>，即 exp1前面的内容不能匹配exp2</td></tr></tbody></table></div><p>举例：</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs arcade"><span class="hljs-string">&quot;中国人&quot;</span>.<span class="hljs-built_in">replace</span>(<span class="hljs-regexp">/(?&lt;=中国)人/</span>, <span class="hljs-string">&quot;rr&quot;</span>) <span class="hljs-comment">// 匹配中国人中的人，将其替换为rr，结果为 中国rr</span><br><span class="hljs-string">&quot;法国人&quot;</span>.<span class="hljs-built_in">replace</span>(<span class="hljs-regexp">/(?&lt;=中国)人/</span>, <span class="hljs-string">&quot;rr&quot;</span>) <span class="hljs-comment">// 结果为 法国人，因为人前面不是中国，所以无法匹配到</span><br></code></pre></td></tr></table></figure><h3 id="条件匹配"><a href="#条件匹配" class="headerlink" title="条件匹配"></a>条件匹配</h3><div class="table-container"><table><thead><tr><th>字符</th><th>描述</th></tr></thead><tbody><tr><td>`(?(id)yes_exp</td><td>no_exp)`</td><td><code>id</code>里的子表达式如果匹配到内容，则匹配yes_exp，否则匹配no_exp</td></tr></tbody></table></div><h3 id="简写字符集"><a href="#简写字符集" class="headerlink" title="简写字符集"></a>简写字符集</h3><p>正则表达式提供一些常用的字符集简写. 如下:</p><div class="table-container"><table><thead><tr><th style="text-align:center">简写</th><th>描述</th></tr></thead><tbody><tr><td style="text-align:center"><code>.</code></td><td>除换行符外的所有字符</td></tr><tr><td style="text-align:center"><code>\w</code></td><td>匹配所有字母数字, 等同于 <code>[a-zA-Z0-9_]</code></td></tr><tr><td style="text-align:center"><code>\W</code></td><td>匹配所有非字母数字, 即符号, 等同于: <code>[^\w]</code></td></tr><tr><td style="text-align:center"><code>\d</code></td><td>匹配数字: <code>[0-9]</code></td></tr><tr><td style="text-align:center"><code>\D</code></td><td>匹配非数字: <code>[^\d]</code></td></tr><tr><td style="text-align:center"><code>\s</code></td><td>匹配所有空格字符, 等同于: <code>[\t\n\f\r\p&#123;Z&#125;]</code></td></tr><tr><td style="text-align:center"><code>\S</code></td><td>匹配所有非空格字符: <code>[^\s]</code></td></tr><tr><td style="text-align:center"><code>\f</code></td><td>匹配一个换页符</td></tr><tr><td style="text-align:center"><code>\n</code></td><td>匹配一个换行符</td></tr><tr><td style="text-align:center"><code>\r</code></td><td>匹配一个回车符</td></tr><tr><td style="text-align:center"><code>\t</code></td><td>匹配一个制表符</td></tr><tr><td style="text-align:center"><code>\v</code></td><td>匹配一个垂直制表符</td></tr><tr><td style="text-align:center"><code>\p</code></td><td>匹配 CR/LF (等同于 <code>\r\n</code>)，用来匹配 DOS 行终止符</td></tr></tbody></table></div><h3 id="正则表达式小抄"><a href="#正则表达式小抄" class="headerlink" title="正则表达式小抄"></a>正则表达式小抄</h3><p><img src="https://ningshixian.github.io/resources/images/regx.png" alt=""></p><h2 id="常用正则"><a href="#常用正则" class="headerlink" title="常用正则"></a>常用正则</h2><ul><li>匹配中文 <code>[\u4e00-\u9fa5]</code></li></ul><ul><li><p>匹配英文字母 <code>[a-zA-Z]</code></p></li><li><p>数字:[0-9]</p></li><li><p>匹配中文，英文字母和数字及下划线,同时判断输入长度：<code>[\u4e00-\u9fa5_a-zA-Z0-9_]&#123;4,10&#125;</code></p></li><li><p>只含有汉字、数字、字母、下划线，下划线位置不限：<code>^[a-zA-Z0-9_\u4e00-\u9fa5]+$</code></p></li><li><p>由数字、26个英文字母或者下划线组成的字符串 <code>^\w+$</code></p></li><li><p>不以某字符串开头</p>  <figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs axapta">^(?!<span class="hljs-built_in">str</span>).*<br></code></pre></td></tr></table></figure></li><li><p>不以某字符串结尾</p>  <figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs axapta">.*(?&lt;!<span class="hljs-built_in">str</span>)$<br></code></pre></td></tr></table></figure></li><li><p>匹配时间</p>  <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 23:59</span><br><span class="hljs-attribute">regex</span> = /^([<span class="hljs-number">01</span>][<span class="hljs-number">0</span>-<span class="hljs-number">9</span>]|[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>-<span class="hljs-number">3</span>]):[<span class="hljs-number">0</span>-<span class="hljs-number">5</span>][<span class="hljs-number">0</span>-<span class="hljs-number">9</span>]$/;<br></code></pre></td></tr></table></figure></li><li><p>匹配日期</p>  <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache">    <span class="hljs-comment"># yyyy-mm-dd</span><br><span class="hljs-attribute">var</span> regex = /^[<span class="hljs-number">0</span>-<span class="hljs-number">9</span>]&#123;<span class="hljs-number">4</span>&#125;-(<span class="hljs-number">0</span>[<span class="hljs-number">1</span>-<span class="hljs-number">9</span>]|<span class="hljs-number">1</span>[<span class="hljs-number">0</span>-<span class="hljs-number">2</span>])-(<span class="hljs-number">0</span>[<span class="hljs-number">1</span>-<span class="hljs-number">9</span>]|[<span class="hljs-number">12</span>][<span class="hljs-number">0</span>-<span class="hljs-number">9</span>]|<span class="hljs-number">3</span>[<span class="hljs-number">01</span>])$/;<br></code></pre></td></tr></table></figure></li><li><p>匹配邮箱地址</p>  <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">email_pattern</span> = &#x27;^[*#\u4e00-\u9fa5 a-zA-Z0-<span class="hljs-number">9</span>_.-]+@[a-zA-Z0-<span class="hljs-number">9</span>-]+(\.[a-zA-Z0-<span class="hljs-number">9</span>-]+)*\.[a-zA-Z0-<span class="hljs-number">9</span>]&#123;<span class="hljs-number">2</span>,<span class="hljs-number">6</span>&#125;$&#x27;<br></code></pre></td></tr></table></figure></li><li><p>匹配电话号码</p>  <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">cellphone_pattern</span> = r&#x27;<span class="hljs-number">1</span>\d&#123;<span class="hljs-number">10</span>&#125;&#x27;<br><span class="hljs-attribute">cellphone_pattern</span> = &#x27;^((<span class="hljs-number">13</span>[<span class="hljs-number">0</span>-<span class="hljs-number">9</span>])|(<span class="hljs-number">14</span>[<span class="hljs-number">0</span>-<span class="hljs-number">9</span>])|(<span class="hljs-number">15</span>[<span class="hljs-number">0</span>-<span class="hljs-number">9</span>])|(<span class="hljs-number">17</span>[<span class="hljs-number">0</span>-<span class="hljs-number">9</span>])|(<span class="hljs-number">18</span>[<span class="hljs-number">0</span>-<span class="hljs-number">9</span>]))\d&#123;<span class="hljs-number">8</span>&#125;$&#x27;<br></code></pre></td></tr></table></figure></li><li><p>匹配身份证号</p>  <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">IDCards_pattern</span> = r&#x27;^([<span class="hljs-number">1</span>-<span class="hljs-number">9</span>]\d&#123;<span class="hljs-number">5</span>&#125;[<span class="hljs-number">12</span>]\d&#123;<span class="hljs-number">3</span>&#125;(<span class="hljs-number">0</span>[<span class="hljs-number">1</span>-<span class="hljs-number">9</span>]|<span class="hljs-number">1</span>[<span class="hljs-number">012</span>])(<span class="hljs-number">0</span>[<span class="hljs-number">1</span>-<span class="hljs-number">9</span>]|[<span class="hljs-number">12</span>][<span class="hljs-number">0</span>-<span class="hljs-number">9</span>]|<span class="hljs-number">3</span>[<span class="hljs-number">01</span>])\d&#123;<span class="hljs-number">3</span>&#125;[<span class="hljs-number">0</span>-<span class="hljs-number">9</span>xX])$&#x27;<br></code></pre></td></tr></table></figure></li><li><p>匹配网页链接</p>  <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">pattern = <span class="hljs-string">r&quot;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|[a-zA-Z]+.\w+\.+[a-zA-Z0-9\/_]+&quot;</span><br></code></pre></td></tr></table></figure></li><li><p>匹配图片链接</p>  <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">pattern = <span class="hljs-string">r&quot;(&lt;img src=(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*,])+)&quot;</span><br></code></pre></td></tr></table></figure></li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><p><a href="https://github.com/ziishaned/learn-regex/blob/master/translations/README-cn.md">正则表达式教程</a></p></li><li><p><a href="https://blog.csdn.net/qq_28633249/article/details/77686976">史上最全的正则表达式-匹配中英文、字母和数字</a></p></li><li><p><a href="https://morvanzhou.github.io/tutorials/python-basic/basic/13-10-regular-expression/">莫烦-正则表达式</a></p></li><li><p><a href="https://blog.csdn.net/csm0912/article/details/81206848">正则表达式中?=和?:和?!的理解</a></p></li><li><p><a href="http://funhacks.net/2016/12/27/regular_expression/">Python 正则表达式 re 模块</a></p></li></ul><ul><li><a href="https://segmentfault.com/a/1190000019266662">正则表达式 - 字符匹配不以某字段开头或者结尾</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VSCode IDE使用指南</title>
    <link href="/2019/08/30/2019-08-30-VSCode%20IDE%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2019/08/30/2019-08-30-VSCode%20IDE%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<p>Visual Studio Code（简称VS Code）是一个由微软开发，同时支持Windows 、 Linux和macOS等操作系统且开放源代码的代码编辑器</p><ul><li>如何配置VSCode</li><li>插件推荐</li><li>利用VS Code进行远程开发</li></ul><h2 id="如何配置VSCode"><a href="#如何配置VSCode" class="headerlink" title="如何配置VSCode"></a>如何配置VSCode</h2><p>Command/Ctrl + ,：就进入了用户配置。在这里是 ui 化的配置，也可以进入 JSON 来配置。 </p><p>Command + Shift + P：搜索 JSON ，可以进入 JSON 来实现配置的变更。</p><p>在工程目录里，同样的操作会多一个选项，则是打开工作区配置。当你打开的一瞬间，会在你的工作区建立一个 .vscode 文件夹，此处的配置会覆盖用户配置。一般都是工程专属的内容，比如 python的虚拟环境等。用户级的功能和插件的功能都是通过这个 JSON 文件去实现的。建议在安装插件之前把默认配置可以看一看。</p><h2 id="插件推荐"><a href="#插件推荐" class="headerlink" title="插件推荐"></a>插件推荐</h2><p>我最喜欢的是它的插件能力，几乎想要啥功能都能找到插件支持，应该不用我安利你们都会喜欢的。工欲善其事必先利其器，开发前先把所以提升效率的利器搭好会让今后慢慢的编程长路舒服很多！</p><h3 id="一、主题及图标"><a href="#一、主题及图标" class="headerlink" title="一、主题及图标"></a>一、主题及图标</h3><ul><li><a href="https://marketplace.visualstudio.com/items?itemName=PKief.material-icon-theme">Material Icon Theme</a>：扁平化的主题<a href="https://so.csdn.net/so/search?q=图标库&amp;spm=1001.2101.3001.7020">图标库</a></li><li><a href="https://marketplace.visualstudio.com/items?itemName=vscode-icons-team.vscode-icons">vscode-icons</a>：VSCode官方出品的图标库</li></ul><h3 id="二、功能强化"><a href="#二、功能强化" class="headerlink" title="二、功能强化"></a>二、功能强化</h3><ul><li><a href="https://marketplace.visualstudio.com/items?itemName=WakaTime.vscode-wakatime">wakatime</a>：编程时间及行为跟踪统计（需注册）</li><li><a href="https://marketplace.visualstudio.com/items?itemName=pnp.polacode">Polacode</a>：代码截图</li><li><a href="https://link.zhihu.com/?target=https%3A//marketplace.visualstudio.com/items%3FitemName%3Dgo-home.go-home">go-home</a>：倒计时还有多久下班。默认是早 9 晚 6</li><li>韭菜盒子：实时股票信息</li><li>filesize：一款在左下角显示文件大小的插件 </li><li>Code Runner：右键里有在终端中直接运行Python文件，并对输出给与颜色标注，方便观看 </li><li><a href="https://marketplace.visualstudio.com/items?itemName=MS-CEINTL.vscode-language-pack-zh-hans">Chinese (Simplified) Language Pack for Visual Studio Code</a>：中文语言包</li><li>Settings Sync：同步你的vscode设置，包括setting文件，插件等（built-in）</li></ul><p>右键 Settings Sync 点击扩展设置，复制GitHub gist ID，再 Command + p搜索&gt;sync 再选择 Sync: Upload / Update Settings 会自动上传你当前的插件及设置信息；最后在新的vs code下粘贴GitHub gist ID，再 Command + p搜索&gt;sync 再选择 Sync: Download Settings 自动下载插件和回复之前的配置！！ </p><h3 id="三、Git-集成插件"><a href="#三、Git-集成插件" class="headerlink" title="三、Git 集成插件"></a>三、Git 集成插件</h3><ul><li>GitLens：可以快速查看代码的编写者、轻松导航和探索 Git 存储库、通过丰富的可视化效果和强大的比较命令获取有效信息，以及执行更多操作</li></ul><h3 id="四、数据库"><a href="#四、数据库" class="headerlink" title="四、数据库"></a>四、数据库</h3><ul><li><a href="https://marketplace.visualstudio.com/items?itemName=cweijan.vscode-mysql-client2">MySQL</a>：支持 MySQL/<a href="https://so.csdn.net/so/search?q=MariaDB&amp;spm=1001.2101.3001.7020">MariaDB</a>, Microsoft SQL Server, PostgreSQL, Redis, and ElasticSearch</li></ul><h3 id="五、编程美化"><a href="#五、编程美化" class="headerlink" title="五、编程美化"></a>五、编程美化</h3><ul><li><p>Markdown all in one：在 VSCode 里编写 Markdown，支持预览</p></li><li><p>indent-rainbow：多个缩进以不同颜色进行高亮显示， 提示代码缩进是否到位 </p></li></ul><h3 id="六、开发效率"><a href="#六、开发效率" class="headerlink" title="六、开发效率"></a>六、开发效率</h3><ul><li><a href="https://marketplace.visualstudio.com/items?itemName=EditorConfig.EditorConfig">EditorConfig for VS Code</a>：代码风格统一</li><li><a href="https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode">Prettier - Code formatter</a>：自动格式化代码</li><li><a href="https://marketplace.visualstudio.com/items?itemName=andyyaldoo.vscode-json">vscode-json</a>：操作美化 json</li><li>LeetCode with labuladong：刷题利器</li><li>Sourcery：它会自动审查和实时重构你的 Python 代码</li><li>Thunder Client：Rest API 客户端，可以卸载臃肿的 PostMan</li><li>autoDocstring：自动创建函数文档格式 docstring，需手动修改</li><li>Mintlify Doc Writer：利用人工智能技术帮你自动写文档</li><li>Python：让VS Code支持python编程，提供了 IntelliSense (Pylance)，linting，调试，代码导航，代码格式化、重构、变量资源管理器等功能</li><li>Pylance：微软推出的代码补全工具。具有Docstrings、参数提示、自动导入、代码补全、代码诊断、引用和跳转、代码大纲、类型检查、支持多工作区、带有类型信息的签名帮助、兼容 IntelliCode 和 Jupyter notebook。需要在settings.json 里进行设置  “python.languageServer”: “Pylance”  , 才能使用</li></ul><p>命令面板→首选项: 打开用户设置→搜索language server→选择 Pylance</p><ul><li>Python Indent：让 VSCode 更好地缩进 Python 代码，代码会更容易观看和书写。</li><li>Python Snippets：在你编写Python代码的时候自动补全代码片段，提高编写效率。如内置字符串、列表、集合、元组和字典片段和代码示例。</li><li>Jupyter: Jupyter是 Jupyter Notebook 的支持插件</li></ul><h3 id="七、AI辅助编程工具"><a href="#七、AI辅助编程工具" class="headerlink" title="七、AI辅助编程工具"></a>七、AI辅助编程工具</h3><ul><li>微软 IntelliCode：一种在 GitHub 项目样本上训练的实验性 AI 编码辅助工具。 您的完成列表由 IntelliCode 确定优先级，以便您最有可能使用的项目位于最前面。</li><li>GitHub Copilot：实时代码补全、注释生成（$10/per month）</li><li>CodeGeeX：中国原创的AI辅助编程工具，完全免费，开源开放。拥有代码生成、代码翻译、代码逐行添加注释功能√</li><li>Tabnine：基本是全语言的，但是根本不看语法，只靠推算，即装即用（cpu 消耗大，笨重）</li><li>Kite：实时Python代码补全，需要下载模型，Kite 的下载量要远大于 Tabnine ！</li><li>CodeGPT：提问、重构代码、归档代码、查找代码中的问题</li><li>Amazon <a href="https://aws.amazon.com/cn/codewhisperer/">CodeWhisperer</a>：亚马逊云科技推出实时AI编程助手Amazon CodeWhisperer Individual，供所有开发人员免费使用，帮助开发者基于注释生成代码，追踪开源参考，扫描查找漏洞。此外，还可以帮助开发者创建代码胜任如下场景，比如常规、耗时的无差别任务，或是在使用不熟悉的API或SDK时构建示例代码等。</li></ul><h2 id="利用VS-Code进行远程开发"><a href="#利用VS-Code进行远程开发" class="headerlink" title="利用VS Code进行远程开发"></a>利用VS Code进行远程开发</h2><p>微软在 PyCon 2019 大会上发布了VS Code Remote ，从 1.35.0 版本正式提供可以在本地编辑远程开发环境的文件的功能。VS Code 使用远程开发插件，不论在哪里，只要有电脑都能方便的打开云端开发环境，非常的方便，是一个大幅提升生产力的好工具！</p><h3 id="远程开发配置"><a href="#远程开发配置" class="headerlink" title="远程开发配置"></a>远程开发配置</h3><p><strong>配置SSH环境变量</strong></p><p>由于Git自带SSH客户端程序，如果你还没装Git的话，这里要先安装 Git，所以配置 Git 的 bin目录到环境变量的 PATH 变量下，这样VS Code连接的时候就能找到它了。</p><p><strong>安装远程开发插件</strong></p><p>要能连上远程主机，首先我们需要下载VS Code远程开发插件，VS Code其实是提供了一个远程开发插件包，包括：</p><ul><li>Remote - SSH - 通过使用 SSH 链接虚拟或者实体Linux主机。</li><li>Remote - Containers – 连接 Docker 开发容器。</li><li>Remote - WSL - 连接 Windows Subsystem for Linux （Linux子系统）</li></ul><p>打开软件的扩展界面，搜索 <code>Remote</code> 开头的插件，也能看到这三个的不同远程开发插件，我们这里连接的是云主机，选择安装 Remote - SSH 插件安装即可。</p><h3 id="配置远程连接"><a href="#配置远程连接" class="headerlink" title="配置远程连接"></a>配置远程连接</h3><p>1、首先点侧边栏的「远程资源管理器」之后点击「设置按钮」，进入远程机器配置界面。<br><img src="/2019/08/30/2019-08-30-VSCode%20IDE%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/640-20230424174322629" class="" title="img"> </p><p> 2、修改 ssh 配置文件，用于登录远程机器，各项含义在图中有说明。<br><img src="https://mmbiz.qpic.cn/mmbiz_png/ceNmtYOhbMRCYaSbicLwbVopKvMTVnRI2h8icWGwx8E3kYk4ZhWDWbWntEOsgniacbiae3olepAWhyUzeDeliaYfdLg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"> </p><p>3、点击连接，登录远程服务器，需要输入几次远程服务器的密码（后面会教你怎么免密登录），输入确认即可。第一次连接会做VS Code Server的初始化工作比较慢，耐心等待。<br><img src="/2019/08/30/2019-08-30-VSCode%20IDE%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/640-20230424174322615" class="" title="img"> </p><p>4、登录成功，即可像操作本地环境一样，在VS Code客户端操作远程云主机上的文件。注意，下图中的「打开文件夹」已经是远端机器上的目录结构了。<br><img src="/2019/08/30/2019-08-30-VSCode%20IDE%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/640-20230424174322667" class="" title="img"> </p><p>5、给远程VS Code 安装插件。安装的插件是在云服务器的VS Code上，对本机的VS Code没有影响，插件在远端提供功能，比如代码审查、自动补齐等等，而这所有的一切就像在本地操作一样，对文件的更改也是直接操作的云主机上的文件，丝滑连接。 </p><p>6、代码编辑与远程终端调试。打开文件编辑的是云服务器的文件，同时可以打开云服务终端，直接在终端操作编译或者查看云服务器信息。<br><img src="/2019/08/30/2019-08-30-VSCode%20IDE%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/640-20230424174322612" class="" title="img"></p><h3 id="SSH免密登录配置"><a href="#SSH免密登录配置" class="headerlink" title="SSH免密登录配置"></a>SSH免密登录配置</h3><p>按照上面的配置步骤，每次连接到远程服务器，都需要输入服务器登录密码很麻烦，可以配置SSH免密登录，免去每次输入密码的烦恼，具体操作步骤如下：</p><ol><li>打开win cmd终端，输入 ssh-keygen -t rsa 生成秘钥对 </li><li>打开生成的秘钥保存路径，拷贝 <code>~/.ssh/id_rsa.pub</code> 内容，添加到到云服务器的 <code>~/.ssh/authorized_keys</code> 文件后面。 </li><li>尝试再次连接，不用输密码了，enjoy！ </li></ol><h3 id="选择远程服务器的虚拟环境"><a href="#选择远程服务器的虚拟环境" class="headerlink" title="选择远程服务器的虚拟环境"></a><a href="https://blog.csdn.net/ssunshining/article/details/123583519">选择远程服务器的虚拟环境</a></h3><ol><li>Ctrl + Shift + P 打开命令面板，</li><li>输入 select…，</li><li>选择需要使用的虚拟环境即可</li></ol><img src="/2019/08/30/2019-08-30-VSCode%20IDE%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/1679288275416-2cd9cbcd-9daa-4e04-b731-7a7d07ddebc7.png" class="" title="img"><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p> <a href="https://zhuanlan.zhihu.com/p/73452541">工具篇-vscode效率提升插件</a> </p><p> <a href="https://www.leiphone.com/news/201911/ESeY1uZvYRdP5V43.html">新版 Kite：实时补全代码，Python 之父都发声力挺！</a> </p><p> <a href="https://mp.weixin.qq.com/s?__biz=MzUyNjQxNjYyMg==&amp;mid=2247494803&amp;idx=3&amp;sn=03608ca1e275414bba356780ebaa86a8&amp;chksm=fa0d8312cd7a0a041d550e20e9b1e9c2be1e87ba23a71a15e73c9eec5f999b1b9cee5782efb8&amp;scene=126&amp;sessionid=1604029083&amp;key=a6a963ce8dc9af21e4278ddbc10ce6143fb180b87c87924bd6cc36ac923408a2c26d85e0e7ba3e2857137a15559d2599e10cc8d7d51a6c5fc2e735397d01d68ccf310f661fd42bb4c02bd91705c76bebcafa63c44341883589c59f26a988bb9c25db32e946bb4f7ee273d5327e572969a3c4325bfa607682c6816ea92b10def7&amp;ascene=1&amp;uin=MjM2MDA1NjcyMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A6uPvof82tWGpt6NxM5SR%2F4%3D&amp;pass_ticket=r5dszQk2lRNNteX%2BZmX6%2B1wF2g3D57o1FKmEWEXG6IdPb9qLFiUO6rf4TKQjjajJ&amp;wx_header=0">利用VS Code进行远程开发，就问你香不香？</a> </p>]]></content>
    
    
    
    <tags>
      
      <tag>远程开发</tag>
      
      <tag>VSCode</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python爬虫笔记</title>
    <link href="/2019/08/13/2019-08-13-python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/"/>
    <url>/2019/08/13/2019-08-13-python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="python爬虫笔记"><a href="#python爬虫笔记" class="headerlink" title="python爬虫笔记"></a>python爬虫笔记</h1><ol><li>爬虫预备知识</li><li>网络请求模块 requests</li><li>JSON数据提取</li><li>re正则取数据</li><li>aiohttp异步爬虫</li></ol><span id="more"></span><h2 id="01-爬虫预备知识"><a href="#01-爬虫预备知识" class="headerlink" title="01. 爬虫预备知识"></a>01. 爬虫预备知识</h2><h4 id="爬虫定义"><a href="#爬虫定义" class="headerlink" title="爬虫定义"></a>爬虫定义</h4><p>网络爬虫（又被称为网页蜘蛛，网络机器人）就是模拟浏览器发送网络请求，接收请求响应，一种按照一定的规则，自动地抓取互联网信息的程序。</p><h4 id="爬虫流程"><a href="#爬虫流程" class="headerlink" title="爬虫流程"></a>爬虫流程</h4><ol><li>向起始url发送请求，并获取响应</li><li>对响应进行提取</li><li>如果提取url，则继续发送请求获取响应</li><li>如果提取数据，则将数据进行保存</li></ol><h4 id="网络模型对应关系"><a href="#网络模型对应关系" class="headerlink" title="网络模型对应关系"></a>网络模型对应关系</h4><ol><li>HTTP、RTSP、FTP ———-&gt; 应用层</li><li>TCP、UDP ———-&gt; 传输层</li><li>IP ———-&gt; 网络层</li><li>数据链路 ———-&gt; 数据链路层</li><li>物理介质 ———-&gt; 物理层</li></ol><h4 id="url-地址格式"><a href="#url-地址格式" class="headerlink" title="url 地址格式"></a>url 地址格式</h4><ol><li>scheme：协议（例如：http, https, ftp）</li><li>host：服务器的 IP 地址或者域名</li><li>port：服务器的端口（如果是走协议默认端口，缺省端口80）</li><li>path：访问资源的路径</li><li>query-string：参数，发送给 http 服务器的数据</li><li>anchor：锚（跳转到网页的指定锚点位置）</li></ol><h4 id="HTTP-请求"><a href="#HTTP-请求" class="headerlink" title="HTTP 请求"></a>HTTP 请求</h4><ul><li><strong>请求方式</strong></li></ul><div class="table-container"><table><thead><tr><th>请求方式</th><th>描述</th></tr></thead><tbody><tr><td>GET</td><td>请求指定的页面信息，并返回实体主体。</td></tr><tr><td>HEAD</td><td>类似于 get 请求，只不过返回的响应中没有具体的内容，用于获取报头</td></tr><tr><td>POST</td><td>向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。</td></tr><tr><td>PUT</td><td>从客户端向服务器传送的数据取代指定的文档的内容</td></tr><tr><td>DELETE</td><td>请求服务器删除指定的页面。</td></tr><tr><td>CONNECT</td><td>HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。</td></tr><tr><td>OPTIONS</td><td>允许客户端查看服务器的性能。</td></tr><tr><td>TRACE</td><td>回显服务器收到的请求，主要用于测试或诊断。</td></tr></tbody></table></div><ul><li><strong>常见请求头</strong></li></ul><div class="table-container"><table><thead><tr><th>请求头</th><th>作用</th></tr></thead><tbody><tr><td><strong>Cookie</strong></td><td>Cookie</td></tr><tr><td><strong>User-Agent</strong></td><td>浏览器名称</td></tr><tr><td><strong>Referer</strong></td><td>页面跳转处</td></tr><tr><td>Host</td><td>主机和端口号</td></tr><tr><td>Connection</td><td>链接类型</td></tr><tr><td>Upgrade-Insecure-Requests</td><td>升级为 HTTPS 请求</td></tr><tr><td>Accept</td><td>传输文件类型</td></tr><tr><td>Accept-Encoding</td><td>文件编解码格式</td></tr><tr><td>x-requested-with : XMLHttpRequest</td><td>ajax 请求</td></tr></tbody></table></div><ul><li>模拟请求时，先在headers中加入User-Agent,如果还不可以请求再尝试加入Referer,还无法访问，在尝试加入Cookie,在最后可以尝试加入Host.</li></ul><h4 id="HTTP-响应"><a href="#HTTP-响应" class="headerlink" title="HTTP 响应"></a>HTTP 响应</h4><p><strong>HTTP 状态码</strong></p><p>当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含 HTTP 状态码的信息头（server header）用以响应浏览器的请求。</p><p>HTTP 状态码的英文为 HTTP Status Code。HTTP 状态码共分为 5 种类型</p><div class="table-container"><table><thead><tr><th>分类</th><th>分类描述</th></tr></thead><tbody><tr><td>1**</td><td>信息，服务器收到请求，需要请求者继续执行操作</td></tr><tr><td>2**</td><td>成功，操作被成功接收并处理</td></tr><tr><td>3**</td><td>重定向，需要进一步的操作以完成请求</td></tr><tr><td>4**</td><td>客户端错误，请求包含语法错误或无法完成请求</td></tr><tr><td>5**</td><td>服务器错误，服务器在处理请求的过程中发生了错误</td></tr></tbody></table></div><p>常见的 HTTP 状态码：</p><ul><li>200 - 请求成功</li><li>301 - 资源（网页等）被永久转移到其它 URL</li><li>404 - 请求的资源（网页等）不存在</li><li>500 - 内部服务器错误</li></ul><h2 id="02-网络请求模块requests"><a href="#02-网络请求模块requests" class="headerlink" title="02. 网络请求模块requests"></a><a href="https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/网络请求模块的使用.md">02. 网络请求模块requests</a></h2><p><code>requests</code> 模块是可以模仿浏览器发送请求获取响应，能够自动帮助我们解压网页内容。</p><p><code>requests</code>模块能够自动帮助我们解压网页内容</p><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fetch</span>(<span class="hljs-params">url</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    url: 要请求的网站url</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    headers = &#123;<span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&#x27;</span>&#125;  <span class="hljs-comment"># 请求时携带的header</span><br>    resp = requests.get(url, headers=headers, verify=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># verify忽略https的ssl验证</span><br>    resp.encoding = resp.apparent_encoding  <span class="hljs-comment"># 猜测编码，防乱码</span><br>    <span class="hljs-keyword">return</span> resp.text<br><br><br>fetch(<span class="hljs-string">&#x27;https://www.baidu.com&#x27;</span>)<br></code></pre></td></tr></table></figure><p>response 常用属性：</p><ul><li>response.text 返回响应内容，响应内容为 str 类型</li><li>respones.content 返回响应内容,响应内容为 bytes 类型</li><li>response.status_code 返回响应状态码</li><li>response.headers 返回响应头</li><li>response.cookies 返回响应的 RequestsCookieJar 对象</li></ul><h4 id="发送-GET-请求"><a href="#发送-GET-请求" class="headerlink" title="发送 GET 请求"></a>发送 GET 请求</h4><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-meta"># 导入模块</span><br>import requests<br><span class="hljs-meta"># 定义请求地址</span><br>url = <span class="hljs-string">&#x27;http://www.baidu.com/s&#x27;</span><br><span class="hljs-meta"># 定义自定义请求头</span><br>headers = &#123;<br>  <span class="hljs-string">&quot;User-Agent&quot;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&quot;</span><br>&#125;<br><span class="hljs-meta"># 定义 GET 请求参数</span><br><span class="hljs-keyword">params</span> = &#123;<br>  <span class="hljs-string">&quot;kw&quot;</span>:<span class="hljs-string">&quot;hello&quot;</span><br>&#125;<br><span class="hljs-meta"># 使用 GET 请求参数发送请求</span><br>response = requests.<span class="hljs-keyword">get</span>(url,headers=headers,<span class="hljs-keyword">params</span>=<span class="hljs-keyword">params</span>)<br><span class="hljs-meta"># 获取响应的 html 内容</span><br>html = response.text<br></code></pre></td></tr></table></figure><p>发送请求时 params 参数作为 GET 请求参数</p><h4 id="发送-POST-请求"><a href="#发送-POST-请求" class="headerlink" title="发送 POST 请求"></a>发送 POST 请求</h4><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 导入模块</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-meta"># 定义请求地址</span><br><span class="hljs-title">url</span> = &#x27;http://www.baidu.com&#x27;<br><span class="hljs-meta"># 定义自定义请求头</span><br><span class="hljs-title">headers</span> = &#123;<br>  <span class="hljs-string">&quot;User-Agent&quot;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&quot;</span><br>&#125;<br><span class="hljs-meta"># 定义post请求参数</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = &#123;</span><br><span class="hljs-class">  &quot;<span class="hljs-title">kw</span>&quot;:&quot;<span class="hljs-title">hello</span>&quot;</span><br><span class="hljs-class">&#125;</span><br><br><span class="hljs-meta"># 使用 POST 请求参数发送请求</span><br><span class="hljs-title">response</span> = requests.post(url,headers=headers,<span class="hljs-class"><span class="hljs-keyword">data</span>=<span class="hljs-keyword">data</span>)</span><br><span class="hljs-meta"># 获取响应的 html 内容</span><br><span class="hljs-title">html</span> = response.text<br></code></pre></td></tr></table></figure><p>发送请求时 data 参数作为 POST 请求参数</p><h4 id="使用代理服务器"><a href="#使用代理服务器" class="headerlink" title="使用代理服务器"></a>使用代理服务器</h4><p>作用：</p><ul><li>让服务器以为不是同一个客户端在请求</li><li>防止我们的真实地址被泄露，防止被追究</li></ul><p>代理分类：</p><ul><li>透明代理(Transparent Proxy)：透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以查到你是谁。</li><li>匿名代理(Anonymous Proxy)：匿名代理比透明代理进步了一点：别人只能知道你用了代理，无法知道你是谁。</li><li>混淆代理(Distorting Proxies)：与匿名代理相同，如果使用了混淆代理，别人还是能知道你在用代理，但是会得到一个假的IP地址，伪装的更逼真</li><li>高匿代理(Elite proxy或High Anonymity Proxy)：可以看出来，高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。</li></ul><blockquote><p>在使用的使用，毫无疑问使用高匿代理效果最好</p></blockquote><p>使用方式</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-comment"># 导入模块</span><br>import requests<br><span class="hljs-comment"># 定义请求地址</span><br>url = &#x27;http://www.baidu.com&#x27;<br><span class="hljs-comment"># 定义自定义请求头</span><br>headers = &#123;<br>  <span class="hljs-string">&quot;User-Agent&quot;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&quot;</span><br>&#125;<br><span class="hljs-comment"># 定义 代理服务器</span><br>proxies = &#123;<br>  <span class="hljs-string">&quot;http&quot;</span>:<span class="hljs-string">&quot;http://IP地址:端口号&quot;</span>,<br>  <span class="hljs-string">&quot;https&quot;</span>:<span class="hljs-string">&quot;https://IP地址:端口号&quot;</span><br>&#125;<br><span class="hljs-comment"># 使用 POST 请求参数发送请求</span><br>response = requests.get(url,headers=headers,proxies=proxies)<br><span class="hljs-comment"># 获取响应的 html 内容</span><br>html = response.text<br></code></pre></td></tr></table></figure><h4 id="发送请求携带-Cookies"><a href="#发送请求携带-Cookies" class="headerlink" title="发送请求携带 Cookies"></a>发送请求携带 Cookies</h4><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-comment"># 导入模块</span><br>import requests<br><span class="hljs-comment"># 定义请求地址</span><br>url = &#x27;http://www.baidu.com&#x27;<br><span class="hljs-comment"># 定义自定义请求头</span><br>headers = &#123;<br>  <span class="hljs-string">&quot;User-Agent&quot;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&quot;</span><br>  <span class="hljs-comment"># 方式一：直接在请求头中携带Cookie内容</span><br>  <span class="hljs-string">&quot;Cookie&quot;</span>: <span class="hljs-string">&quot;Cookie值&quot;</span><br>&#125;<br><span class="hljs-comment"># 方式二：定义 cookies 值</span><br>cookies = &#123;<br>  <span class="hljs-string">&quot;xx&quot;</span>:<span class="hljs-string">&quot;yy&quot;</span><br>&#125;<br><span class="hljs-comment"># 使用 POST 请求参数发送请求</span><br>response = requests.get(url,headers=headers,cookies=cookies)<br><span class="hljs-comment"># 获取响应的 html 内容</span><br>html = response.text<br></code></pre></td></tr></table></figure><h2 id="03-JSON数据提取"><a href="#03-JSON数据提取" class="headerlink" title="03. JSON数据提取"></a><a href="https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/JSON数据提取.md">03. JSON数据提取</a></h2><p>JSON (JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。</p><p>JSON相关的方法：</p><ul><li>json.loads        json字符串 转 Python数据类型</li><li>json.dumps     Python数据类型 转 json字符串</li><li>json.load          json文件 转 Python数据类型</li><li>json.dump       Python数据类型 转 json文件</li></ul><p>JSON数据提取代码示例：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta">#!/usr/bin/python3</span><br><span class="hljs-meta"># -*- coding: utf-8 -*-</span><br><span class="hljs-meta"># 导入模块</span><br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-meta"># json.loads json字符串 转 Python数据类型</span><br><span class="hljs-title">json_string</span> = &#x27;&#x27;&#x27;<br>&#123;<br>    <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;crise&quot;</span>,<br>    <span class="hljs-string">&quot;age&quot;</span>: <span class="hljs-number">18</span>,<br>    <span class="hljs-string">&quot;parents&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;monther&quot;</span>: <span class="hljs-string">&quot;妈妈&quot;</span>,<br>        <span class="hljs-string">&quot;father&quot;</span>: <span class="hljs-string">&quot;爸爸&quot;</span><br>    &#125;<br>&#125;<br>&#x27;&#x27;&#x27;<br><span class="hljs-title">print</span>(<span class="hljs-string">&quot;json_string数据类型：&quot;</span>,<span class="hljs-class"><span class="hljs-keyword">type</span>(<span class="hljs-title">json_string</span>))</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = json.loads(<span class="hljs-title">json_string</span>)</span><br><span class="hljs-title">print</span>(<span class="hljs-string">&quot;data数据类型：&quot;</span>,<span class="hljs-class"><span class="hljs-keyword">type</span>(<span class="hljs-title">data</span>))</span><br><br><span class="hljs-meta"># json.dumps Python数据类型 转 json字符串</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = &#123;</span><br><span class="hljs-class">    &quot;<span class="hljs-title">name</span>&quot;: &quot;<span class="hljs-title">crise</span>&quot;,</span><br><span class="hljs-class">    &quot;<span class="hljs-title">age</span>&quot;: 18,</span><br><span class="hljs-class">    &quot;<span class="hljs-title">parents</span>&quot;: &#123;</span><br><span class="hljs-class">        &quot;<span class="hljs-title">monther</span>&quot;: &quot;妈妈&quot;,</span><br><span class="hljs-class">        &quot;<span class="hljs-title">father</span>&quot;: &quot;爸爸&quot;</span><br><span class="hljs-class">    &#125;</span><br>&#125;<br><span class="hljs-title">print</span>(<span class="hljs-string">&quot;data数据类型：&quot;</span>,<span class="hljs-class"><span class="hljs-keyword">type</span>(<span class="hljs-title">data</span>))</span><br><span class="hljs-title">json_string</span> = json.dumps(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br><span class="hljs-title">print</span>(<span class="hljs-string">&quot;json_string数据类型：&quot;</span>,<span class="hljs-class"><span class="hljs-keyword">type</span>(<span class="hljs-title">json_string</span>))</span><br></code></pre></td></tr></table></figure><h2 id="04-re-正则表达式提取数据"><a href="#04-re-正则表达式提取数据" class="headerlink" title="04. re 正则表达式提取数据"></a><a href="https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/06 - 正则表达式 提取数据.md">04. re 正则表达式提取数据</a></h2><h6 id="正则表达式就是记录文本规则的代码"><a href="#正则表达式就是记录文本规则的代码" class="headerlink" title="正则表达式就是记录文本规则的代码"></a>正则表达式就是记录文本规则的代码</h6><p>re 模块常用方法：</p><ul><li><p><strong>match</strong> 开头匹配，只匹配一次</p></li><li><p><strong>search</strong> 全局匹配，只匹配一次</p></li><li><p><strong>findall</strong> 匹配所有符号条件的数据，返回是 结果列表</p><p>优点：使用简单，缺点：必须把所有数据搜索返回再返回（如果数据量小）</p></li><li><p><strong>finditer</strong> 迭代对象，迭代 Match 对象</p><p>优点：找到就返回，可以边找边返回（如果数据量大）</p></li></ul><h2 id="05-aiohttp异步爬虫"><a href="#05-aiohttp异步爬虫" class="headerlink" title="05. aiohttp异步爬虫"></a>05. aiohttp异步爬虫</h2><p>异步爬虫不同于多进程爬虫，它使用单线程(即仅创建一个事件循环，然后把所有任务添加到事件循环中)就能并发处理多任务。在轮询到某个任务后，当遇到耗时操作(如请求URL)时，挂起该任务并进行下一个任务，当之前被挂起的任务更新了状态(如获得了网页响应)，则被唤醒，程序继续从上次挂起的地方运行下去。极大的减少了中间不必要的等待时间。</p><p><a href="https://blog.csdn.net/weixin_30808693/article/details/99664117">参考1</a>    <a href="https://blog.csdn.net/sinat_33924041/article/details/87927406">参考2</a></p><h4 id="aiohttp-基本介绍"><a href="#aiohttp-基本介绍" class="headerlink" title="aiohttp 基本介绍"></a>aiohttp 基本介绍</h4><p><a href="https://docs.aiohttp.org/">aiohttp</a> 可以用来实现<strong>异步</strong>网页请求以及作为异步服务器。<a href="https://docs.aiohttp.org/">aiohttp</a> 是一个利用asyncio的库，性能会比同步的模块提高不少。</p><p><code>aiohttp</code>改写 requests 爬虫代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> asyncio<br><br><span class="hljs-keyword">import</span> aiohttp<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">fetch</span>(<span class="hljs-params">url</span>):<br>    headers = &#123;<span class="hljs-string">&#x27;content-type&#x27;</span>: <span class="hljs-string">&#x27;application/json&#x27;</span>&#125;<br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> aiohttp.ClientSession() <span class="hljs-keyword">as</span> session:<br>        <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> session.get(url, headers=headers, verify_ssl=<span class="hljs-literal">False</span>) <span class="hljs-keyword">as</span> resp:<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">await</span> resp.text(errors=<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    start_time = time.time()<br>    loop = asyncio.get_event_loop()<br>    get_future = asyncio.ensure_future(fetch(<span class="hljs-string">&#x27;https://www.baidu.com&#x27;</span>)) <span class="hljs-comment"># ?</span><br>    loop.run_until_complete(get_future)<br>    <span class="hljs-built_in">print</span>(get_future.result())<br>    loop.close()<br></code></pre></td></tr></table></figure><h4 id="批量url爬取"><a href="#批量url爬取" class="headerlink" title="批量url爬取"></a>批量url爬取</h4><p>有大量url时，每个都开一个会话显然是不好的，于是会写成这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> asyncio<br><br><span class="hljs-keyword">import</span> aiohttp<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">fetch</span>(<span class="hljs-params">session, id_, url</span>):<br>    headers = &#123;<span class="hljs-string">&#x27;content-type&#x27;</span>: <span class="hljs-string">&#x27;application/json&#x27;</span>&#125;<br>    data = json.dumps(&#123;<span class="hljs-string">&quot;XX&quot;</span>: <span class="hljs-string">&quot;YY&quot;</span>&#125;)<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> session.post(url, data=data, headers=headers, verify_ssl=<span class="hljs-literal">False</span>) <span class="hljs-keyword">as</span> resp:<br>            <span class="hljs-keyword">assert</span> resp.status == <span class="hljs-number">200</span><br>            <span class="hljs-built_in">print</span>(resp.charset)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-keyword">await</span> resp.text())<br>            <span class="hljs-built_in">print</span>(<span class="hljs-keyword">await</span> resp.read())<br>            <span class="hljs-comment"># print(await resp.json())# aiohttp.client_exceptions.ContentTypeError: 0, message=&#x27;Attempt to decode JSON with unexpected mimetype: text/plain;charset=utf-8&#x27;</span><br>            <span class="hljs-comment"># The correct way of use is as follows:</span><br>            res = <span class="hljs-keyword">await</span> resp.json(content_type=<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">return</span> res<br>    <span class="hljs-keyword">except</span> Exception:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;id_&#125;</span>, url: <span class="hljs-subst">&#123;url&#125;</span> error happened:&quot;</span>)<br>        <span class="hljs-built_in">print</span>(traceback.format_exc())<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">fetch_all</span>(<span class="hljs-params">urls</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    urls: list[(id_, url)]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> aiohttp.ClientSession() <span class="hljs-keyword">as</span> session:<br>        datas = <span class="hljs-keyword">await</span> asyncio.gather(*[fetch(session, id_, url) <span class="hljs-keyword">for</span> id_, url <span class="hljs-keyword">in</span> urls], return_exceptions=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">for</span> ind, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(urls):<br>            id_, url = data<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(datas[ind], Exception):<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;id_&#125;</span>, <span class="hljs-subst">&#123;url&#125;</span>: ERROR&quot;</span>)<br>        <span class="hljs-keyword">return</span> datas<br><br><br>urls = [(i, <span class="hljs-string">f&#x27;https://www.baidu.com/?tn=<span class="hljs-subst">&#123;i&#125;</span>&#x27;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>)]<br>loop = asyncio.new_event_loop()  <br><span class="hljs-comment"># loop = asyncio.get_event_loop()</span><br>task = loop.create_task(fetch_all(urls))<br>loop.run_until_complete(task)<br><span class="hljs-built_in">print</span>(task.result())<span class="hljs-comment"># save data</span><br>loop.close()<br></code></pre></td></tr></table></figure><h4 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h4><p>如果url太多，可能会报错<code>ValueError: too many file descriptors in select()</code>，根据<a href="https://stackoverflow.com/questions/47675410/python-asyncio-aiohttp-valueerror-too-many-file-descriptors-in-select-on-win">stackoverflow</a>所述，<code>aiohttp</code>默认设置中一次可以打开100个连接，而Windows一次最多只能打开64个<code>socket</code>，所以可以在<code>fetch_all</code>中添加一行：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">connector = aiohttp.<span class="hljs-constructor">TCPConnector(<span class="hljs-params">limit</span>=60)</span>  # <span class="hljs-number">60</span>小于<span class="hljs-number">64</span>。也可以改成其他数<br>async <span class="hljs-keyword">with</span> aiohttp.<span class="hljs-constructor">ClientSession(<span class="hljs-params">connector</span>=<span class="hljs-params">connector</span>)</span> <span class="hljs-keyword">as</span> session:<br>    ...<br></code></pre></td></tr></table></figure><p>此外，为防止爬的过客，可以使用信号量来控制：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">import</span> aiohttp<br><span class="hljs-keyword">import</span> asyncio<br><br>sem = asyncio.Semaphore(<span class="hljs-number">10</span>) <span class="hljs-comment"># 信号量，控制协程数，防止爬的过快</span><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_title</span>(<span class="hljs-params">url</span>):<br>    <span class="hljs-keyword">with</span>(<span class="hljs-keyword">await</span> sem):<br>        <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> aiohttp.ClientSession() <span class="hljs-keyword">as</span> session:<br>            <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> session.request(<span class="hljs-string">&#x27;GET&#x27;</span>, url) <span class="hljs-keyword">as</span> resp:  <span class="hljs-comment"># 提出请求</span><br>                html = <span class="hljs-keyword">await</span> resp.read()<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/%E7%88%AC%E8%99%AB%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.md">从零到一 Python 爬虫</a></p><p><a href="https://blog.csdn.net/SL_World/article/details/86633611">Python实战异步爬虫(协程)+分布式爬虫(多进程)</a></p><p><a href="https://baozoulin.gitbook.io/-python/jia-su-ni-de-pa-chong/42-jia-su-pa-866b3a-yi-bu-jia-zai-asyncio">4.2 加速爬虫: 异步加载 Asyncio</a></p><p><a href="[https://luca-notebook.readthedocs.io/zh_CN/latest/c01/%E7%94%A8aiohttp%E5%86%99%E7%88%AC%E8%99%AB.html](https://luca-notebook.readthedocs.io/zh_CN/latest/c01/用aiohttp写爬虫.html">用aiohttp写爬虫</a>)</p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>Python</tag>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>     Git免输入账号密码设置方法</title>
    <link href="/2019/08/12/2019-08-12-Git%E5%85%8D%E8%BE%93%E5%85%A5%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81%E8%AE%BE%E7%BD%AE%E6%96%B9%E6%B3%95/"/>
    <url>/2019/08/12/2019-08-12-Git%E5%85%8D%E8%BE%93%E5%85%A5%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81%E8%AE%BE%E7%BD%AE%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="Git免输入账号密码设置方法"><a href="#Git免输入账号密码设置方法" class="headerlink" title="Git免输入账号密码设置方法"></a>Git免输入账号密码设置方法</h2><p>执行 <code>git pull</code> 或者 <code>git push</code> 的时候都要输入用户名和密码，很是麻烦，如何免输入账号密码呢？</p><p>创建 git-credentials 文件并写入对应的 github 或者其他 git 服务器的用户名和密码，后面即可使用这些配置进行免密传输了，具体设置如下：</p><h3 id="Linux-Mac方法："><a href="#Linux-Mac方法：" class="headerlink" title="Linux/Mac方法："></a>Linux/Mac方法：</h3><ol><li><p>创建文件，进入文件，输入内容：  </p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> ~<br>touch <span class="hljs-string">.git-credentials</span><br>vim <span class="hljs-string">.git-credentials</span><br>https:<span class="hljs-string">//</span>&#123;username&#125;:&#123;password&#125;@github.com<br></code></pre></td></tr></table></figure></li><li><p>在终端下输入： </p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs llvm">git config --<span class="hljs-keyword">global</span> credential.helper <span class="hljs-keyword">store</span><br></code></pre></td></tr></table></figure><p>打开~/.gitconfig文件，会发现多了一项:  </p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[credential]</span><br><span class="hljs-attr">helper</span> = store<br></code></pre></td></tr></table></figure></li><li><p>其实直接执行 <code>git config --global credential.helper store</code>，然后调用一次 <code>git push xxx</code> 操作，输入一次账户和密码之后，git就会自动生成一个 .git-credentials 文件，然后将本次的账号密码信息保存进文件中。 </p></li></ol><h3 id="Windows方法："><a href="#Windows方法：" class="headerlink" title="Windows方法："></a>Windows方法：</h3><p>方法同上面，只是第一步创建 git-credentials 有点不同。</p><ol><li>借助git bash，进入创建的系统用户名目录（一般为C:\users\Administrator）</li><li>然后用touch创建文件 .git-credentials,</li><li>后面的操作同上</li></ol><p><strong>方法二</strong></p><p><strong>1 添加环境变量</strong></p><p>在windows中添加一个HOME环境变量，变量名:HOME,变量值：%USERPROFILE%</p><p><strong>2 创建git用户名和密码存储文件</strong></p><p>进入%HOME%目录，新建一个名为”_netrc”的文件，文件中内容格式如下：<br><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">machine &#123;git account <span class="hljs-type">name</span>&#125;.github.com<br><span class="hljs-keyword">login</span> your-usernmae<br><span class="hljs-keyword">password</span> your-<span class="hljs-keyword">password</span><br></code></pre></td></tr></table></figure></p><p>重新打开git bash即可，无需再输入用户名和密码</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/tq384998430/article/details/77770127">Git免输入账号密码设置方法</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git指令整理&amp;代码版本管理</title>
    <link href="/2019/08/12/2019-08-12-Git%E6%8C%87%E4%BB%A4%E4%BB%8B%E7%BB%8D&amp;%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/"/>
    <url>/2019/08/12/2019-08-12-Git%E6%8C%87%E4%BB%A4%E4%BB%8B%E7%BB%8D&amp;%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="Git指令整理-amp-代码版本管理"><a href="#Git指令整理-amp-代码版本管理" class="headerlink" title="Git指令整理&amp;代码版本管理"></a>Git指令整理&amp;代码版本管理</h1><p>Git可视化学习可以参考 <a href="https://dev.to/lydiahallie/cs-visualized-useful-git-commands-37p1">🌳🚀 CS Visualized: Useful Git Commands</a>，基本上看完就会了！！！</p><ol><li>Git是什么？</li><li>基本命令（init clone add commit push pull）</li><li>分支（brach checkout merge rebase）</li><li>如何使用Git进行团队协作</li><li>其他命令（前置拉取更新）<h2 id="1-Git是什么？"><a href="#1-Git是什么？" class="headerlink" title="1. Git是什么？"></a>1. Git是什么？</h2></li></ol><ul><li>Linus(Linux之父)花了两周时间自己用C写了一个分布式版本控制系统，这就是Git！ </li><li>Git是目前世界上最先进的分布式版本控制系统。 </li></ul><h3 id="工作原理-流程："><a href="#工作原理-流程：" class="headerlink" title="工作原理 / 流程："></a>工作原理 / 流程：</h3><p><img src="https://ningshixian.github.io/resources/images/Git%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%9B%BE.png#id=mgulq&amp;originHeight=263&amp;originWidth=853&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><ul><li>Workspace：工作区</li><li>Index / Stage：暂存区</li><li>Repository：仓库区（或本地仓库）</li><li>Remote：远程仓库</li></ul><h2 id="2-基本命令"><a href="#2-基本命令" class="headerlink" title="2. 基本命令"></a>2. 基本命令</h2><h3 id="2-1-git-config-环境设置命令"><a href="#2-1-git-config-环境设置命令" class="headerlink" title="2.1. git config: 环境设置命令"></a>2.1. git config: 环境设置命令</h3><p>通常情况下，安装完Git后的第一件事就是设置<strong>用户名称</strong>和<strong>邮件地址</strong>。每一个Git的提交都会使用这些信息，如果不设置则无法进行提交。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$ git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.name</span> <span class="hljs-string">&quot;goto456&quot;</span> <span class="hljs-comment">// 设置用户名称</span><br>$ git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.email</span> <span class="hljs-string">&quot;goto456@126.com&quot;</span> <span class="hljs-comment">// 设置邮件地址</span><br></code></pre></td></tr></table></figure><blockquote><p>使用<code>--global</code>参数表示设置了全局的环境，如果想对与特定的项目使用不同的用户名和邮件地址，则可已在该项目目录下不使用<code>--global</code>参数设置不同的用户名和邮件地址。</p></blockquote><p><code>git config --list</code>命令可以列出当前Git所有的配置信息。</p><h3 id="2-2-git-init-初始化本地仓库"><a href="#2-2-git-init-初始化本地仓库" class="headerlink" title="2.2. git init: 初始化本地仓库"></a>2.2. git init: 初始化本地仓库</h3><p>获取一个 Git 仓库有2中方法：</p><blockquote><ol><li>本地初始化一个仓库</li><li>从远程克隆一个仓库到本地</li></ol></blockquote><p>对于第1种方式，如果想对本地现有的一个项目用 Git 来管理，可以直接进入该项目的目录下执行如下命令，就可以将其初始化成一个 Git 仓库了。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">git init<br>git remote <span class="hljs-keyword">add </span><span class="hljs-keyword">origin </span>...<br>git <span class="hljs-keyword">add </span>.     <br>git commit -m <span class="hljs-string">&quot;Initial commit&quot;</span><br>git push -u <span class="hljs-keyword">origin </span>master<br></code></pre></td></tr></table></figure><h3 id="2-3-git-clone-克隆远程仓库到本地"><a href="#2-3-git-clone-克隆远程仓库到本地" class="headerlink" title="2.3. git clone: 克隆远程仓库到本地"></a>2.3. git clone: 克隆远程仓库到本地</h3><p>对于第二种方式，也是最常用的方式，比如你在 GitHub 上（或者其他代码托管网站）已经建立了一个项目，你就可以将该项目从远程克隆到本地，这就有了该项目在本地的 Git 仓库。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">git <span class="hljs-built_in">clone</span> git@github.com:goto456/leetcode.git // 通过ssh方式克隆</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">git <span class="hljs-built_in">clone</span> https://github.com/goto456/leetcode.git // 通过https方式克隆</span><br></code></pre></td></tr></table></figure><p>以上2种方式有如下区别：</p><blockquote><p><strong>1. https方式</strong>：不管是谁，只要拿到该项目的 url 可以随便 clone，但是在 push 到远程的时候需要验证用户名和密码；<br><strong>2. ssh方式</strong>：需要现将你电脑的SSH key（SSH公钥）添加到GitHub（或者其他代码托管网站）上，这样在 clone 项目和 push 项目到远程时都不需要输入用户名和密码。</p></blockquote><p>如何生成<code>SSH key</code>，参见下一条命令：ssh-keygen</p><h3 id="2-4-ssh-keygen-生成SSH公钥"><a href="#2-4-ssh-keygen-生成SSH公钥" class="headerlink" title="2.4. ssh-keygen: 生成SSH公钥"></a>2.4. ssh-keygen: 生成SSH公钥</h3><p>生成公钥之前先检查系统中是否已经有了公钥，公钥一般在<code>~/.ssh/</code>目录下。如果该目录下存在<code>id_rsa.pub</code>文件，这就是公钥（id_rsa 文件是私钥）；如果不存在此文件，甚至连<code>.ssh</code>目录都不存在，则需要用 ssh-keygen 命令来生成。如下所示：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>ssh-keygen -t rsa<br><span class="hljs-variable">$ </span>ssh-keygen -t rsa –C “youremail<span class="hljs-variable">@example</span>.com” /<span class="hljs-regexp">/ 然后一直按回车键</span><br></code></pre></td></tr></table></figure><p>这就可以生成 SSH key 了，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。</p><p>登录github，打开” settings”中的SSH Keys页面，然后点击“Add SSH Key”，填上任意title，在Key文本框里黏贴id_rsa.pub文件的内容。_PS: 秘钥所在目录 ~/.ssh_</p><h3 id="2-5-git-status-查看当前状态"><a href="#2-5-git-status-查看当前状态" class="headerlink" title="2.5. git status 查看当前状态"></a>2.5. git status 查看当前状态</h3><p>可以在任何时候使用该命令查看当前的状态。一般会显示当前所处的分支，以及当前工作区是否干净，是否有文件要提交。</p><h3 id="2-6-git-add-添加到暂存区"><a href="#2-6-git-add-添加到暂存区" class="headerlink" title="2.6. git add: 添加到暂存区"></a>2.6. git add: 添加到暂存区</h3><p>无论你新增了一个文件或者对已有的文件进行了修改，都需要将其添加到暂存区，然后提交到版本库。</p><h3 id="2-7-git-commit-提交到版本库"><a href="#2-7-git-commit-提交到版本库" class="headerlink" title="2.7. git commit: 提交到版本库"></a>2.7. git commit: 提交到版本库</h3><p>该命令是将添加到暂存去的变更提交到版本库，主要有一下几个常用的：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ git commit -m <span class="hljs-string">&quot;变更的说明信息&quot;</span>       <span class="hljs-regexp">//</span>标准用法，提交到版本库并填写相关说明信息<br>$ git commit -am <span class="hljs-string">&quot;变更的说明信息&quot;</span>     <span class="hljs-regexp">//</span>加上-a参数，则不需要上一步的git add过程，可以直接将修改过的变更直接提交到版本库，但不会把新增的文件提交<br></code></pre></td></tr></table></figure><p><strong>注：第2条命令一般在不想把新增的文件提交到版本库时用的比较多</strong></p><p>执行完git commit命令提交到版本库后，一次简单的流程就算走完了。接下来可以通过git log命令来查看所有的提交历史。</p><h3 id="2-8-git-log-查看提交历史"><a href="#2-8-git-log-查看提交历史" class="headerlink" title="2.8. git log: 查看提交历史"></a>2.8. git log: 查看提交历史</h3><p>如果发现提交了一个错误版本，想回退到上个版本时，就可以通过该命令来查看所有的历史版本，以选择回退到历史的哪个版本。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-symbol">$</span> git <span class="hljs-built_in">log</span>                  <span class="hljs-comment">//查看历史版本</span><br><span class="hljs-symbol">$</span> git <span class="hljs-built_in">log</span> –-pretty=oneline<br><span class="hljs-symbol">$</span> git <span class="hljs-built_in">log</span> --graph     <span class="hljs-comment">//以图形的方式查看历史（这个我比较常用，能很好的显示各个分支之间的关系）</span><br></code></pre></td></tr></table></figure><h3 id="2-9-git-diff-显示变更内容"><a href="#2-9-git-diff-显示变更内容" class="headerlink" title="2.9. git diff: 显示变更内容"></a>2.9. git diff: 显示变更内容</h3><p>当你对文件进行了修改，想查看进行了哪些修改时，可以通过该命令查看。<br><code>git diff</code>命令会显示修改的文件中哪些内容进行了修改，包括新增了哪些内容，删除了哪些内容。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ git diff                            <span class="hljs-regexp">//</span>后面不接参数，表示显示所有修改文件的变更<br>$ git diff README.md      <span class="hljs-regexp">//</span>后面接文件名，表示只显示该文件的变更<br></code></pre></td></tr></table></figure><p><strong>git比较两个分支的文件的差异</strong></p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gcode">git diff bra<span class="hljs-symbol">nch1</span> bra<span class="hljs-symbol">nch2</span> --stat   <span class="hljs-comment">//显示出所有有差异的文件列表</span><br>git diff bra<span class="hljs-symbol">nch1</span> bra<span class="hljs-symbol">nch2</span> 文件名<span class="hljs-comment">(带路径)</span>   <span class="hljs-comment">//显示指定文件的详细差异</span><br>git diff bra<span class="hljs-symbol">nch1</span> bra<span class="hljs-symbol">nch2</span>                   <span class="hljs-comment">//显示出所有有差异的文件的详细差异</span><br></code></pre></td></tr></table></figure><h3 id="2-10-⭐️git-reset-回退版本"><a href="#2-10-⭐️git-reset-回退版本" class="headerlink" title="2.10. ⭐️git reset: 回退版本"></a>2.10. ⭐️git reset: 回退版本</h3><p>常用于回退版本或者在各个版本间进行跳跃切换。 我们可以先用 <code>git log</code> 看一下当前历史版本，如果要回退到前一个版本，则只需要输入：<code>git reset HEAD~1</code>，如果需要回退到前2个版本，命令是：<code>git reset HEAD~2</code>，回退到前n个版本就是：<code>git reset HEAD~n</code></p><p><strong>注意：在 Git 中，用 </strong><code>**HEAD**</code><strong> 表示指向当前版本的指针</strong></p><p>如果需要回退到任何一个版本，则需要替换成该版本的 <code>commit id</code> 就可以了，例如：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">git reset a8336834b50daafa<span class="hljs-number">0793370</span><br></code></pre></td></tr></table></figure><p><strong>一般情况下会加一个 </strong><code>**--hard**</code><strong> 参数：</strong><code>**git reset --hard HEAD~1**</code><strong> 或 </strong><code>**git reset --hard a8336834b50daafa0793370**</code><strong>，表示回退到某个版本并且丢弃调工作区进行的修改，而不加该参数表示回退到某个版本但保留工作区的修改。</strong></p><p>再次提交-强制:</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">git push -u origin <span class="hljs-keyword">master</span> <span class="hljs-title">-f</span><br></code></pre></td></tr></table></figure><p><strong>在重置之前可以通过从master创建一个分支来维护当前的本地提交：</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># 新建一个分支，并切换到该分支<br>git checkout -b [branch]<br><br># 下载远程仓库的所有变动，而不尝试合并或rebase任何东西<br>git <span class="hljs-keyword">fetch</span> <span class="hljs-comment">--all</span><br><br># 重置当前分支的HEAD为指定<span class="hljs-keyword">commit</span>，同时重置暂存区和工作区，与指定<span class="hljs-keyword">commit</span>一致<br>git <span class="hljs-keyword">reset</span> <span class="hljs-comment">--hard origin/master</span><br></code></pre></td></tr></table></figure><h3 id="2-11-git-push-推送本地分支到远程"><a href="#2-11-git-push-推送本地分支到远程" class="headerlink" title="2.11. git push: 推送本地分支到远程"></a>2.11. git push: 推送本地分支到远程</h3><p>当修改完成，本地的改动都已经提交到本地库，则可以将本地分支推送到远程代码库了。</p><p>命令：<code>git push origin master</code><br><code>origin</code> 表示远程代码库的一个别名（也可以修改为其他名字，可通过 <code>git remote</code> 修改），<code>master</code> 表示需要推送的分支名称。</p><p>如果，push 的过程中提示当前分支进度落后于远程的分支，则需要通过 <code>git pull</code> 命令来拉取远程最新状态和本地分支进行合并，完成之后再 push 到远程就可以了。</p><p><strong>如果本地新建了一个分支，如何提交分支数据到远程服务器？</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git push origin &lt;local_branch_name&gt;:&lt;remote_branch_name&gt;<br></code></pre></td></tr></table></figure><br>例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git push origin branch_abc: branch_abc<br></code></pre></td></tr></table></figure></p><h3 id="2-12-git-pull-拉取远程分支到本地并合并"><a href="#2-12-git-pull-拉取远程分支到本地并合并" class="headerlink" title="2.12. git pull: 拉取远程分支到本地并合并"></a>2.12. git pull: 拉取远程分支到本地并合并</h3><p>一般是本地分支的进度落后于远程分支时，需要使用该命令<strong>（pull = fetch + merge）</strong></p><p>命令：<code>git pull origin master</code><br><code>origin</code> 表示远程代码库的一个别名（也可以修改为其他名字，可通过 <code>git remote</code> 修改），<code>master</code> 表示需要拉取合并的分支名称。</p><p><strong>常用 </strong><code>**git pull --rebase origin master**</code><strong> 用 rebase 的方式进行，不会产生 merge 保持分支干净、整洁</strong></p><p><strong>PS：强制拉取更新</strong></p><ul><li>回到远程仓库的状态 </li><li><strong>抛弃本地所有的修改</strong>，回到远程仓库的状态。 </li></ul><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-comment"># 下载远程仓库的所有变动，而不尝试合并或rebase任何东西</span><br>git fetch <span class="hljs-comment">--all</span><br><br><span class="hljs-comment"># 将主分支重置为您刚刚获取的内容</span><br>git reset <span class="hljs-comment">--hard origin/master</span><br></code></pre></td></tr></table></figure><h2 id="3-分支"><a href="#3-分支" class="headerlink" title="3. 分支"></a>3. 分支</h2><h3 id="3-1-git-branch-分支操作"><a href="#3-1-git-branch-分支操作" class="headerlink" title="3.1. git branch: 分支操作"></a>3.1. git branch: 分支操作</h3><p>命令：<code>git branch</code> 用于显示本地所有分支以及当前所在哪个分支</p><p>命令：<code>git branch branchName</code> 用于新建名为 branchName 的新分</p><p>命令：<code>git branch -d branchName</code> 用于删除名为 branchName 的分支。</p><p>命令：<code>git branch -D branchName</code> 用于强制删除分支。</p><p>删除一个远程分支:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(main)$ git push origin --delete my-branch<br></code></pre></td></tr></table></figure><br>你也可以:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(main)$ git push origin :my-branch<br></code></pre></td></tr></table></figure></p><h3 id="3-2-git-checkout-分支间切换"><a href="#3-2-git-checkout-分支间切换" class="headerlink" title="3.2. git checkout: 分支间切换"></a>3.2. git checkout: 分支间切换</h3><p>该命令除了进行分支间切换功能外，还可以用来丢弃工作区中的修改内容，这里就不作介绍了，仅介绍分之间的切换功能。</p><p>命令：<code>git checkout branchName</code> 用于从当前分支切换到名为 branchName 的分支上。</p><p>命令：<code>git checkout -b branchName</code> 用于新建名为 branchName 的分支并切换到该分支上。</p><p>从别人正在工作的远程分支签出(checkout)一个分支：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 首先, 从远程拉取(fetch) 所有分支:</span><br>git fetch --<span class="hljs-built_in">all</span><br><br><span class="hljs-comment"># 假设你想要从远程的daves分支签出到本地的daves</span><br>git checkout --track origin/daves<br><br><span class="hljs-comment"># --track 是 git checkout -b [branch] [remotename]/[branch] 的简写)</span><br><span class="hljs-comment"># 这样就得到了一个daves分支的本地拷贝, 任何推过(pushed)的更新，远程都能看到.</span><br></code></pre></td></tr></table></figure></p><h3 id="3-3-git-merge-合并分支"><a href="#3-3-git-merge-合并分支" class="headerlink" title="3.3. git merge: 合并分支"></a>3.3. git merge: 合并分支</h3><p>该命令用于合并两个分支。</p><p><strong>命令：</strong><code>**git merge branchName**</code><strong> 用于将名为 branchName 的分支合并到当前分支。</strong></p><p>有两种合并方式：</p><ol><li>fast-forward 方式合并：<br>命令：<code>git merge dev</code></li></ol><p><img src="https://raw.githubusercontent.com/goto456/markdown-pictures/master/wengeblog/git/git-20.png#id=YI7KL&amp;originHeight=268&amp;originWidth=1297&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><ol><li>非fast-forward 方式合并：<br>命令：<code>git merge dev</code></li></ol><p><img src="https://raw.githubusercontent.com/goto456/markdown-pictures/master/wengeblog/git/git-21.png#id=VTMXk&amp;originHeight=230&amp;originWidth=1298&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><p><strong>注意两种方式的区别：fast-forward 方式仅仅是移动了 HEAD 指针，而非 fast-forward 方式则是新建了一个节点</strong></p><h3 id="3-4-git-rebase-分支的变基"><a href="#3-4-git-rebase-分支的变基" class="headerlink" title="3.4. git rebase: 分支的变基"></a>3.4. git rebase: 分支的变基</h3><p>命令：<code>git rebase master</code> 将当前分支 rebase 到 master 分支<br>命令：<code>git rebase master dev</code> 将 dev 分支 rebase 到 master 分支</p><p>这种操作很难用语言解释，我用一个图来说明其与 merge 操作的区别：</p><p><img src="https://raw.githubusercontent.com/goto456/markdown-pictures/master/wengeblog/git/git-22.png#height=565&amp;id=DSALF&amp;originHeight=816&amp;originWidth=786&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=&amp;width=544" alt=""><br>由图可知，<strong>rebase 操作相当于将 C3 节点拿下来换了一个位置重新放置</strong>。最后不会产生合并的痕迹，所有分支都是同一条直线。</p><h3 id="3-5-rebase合并多个提交commit"><a href="#3-5-rebase合并多个提交commit" class="headerlink" title="3.5 rebase合并多个提交commit"></a>3.5 rebase合并多个提交commit</h3><p>当你的项目充满了无用的 <code>commit</code> 纪录，如果有一天线上出现了紧急问题，你需要回滚代码，却发现海量的 <code>commit</code> 需要一条条来看。</p><p>rebase用途：合并多次提交纪录，然后再merge到主干</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">git rebase -i  [startpoint]  [endpoint]<br><br>`-i`的意思是`--interactive`，即弹出交互式的界面让用户编辑完成合并操作<br>`[startpoint] [endpoint]`则指定了一个编辑区间<br>如果不指定`[endpoint]`，则该区间的终点默认是当前分支<br></code></pre></td></tr></table></figure><ol><li>我们来合并最近的 4 次提交纪录，执行：</li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">git</span> rebase -i HEAD~<span class="hljs-number">4</span><br></code></pre></td></tr></table></figure><ol><li>这时候，会自动进入 <code>vi</code> 编辑模式：<br>将后面两个改成squash，就是合并到第一个上去</li></ol><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">p cacc52da <span class="hljs-built_in">add</span>: qrcode<br>s f072ef48 update: indexeddb hack<br>s <span class="hljs-number">4e84901</span><span class="hljs-keyword">a</span> feat: <span class="hljs-built_in">add</span> indexedDB floder<br>s <span class="hljs-number">8</span>f33126c feat: <span class="hljs-built_in">add</span> test2.js<br></code></pre></td></tr></table></figure><ul><li>4次提交的信息会倒序排列，最上面的是第四次提交，最下面的是最近一次提交。 <ul><li>有几个命令需要注意一下：</li><li>pick：保留该commit（缩写:p）</li><li>reword：保留该commit，但我需要修改该commit的注释（缩写:r）</li><li>edit：保留该commit, 但我要停下来修改该提交(不仅仅修改注释)（缩写:e）</li><li>squash：将该commit和前一个commit合并（缩写:s）</li><li>fixup：将该commit和前一个commit合并，但我不要保留该提交的注释信息（缩写:f）</li><li>exec：执行shell命令（缩写:x）</li><li>drop：我要丢弃该commit（缩写:d）</li></ul></li></ul><ol><li>按照如上命令来修改你的提交纪录，然后<code>wq</code>保存退出，Git会压缩提交历史。 </li><li>最后，修改一下合并后的commit的描述信息，编辑完保存即可完成了. </li><li><p>如果你想放弃这次压缩的话，执行以下命令：  </p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">git rebase <span class="hljs-comment">--abort</span><br></code></pre></td></tr></table></figure></li><li><p>同步到远程仓库<br>若强制推送提示后出现 “You are not allowed to force push code to a protected branch on this project.”<br>解决方案: <a href="https://github.com/LeachZhou/blog/issues/11">https://github.com/LeachZhou/blog/issues/11</a><br>解决方案: <a href="https://backlog.com/git-tutorial/cn/stepup/stepup2_8.html">https://backlog.com/git-tutorial/cn/stepup/stepup2_8.html</a> </p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">git push -u origin <span class="hljs-keyword">master</span><br><span class="hljs-title">git</span> push -u origin <span class="hljs-keyword">master</span> <span class="hljs-title">-f</span><span class="hljs-comment"># 强制push</span><br></code></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs subunit">E:\xxx&gt;git push -f origin master<br>Total 0 (delta 0), reused 0 (delta 0)<br>remote: GitLab: You are not allowed to force push code to a protected branch on this project.<br>To http://git.xxx.cn/xxx/xxx<br> ! [remote rejected] master -&gt; master (pre-receive hook declined)<br><span class="hljs-keyword">error: </span>failed to push some refs to &#x27;http://xxx@git.xxx.cn/xxx/xxx.git&#x27;<br></code></pre></td></tr></table></figure></li><li><p>git放弃修改，强制覆盖本地代码  </p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">//</span> 从远程拉取所有内容<br>git fetch --all<br><br><span class="hljs-regexp">//</span> reset 本地代码<br>git reset --hard origin/master<br><br><span class="hljs-regexp">//</span> 重启拉取对齐<br>git pull<br></code></pre></td></tr></table></figure></li></ol><h3 id="3-6-提交-Merge-Request"><a href="#3-6-提交-Merge-Request" class="headerlink" title="3.6 提交 Merge Request"></a>3.6 提交 Merge Request</h3><p>当完成功能后提交到远程后，需要经过下述流程：</p><ol><li>提交 Merge Request：通过「＋Create Merge Request」按钮创建一个 Merge Request；</li><li>必须指定「Assignee」：指定需要 review 你代码的同学，禁止指定自己；</li><li>更改「Target branch」：改变为需要合并进去的目标分支；</li><li>设置合并后删除被合并分支的选项：勾选「Remove source branch when merge request is accepted.」选项，在合并完成后删除源分支，控制分支总数量；</li><li>使用「Submit Merge Request」提交合并请求。</li></ol><p>完成上述设置后，相关同学将会收到邮件通知，此时可进入 GitLab 进行 code review。<br>如果发现问题则对问题代码进行点评并拒绝关闭申请，反之则通过合并申请。<br>合并申请通过后留下的 Merge Request 记录，也就记录了 code reviewer 。</p><h2 id="4-多人协同开发代码版本管理流程"><a href="#4-多人协同开发代码版本管理流程" class="headerlink" title="4. 多人协同开发代码版本管理流程"></a>4. 多人协同开发代码版本管理流程</h2><p>在团队协作过程中一般会有多个分支，比如有默认的 master 分支，有用于开发的 dev 分支，还有用于测试的 test 分支，用于对外发布的 release 分支，以及每个开发人员开发不同功能时用到的 feature_xx 分支等等。</p><p>公司中一般是用 GitLab 搭建的代码托管服务，几个人的小团队也可以自己搭建。</p><p>每个团队业务不一样，分支数量的设置也会不一样，下面我介绍一下我们团队的分支设置，以及普通开发人员和项目 leader 对不同分支的不同权限以及不同的操作。</p><p><img src="https://ningshixian.github.io/resources/images/%E5%A4%9A%E4%BA%BA%E5%8D%8F%E5%90%8C%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B.png#id=jvG4X&amp;originHeight=440&amp;originWidth=898&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt=""></p><h3 id="4-1-分支设置"><a href="#4-1-分支设置" class="headerlink" title="4.1 分支设置"></a>4.1 分支设置</h3><p>我们常用的分支有3个（master 分支、dev 分支、test 分支）以及若干个 feature_xx 分支。</p><ol><li><code>master</code> 分支：是主分支，是最终上线代码的分支，该分支被设置被保护分支（锁住），普通开发人员没有权限操作，只有团队 leader 有合并的权限；</li><li><code>dev</code> 分支：是用于开发的分支，该分支被设置为默认 clone 的分支，也用于合并到 master 之前进行测试的分支，普通开发人员从远程 clone 到本地的默认分支，可以进行合并等操作；</li><li><code>test</code> 分支：是用于测试的分支，测试人员可以将自己开发分支中的修改合并到 test 分支在测试环境进行测试，一般该分支不合并到任何分支；</li><li><code>feature_xx</code> 分支：是用户开发自己模块功能的特征分支，可以叫 feature_login, feature_ui, feature_payment 等与开发的功能相关的名称，该分支上的功能开发完、测试无误后可合并到 dev 分支上。</li></ol><h3 id="4-2-普通开发人员的操作"><a href="#4-2-普通开发人员的操作" class="headerlink" title="4.2 普通开发人员的操作"></a>4.2 普通开发人员的操作</h3><p><a href="https://blog.csdn.net/qq_34206198/article/details/52055395">将master分支内容合并到dev分支</a></p><p>普通开发人员，一般按照如下几个步骤来进行开发、测试工作就可以了：</p><ol><li>将远程 dev 分支 clone 到本地，例如：<code>git clone git@github.com:goto456/test.git</code>；</li><li>从 dev 分支拉出（新建）自己的 feature 分支用于开发，例如：<code>git checkout -b feature_login</code>；</li><li>在自己的 feature 分支上进行开发工作；</li><li>开发完了用 add、commit 等操作提交到当前分支；</li><li>如果需要在测试环境进行测试，则将远程 test 分支拉到本地，例如：<code>git branch test origin/test</code>;</li><li>将自己的 feature 分支合并到 test 分支，并将 test 分支 push 到远程，例如：<code>git rebase test</code>, <code>git checkout test</code>, <code>git merge feature_login</code>, <code>git push origin test</code>；<strong>（注意：我们推荐用 rebase 来合并，以保证分支的整洁、美观）</strong></li><li>通过公司的发布平台将远程 test 分支发布到测试环境进行测试；</li><li>如果测试没问题或者开始就不需要测试，这可以直接将当前 feature 分支合并到 dev 分支，并 push 到远程库，例如：<code>git rebase dev</code>, <code>git checkout dev</code>, <code>git merge feature_login</code>, <code>git push origin dev</code>；<strong>（注意：我们推荐用 rebase 来合并，以保证分支的整洁、美观）</strong></li><li>这时表示该功能已经开发完成了，代码的 review 以及发布，需要团队 leader 在合并到 master 操作时进行；这时可以删除了自己的 feature 分支，例如：<code>git branch -d feature_login</code>；</li><li><strong>如果在 push 到远程的时候提示需要先 pull 时，我们推荐使用 rebase 的方式：</strong><code>**git pull --rebase**</code><strong> 以保持分支的整洁、美观。</strong></li></ol><h3 id="4-3-团队-leader-的操作"><a href="#4-3-团队-leader-的操作" class="headerlink" title="4.3 团队 leader 的操作"></a>4.3 团队 leader 的操作</h3><p>因为只有 leader 有操作 master 分支的权限，所以需要完成 dev 分支到 master 分支的合并，以及后续打 tag 和正式上线发布的工作：</p><ol><li>先切换到 dev 分支，并拉取最新的状态，例如：<code>git checkout dev</code>, <code>git pull --rebase origin dev</code>；</li><li>进行代码 review 等过程后，合并到 master 分支，例如：<code>git rebase master</code>, <code>git checkout master</code>, <code>git merge dev</code>;<strong>（注意：我们推荐用 rebase 来合并，以保证分支的整洁、美观）</strong></li><li>为本次完成的版本打上标签，例如：<code>git tag v1.0 -m &quot;release version 1.0&quot;</code>；</li><li>将本地合并后的 master 分支以及标签 push 到远程库，例如：<code>git push orgin master --tags</code>。</li></ol><h2 id="5-其他命令"><a href="#5-其他命令" class="headerlink" title="5. 其他命令"></a>5. 其他命令</h2><h3 id="查看分支的版本历史"><a href="#查看分支的版本历史" class="headerlink" title="查看分支的版本历史"></a>查看分支的版本历史</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs 1c">git <span class="hljs-built_in">log</span><br>git reflog<span class="hljs-meta"># 显示当前分支的最近几次提交</span><br>git <span class="hljs-built_in">log</span> –-pretty=oneline<br>git <span class="hljs-built_in">log</span> -S [keyword]<span class="hljs-meta"># 搜索提交历史，根据关键词</span><br><br><span class="hljs-meta"># 开头的即是commit id号</span><br></code></pre></td></tr></table></figure><h3 id="删除上一次提交-commit"><a href="#删除上一次提交-commit" class="headerlink" title="删除上一次提交(commit)"></a>删除上一次提交(commit)</h3><p>如果你需要删除推了的提交(pushed commits)，你可以使用下面的方法。可是，这会不可逆的改变你的历史，也会搞乱那些已经从该仓库拉取(pulled)了的人的历史。简而言之，如果你不是很确定，千万不要这么做。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">$ git reset --hard HEAD^<br>$ git push -f [remote] [branch]<br></code></pre></td></tr></table></figure><br>如果你还没有推到远程, 把Git重置(reset)到你最后一次提交前的状态就可以了(同时保存暂存的变化):<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git reset --soft HEAD~<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure></p><h3 id="我需要提交到一个新分支，但错误的提交到了main"><a href="#我需要提交到一个新分支，但错误的提交到了main" class="headerlink" title="我需要提交到一个新分支，但错误的提交到了main"></a>我需要提交到一个新分支，但错误的提交到了main</h3><p>在main下创建一个新分支，不切换到新分支,仍在main下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(main)$ git branch my-branch<br></code></pre></td></tr></table></figure><br>把main分支重置到前一个提交:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(main)$ git reset --hard HEAD^<br></code></pre></td></tr></table></figure><br>签出(checkout)刚才新建的分支继续工作:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(main)$ git checkout my-branch<br></code></pre></td></tr></table></figure></p><h3 id="commit写错-撤销重写"><a href="#commit写错-撤销重写" class="headerlink" title="commit写错-撤销重写"></a>commit写错-撤销重写</h3><p>或者说，把暂存的内容添加到上一次的提交(commit)<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">git</span> <span class="hljs-string">commit</span> <span class="hljs-built_in">--m</span> <span class="hljs-string">&#x27;提交信息&#x27;</span><br><span class="hljs-string">git</span> <span class="hljs-string">commit</span> <span class="hljs-built_in">--amend</span> -<span class="hljs-string">m</span> <span class="hljs-string">&#x27;修改后的提交信息&#x27;</span><br></code></pre></td></tr></table></figure></p><h3 id="强制-push-报错"><a href="#强制-push-报错" class="headerlink" title="强制 push 报错"></a>强制 push 报错</h3><blockquote><p>E:\xxx&gt;git push -f origin master<br>Total 0 (delta 0), reused 0 (delta 0)<br>remote: GitLab: You are not allowed to force push code to a protected branch on this project.<br>To <a href="http://git.xxx.cn/xxx/xxx">http://git.xxx.cn/xxx/xxx</a><br> ! [remote rejected] master -&gt; master (pre-receive hook declined)<br>error: failed to push some refs to ‘<a href="http://xxx@git.xxx.cn/xxx/xxx.git&#39;">http://xxx@git.xxx.cn/xxx/xxx.git’</a></p></blockquote><p>问题：若强制推送 push -f 提示 You are not allowed to force push code to a protected branch on this project.<br>解决方案: <a href="https://github.com/LeachZhou/blog/issues/11">Gitlab强制推送提示”You are not allowed to force push code to a protected branch on this project.” #11 </a><br>PS：由于当前分支的版本低于远程分支的版本，强制推送—force 将新的代码覆盖掉远程仓库版本代码，这样子就达到了撤销远程仓库代码一样的效果</p><h3 id="意外硬重置-hard-reset-，如何找回修改的内容"><a href="#意外硬重置-hard-reset-，如何找回修改的内容" class="headerlink" title="意外硬重置(hard reset)，如何找回修改的内容"></a>意外硬重置(hard reset)，如何找回修改的内容</h3><p>如果你意外的做了 git reset —hard, 你通常能找回你的提交(commit), 因为Git对每件事都会有日志，且都会保存几天。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git reflog<br></code></pre></td></tr></table></figure><br>你将会看到一个你过去提交(commit)的列表, 和一个重置的提交。选择你想要回到的提交(commit)的SHA，再重置一次:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git reset --hard SHA1234<br></code></pre></td></tr></table></figure><br>这样就完成了。</p><h3 id="★★强制拉取更新"><a href="#★★强制拉取更新" class="headerlink" title="★★强制拉取更新"></a>★★强制拉取更新</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># 新建一个分支，并切换到该分支<br>git checkout -b [branch]<br><br># 下载远程仓库的所有变动，而不尝试合并或rebase任何东西<br>git <span class="hljs-keyword">fetch</span> <span class="hljs-comment">--all</span><br><br># 重置当前分支的HEAD为指定<span class="hljs-keyword">commit</span>，同时重置暂存区和工作区，与指定<span class="hljs-keyword">commit</span>一致<br>git <span class="hljs-keyword">reset</span> <span class="hljs-comment">--hard origin/master</span><br></code></pre></td></tr></table></figure><h3 id="★★git-添加-gitignore-规则无效"><a href="#★★git-添加-gitignore-规则无效" class="headerlink" title="★★git 添加 gitignore 规则无效"></a>★★git 添加 gitignore 规则无效</h3><p>在使用git的时候我们有时候需要忽略一些文件或者文件夹。我们一般在仓库的根目录创建.gitignore文件</p><p>在提交之前，修改.gitignore文件，添加需要忽略的文件。然后再做 <code>add commit push</code> 等</p><p>但是有时在使用过称中，需要对.gitignore文件进行再次的修改。</p><p>这次我们需要清除一下缓存cache，才能是.gitignore 生效。</p><p>具体做法：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">git rm -r --<span class="hljs-keyword">cached </span>.  <span class="hljs-comment">#清除缓存</span><br>git <span class="hljs-keyword">add </span>. <span class="hljs-comment">#重新trace file</span><br>git commit -m <span class="hljs-string">&quot;update .gitignore&quot;</span> <span class="hljs-comment">#提交和注释</span><br>git push <span class="hljs-keyword">origin </span>master <span class="hljs-comment">#可选，如果需要同步到remote上的话</span><br></code></pre></td></tr></table></figure><p>这样就能够使修改后的.gitignore生效。</p><p><a href="https://blog.csdn.net/mingjie1212/article/details/51689606">参考1</a>、<a href="https://blog.csdn.net/u010940300/article/details/49204597">参考2</a></p><h3 id="git拉取指定分支下的文件"><a href="#git拉取指定分支下的文件" class="headerlink" title="git拉取指定分支下的文件"></a>git拉取指定分支下的文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git checkkout 「branch」&lt;file_name&gt;<br></code></pre></td></tr></table></figure><p>实例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git checkout origin/feature forms/common/commen.php<br></code></pre></td></tr></table></figure></p><h3 id="★★常见问题"><a href="#★★常见问题" class="headerlink" title="★★常见问题"></a>★★常见问题</h3><blockquote><p>ssh登录出现异常警告：WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!</p></blockquote><p><strong>原因分析：</strong></p><p>It is also possible that a host key has just been changed.</p><p>之前，与远程库建立首次连接时，双方相互记录了对方的公钥（ssh基于非对称密钥技术），在ssh服务主机重装系统后，公钥改变了，而ssh连接仍以旧版本公钥，自然是无法与新系统连接的。</p><p><a href="https://www.cnblogs.com/johnchain/archive/2013/04/08/3006631.html"><strong>解决方案：</strong></a></p><p>删除~/.ssh/known_hosts文件，或者如果你可以判断出known_hosts中原ssh服务器的公钥，删去那部分，然后后再次建立新的连接，即可获得新的公钥。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzUzMjk1MDI0OQ==&amp;mid=2247485617&amp;idx=3&amp;sn=ca65184052665495c51494454a19fade&amp;chksm=faaa3537cdddbc21333656c70319507b728e314c186426b26329e6a882da80ffe6c26cf265bd&amp;scene=0&amp;xtrack=1#rd">Git使用教程：最详细、最傻瓜、最浅显、真正手把手教！</a><br><a href="">简明 Git 教程。浅显易懂，快速入门</a><br><a href="https://mp.weixin.qq.com/s/PXu2YcNXvaRVxliKvnnYQg">45 个 Git 经典操作场景，专治不会合代码</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>优雅部署python代码</title>
    <link href="/2019/08/12/2019-08-12-%E4%BC%98%E9%9B%85%E9%83%A8%E7%BD%B2python%E4%BB%A3%E7%A0%81/"/>
    <url>/2019/08/12/2019-08-12-%E4%BC%98%E9%9B%85%E9%83%A8%E7%BD%B2python%E4%BB%A3%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<h2 id="优雅部署python代码"><a href="#优雅部署python代码" class="headerlink" title="优雅部署python代码"></a>优雅部署python代码</h2><blockquote><p>搬运《Python 最佳实践指南2018》文档中的‘优雅部署python代码’一节</p></blockquote><ol><li>代码打包-第三方库</li><li>代码冻结-可执行文件</li></ol><span id="more"></span><h3 id="1-代码打包"><a href="#1-代码打包" class="headerlink" title="1. 代码打包"></a>1. 代码打包</h3><p>打包你的代码，将它共享出去，让其他的开发者使用。例如，将其打包成一个<code>库</code>分享给其他开发者. 这种做法适合发布代码给其他开发人员，但是不适合发布应用程序给终端用户使用。</p><p><a href="https://python-packaging-user-guide.readthedocs.io/">Python 打包指南</a> 提供了创建和维护 Pythond 包的一个延伸的指导。</p><h3 id="2-代码冻结"><a href="#2-代码冻结" class="headerlink" title="2. 代码冻结"></a>2. 代码冻结</h3><p>『冻结』你的代码是指创建单个可执行文件，文件里包含所有程序代码以及 Python 解释器。进行这种分发的好处是你的用户不需要安装所要求的 Python 版本（或其他）即可直接运行你的应用程序。<br>冻结的一个缺点是它会增加大约 2-12 MB 的发行大小。另外，如果修补了 Python 的安全漏洞， 你将需要独立负责更新分发的应用程序。</p><p><strong>Windows 下的解决方案</strong>：<strong>PyInstaller</strong></p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">pip install pyinstaller<br>pyinstaller <span class="hljs-comment">--onefile script.py</span><br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://learnku.com/docs/python-guide/2018/shipping-freezing/3287">Python 最佳实践指南2018-优雅部署python代码</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何与Linux服务器优雅的交互</title>
    <link href="/2019/08/12/2019-08-12-%E5%A6%82%E4%BD%95%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BC%98%E9%9B%85%E7%9A%84%E4%BA%A4%E4%BA%92/"/>
    <url>/2019/08/12/2019-08-12-%E5%A6%82%E4%BD%95%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BC%98%E9%9B%85%E7%9A%84%E4%BA%A4%E4%BA%92/</url>
    
    <content type="html"><![CDATA[<h1 id="如何与Linux服务器优雅的交互"><a href="#如何与Linux服务器优雅的交互" class="headerlink" title="如何与Linux服务器优雅的交互"></a>如何与Linux服务器优雅的交互</h1><ol><li>远程连接Linux客户端</li><li>ssh免密快速登录</li><li>内网穿透（跨网段访问服务器）</li><li>文件传输与同步 syncthing</li><li>多开发环境管理 Anaconda</li><li>多任务管理 nohup screen</li><li>睡觉调参模式 python-fire</li><li>jupyter notebook</li><li>单任务全霸占模式 run-one</li></ol><span id="more"></span><h2 id="0-远程连接Linux客户端"><a href="#0-远程连接Linux客户端" class="headerlink" title="0. 远程连接Linux客户端"></a>0. 远程连接Linux客户端</h2><p>主要介绍 <strong>MobaXterm</strong>、XManager、secureCRT、Putty、Cygwin、FileZilla.</p><p><strong>1. MobaXterm</strong></p><p>MobaXterm：远程计算的终极工具箱。</p><p>它提供了所有重要的远程网络工具 （如SSH、 X11、 RDP、 VNC、 FTP、 MOSH 等等），<br>以及 Windows 桌面上的 Unix 命令（bash、 ls、 cat、sed、 grep、 awk、 rsync等等），而这些都是由一个开箱即用的单一的便携程序所提供。<br>MobaXterm 对个人使用免费，你可以在<a href="http://mobaxterm.mobatek.net/download.html">这里</a>下载 MobaXterm （单文件纯绿色软件，下载过来exe包直接运行即可，不需要任何的安装过程）。</p><p>参考 <a href="https://www.freebuf.com/column/163631.html">免费开源SSH客户端神器MobaXterm，是时候puttty、Xshell和SecureCRT说Byebye了</a></p><ul><li>多协议，多Session管理<ul><li>传输协议上支持SSH, X11, RDP, VNC, FTP,MOSH, … 等大量远程网络协议；</li><li>而且所有协议统一session管理，打开一个终端，所有功能都可以直接使用免登陆。</li></ul></li><li>RDP 连接Windows远程桌面</li><li>图形化的SSH隧道管理</li><li>Linux shell小工具<ul><li>MobaXterm引入很多Linux shell的小工具直接在Windows使用</li></ul></li><li>SFTP传输<ul><li>SSH登陆后左边会自动列出sftp文件传输窗口，可以随手进行文件上传和下载；</li></ul></li><li>多窗口和分屏<ul><li>MXterm也支持内置多标签页、横向纵向2分屏和田字形4分屏，用于一个窗口内管理多个连接</li></ul></li></ul><p><strong>2. XManager</strong></p><p>Xmanager是一个工具集合，里面包括了xshell，xftp，xbrowser,xstart,xlpd等等功能，<br>其中最常用的就是xshell和xftp，我个人认为xshell是最好用的ssh工具，这也是我目前每天使用的工具。</p><p>网址：<a href="http://www.netsarang.com/products/xmg_detail.html">http://www.netsarang.com/products/xmg_detail.html</a></p><p>Xshell4-Xftp4整合绿色便携版，免安装直接运行使用【亲测好用】-<a href="http://download.csdn.net/download/chenchunlin526/9990854">CSDN下载</a></p><p><em>* 注意：不要用盗版汉化版的ssh软件，可能植入木马，窃取你的用户名密码，证书 </em></p><p>优点：</p><ol><li>xshell的设计简洁，支持多标签模式，默认可以进行自动登录，方便进行快速设置主题，字体等，</li><li>可以一键连接xftp工具。</li></ol><p>缺点：</p><ol><li>收费（home and school users可以申请免费使用xshell与xftp）</li><li>只有windows版本。</li></ol><p><strong>3. secureCRT</strong></p><p>SecureCRT也是一款功能强大的工具，比较好用。但是也是收费的（钱不是白收的），同时支持windows，mac，linux，iOS等多系统平台。是mac用户的首选。<br>SecureCRT除了包括一般工具都有的特点之外，还包括自动注册、对不同主机保持不同的特性、打印功能、颜色设置、可变屏幕尺寸、用户定义的键位图等等功能。</p><p>网址：<a href="https://www.vandyke.com/products/securecrt/">https://www.vandyke.com/products/securecrt/</a></p><p><strong>4. Putty</strong></p><p>putty是最简单的SSH工具，无需安装，支持多系统版本，下载后就可以直接使用</p><p>网址：<a href="https://www.putty.org/">https://www.putty.org/</a></p><p>网址：<a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/">https://www.chiark.greenend.org.uk/~sgtatham/putty/</a></p><p>缺点：</p><ol><li>不支持标签模式；</li><li>默认设置不友好，很多功能都需要额外配置才行，例如自动登录功能；</li><li>不能传输文件；</li><li>没有X11，需要配置Xming工具；</li><li>默认keepalives没有设置，一段时间不操作后会断开。</li></ol><p><strong>5. Cygwin</strong></p><p>Cygwin 是一款 GNU 和开源工具的大杂烩，提供的功能近似于一个 Windows 平台下的 Linux。</p><p><strong>6. FileZilla</strong></p><p>接下来的一个工具是FileZilla，这个工具主要方便我们将本机上的文件传输到服务器端。FileZilla使用起来也十分容易。</p><p>只需在上面输入相应的主机名、用户名和密码就行了，端口还是输入22，使用sftp</p><h3 id="通过堡垒机登陆Linux服务器"><a href="#通过堡垒机登陆Linux服务器" class="headerlink" title="通过堡垒机登陆Linux服务器"></a>通过堡垒机登陆Linux服务器</h3><p>最近公司线上服务器都换上了堡垒机登陆，这样每个开发人员都只分配堡垒机账号，然后运维人员划分对应的权限机器就好了。</p><p><strong>相比之前的登陆有很多优点：</strong></p><ul><li>只需给开发人员分配一个账号和密码，避免机器过多账号繁多的情况；</li><li>运维人员分配机器权限便捷，只需给开发人员账号分配权限机器即可；</li><li>相对跳板机来说更安全，堡垒机登陆到线上服务器后，可以记录每个用户的所有操作记录；</li></ul><p><strong>大概说下堡垒机的使用机制：</strong></p><ul><li>开发人员使用自己的账号登陆堡垒机，输入账号和密码</li><li>登陆成功后，堡垒机会列出该账户下有权限可操作的服务器列表，每个服务器对应一个数字</li><li>开发人员输入对应的服务器数字或者选择搜索选项搜索服务器</li><li>如果是直接选择了服务器，则会列出可用的账户，如果是搜索服务器，则需输入服务器IP</li><li>然后堡垒机会列出可用的账号列表，每个账户对应一个数字</li><li>输入账户对应的数字，选择使用哪个账户操作服务器，进入服务器，登陆完成</li></ul><p>也就是说我进入一台线上服务器，需要输入账号密码-&gt;选择/搜索服务器-&gt;选择操作账户-&gt;登陆成功这几个步骤。</p><p><strong>具体步骤</strong></p><p>使用XShell连接服务器<a href="https://blog.csdn.net/h8178/article/details/78250710">图解</a></p><ol><li>点击新建会话，填写堡垒机的地址与端口号</li><li>点击用户身份验证，填写堡垒机的用户名与密码</li><li>保存会话连接</li><li>连接堡垒机，输入OTP动态密码，选择相应的服务器</li></ol><h2 id="1-ssh免密快速登录"><a href="#1-ssh免密快速登录" class="headerlink" title="1. ssh免密快速登录"></a>1. ssh免密快速登录</h2><p>实现快速远程登录服务器，可以将你的服务器信息写入ssh配置文件中：</p><ul><li><p>ssh配置文件位于：~/.ssh/config </p></li><li><p>修改文件内容</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">Host test<br><span class="hljs-keyword">User</span> <span class="hljs-title">lhadmin</span><br>Hostname <span class="hljs-number">10.231</span>.<span class="hljs-number">135.146</span><br>Port <span class="hljs-number">22</span><br><br>Host github.com<br>HostName github.com<br><span class="hljs-keyword">User</span> <span class="hljs-title">ningshixian</span><br>IdentityFile /Users/xxx/.ssh/id_rsa<br></code></pre></td></tr></table></figure><p>Host后面你自己起个名；</p><p>User后面是你的服务器用户名；</p><p>Hostname是服务器IP；</p><p>Port是端口号；</p></li><li><p>保存后 <code>source ~/.bash_profile*</code> 或者 <code>*source ~/.bashrc*</code> 激活一下启动脚本</p></li><li>快捷登录：ssh test</li></ul><p>然后，经过第一步后，只需要再敲密码就可以进入啦。如何实现免密登录呢？也很简单，把你PC端的ssh公钥写入服务器的ssh信任列表里就可以啦。首先用<code>ssh-keygen</code>命令生成rsa密钥对（生成一只私钥和一只公钥），一路enter即可，但是注意：</p><p><img src="https://pic1.zhimg.com/80/v2-4caf762d4648f366adcb23dc05fdad94_720w.jpg" alt="img"></p><p><em>之前有已经生成过的同学在此处就选择n吧，没有生成过的同学就一路next～</em></p><p>然后去 <em>~/.ssh/</em> 文件夹下将公钥发送到服务器上的某文件夹里：</p><p><img src="https://pic2.zhimg.com/80/v2-505040d0a11a5d5ba45af596377e5e91_720w.jpg" alt="img"></p><p>然后去服务器上，把你PC端的公钥丢进ssh信任列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br></code></pre></td></tr></table></figure><p>好啦～搞定啦，再回到你的PC端登录试试吧，是不是连输入密码都省掉啦。</p><h2 id="2-内网穿透（跨网段访问服务器）"><a href="#2-内网穿透（跨网段访问服务器）" class="headerlink" title="2. 内网穿透（跨网段访问服务器）"></a>2. 内网穿透（跨网段访问服务器）</h2><p>但是注意哦，如果你的服务器是在局域网内，那你的PC离开这个局域网的时候当然就找不到你的服务器啦。想要在家里用GPU服务器？很简单，小夕教你分分钟内网穿透！</p><p>在内网穿透方面，小夕试了好几种方案后，感觉还是<strong><a href="https://link.zhihu.com/?target=https%3A//hsk.oray.com/download/">花生壳</a></strong>对新手最友好也最稳定。我们的内网穿透只需要将服务器内网ip以及22端口号（即ssh端口号）映射到外网ip的某个端口号。这个过程使用花生壳非常简单，在网上有很多教程，小夕就不啰嗦啦。之后我们要做的就是将这个外网ip和端口号也封装成一条命令，比如花生壳分配给我们的外网ip是103.44.145.240，端口是12560，那么只需要把这个写入客户端shell启动脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">alias</span> sshdlnlp_remote=<span class="hljs-string">&quot;ssh -p 12560 dlnlp@103.44.145.240&quot;</span><br>（别忘用<span class="hljs-built_in">source</span>刷新启动脚本）<br></code></pre></td></tr></table></figure><p>之后就可以在世界各地用一条命令访问你的gpu服务器啦。</p><p>—— 10.19 更新 ——</p><p>放弃花生壳了，改用更稳定并且开源免费的<a href="https://link.zhihu.com/?target=https%3A//github.com/fatedier/frp">frp</a>了，当然这个需要你事先有一台外网服务器，推荐阿里云（学生每月10元）。在你的外网服务器上用frp搭一个反向代理超级容易，傻子都能学会，去frp的github看一下吧～亲测比花生壳好用最少一万倍，并且超级稳定。</p><h2 id="3-文件传输与同步"><a href="#3-文件传输与同步" class="headerlink" title="3. 文件传输与同步"></a>3. 文件传输与同步</h2><p>对于一次性的文件传输，这方面最简单的当然还是直接使用scp命令啦，文件夹和文件都能轻松传输。</p><p>但是我们做深度学习的话，在服务器端大面积改代码、重量级调试的话还是不方便，毕竟服务器上没有图形界面，大部分人还是用不惯vim的，那么能不能在PC端用漂亮的编辑器修改代码，将修改结果实时的同步到服务器端呢？当然可以！这里小夕推荐文件同步神器<strong><a href="https://link.zhihu.com/?target=https%3A//syncthing.net/">syncthing</a></strong>。</p><p>剩下的就是傻瓜式配置啦。<strong>记得要更改文件夹刷新频率哦</strong>（默认是60秒，我们可以改的短一点，比如3秒），这样在客户端我们用漂亮的文本编辑器对代码的改动就能实时的同步到服务器上啦，在服务器端就只需要负责运行就可以咯。</p><h2 id="4-多开发环境管理"><a href="#4-多开发环境管理" class="headerlink" title="4. 多开发环境管理"></a><strong>4. 多开发环境管理</strong></h2><p>如果不幸你的GPU服务器并不是你一个人用，那么这时多人（尤其是混入小白多话）经常把服务器默认的python环境弄的乌烟瘴气，比如有人用python2，有人用python3，有人用tensorflow1.3，有人用0.12等…最后导致大家的程序全跑崩了。</p><p>所以在服务器端管理深度学习的开发环境是极其必要的，这里<strong><a href="https://link.zhihu.com/?target=https%3A//anaconda.org/">anaconda</a></strong>直接搞定！每个人建立和管理自己的开发环境，包括python版本、各种库的版本等，互不干扰。而且在发布project时，也方便直接将环境导出为requirements文件，免得自己去手写啦。</p><h2 id="5-多任务管理（并行调参）"><a href="#5-多任务管理（并行调参）" class="headerlink" title="5. 多任务管理（并行调参）"></a><strong>5. 多任务管理（并行调参）</strong></h2><p>如果你的服务器上有多个GPU，或者你的任务消耗GPU资源不多，那么并行的训练模型调参数是极大提高开发效率的！这里小夕给出几种场景下的常用方案：</p><p>1、比如我们在服务器上除了训练还要接着干别的事情（比如还要捣鼓一下贪吃蛇什么的），那么我们就可以直接将训练任务挂后台。具体如下。</p><p>在linux中，在命令后面加上 <em>&amp;</em> 符号可以将命令在后台执行，为了能看到训练日志，我们当时还需要输出重定向（否则会打印到屏幕上干扰正常工作的），所以比如我们调batchsize参数时可以这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">dlnlp@ubuntu:~$ python train.py --batchsize=16 &gt; log_batch16.txt &amp;<br></code></pre></td></tr></table></figure><p>当然再挂上其他batchsize大小，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">dlnlp@ubuntu:~$ python train.py --batchsize=16 &gt; log_batch16.txt &amp;<br>dlnlp@ubuntu:~$ python train.py --batchsize=64 &gt; log_batch64.txt &amp;<br>dlnlp@ubuntu:~$ python train.py --batchsize=128 &gt; log_batch128.txt &amp;<br></code></pre></td></tr></table></figure><p>通过 <em>jobs</em> 命令可以看到后台任务的运行状况（running、stopped等），通过 <em>bg [任务号]</em> 可以让后台stopped的命令继续running，通过 <em>fg [任务号]</em> 可以让后台的任务来前台执行。对于前台已经执行起来的任务，可以 <em>ctrl+z</em> 来丢进后台（丢后台时stop了的话用bg让其run起来）。</p><blockquote><p>感谢微信用户A Bad Candy在微信订阅号后台留言提醒上面的丢后台方法会在ssh断开连接后进程终止，因此：</p></blockquote><p>如果我们还不希望ssh断开后导致训练任务终止，那么需要再在命令前面加上 <em>nohup 。如：</em></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">dlnlp@ubuntu:~$ nohup python train.py --batchsize=16 &gt; log_batch16.txt &amp;<br></code></pre></td></tr></table></figure><p>2、如果我们特别着急，不仅要并行挂着很多训练任务，而且都要实时的监控它们的训练进展，那么使用 <em>screen</em>命令吧，这个命令就相当于可以让你同时开很多个窗口（就像桌面上那样，你可以开很多应用程序的很多窗口），而且多个窗口之间可以轻松切换，同样这种方法不会因为ssh的断开而停止训练任务。</p><p>具体的操作可以直接在linux下 <em>man screen</em> 来查看screen命令的帮助文档。英文恐惧症的童鞋可以看本文参考文献[1]。</p><h2 id="6-睡觉调参模式（串行调参）"><a href="#6-睡觉调参模式（串行调参）" class="headerlink" title="6. 睡觉调参模式（串行调参）"></a><strong>6. 睡觉调参模式（串行调参）</strong></h2><p>大部分场合下我们没有那么多充裕的GPU可以用，我们一般只能一次挂一个任务，但是我们又有很重的调参任务，那怎么办呢？</p><p>依然很简单啦，首先，装好<strong><a href="https://link.zhihu.com/?target=https%3A//github.com/google/python-fire">python-fire</a></strong>这个工具。</p><p>它可以非常轻松的将你的python程序变成命令行程序，并且可以轻松的将你要调的参数封装成命令行参数的形式。</p><p>然后，写一个调参shell脚本，把你要调的参数全都写进去！比如就像这样：</p><p><img src="https://pic4.zhimg.com/80/v2-65c4751a21d99fc1f642d3446414bf3b_720w.jpg" alt="img"></p><p>（当然别忘在代码里将训练的summary写到某个文件里）</p><p>然后就可以挂上这个脚本去睡觉啦～睡到天亮发现各个最优参数都找到了，超级开心有木有。</p><h2 id="7-关于jupyter-notebook"><a href="#7-关于jupyter-notebook" class="headerlink" title="7. 关于jupyter notebook"></a><strong>7. 关于jupyter notebook</strong></h2><p><strong><a href="https://link.zhihu.com/?target=http%3A//jupyter.org/">jupyter notebook</a></strong>这个神器小夕在历史文章中写过啦，也是一个重量级调参神器！或者直接可以说深度学习神器！在服务器端依然犀利的无可替代，只需要如下的tricks。</p><p>1、服务器端开启jupyter notebook后</p><p><img src="https://pic3.zhimg.com/80/v2-e777f00637e4c3fa6ce06cc1a710416a_720w.jpg" alt="img"></p><p>然后复制最后那一行的 <em>token=xxx</em> ，这个token就是远程访问的密码！同时记下 <em>最后那行显示的端口号</em> 8888（因为如果服务器上同时开多个的话，端口号就不一定是8888了哦），然后去PC端做一个端口映射！即通过ssh隧道来将服务器端的8888端口号映射到本地（PC端）的某个端口（如1234）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh -L 1234:localhost:8888 dlnlp@102.10.60.23<br></code></pre></td></tr></table></figure><p>（这个操作同样可以用于远程监视服务器端tensorboard）</p><p>这时就可以在PC端的浏览器</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">http://localhost:1234<br></code></pre></td></tr></table></figure><p>直接访问服务器上的jupyter notebook啦～当然，访问时会让你输入密码，这时就输入之前记下的那个token哦。</p><p>2、让jupyer notebook跟anaconda开发环境融合。</p><p>默认的情况下jupyter notebook是运行在系统默认环境里的，如果要让它运行在我们自己用ananconda创建的环境中，要进入那个环境中，然后安装 <em>nb_conda</em> 这个库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda install nb_conda<br></code></pre></td></tr></table></figure><p>这时再开启jupyter notebook就能选择在我们这个环境里运行代码啦。</p><h2 id="8-单任务全霸占模式"><a href="#8-单任务全霸占模式" class="headerlink" title="8. 单任务全霸占模式"></a><strong>8. 单任务全霸占模式</strong></h2><p>有时我们的训练任务非常重要且急迫，且绝对不允许被别人挤崩，或者我们明知要占用全部GPU资源了，那么这时我们就可以。。。emmm事先说明，非必要时刻请勿频繁使用哦：</p><p>使用linux中的 <em>run-one</em> 命令，这个命令可以保证同一条命令最多同时运行一个。比如 run-one python xxx 就会只允许运行一个python程序，后来的python程序在这个python程序执行完毕前是得不到执行的（一执行就会出错返回）。所以我们可以写入.bashrc：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">alias</span> python=<span class="hljs-string">&#x27;run-one python&#x27;</span><br></code></pre></td></tr></table></figure><p>（别忘source激活哦）</p><p>这时</p><p><img src="https://pic2.zhimg.com/80/v2-5c6c877a01a7cde6e398854b921253c9_720w.jpg" alt="img"></p><p>看，我通过将第一个python挂到后台了，后面的python完全执行不起来。除非前一个python结束。（所以其他小伙伴可能以为自己的程序出问题了，然后emmm陷入了无尽的困惑）</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/32496193">如何与深度学习服务器优雅的交互？（长期更新）</a></p><p><a href="https://blog.csdn.net/h8178/article/details/78250710">使用Xshell通过堡垒机登陆Linux</a></p><p><a href="http://kaito-kidd.com/2016/04/06/expect/">堡垒机Expect登陆服务器</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux的常用命令</title>
    <link href="/2019/08/11/2019-08-11-linux%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <url>/2019/08/11/2019-08-11-linux%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h1 id="linux-的一些常用命令"><a href="#linux-的一些常用命令" class="headerlink" title="linux 的一些常用命令"></a>linux 的一些常用命令</h1><ul><li><p>文件目录操作</p></li><li><p>日常开关机</p></li><li><p>用户管理</p></li><li><p>搜索命令</p></li><li><p>文本内容搜索</p></li><li><p>任务操作</p></li><li><p>系统管理</p></li><li><p>网络相关</p></li><li><p>ssh远程登录服务器</p></li><li><p>scp远程拷贝文件</p></li><li><p>解压tar文件</p></li><li><p>其他应用</p></li></ul><h2 id="文件目录操作"><a href="#文件目录操作" class="headerlink" title="文件目录操作"></a>文件目录操作</h2><ul><li>文件浏览</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">cat, more, less, head, tail<br></code></pre></td></tr></table></figure><ul><li>ls 显示文件 / 目录属性</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plain">常见参数：<br>-l ：列出长数据串，包含文件的属性与权限数据等  <br>-a ：列出全部的文件，连同隐藏文件（开头为。的文件）一起列出来（常用）  <br>-d ：仅列出目录本身，而不是列出目录的文件数据  <br>-h ：将文件容量以较易读的方式（GB，kB 等）列出来  <br>-R ：连同子目录的内容一起列出（递归列出），等于该目录下的所有文件都会显示出来<br><br>示例： ls -l /tmp<br></code></pre></td></tr></table></figure><ul><li><p>pwd 显示当前路径</p></li><li><p>mkdir 创建目录</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">mkdir  newdir<br>mkdir  -p  newdir/newdir   #递归创建目录<br></code></pre></td></tr></table></figure><ul><li><p>touch 创建文件</p></li><li><p>mv 移动文件 / 重命名</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">mv filename dirname  移动文件<br>mv oldfilename newfilename 对文件重命名<br></code></pre></td></tr></table></figure><ul><li>rm 删除文件</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plain">-i： 交互式<br>-r： 递归<br>-f： 强制<br></code></pre></td></tr></table></figure><ul><li>ln 创建链接</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">ln /etc/passwd passwd  创建硬链接<br>ln -s 创建软链接<br></code></pre></td></tr></table></figure><ul><li>chmod 更改文件权限</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">r=4,w=2,x=1<br>chmod xxx filename<br></code></pre></td></tr></table></figure><h2 id="日常开关机"><a href="#日常开关机" class="headerlink" title="日常开关机"></a>日常开关机</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plain">shutdown<br>-r             关机重启<br>-h             关机不重启<br>now            立刻关机<br></code></pre></td></tr></table></figure><h2 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h2><p>实现用户账号的管理，要完成的工作主要有如下几个方面：</p><ul><li><p>用户账号的添加、删除与修改</p></li><li><p>用户密码的管理</p></li><li><p>用户组的管理</p></li></ul><h3 id="Linux系统用户账号的管理"><a href="#Linux系统用户账号的管理" class="headerlink" title="Linux系统用户账号的管理"></a>Linux系统用户账号的管理</h3><ol><li>添加新的用户账号使用 useradd 命令，其语法如下：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs plain">useradd [-u|-g|-m|-d|-s] username<br><br>其中各选项含义如下：<br><br>-c comment 指定一段注释性描述。<br>-d 目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，可以创建主目录。<br>-g 用户组 指定用户所属的用户组。<br>-G 用户组，用户组 指定用户所属的附加组。<br>-s Shell文件 指定用户的登录Shell。<br>-u 用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号。<br>-m/M：创建家目录 /M 不创建<br><br>举例：<br>添加用户：`useradd -m ningshixian `<br>然后设置密码:  `passwd ningshixian`<br></code></pre></td></tr></table></figure><ol><li>删除用户：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">userdel  -r  用户名<br></code></pre></td></tr></table></figure><ol><li>修改帐号</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">usermod 选项 用户名<br>usermod -s /bin/ksh -d /home/z –g developer sam<br></code></pre></td></tr></table></figure><ol><li>查看用户：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">cat /etc/passwd<br></code></pre></td></tr></table></figure><h3 id="Linux系统用户组的管理"><a href="#Linux系统用户组的管理" class="headerlink" title="Linux系统用户组的管理"></a>Linux系统用户组的管理</h3><ol><li>增加一个新的用户组使用groupadd命令。 其格式如下：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">groupadd 选项 用户组<br></code></pre></td></tr></table></figure><ol><li>如果要删除一个已有的用户组，使用groupdel命令， 其格式如下：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">groupdel 用户组<br></code></pre></td></tr></table></figure><ol><li>修改用户所在组</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plain">强行设置某个用户所在组<br>sermod -g 用户组 用户名```<br><br>把某个用户改为 group(s) <br>usermod -G 用户组 用户名```<br><br>把用户添加进入某个组(s），注意：原来的用户组还存在<br>usermod -a -G 用户组 用户名```<br></code></pre></td></tr></table></figure><ol><li>查看 所有用户 及 所有用户组</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plain">`groups` 查看当前登录用户的组内成员<br><br>`groups 52php` 查看52php用户所在的组，以及组内成员<br><br>`whoami` 查看当前登录用户名 <br><br>/etc/group 文件包含所有组<br><br>/etc/shadow 和 /etc/passwd 系统存在的所有用户名<br></code></pre></td></tr></table></figure><ol><li>查看当前活跃的用户列表：<code>w</code></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">cat /etc/passwd|grep -v nologin|grep -v halt|grep -v shutdown|awk -F&quot;:&quot; &#x27;&#123; print $1&quot;|&quot;$3&quot;|&quot;$4 &#125;&#x27;|more<br></code></pre></td></tr></table></figure><h3 id="权限分配"><a href="#权限分配" class="headerlink" title="权限分配"></a>权限分配</h3><ol><li>用户/用户组管理</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">chown -R nsx:nsx /usr/hadoop/<br></code></pre></td></tr></table></figure><ol><li>让普通用户拥有root的权限:</li></ol><p><a href="https://blog.csdn.net/lglglgl/article/details/46932001">https://blog.csdn.net/lglglgl/article/details/46932001</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plain">以root身份登录，然后输入<br><br>`usermod -g root ningshixian`<br><br>执行完后username即归属于root组了，可以再输入<br><br>`id ningshixian`<br><br>查看输出验证一下，如果看到类似下面的输出：<br><br>`uid=502(ningshixian) gid=0(root) groups=0(root)`<br><br>就表示OK了<br></code></pre></td></tr></table></figure><h2 id="搜索命令"><a href="#搜索命令" class="headerlink" title="搜索命令"></a>搜索命令</h2><ul><li><p>寻找 ls 命令所在位置: <code>which ls</code> </p></li><li><p>寻找特定档案：<code>locate passwd</code> </p></li><li><p>寻找特定档案：<code>find</code> </p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs plain">用法： find [PATH] [option] [action]  <br>find 支持正则匹配  <br>-name   按照文件名查找文件。<br>-perm   按照文件权限来查找文件。<br>-user   按照文件属主来查找文件。<br>-group  按照文件所属的组来查找文件。<br>-mtime -n +n  按照文件的更改时间来查找文件<br>-type  查找某一类型的文件  <br><br>实例：<br><br>find / -name *.txt   <br>find . -name *.pyc exec rm -rf &#123;&#125;\; 找出 pyc 文件并删除<br>find . -type d  查找目录<br></code></pre></td></tr></table></figure><h2 id="文本操作"><a href="#文本操作" class="headerlink" title="文本操作"></a>文本操作</h2><ul><li><p>全文查看 <code>cat/tac</code><br>语法：  <code>cat/tac filename</code> </p></li><li><p>分页查看 <code>less/more</code><br>语法：  <code>less/more filename</code> </p></li><li><p>头尾查看 <code>head/tail</code><br>语法：  <code>head filename</code> </p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plain">常见用法：<br>head -10 /etc/passwd  查看文件前 10 行<br>tail -5 /etc/passwd  查看文件后 5 行<br></code></pre></td></tr></table></figure><ul><li>统计命令 <code>wc</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs plain">wc [-lwm]<br>选项：<br>-l  ：仅列出行；<br>-w  ：仅列出多少字（英文单字)；<br>-m  ：多少字符；<br><br>常见用法<br><br>wc /etc/passwd<br>    50   94   2550 /etc/passwd<br>    行数 单词数 字节数<br>wc -l /etc/passwd   #统计行数很常用<br>wc -w /etc/passwd   #统计单词出现次数<br>wc -m /etc/passwd   #统计文件的字节数<br></code></pre></td></tr></table></figure><ul><li>排序命令 <code>sort</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plain">sort [-bcfMnrtk](源文件)[-o 输出文件』<br><br>常用<br>1 按文本排序<br>sort filename<br>2 按数字排序<br>sort -n<br>3 在输出内容中去除重复行<br>sort -u<br>4 反序  <br>sort -r  <br>5 指定列排序<br>sort -k 列号 -t 分隔符<br></code></pre></td></tr></table></figure><ul><li>文本比较  <code>diff</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plain">diff file1 file2<br>-i 忽略大小写<br>-w 忽略空白字符<br>-b 忽略空格数量的改变<br>-u 统一显示，比较信息，一般用于生成 patch 文件，在 svn 中版本控制很有用<br>diff -u file1 file2 &amp;gt; difference.patch<br></code></pre></td></tr></table></figure><ul><li>查找命令——<code>grep [options]</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plain">　　主要参数<br>　　[options] 主要参数：<br>　　－c：只输出匹配行的计数。<br>　　－I：不区分大 小写（只适用于单字符)。<br>　　－h：查询多文件时不显示文件名。<br>　　－l：查询多文件时只输出包含匹配字符的文件名。<br>　　－n：显示匹配行及 行号。<br>　　－s：不显示不存在或无匹配文本的错误信息。<br>　　－v：显示不包含匹配文本的所有行。<br><br><br>常见使用：<br><br>1 grep root /etc/passwd   匹配文件中有 root 的行<br>2 grep ^root /etc/passwd  匹配文件中以 root 开头的行<br>3 grep -v root /etc/passwd 匹配文件中无 root 的行<br>4 grep -v ^$   test   过滤文件中的空行<br>5 cat xxx.txt | grep -n error<br></code></pre></td></tr></table></figure><ul><li><p>替换 / 查找 / 删除命令——<code>sed</code> </p></li><li><p>强大的文本分析命令——<code>awk</code> </p></li></ul><h2 id="任务操作"><a href="#任务操作" class="headerlink" title="任务操作"></a>任务操作</h2><p>定时任务:  <code>contab</code></p><p>任务调度 &amp; / <code>nohup</code>, 将任务打入后台</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">python test.py&amp;  <br>nohup python test.py&amp;<br></code></pre></td></tr></table></figure><h2 id="系统管理"><a href="#系统管理" class="headerlink" title="系统管理"></a>系统管理</h2><ul><li><p>系统负载：<code>uptime</code> </p></li><li><p>显示内存： <code>free [-b|k|m|g]</code> </p></li><li><p>监控内存：<code>vmstat -t 间隔 -d 次数</code><br>vmstat 是 Linux 中的常用工具，可对操作系统的虚拟内存、进程、CPU 等的整体情况进行监视。 </p></li><li><p>显示所有进程信息：ps </p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plain">ps [option]<br>-A ：所有的进程均显示出来  <br>-a ：不与 terminal 有关的所有进程  <br>-u ：有效用户的相关进程  <br>-x ：一般与 a 连用 可列出较完整的信息  <br>-l ：以长列表形式显示<br>-o ：自定义显示<br><br>常用：<br><br>ps -aux/-ef  显示所有进程信息<br>ps -ax -o pid,%cpu,%mem --sort=-%cpu,-%mem | head -10 显示 cpu 以及内存占用最高的 10 个进程<br></code></pre></td></tr></table></figure><ul><li>性能分析：<code>top</code><br>Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况<br>按进程的CPU使用率排序: 运行top命令后，键入大写P。<br>按进程的内存使用率排序: 运行top命令后，键入大写M。<br><strong>内容解释：</strong><br>第一行是任务队列信息 </li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">13:22:30 up8 min,  4 users,  load average: 0.14, 0.38, 0.25<br>当前时间 系统运行时间 当前登录用户数 系统负载<br></code></pre></td></tr></table></figure><p>第二是进程信息<br>第三行是CPU信息<br>第四五行为内存信息<br>进程信息  </p><ul><li>系统活动情况报告：<code>sar</code><br>Linux 上最为全面的系统性能分析工具之一，可以从多方面对系统的活动进行报告，包括：文件的读写情况、系统调用的使用情况、磁盘 I/O、CPU 效率、内存使用状况、进程活动及 IPC 有关的活 </li></ul><h2 id="网络相关"><a href="#网络相关" class="headerlink" title="网络相关"></a>网络相关</h2><ul><li>查看设置网卡参数：<code>ifconfig</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain">命令格式：<br>ifconfig 『网络设备』 『参数』<br>常用<br>ifconfig eth0 up/down   启用或关闭指定网卡<br>ifconfig eth0      显示网卡信息<br></code></pre></td></tr></table></figure><ul><li>测试网络连通性：<code>ping</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs plain">ping 『参数』 『主机名或 IP 地址』<br>参数：<br>   -d 使用 Socket 的 SO_DEBUG 功能。<br>   -f  极限检测。大量且快速地送网络封包给一台机器，看它的回应。<br>   -n 只输出数值。<br>   -q 不显示任何传送封包的信息，只显示最后的结果。<br>   -r 忽略普通的 Routing Table，直接将数据包送到远端主机上。通常是查看本机的网络接口是否有问题。<br>   -R 记录路由过程。<br>   -v 详细显示指令的执行过程。<br><br>常用<br><br>ping www.163.com<br>ping -R www.163.com<br></code></pre></td></tr></table></figure><ul><li>查看网络状态：<code>netstat</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plain">netstat [option]<br>参数：     <br>   -a： 显示所有<br>   -n： 以 ip 形式显示<br>   -p： 显示进程<br>   -r： 显示路由表<br>   -t： 只显示 tcp<br>   -u： 只显示 udp<br>   -i： 显示网络接口<br><br>常用：<br><br>1 netstat<br>2 netstat -nu 只显示 udp / -t 只显示 tcp<br>3 netstat -r  显示路由表，作用同 route  <br><br>查看 Linux 下一些端口是否打开的命令<br>netstat -an|grep: 端口号<br></code></pre></td></tr></table></figure><ul><li>查看网络状态：<code>ss</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plain">ss  -s  我想查看当前服务器的网络连接统计<br>ss -l   我想查看所有打开的网络端口<br>ss -a   查看这台服务器上所有的 socket 连接<br></code></pre></td></tr></table></figure><ul><li><p>查看设置网卡参数：<code>ip</code> </p></li><li><p>查看路由以及添加路由：<code>route</code> </p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs plain">route [option] [action]<br>参数：<br>   -n 不解析名字<br>   -v 显示详细的处理信息<br>   -f 清除所有网关入口的路由表。<br>   -p 与 add 命令一起使用时使路由具有永久性。<br>   add: 添加一条新路由。<br>   del: 删除一条路由。<br>   -net: 目标地址是一个网络。<br>   -host: 目标地址是一个主机。<br><br>常用<br><br>route -n  显示路由表<br>route add/del default gw 192.168.120.1  删除和添加设置默认网关<br>route add -net 172.25.0.0 netmask 255.255.0.0 dev eth0 添加网关<br></code></pre></td></tr></table></figure><ul><li>查看路由轨迹：<code>traceroute</code><br><code>traceroute</code>追踪网络数据包的路由途径 </li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plain">traceroute[参数](主机)<br>参数：<br>   -I 使用 ICMP 回应取代 UDP 资料信息。<br>   -d 使用 Socket 层级的排错功能。<br>   -s 设置本地主机送出数据包的 IP 地址。<br>   -n 只显示 IP<br><br>常用<br><br>traceroute www.163.com<br>traceroute -n www.163.com  显示 IP 地址，不查主机名<br></code></pre></td></tr></table></figure><h2 id="远程登录服务器"><a href="#远程登录服务器" class="headerlink" title="远程登录服务器"></a>远程登录服务器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">ssh user@localhost<br></code></pre></td></tr></table></figure><h2 id="远程拷贝文件"><a href="#远程拷贝文件" class="headerlink" title="远程拷贝文件"></a>远程拷贝文件</h2><p>用于在 Linux 下进行远程拷贝文件的命令 <code>scp</code></p><p>上传本地文件到远程机器指定目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">scp ~/test/a.out pi@192.168.1.178:/home/pi<br></code></pre></td></tr></table></figure><h2 id="解压-tar-文件"><a href="#解压-tar-文件" class="headerlink" title="解压 tar 文件"></a>解压 tar 文件</h2><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">tar</span> xvf archive_name.tar<br></code></pre></td></tr></table></figure><h1 id="常用问题排查命令"><a href="#常用问题排查命令" class="headerlink" title="常用问题排查命令"></a>常用问题排查命令</h1><ul><li>释放当前被占用端口</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plain">1.  ``` netstat -tln ```  查看系统当前所有被占用端口；<br>2.  ``` lsof -i :9001 ```  看到当前被占用的端口的进程<br>3.  ``` ps id ``` <br>4.  ``` kill -9 id ```  杀掉进程<br></code></pre></td></tr></table></figure><ul><li>根据名称查找进程 id</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">ps -ef | grep Name<br></code></pre></td></tr></table></figure><ul><li>查看某一端口被哪个进程占用 (3 种方式)</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain">netstat -anl | grep &quot;端口号&quot;  ``` <br><br>netstat -tunpl | grep 端口号 ``` <br><br>lsof -i: 端口号<br></code></pre></td></tr></table></figure><ul><li>通过 PID 查看进程完整信息  <code>ll /proc/PID</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain">解释<br>1. cwd 符号链接的是进程运行目录；<br>2. exe 符号连接就是执行程序的绝对路径；<br>3. cmdline 就是程序运行时输入的命令行命令；<br>4. environ 记录了进程运行时的环境变量；<br></code></pre></td></tr></table></figure><ul><li>du: 显示指定的目录或文件所占用的磁盘空间</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plain">df -h                       统计整体磁盘情况<br>du -bsh /usr/               查看指定目录磁盘占用情况<br>du -sh                      查看当前目录总共占的容量<br>du -ah --max-depth=1 /      查看根目录每个文件夹的占用情况<br></code></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.cnblogs.com/52php/p/5677628.html">《Linux 新建用户、用户组，给用户分配权限（chown、useradd、groupadd、userdel、usermod、passwd、groupdel）》</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>超详细 Nginx 极简教程，傻瓜一看也会！</title>
    <link href="/2019/02/03/2019-02-03-Nginx%20%E6%9E%81%E7%AE%80%E6%95%99%E7%A8%8B/"/>
    <url>/2019/02/03/2019-02-03-Nginx%20%E6%9E%81%E7%AE%80%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="Nginx-极简教程"><a href="#Nginx-极简教程" class="headerlink" title="Nginx 极简教程"></a>Nginx 极简教程</h2><ul><li>Nginx代理工具的配置</li><li>Nginx配置实战</li><li>负载均衡配置</li></ul><span id="more"></span><h3 id="Nginx代理工具的配置"><a href="#Nginx代理工具的配置" class="headerlink" title="Nginx代理工具的配置"></a>Nginx代理工具的配置</h3><p>&ensp;&ensp;&ensp;&ensp;nginx作为一个极其重要的代理工具，我们就单独拎出来讨论讨论。</p><h4 id="什么是Nginx"><a href="#什么是Nginx" class="headerlink" title="什么是Nginx?"></a>什么是Nginx?</h4><p>Nginx (engine x) 是一款轻量级的Web 服务器 、反向代理服务器及电子邮件（IMAP/POP3）代理服务器。</p><h4 id="什么是反向代理？"><a href="#什么是反向代理？" class="headerlink" title="什么是反向代理？"></a>什么是反向代理？</h4><p>反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。</p><p><img src="https://img2020.cnblogs.com/other/1218593/202007/1218593-20200714150739594-1361995225.webp" alt="img"></p><h4 id="安装nginx"><a href="#安装nginx" class="headerlink" title="安装nginx"></a>安装nginx</h4><p>nginx官网下载地址：<a href="http://nginx.org/">http://nginx.org</a>，发布版本分为 Linux 和 windows 版本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ sudo apt-get install nginx<br></code></pre></td></tr></table></figure><p>默认情况下，Nginx 会被安装在 /usr/local/nginx</p><h4 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h4><p>基本操作的命令还是要了解一下的<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 快速关闭Nginx，可能不保存相关信息，并迅速终止web服务。</span><br>$ nginx -s stop<br><span class="hljs-comment"># 平稳关闭Nginx，保存相关信息，有安排的结束web服务。</span><br>$ nginx -s quit <br><span class="hljs-comment"># 因改变了Nginx相关配置，需要重新加载配置而重载。</span><br>$ nginx -s reload<br><span class="hljs-comment"># 重新打开日志文件。</span><br>$ nginx -s reopen<br><span class="hljs-comment"># 为 Nginx 指定一个配置文件，来代替缺省的。</span><br>$ nginx -c filename<br><span class="hljs-comment"># 不运行，而仅仅测试配置文件</span><br>$ nginx -t<br><span class="hljs-comment"># 显示 nginx 的版本，编译器版本和配置参数。</span><br>$ nginx -V<br></code></pre></td></tr></table></figure></p><p>如果不想每次都敲命令，可以在nginx安装目录下新添一个启动批处理文件startup.bat，双击即可运行。内容如下：</p><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs dos">@<span class="hljs-built_in">echo</span> off<br><span class="hljs-comment">rem 如果启动前已经启动nginx并记录下pid文件，会kill指定进程</span><br>nginx.exe -s stop<br><span class="hljs-comment"></span><br><span class="hljs-comment">rem 测试配置文件语法正确性</span><br>nginx.exe -t -c conf/nginx.conf<br><span class="hljs-comment"></span><br><span class="hljs-comment">rem 显示版本信息</span><br>nginx.exe -v<br><span class="hljs-comment"></span><br><span class="hljs-comment">rem 按照指定配置去启动nginx</span><br>nginx.exe -c conf/nginx.conf<br></code></pre></td></tr></table></figure><p>如果是运行在 Linux 下，写一个 shell 脚本，大同小异。</p><h3 id="nginx-配置实战"><a href="#nginx-配置实战" class="headerlink" title="nginx 配置实战"></a>nginx 配置实战</h3><p>我始终认为，各种开发工具的配置还是结合实战来讲述，会让人更易理解。</p><h4 id="http反向代理配置"><a href="#http反向代理配置" class="headerlink" title="http反向代理配置"></a>http反向代理配置</h4><p>我们先实现一个小目标：不考虑复杂的配置，仅仅是完成一个 http 反向代理。</p><h4 id="nginx-conf-配置文件如下："><a href="#nginx-conf-配置文件如下：" class="headerlink" title="nginx.conf 配置文件如下："></a>nginx.conf 配置文件如下：</h4><p>注：conf / nginx.conf 是 nginx 的默认配置文件。你也可以使用 nginx -c 指定你的配置文件</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">#运行用户</span><br><span class="hljs-comment">#user somebody;</span><br><br><span class="hljs-comment">#启动进程,通常设置成和cpu的数量相等</span><br>worker_processes  <span class="hljs-number">1</span>;<br><br><span class="hljs-comment">#全局错误日志</span><br>error_log  D:<span class="hljs-regexp">/Tools/</span>nginx-<span class="hljs-number">1.10</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/logs/</span>error.log;<br>error_log  D:<span class="hljs-regexp">/Tools/</span>nginx-<span class="hljs-number">1.10</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/logs/</span>notice.log  notice;<br>error_log  D:<span class="hljs-regexp">/Tools/</span>nginx-<span class="hljs-number">1.10</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/logs/i</span>nfo.log  info;<br><br><span class="hljs-comment">#PID文件，记录当前启动的nginx的进程ID</span><br>pid        D:<span class="hljs-regexp">/Tools/</span>nginx-<span class="hljs-number">1.10</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/logs/</span>nginx.pid;<br><br><span class="hljs-comment">#工作模式及连接数上限</span><br>events &#123;<br>   worker_connections <span class="hljs-number">1024</span>;    <span class="hljs-comment">#单个后台worker process进程的最大并发链接数</span><br>&#125;<br><br><span class="hljs-comment">#设定http服务器，利用它的反向代理功能提供负载均衡支持</span><br>http &#123;<br>   <span class="hljs-comment">#设定mime类型(邮件支持类型),类型由mime.types文件定义</span><br>   include       D:<span class="hljs-regexp">/Tools/</span>nginx-<span class="hljs-number">1.10</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/conf/mim</span>e.types;<br>   default_type  application/octet-stream;<br><br>   <span class="hljs-comment">#设定日志</span><br>   log_format  main  <span class="hljs-string">&#x27;\[$remote_addr\] \- \[$remote_user\] \[$time_local\] &quot;$request&quot; &#x27;</span><br>                     <span class="hljs-string">&#x27;$status $body\_bytes\_sent &quot;$http_referer&quot; &#x27;</span><br>                     <span class="hljs-string">&#x27;&quot;$http\_user\_agent&quot; &quot;$http\_x\_forwarded_for&quot;&#x27;</span>;<br><br>   access_log    D:<span class="hljs-regexp">/Tools/</span>nginx-<span class="hljs-number">1.10</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/logs/</span>access.log main;<br>   rewrite_log     on;<br><br>   <span class="hljs-comment">#sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，对于普通应用，</span><br>   <span class="hljs-comment">#必须设为 on,如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，以平衡磁盘与网络I/O处理速度，降低系统的uptime.</span><br>   sendfile        on;<br>   <span class="hljs-comment">#tcp_nopush     on;</span><br><br>   <span class="hljs-comment">#连接超时时间</span><br>   keepalive_timeout  <span class="hljs-number">120</span>;<br>   tcp_nodelay        on;<br><br>   <span class="hljs-comment">#gzip压缩开关</span><br>   <span class="hljs-comment">#gzip  on;</span><br><br>   <span class="hljs-comment">#设定实际的服务器列表</span><br>   upstream zp_server1&#123;<br>       server <span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">8089</span>;<br>   &#125;<br><br>   <span class="hljs-comment">#HTTP服务器</span><br>   server &#123;<br>       <span class="hljs-comment">#监听80端口，80端口是知名端口号，用于HTTP协议</span><br>       listen       <span class="hljs-number">80</span>;<br><br>       <span class="hljs-comment">#定义使用www.xx.com访问</span><br>       server_name  www.javastack.cn;<br><br>       <span class="hljs-comment">#首页</span><br>       index index.html<br><br>       <span class="hljs-comment">#指向webapp的目录</span><br>       root D:_WorkspaceProjectgithubzpSpringNotesspring-securityspring-shirosrcmainwebapp;<br><br>       <span class="hljs-comment">#编码格式</span><br>       charset utf-<span class="hljs-number">8</span>;<br><br>       <span class="hljs-comment">#代理配置参数</span><br>       proxy\_connect\_timeout <span class="hljs-number">180</span>;<br>       proxy\_send\_timeout <span class="hljs-number">180</span>;<br>       proxy\_read\_timeout <span class="hljs-number">180</span>;<br>       proxy\_set\_header Host <span class="hljs-variable">$host</span>;<br>       proxy\_set\_header X-Forwarder-For <span class="hljs-variable">$remote_addr</span>;<br><br>       <span class="hljs-comment">#反向代理的路径（和upstream绑定），location 后面设置映射的路径</span><br>       location / &#123;<br>           proxy_pass http:<span class="hljs-regexp">//</span>zp_server1;<br>       &#125;<br><br>       <span class="hljs-comment">#静态文件，nginx自己处理</span><br>       location ~ ^<span class="hljs-regexp">/(images|javascript|js|css|flash|media|static)/</span> &#123;<br>           root D:_WorkspaceProjectgithubzpSpringNotesspring-securityspring-shirosrcmainwebappiews;<br>           <span class="hljs-comment">#过期30天，静态文件不怎么更新，过期可以设大一点，如果频繁更新，则可以设置得小一点。</span><br>           expires <span class="hljs-number">30</span>d;<br>       &#125;<br><br>       <span class="hljs-comment">#设定查看Nginx状态的地址</span><br>       location /NginxStatus &#123;<br>           stub_status           on;<br>           access_log            on;<br>           auth_basic            <span class="hljs-string">&quot;NginxStatus&quot;</span>;<br>           auth\_basic\_user_file  conf/htpasswd;<br>       &#125;<br><br>       <span class="hljs-comment">#禁止访问 .htxxx 文件</span><br>       location ~ /.ht &#123;<br>           deny all;<br>       &#125;<br><br>       <span class="hljs-comment">#错误处理页面（可选择性配置）</span><br>       <span class="hljs-comment">#error_page   404              /404.html;</span><br>       <span class="hljs-comment">#error_page   500 502 503 504  /50x.html;</span><br>       <span class="hljs-comment">#location = /50x.html &#123;</span><br>       \<span class="hljs-comment">#    root   html;</span><br>       <span class="hljs-comment">#&#125;</span><br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>好了，让我们来试试吧：</p><p>启动 webapp，注意启动绑定的端口要和nginx中的 upstream 设置的端口保持一致。</p><p>更改 host：</p><p>在 C:Windows\System32\drivers\etc 目录下的host文件中添加一条DNS 记录127.0.0.1 www.javastack.cn 启动前文中 startup.bat 的命令</p><p>在浏览器中访问 www.javastack.cn，不出意外，已经可以访问了。</p><h3 id="负载均衡配置"><a href="#负载均衡配置" class="headerlink" title="负载均衡配置"></a>负载均衡配置</h3><p>上一个例子中，代理仅仅指向一个服务器。</p><p>但是，网站在实际运营过程中，多半都是有多台服务器运行着同样的app，这时需要使用负载均衡来分流。</p><p>nginx也可以实现简单的负载均衡功能。</p><p>假设这样一个应用场景：将应用部署在 192.168.1.11:80、192.168.1.12:80、192.168.1.13:80 三台linux环境的服务器上。网站域名叫 www.javastack.cn，公网IP为 192.168.1.11。在公网IP所在的服务器上部署 nginx，对所有请求做负载均衡处理。</p><p>nginx.conf 配置如下：</p><figure class="highlight puppet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs puppet"><span class="hljs-keyword">http</span> &#123;<br>    <span class="hljs-comment">#设定mime类型,类型由mime.type文件定义</span><br>   <span class="hljs-literal">include</span>       /etc/nginx/mime.types;<br>   default_type  application/octet-stream;<br>   <span class="hljs-comment">#设定日志格式</span><br>   access_log    /var/log/nginx/access.log;<br><br>   <span class="hljs-comment">#设定负载均衡的服务器列表</span><br>   upstream load\_balance\_server &#123;<br>       <span class="hljs-comment">#weigth参数表示权值，权值越高被分配到的几率越大</span><br>       server <span class="hljs-number">192.168</span>.<span class="hljs-number">1.11</span>:<span class="hljs-number">80</span>   weight=<span class="hljs-number">5</span>;<br>       server <span class="hljs-number">192.168</span>.<span class="hljs-number">1.12</span>:<span class="hljs-number">80</span>   weight=<span class="hljs-number">1</span>;<br>       server <span class="hljs-number">192.168</span>.<span class="hljs-number">1.13</span>:<span class="hljs-number">80</span>   weight=<span class="hljs-number">6</span>;<br>   &#125;<br><br>  #HTTP服务器<br>  <span class="hljs-keyword">server</span> &#123;<br>       <span class="hljs-comment">#侦听80端口</span><br>       listen       <span class="hljs-number">80</span>;<br><br>       <span class="hljs-comment">#定义使用www.xx.com访问</span><br>       server_name  www.javastack.cn;<br><br>       <span class="hljs-comment">#对所有请求进行负载均衡请求</span><br>       location / &#123;<br>           <span class="hljs-literal">root</span>        /<span class="hljs-literal">root</span>;                 <span class="hljs-comment">#定义服务器的默认网站根目录位置</span><br>           index       index.html index.htm;  <span class="hljs-comment">#定义首页索引文件的名称</span><br>           proxy_pass  http://load\_balance\_server ;<span class="hljs-comment">#请求转向load\_balance\_server 定义的服务器列表</span><br><br>           <span class="hljs-comment">#以下是一些反向代理的配置(可选择性配置)</span><br>           <span class="hljs-comment">#proxy_redirect off;</span><br>           <span class="hljs-literal">proxy</span>\_set\_header Host <span class="hljs-variable">$host</span>;<br>           <span class="hljs-literal">proxy</span>\_set\_header X-Real-IP <span class="hljs-variable">$remote_addr</span>;<br>           <span class="hljs-comment">#后端的Web服务器可以通过X-Forwarded-For获取用户真实IP</span><br>           <span class="hljs-literal">proxy</span>\_set\_header X-Forwarded-For <span class="hljs-variable">$remote_addr</span>;<br>           <span class="hljs-literal">proxy</span>\_connect\_timeout <span class="hljs-number">90</span>;          <span class="hljs-comment">#nginx跟后端服务器连接超时时间(代理连接超时)</span><br>           <span class="hljs-literal">proxy</span>\_send\_timeout <span class="hljs-number">90</span>;             <span class="hljs-comment">#后端服务器数据回传时间(代理发送超时)</span><br>           <span class="hljs-literal">proxy</span>\_read\_timeout <span class="hljs-number">90</span>;             <span class="hljs-comment">#连接成功后，后端服务器响应时间(代理接收超时)</span><br>           <span class="hljs-literal">proxy</span>\_buffer\_size <span class="hljs-number">4</span>k;              <span class="hljs-comment">#设置代理服务器（nginx）保存用户头信息的缓冲区大小</span><br>           proxy_buffers <span class="hljs-number">4</span> <span class="hljs-number">32</span>k;               <span class="hljs-comment">#proxy_buffers缓冲区，网页平均在32k以下的话，这样设置</span><br>           <span class="hljs-literal">proxy</span>\_busy\_buffers_size <span class="hljs-number">64</span>k;       <span class="hljs-comment">#高负荷下缓冲大小（proxy_buffers*2）</span><br>           <span class="hljs-literal">proxy</span>\_temp\_file\_write\_size <span class="hljs-number">64</span>k;    <span class="hljs-comment">#设定缓存文件夹大小，大于这个值，将从upstream服务器传</span><br><br>           client\_max\_body_size <span class="hljs-number">10</span>m;          <span class="hljs-comment">#允许客户端请求的最大单文件字节数</span><br>           client\_body\_buffer_size <span class="hljs-number">128</span>k;      <span class="hljs-comment">#缓冲区代理缓冲用户端请求的最大字节数</span><br>       &#125;<br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="网站有多个webapp的配置"><a href="#网站有多个webapp的配置" class="headerlink" title="网站有多个webapp的配置"></a>网站有多个webapp的配置</h4><p>当一个网站功能越来越丰富时，往往需要将一些功能相对独立的模块剥离出来，独立维护。这样的话，通常，会有多个 webapp。</p><p>举个例子：假如 www.javastack.cn 站点有好几个webapp，finance（金融）、product（产品）、admin（用户中心）。访问这些应用的方式通过上下文(context)来进行区分:</p><ul><li>www.javastack.cn/finance/</li><li>www.javastack.cnproduct/</li><li>www.javastack.cn/admin/</li></ul><p>我们知道，http的默认端口号是80，如果在一台服务器上同时启动这3个 webapp 应用，都用80端口，肯定是不成的。所以，这三个应用需要分别绑定不同的端口号。</p><p>那么，问题来了，用户在实际访问 www.javastack.cn 站点时，访问不同 webapp，总不会还带着对应的端口号去访问吧。所以，你再次需要用到反向代理来做处理。</p><p>配置也不难，来看看怎么做吧：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-section">http</span> &#123;<br>   <span class="hljs-comment">#此处省略一些基本配置</span><br><br>   <span class="hljs-section">upstream</span> product_server&#123;<br>       <span class="hljs-attribute">server</span> www.javastack.cn:<span class="hljs-number">8081</span>;<br>   &#125;<br><br>   <span class="hljs-section">upstream</span> admin_server&#123;<br>       <span class="hljs-attribute">server</span> www.javastack.cn:<span class="hljs-number">8082</span>;<br>   &#125;<br><br>   <span class="hljs-section">upstream</span> finance_server&#123;<br>       <span class="hljs-attribute">server</span> www.javastack.cn:<span class="hljs-number">8083</span>;<br>   &#125;<br><br>   <span class="hljs-section">server</span> &#123;<br>       <span class="hljs-comment">#此处省略一些基本配置</span><br>       <span class="hljs-comment">#默认指向product的server</span><br>       <span class="hljs-section">location</span> / &#123;<br>           <span class="hljs-attribute">proxy_pass</span> http://product_server;<br>       &#125;<br><br>       <span class="hljs-section">location</span> /product/&#123;<br>           <span class="hljs-attribute">proxy_pass</span> http://product_server;<br>       &#125;<br><br>       <span class="hljs-section">location</span> /admin/ &#123;<br>           <span class="hljs-attribute">proxy_pass</span> http://admin_server;<br>       &#125;<br><br>       <span class="hljs-section">location</span> /finance/ &#123;<br>           <span class="hljs-attribute">proxy_pass</span> http://finance_server;<br>       &#125;<br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="https反向代理配置"><a href="#https反向代理配置" class="headerlink" title="https反向代理配置"></a>https反向代理配置</h4><p>一些对安全性要求比较高的站点，可能会使用 HTTPS（一种使用ssl通信标准的安全HTTP协议）。</p><p>这里不科普 HTTP 协议和 SSL 标准。但是，使用 nginx 配置 https 需要知道几点：</p><ul><li>HTTPS 的固定端口号是 443，不同于 HTTP 的 80 端口</li><li>SSL 标准需要引入安全证书，所以在 nginx.conf 中你需要指定证书和它对应的 key</li></ul><p>其他和 http 反向代理基本一样，只是在 Server 部分配置有些不同。</p><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-meta">#HTTP服务器</span><br> server &#123;<br>     <span class="hljs-meta">#监听443端口。443为知名端口号，主要用于HTTPS协议</span><br>     listen       <span class="hljs-number">443</span> ssl;<br><br>     <span class="hljs-meta">#定义使用www.xx.com访问</span><br>     server_name  www.javastack.cn;<br><br>     <span class="hljs-meta">#ssl证书文件位置(常见证书文件格式为：crt/pem)</span><br>     ssl_certificate      cert.pem;<br>     <span class="hljs-meta">#ssl证书key位置</span><br>     ssl\_certificate\_key  cert.key;<br><br>     <span class="hljs-meta">#ssl配置参数（选择性配置）</span><br>     ssl\_session\_cache    <span class="hljs-keyword">shared</span>:SSL:<span class="hljs-number">1</span>m;<br>     ssl\_session\_timeout  <span class="hljs-number">5</span>m;<br>     <span class="hljs-meta">#数字签名，此处使用MD5</span><br>     ssl_ciphers  HIGH:!aNULL:!MD5;<br>     ssl\_prefer\_server_ciphers  on;<br><br>     <span class="hljs-keyword">location</span> / &#123;<br>         root   /root;<br>         <span class="hljs-keyword">index</span>  <span class="hljs-keyword">index</span>.html <span class="hljs-keyword">index</span>.htm;<br>     &#125;<br> &#125;<br></code></pre></td></tr></table></figure><h4 id="静态站点配置"><a href="#静态站点配置" class="headerlink" title="静态站点配置"></a>静态站点配置</h4><p>有时候，我们需要配置静态站点(即 html 文件和一堆静态资源)。</p><p>举例来说：如果所有的静态资源都放在了 /app/dist 目录下，我们只需要在 nginx.conf 中指定首页以及这个站点的 host 即可。</p><p>配置如下：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">worker_processes</span>  <span class="hljs-number">1</span>;<br><br><span class="hljs-section">events</span> &#123;<br>   <span class="hljs-attribute">worker_connections</span>  <span class="hljs-number">1024</span>;<br>&#125;<br><br><span class="hljs-section">http</span> &#123;<br>   <span class="hljs-attribute">include</span>       mime.types;<br>   <span class="hljs-attribute">default_type</span>  application/octet-stream;<br>   <span class="hljs-attribute">sendfile</span>        <span class="hljs-literal">on</span>;<br>   <span class="hljs-attribute">keepalive_timeout</span>  <span class="hljs-number">65</span>;<br><br>   <span class="hljs-attribute">gzip</span> <span class="hljs-literal">on</span>;<br>   <span class="hljs-attribute">gzip_types</span> text/plain application/x-javascript text/css application/xml text/javascript application/javascript image/jpeg image/gif image/png;<br>   <span class="hljs-attribute">gzip_vary</span> <span class="hljs-literal">on</span>;<br><br>   <span class="hljs-section">server</span> &#123;<br>       <span class="hljs-attribute">listen</span>       <span class="hljs-number">80</span>;<br>       <span class="hljs-attribute">server_name</span>  static.zp.cn;<br><br>       <span class="hljs-section">location</span> / &#123;<br>           <span class="hljs-attribute">root</span> /app/dist;<br>           <span class="hljs-attribute">index</span> index.html;<br>           <span class="hljs-comment">#转发任何请求到 index.html</span><br>       &#125;<br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>然后，添加 HOST：</p><p>127.0.0.1 static.zp.cn，此时，在本地浏览器访问 static.zp.cn ，就可以访问静态站点了。</p><h4 id="跨域解决方案"><a href="#跨域解决方案" class="headerlink" title="跨域解决方案"></a>跨域解决方案</h4><p>web 领域开发中，经常采用前后端分离模式。这种模式下，前端和后端分别是独立的 web 应用程序，例如：后端是 Java 程序，前端是 React 或 Vue 应用，更多请看这篇文章《<a href="http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&amp;mid=2247485898&amp;idx=1&amp;sn=32626e4f241d5f04684736b52afafb10&amp;chksm=eb538cfcdc2405ea368fa092c0af8e424ad843198274426cca275acc8ed83d86497ec3a54acc&amp;scene=21#wechat_redirect">到底什么是跨域，及解决方案</a>》。</p><p>各自独立的 web app 在互相访问时，势必存在跨域问题。解决跨域问题一般有两种思路：</p><h5 id="CORS"><a href="#CORS" class="headerlink" title="CORS"></a>CORS</h5><p>在后端服务器设置 HTTP 响应头，把你需要运行访问的域名加入加入 Access-Control-Allow-Origin 中。</p><h5 id="jsonp"><a href="#jsonp" class="headerlink" title="jsonp"></a>jsonp</h5><p>把后端根据请求，构造json数据，并返回，前端用 jsonp 跨域。</p><p>这两种思路，本文不展开讨论。</p><p>需要说明的是，nginx 根据第一种思路，也提供了一种解决跨域的解决方案。</p><p>举例：www.javastack.cn 网站是由一个前端 app ，一个后端 app 组成的。前端端口号为 9000， 后端端口号为 8080。</p><p>前端和后端如果使用 http 进行交互时，请求会被拒绝，因为存在跨域问题。来看看，nginx 是怎么解决的吧：</p><p>首先，在 enable-cors.conf 文件中设置 cors ：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-comment"># allow origin list</span><br><span class="hljs-attribute">set</span> <span class="hljs-variable">$ACAO</span> <span class="hljs-string">&#x27;*&#x27;</span>;<br><br><span class="hljs-comment"># set single origin</span><br><span class="hljs-attribute">if</span> (<span class="hljs-variable">$http_origin</span> ~\* (www.javastack.cn)$) &#123;<br> <span class="hljs-attribute">set</span> <span class="hljs-variable">$ACAO</span> <span class="hljs-variable">$http_origin</span>;<br>&#125;<br><br><span class="hljs-attribute">if</span> (<span class="hljs-variable">$cors</span> = <span class="hljs-string">&quot;trueget&quot;</span>) &#123;<br>   <span class="hljs-attribute">add_header</span> <span class="hljs-string">&#x27;Access-Control-Allow-Origin&#x27;</span> <span class="hljs-string">&quot;<span class="hljs-variable">$http_origin</span>&quot;</span>;<br>   <span class="hljs-attribute">add_header</span> <span class="hljs-string">&#x27;Access-Control-Allow-Credentials&#x27;</span> <span class="hljs-string">&#x27;true&#x27;</span>;<br>   <span class="hljs-attribute">add_header</span> <span class="hljs-string">&#x27;Access-Control-Allow-Methods&#x27;</span> <span class="hljs-string">&#x27;GET, POST, OPTIONS&#x27;</span>;<br>   <span class="hljs-attribute">add_header</span> <span class="hljs-string">&#x27;Access-Control-Allow-Headers&#x27;</span> <span class="hljs-string">&#x27;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&#x27;</span>;<br>&#125;<br><br><span class="hljs-attribute">if</span> (<span class="hljs-variable">$request_method</span> = <span class="hljs-string">&#x27;OPTIONS&#x27;</span>) &#123;<br> <span class="hljs-attribute">set</span> <span class="hljs-variable">$cors</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;cors&#125;</span>options&quot;</span>;<br>&#125;<br><br><span class="hljs-attribute">if</span> (<span class="hljs-variable">$request_method</span> = <span class="hljs-string">&#x27;GET&#x27;</span>) &#123;<br> <span class="hljs-attribute">set</span> <span class="hljs-variable">$cors</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;cors&#125;</span>get&quot;</span>;<br>&#125;<br><br><span class="hljs-attribute">if</span> (<span class="hljs-variable">$request_method</span> = <span class="hljs-string">&#x27;POST&#x27;</span>) &#123;<br> <span class="hljs-attribute">set</span> <span class="hljs-variable">$cors</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;cors&#125;</span>post&quot;</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>接下来，在你的服务器中 include enable-cors.conf 来引入跨域配置：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-comment"># ----------------------------------------------------</span><br><span class="hljs-comment"># 此文件为项目 nginx 配置片段</span><br><span class="hljs-comment"># 可以直接在 nginx config 中 include（推荐）</span><br><span class="hljs-comment"># 或者 copy 到现有 nginx 中，自行配置</span><br><span class="hljs-comment"># www.javastack.com 域名需配合 dns hosts 进行配置</span><br><span class="hljs-comment"># 其中，api 开启了 cors，需配合本目录下另一份配置文件</span><br><span class="hljs-comment"># ----------------------------------------------------</span><br><span class="hljs-section">upstream</span> front_server&#123;<br> <span class="hljs-attribute">server</span> www.javastack.cn:<span class="hljs-number">9000</span>;<br>&#125;<br><span class="hljs-section">upstream</span> api_server&#123;<br> <span class="hljs-attribute">server</span> www.javastack.cn:<span class="hljs-number">8080</span>;<br>&#125;<br><br><span class="hljs-section">server</span> &#123;<br> <span class="hljs-attribute">listen</span>       <span class="hljs-number">80</span>;<br> <span class="hljs-attribute">server_name</span>  www.javastack.cn;<br><br> <span class="hljs-section">location</span> <span class="hljs-regexp">~ ^/api/</span> &#123;<br>   <span class="hljs-attribute">include</span> enable-cors.conf;<br>   <span class="hljs-attribute">proxy_pass</span> http://api_server;<br>   <span class="hljs-attribute">rewrite</span> <span class="hljs-string">&quot;^/api/(.*)$&quot;</span> /<span class="hljs-variable">$1</span> <span class="hljs-literal">break</span>;<br> &#125;<br><br> <span class="hljs-section">location</span> <span class="hljs-regexp">~ ^/</span> &#123;<br>   <span class="hljs-attribute">proxy_pass</span> http://front_server;<br> &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>到此，就完成了。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.cnblogs.com/javastack/p/13299177.html">https://www.cnblogs.com/javastack/p/13299177.html</a></p>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ubuntu</tag>
      
      <tag>server</tag>
      
      <tag>配置</tag>
      
      <tag>docker</tag>
      
      <tag>nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>傅里叶变换</title>
    <link href="/2019/01/08/2019-01-08-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/"/>
    <url>/2019/01/08/2019-01-08-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/</url>
    
    <content type="html"><![CDATA[<h3 id="给傻子写的傅里叶变换博文"><a href="#给傻子写的傅里叶变换博文" class="headerlink" title="给傻子写的傅里叶变换博文"></a>给傻子写的傅里叶变换博文</h3><p>傅里叶分析不仅仅是一个数学工具，更是一种可以彻底颠覆一个人以前世界观的思维模式。但不幸的是，傅里叶分析的公式看起来太复杂了，所以很多大一新生上来就懵圈并从此对它深恶痛绝。老实说，这么有意思的东西居然成了大学里的杀手课程，不得不归咎于编教材的人实在是太严肃了。（您把教材写得好玩一点会死吗？会死吗？）所以我一直想写一个有意思的文章来解释傅里叶分析，有可能的话高中生都能看懂的那种。</p><p>所以，不管读到这里的您从事何种工作，我保证您都能看懂，并且一定将体会到通过傅里叶分析看到世界另一个样子时的快感。至于对于已经有一定基础的朋友，也希望不要看到会的地方就急忙往后翻，仔细读一定会有新的发现。</p><p><a href="http://blog.jobbole.com/70549/">如果看了此文你还不懂傅里叶变换，那就过来掐死我吧【完整版】</a></p><span id="more"></span>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>个人秋招笔面试经验整理</title>
    <link href="/2019/01/03/2019-01-03-%E4%B8%AA%E4%BA%BA%E7%A7%8B%E6%8B%9B%E7%AC%94%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C%E6%95%B4%E7%90%86/"/>
    <url>/2019/01/03/2019-01-03-%E4%B8%AA%E4%BA%BA%E7%A7%8B%E6%8B%9B%E7%AC%94%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="一个码农的秋招笔面试经验整理"><a href="#一个码农的秋招笔面试经验整理" class="headerlink" title="一个码农的秋招笔面试经验整理"></a>一个码农的秋招笔面试经验整理</h2><h3 id="个人背景"><a href="#个人背景" class="headerlink" title="个人背景"></a>个人背景</h3><ul><li>普通本科，985硕士</li><li>计算机科学与技术专业</li><li>本科期间只做过一些app开发和网站开发的工作</li><li>读研期间主要研究方向是自然语言处理，有一些实验室项目</li><li>除此之外，无任何比赛获奖和实习经历</li></ul><h3 id="投递方向"><a href="#投递方向" class="headerlink" title="投递方向"></a>投递方向</h3><p>国内的NLP方向的岗位一般叫做NLP研究员/算法工程师/工程师</p><h3 id="一点点经验总结"><a href="#一点点经验总结" class="headerlink" title="一点点经验总结"></a>一点点经验总结</h3><ul><li><p>校招是社会和企业为从未步入职场的同学特意准备的一个绿色通道。在这个通道里，我们将候选人当做一张白纸，可以没有经验、没有行业常识，只要对工作充满热情、有还算聪明的头脑，基本都算是合格的。企业会为这些同学准备培训，并留足适应期，有的甚至还会安排一对一的老员工来带。</p></li><li><p>项目很重要</p></li></ul><span id="more"></span><h3 id="投递公司心路历程："><a href="#投递公司心路历程：" class="headerlink" title="投递公司心路历程："></a>投递公司心路历程：</h3><ul><li>8月底-9月初：只投递BATJ、TMD、网易这种知名的大厂，以及自己感兴趣的方向</li><li><script type="math/tex">9月初-9月底</script>：经历N多次失败后，开始眼界放低，开启互联网领域广撒网模式，互联网巨头+独角兽公司+新兴企业，开始大批量投递</li><li>10月初：国庆回家休息了7天。秋招基本颗粒无收+全程崩溃，感觉是受挫到不行，一度觉得找不到工作了%&gt;_&lt;%</li><li>10月中旬-11月中旬：开始想传统行业巨头（伊利、蒙牛）、移动、房地产开始投递。</li></ul><h3 id="Research-or-Product"><a href="#Research-or-Product" class="headerlink" title="Research or Product ?"></a>Research or Product ?</h3><ul><li>想做research，有不错的论文或比赛经历，对某个方向研究的比较深入，但做系统的经历相对匮乏，则偏research的部门可能更赏识你；</li><li>想做产品，并且做系统、啪代码的能力不错，论文也刷了不少但是科研热情不高，则你可能跟业务部门更match。</li></ul><h4 id="偏research-通用平台的："><a href="#偏research-通用平台的：" class="headerlink" title="偏research/通用平台的："></a>偏research/通用平台的：</h4><ul><li>百度：AIG NLP部（以前的IDL的NLP团队合并进去了，方向全覆盖没弱项，王海峰和吴华两位巨佬坐镇，没争议的国内最强NLP团队（忽略MSRA的话））</li><li>阿里：达摩院（以前的idst合并进去了，主要做NLP基础技术，近年来SQuAD也刷的风生水起，不过对话方向貌似在小蜜）</li><li>腾讯：AI lab（属于TEG事业群，论文高产，听说最近也在努力跟业务结合）</li><li>网易游戏：伏羲AI实验室（新成立不久的，不过终面时感觉面试官实力挺赞）</li><li>京东：AI lab（感觉方向有点对标阿里，团队实力不错的）</li><li>滴滴：AI lab（感觉滴滴还是DM领域比较厉害，NLP领域业务不多，主要是语言模型，检索和翻译，不过package挺给力的）</li><li>商汤：商汤研究院（没错，商汤研究院也有NLP，不过当然没有CV做的好）</li></ul><h4 id="偏业务-产品的："><a href="#偏业务-产品的：" class="headerlink" title="偏业务/产品的："></a>偏业务/产品的：</h4><ul><li>百度：度秘部（高T云集，百度主战略DuerOS所在部门，NLP实力没得质疑）、智能客服部（听说团队拿过百度最高奖的百万美金，技术落地to B业务没得说）</li><li>阿里：智能服务部（阿里小蜜，杭州的X lab实力派，北京的Y lab也前景不错）、AI labs（比较新，主要做产品，比如天猫精灵）</li><li>腾讯：微信事业群（北京的模式识别中心应该是腾讯内部AI做的最靠谱的部门了，有微信这种10亿用户的业务，感觉NLP想做的烂都难）</li><li>网易：有道事业部（当时面试时间冲突放弃了，不过有道NLP产品这么成功，想必也不会差）</li></ul><h3 id="网申（NLP算法工程师）"><a href="#网申（NLP算法工程师）" class="headerlink" title="网申（NLP算法工程师）"></a>网申（NLP算法工程师）</h3><blockquote><p>把握提前批，虽然比较难，但是相当于多一次机会；(很多核心部门的NLP岗的hc在提前批就用光了)<br>尽量寻找内推，一般可以直接进笔试环节</p></blockquote><ul><li>途家 没消息</li><li>顺丰 做完性格测试没消息</li><li>唯品会 没消息</li><li>网易有道 没消息</li><li>新浪 没消息</li><li>伊利 没消息</li><li>流利说 没消息</li><li>Oppo 没消息</li><li>快手 没消息</li><li>腾讯内推 没消息</li><li>微策略 没消息</li></ul><hr><ul><li>小米 投递内推成功，笔试挂</li><li>商汤科技 听说很难，没做笔试</li><li>科大讯飞 笔试挂</li><li>网易（NLP算法工程师） 挂</li><li>作业帮 提前批一面挂 网申笔试挂</li><li>深信服 （笔试挂，记得是只能用C++，不会）</li><li>猎豹移动 宣讲会现场笔试挂</li><li>爱奇艺 笔试挂</li><li>招行网络科技 笔试错过了</li><li>携程 笔试挂</li><li>美团 笔试挂</li><li>平安科技 笔试挂</li><li>瓜子二手车 笔试挂</li><li>马蜂窝 现场宣讲笔试挂</li><li>搜狗 笔试挂</li><li>58 笔试挂</li><li><p>多益网络 笔试挂，还要下载多益专用的软件，体验不好<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fytkbko0m4j30gq06074w.jpg" alt=""></p></li><li><p>小红书 笔试挂</p></li><li>Vipkid 现场宣讲笔试挂(我记得宣讲抽奖环节抽中一个U盘)</li><li>知乎 笔试挂</li><li>贝壳 现场宣讲笔试挂（贝壳今年给的很高，投递很晚，有点可惜）</li><li>CVTE 笔试挂</li><li>去哪儿 笔试挂</li><li>华润集团 笔试挂</li><li>途家 笔试挂</li><li>阿里巴巴 笔试挂（太难）</li><li>滴滴 笔试挂</li><li>B站 笔试挂</li><li>拼多多 笔试挂</li><li>迅雷 笔试挂</li></ul><hr><ul><li><p>京东 笔试过一面挂<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fytkc3bgzoj31ke0icwhc.jpg" alt=""></p></li><li><p>华为 笔试过（一般3道编程题，做出一道即可）一面挂（撞枪口上了）</p></li><li>海康威视 电话面挂</li><li>头条 笔试过一面挂</li><li>360 内推的，一面挂</li><li>最右 一面挂</li><li>百度 笔试过一面过二面挂(面试官是我见过的最nice的，问的问题比较基础，手撕两道简单的编程题，大部分是关于项目的问题)</li><li>中兴（开发） 直接面试，很随意很简单，薪资太低</li><li>南方航空 笔试过面试放弃</li><li>中国建行 笔试挂面试放弃</li><li>中移动 北京一面，聊了聊项目和工作意向，二面放弃</li><li>各种国企银行…</li><li>龙湖地产 直接一面（聊项目，python，Linux相关），二面就定了谈岗位和薪资（整个过程很快很快，薪资待遇很好，让人有种中彩票的感觉？）</li></ul><h3 id="建议学习路径"><a href="#建议学习路径" class="headerlink" title="建议学习路径"></a>建议学习路径</h3><p>我是研二下学期暑假快结束了才开始准备，已经是很晚了，后面只能边复习边刷题边准备笔面试，很辛苦~<br>以下是我给出的秋招准备方法，当然春招的时候走一波实习是最好的，可以拿来练手增加经验。</p><h5 id="复习路径"><a href="#复习路径" class="headerlink" title="复习路径"></a>复习路径</h5><ol><li><p>笔试基础（很重要）</p><p><a href="https://www.zhihu.com/question/24964987">互联网公司最常见的面试算法题有哪些？</a></p><blockquote><p>基本都是leetcode简单级别的或者剑指offer上的原题；</p><p>编程语言的话，放心的用python就好；</p><p>概率统计记得要翻一翻回忆下</p><p>数据结构：字符串、链表、数组、堆、哈希表、树（Trie树、后缀树、红黑树、B树、R树）、图（遍历：BFS、DFS、Dijkstra）</p><p>算法：基于各个数据结构的查找（二分、二叉树）、排序、树的遍历、排列组合概率、分治递归回溯、贪心算法、动态规划（套路：最大、最小，最长、最优，计数；离散问题：容易设计状态（01背包）；最优子结构：N-1可以推导出N）、递归（递归（递推公式)+缓存（重叠子问题）+边界条件；最优子结构：子问题最优决策可导出原问题最优决策+无后效性；重叠子问题：去冗余+空间换时间；边界条件：递归结束的设计、海量数据、字符串匹配相关</p></blockquote><ul><li>一开始是刷<a href="https://leetcode.com/">LeetCode</a>，但是不太习惯英文，刷了不到50题放弃了（刚开始做leetcode中等难度都要想好久甚至做不出来，后来刷了一段时间后基本就很容易了）；</li><li>后来是在<a href="https://www.nowcoder.com/">牛客网</a>上，根据《剑指offer》学习数据结构的面试算法题</li><li>王道程序员求职宝典</li><li><a href="https://oi-wiki.org/">OI-wiki</a> 是一个整合了编程竞赛有趣又实用的知识站点，内容含有竞赛中的基础知识、常见题型、解题思&lt;路以及常用工具等</li><li>《王道程序员求职宝典》</li></ul></li><li><p>面试基础（按顺序来）</p><ul><li>七月在线的课程，看完后会对给的代码进行复现，自己敲一遍(不止一遍)。</li></ul></li></ol><ul><li><p><a href="https://www.coursera.org/learn/machine-learning"><strong>吴恩达的机器学习课程</strong></a>，是 Coursera上最受欢迎的机器学习课程 ★★</p></li><li><p>周志华的《西瓜书》</p></li><li><p>李航的《统计学习方法》</p></li><li><p>《Speech and Language Processing》<a href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf">PDF</a></p></li><li><p><a href="http://t.cn/RCvrRIO">斯坦福CS224n</a></p></li><li><p>Ian Goodfellow，Yoshua Bengio和Aaron Courville 合著的<strong>《深度学习》</strong>，这是学习深度学习最全面的书籍，涵盖了比其他所有课程都要多的主题。★★★ （<a href="http://www.deeplearningbook.org/）（必读）">http://www.deeplearningbook.org/）（必读）</a> <a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf">中文版下载</a></p></li><li><p><a href="http://www.cnblogs.com/tornadomeet/p/3395593.html">《机器学习&amp;数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）》</a></p><ul><li><p><a href="http://deeplearning.stanford.edu/wiki/index.php/Main_Page">《UFLDL-斯坦福大学Andrew Ng教授“Deep Learning”教程》</a></p><p>介绍:本教程将阐述无监督特征学习和深度学习的主要观点。通过学习，你也将实现多个功能学习/深度学习算法，能看到它们为你工作，并学习如何应用/适应这些想法到新问题上。</p></li><li><p><a href="PRML algorithms implemented in Python">PRML一书中所有算法的python实现！！</a></p></li></ul></li></ul><h5 id="资料整理（2020-04-20更新）"><a href="#资料整理（2020-04-20更新）" class="headerlink" title="资料整理（2020-04-20更新）"></a>资料整理（2020-04-20更新）</h5><ul><li><p><a href="https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese">DarLiner/Algorithm_Interview_Notes-Chinese</a>✨ 564</p><p>2018/2019/校招/春招/秋招/自然语言处理(NLP)/深度学习(Deep Learning)/机器学习(Machine Learning)/C/C++/Python/面试笔记.强烈建议学习！一个关于国内知名互联网企业笔试和面试经验的资源库，除了初步梳理和介绍的机器学习领域重要的基础知识和脉络结构之外，还总结了一些国内互联网名企网招、校招笔试面试时的内容和套路，非常值得立志进入这些企业的小伙伴们参考，而且是纯中文的哦！</p></li><li><p><a href="https://github.com/amusi/AI-Job-Notes">AI算法岗求职攻略（涵盖准备攻略、刷题指南、内推和AI公司清单等资料）</a>✨ 2.1K<br>AI算法岗求职攻略：涵盖校招时间表、准备攻略、刷题指南、内推、AI公司清单和答疑等资料</p></li><li><p><a href="https://github.com/scutan90/DeepLearning-500-questions">深度学习500问</a> ✨37.5K</p><p>深度学习500问，以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述，以帮助自己及有需要的读者。 全书分为18个章节，50余万字。</p></li><li><p><a href="https://github.com/TheAlgorithms/Python">All Algorithms implemented in Python</a>✨69.9K<br>该项目用Python实现了所有的排序算法，包括插入排序、冒泡排序、快速排序、选择排序、归并排序等+动图</p></li><li><p><a href="https://github.com/taizilongxu/interview_python">关于Python的面试题</a> ✨12.3K</p></li><li><p><a href="http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000">廖雪峰的Python教程</a> （推荐）</p></li><li><p><a href="https://github.com/CyC2018/CS-Notes">CyC2018/CS-Notes</a>✨97.9K<br>技术面试必备基础知识、Leetcode、计算机操作系统、计算机网络、系统设计、Java、Python、C++ </p></li><li><p><a href="https://github.com/apachecn/Interview">apachecn/Interview</a> ✨ 5.8K</p><p>结构化算法刷题训练指南（Interview = 简历指南 + LeetCode + Kaggle）</p></li><li><p><a href="https://github.com/kriadmin/30-seconds-of-python-code">30-seconds-of-python-code</a> ✨6.3K</p><p>精选的有用python代码段集合，开发者可以在30秒或更短的时间内理解这些代码段。</p></li><li><p><a href="https://github.com/keon/algorithms">algorithms</a> ✨17.2K</p><p>Minimal examples of data structures and algorithms in Python</p></li><li><p><strong><a href="https://github.com/MLEveryday/100-Days-Of-ML-Code">100天掌握机器学习</a> ✨15.3K</strong></p><p>100-Days-Of-ML-Code中文版</p></li><li><p><a href="https://github.com/fengdu78/lihang-code">《统计学习方法》的Python 3.6复现</a> ✨9.9K</p></li><li><p><a href="https://github.com/lijin-THU/notes-python">lijin-THU/notes-python</a> ✨5.5K<br>中文 Python 笔记笔记的实体书：《自学Python——编程基础、科学计算及数据分析》</p></li><li><p><a href="https://github.com/norvig/pytudes">完善编程技巧Python程序</a> ✨13.6K</p><p>资源 | 想用Python学机器学习？Google大神替你写好了所有的编程示范代码<br>全球第一的AI教科书作者、Google Research总监Peter Novig就专门为初学者做了一个关于Python编程示范操作的GitHub项目</p></li></ul><h2 id="找工作的必备工具"><a href="#找工作的必备工具" class="headerlink" title="找工作的必备工具"></a>找工作的必备工具</h2><h3 id="1、简历工具"><a href="#1、简历工具" class="headerlink" title="1、简历工具"></a>1、简历工具</h3><ul><li><a href="https://www.wondercv.com">超级简历（推荐）</a>√</li><li><a href="https://www.zhiyeapp.com/">知页简历</a></li><li><a href="http://cv.ftqq.com/#">冷熊简历</a></li><li><a href="http://cv.qiaobutang.com/tpl/">乔布简历</a></li><li><a href="http://jayveehe.github.io/about/resume.html">Jayvee He的简历</a></li></ul><h3 id="2、收集面经类"><a href="#2、收集面经类" class="headerlink" title="2、收集面经类"></a>2、收集面经类</h3><p>（1）应届生论坛</p><p>（2）牛客网</p><p>没事就多看看，自己多做整理，内化成自己的知识。</p><h3 id="3、了解公司八卦吐槽类"><a href="#3、了解公司八卦吐槽类" class="headerlink" title="3、了解公司八卦吐槽类"></a>3、了解公司八卦吐槽类</h3><p>（1）脉脉</p><p>（2）看准网</p><p>（3）职友集</p><p>但也不要全当真哈，适当参考即可～</p><h3 id="4、薪酬参考类"><a href="#4、薪酬参考类" class="headerlink" title="4、薪酬参考类"></a>4、薪酬参考类</h3><p>（1）看准网</p><p>（2）校招薪水（公主号）对应小程序—offershow</p><p>薪酬一般不好直接打听嘛，所以可以参考一下这些渠道，做到心里有数～</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://ask.julyedu.com/question/86859?from=timeline">非科班生拿BAT算法SP offer面经</a></p><p><a href="https://github.com/jwasham/coding-interview-university">一个完整的计算机科学研究计划成为一名Google软件工程师</a></p><p><a href="https://github.com/conanhujinming/tips_for_interview/blob/master/就业分享_浙江大学计算机学院_conanhujinming.pdf">https://github.com/conanhujinming/tips_for_interview/blob/master/%E5%B0%B1%E4%B8%9A%E5%88%86%E4%BA%AB_%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2_conanhujinming.pdf</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>笔面试</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>✨NLP入门极简教程</title>
    <link href="/2019/01/01/2019-01-01-%E2%9C%A8NLP%E5%85%A5%E9%97%A8%E6%9E%81%E7%AE%80%E6%95%99%E7%A8%8B/"/>
    <url>/2019/01/01/2019-01-01-%E2%9C%A8NLP%E5%85%A5%E9%97%A8%E6%9E%81%E7%AE%80%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>任何列书单和在线视频超过10条的，基本都是混子。列了几十本书和几百小时在线视频的人，大部分自己连1/10都没读完过。这种人统称为“书单教育家”，代表人物就是罗文益。</p><p>要我说，读一本deep learning和PRML，读python和numpy的官方文档，看一个Andrew Ng和Hinton的coursera，剩下的读一读经典paper和github开源代码，动手造造轮子，就完全足够入门了。</p><h2 id="在线课程推荐"><a href="#在线课程推荐" class="headerlink" title="在线课程推荐"></a>在线课程推荐</h2><ul><li>【双语字幕】斯坦福CS224n《深度学习自然语言处理》课程(2017) by Chris Manning &amp; Richard Socher<br>推荐理由：面向NLP的深度学习课程，从基础神经网路讲起，到概率模型，再到word2vec，最后讲到RNN模型在NLP领域的应用等，是NLP方向的必修课。<br>#bilibili#搬运：<a href="https://www.bilibili.com/video/av13383754/">https://www.bilibili.com/video/av13383754/</a></li><li>【双语字幕】斯坦福CS224n《深度学习自然语言处理》课程(2019) by Chris Manning<br>上一门课程的升级版，补充了更多内容<br>#bilibili#搬运：<a href="https://www.bilibili.com/video/BV1Eb411H7Pq?share_source=copy_web">https://www.bilibili.com/video/BV1Eb411H7Pq?share_source=copy_web</a></li><li>李宏毅老师的系列课程<br>地址：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses.html</a></li><li>林轩田老师的课程，机器学习基石和机器学习技法<br>主页：  <a href="https://www.csie.ntu.edu.tw/~htlin/mooc/">https://www.csie.ntu.edu.tw/~htlin/mooc/</a></li><li><a href="https://github.com/yandexdataschool/nlp_course">YSDA course in Natural Language Processing</a><br>这门课程是2021年的，使用Tensorflow/Keras 以及 Pytorch 作为实现框架！！！ </li></ul><p>2018线上课程的文字版讲解：<a href="https://lena-voita.github.io/nlp_course.html#preview_text_clf">https://lena-voita.github.io/nlp_course.html#preview_text_clf</a><br>    每周的课程 PPT 和研讨会材料位于 ./week* 文件夹中</p><h2 id="机器学习-书籍推荐"><a href="#机器学习-书籍推荐" class="headerlink" title="机器学习 书籍推荐"></a>机器学习 书籍推荐</h2><ol><li><p>周志华的<a href="https://pan.baidu.com/s/1hscnaQC">《机器学习》</a>作为通读教材，不用深入，从宏观上了解机器学习 </p></li><li><p>李航的<a href="https://pan.baidu.com/s/1dF2b4jf">《统计学习方法》</a>作为经典的深入案例，仔细研究几个算法的来龙去脉 | <a href="https://github.com/WenDesi/lihang_book_algorithm">书中的代码python实现</a> </p></li><li><a href="https://github.com/exacity/deeplearningbook-chinese">《Deep Learning》</a>Ian Goodfellow / Yoshua Bengio / Aaron Courville<br>(花书)不是一次就能读完读透的，已经有基础的同学可以当作工具书来用，在碰到难题或者面试前都可以翻    </li><li><a href="https://nndl.github.io/">《神经网络与深度学习》</a> 邱锡鹏</li><li>模式识别与机器学习(PRML)<br><a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">下载主页</a>  <a href="http://prml.github.io/">本书代码</a>  <a href="https://github.com/ctgk/PRML">机器学习经典教材《PRML》所有代码实现</a> </li></ol><h2 id="NLP-书籍推荐"><a href="#NLP-书籍推荐" class="headerlink" title="NLP 书籍推荐"></a>NLP 书籍推荐</h2><ol><li>《统计自然语言处理（第二版）》宗成庆<br>主要内容：本书介绍了统计自然语言处理的基本概念、理论方法和最新研究进展，内容包括语言模型、隐马尔可夫模型、语料库技术、汉语自动分词与词性标注、句法分析、词义消歧、篇章分析、统计机器翻译、语音翻译、文本分类、信息检索与问答系统、自动文摘和信息抽取、口语信息处理与人机对话系统等。</li><li>《Introduction to Information Retrieval》Chris Manning<br>排名/搜索领域的一本好书。本书在国内有中译本《信息检索导论》，必读-第2、6章</li><li>《Neural Network Methods for Natural Language Processing》<br>本书内容涵盖了自然语言处理的方方面面，从底层的词法分词、语法分析和语义分析，到和应用更为接近的自然语言处理任务要、对话系统等。书中将自然语言处理、计算语言学以及语音识别等内容融合在一起，把各种技术相互联系起来，让读者了解怎样才能最佳地利用每种技术，怎样才能将各种技术结合起来使用。本书在国内有中译本《自然语言处理综论》</li><li>进阶可选《Speech and Language Processing》<br>对NLP领域神经网络应用的深入介绍</li></ol><p>下载：<a href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf">https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf</a></p><h2 id="NLP-课外补充书籍"><a href="#NLP-课外补充书籍" class="headerlink" title="NLP 课外补充书籍"></a>NLP 课外补充书籍</h2><ol><li>《数学之美》+《智能问答》+《机器翻译》+《知识图谱》</li><li>《自然语言处理实战》 20年底出版，配有Keras代码，比其他深度学习框架都容易些。</li></ol><h2 id="面试相关-书籍推荐"><a href="#面试相关-书籍推荐" class="headerlink" title="面试相关 书籍推荐"></a>面试相关 书籍推荐</h2><ol><li>《百面机器学习》《百面深度学习》Hulu团队出品，基本是面试必备了，可以迅速不就遗漏知识点</li><li>川大毕业极客创建项目《深度学习500问》<br>地址： <a href="https://github.com/yuquanle/DeepLearning-500-questions">https://github.com/yuquanle/DeepLearning-500-questions</a></li></ol><h2 id="炼丹调参"><a href="#炼丹调参" class="headerlink" title="炼丹调参"></a>炼丹调参</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/24720954">深度学习网络调参技巧</a></li><li>《Deep Learning for Vision: Tricks of the Trade》Marc’Aurelio Ranzato 在 CVPR 上 的 presentation slides/talk（Youtube 等地方可以搜到）。caffe 作者之一贾扬清推荐。涉及到了许多 DL 的调参技巧（在 slides 比较靠后的地方）</li><li><a href="http://mp.weixin.qq.com/s?biz=MzA3MzI4MjgzMw==&amp;mid=2650738568&amp;idx=1&amp;sn=e3492aef7d67cbd20c982c348600d42a&amp;chksm=871acbf6b06d42e0b7ae37438befc2f301ab3c3e5cf616a08acc94ab4e8032ada246c1e81ce4&amp;scene=21#wechat_redirect">机器学习算法如何调参？这里有一份神经网络学习速率设置指南</a></li><li><a href="http://mp.weixin.qq.com/s?biz=MzA3MzI4MjgzMw==&amp;mid=2650744897&amp;idx=1&amp;sn=c1bdf8f8e6daeab3abccef549c6c213e&amp;chksm=871aec3fb06d65296bf2436a671eaebbf21cbdeb8995d388e8972accea14d0a558e2ddce92dd&amp;scene=21#wechat_redirect">构建深度神经网络，我有 20 条「不成熟」的小建议</a></li><li><a href="http://mp.weixin.qq.com/s?biz=MzA3MzI4MjgzMw==&amp;mid=2650743506&amp;idx=2&amp;sn=df5d98de3bfa8ea4e2460bbb5d0659cb&amp;chksm=871ae6acb06d6fba90c7e46ed8ad1a188acbbf4bd0a260c33a2b7d0f16ef8d1162f4cb5b3e10&amp;scene=21#wechat_redirect">入门 深度学习模型的简单优化技巧</a></li><li><a href="http://mp.weixin.qq.com/s?biz=MzA3MzI4MjgzMw==&amp;mid=2650742657&amp;idx=1&amp;sn=92ff1cd3481155430d51349caead02a2&amp;chksm=871adbffb06d52e9fb8f5b152f7e43954e340ba2bec1204fb5d4a572864f1832cec0ebdfc455&amp;scene=21#wechat_redirect">谷歌机器学习 43 条规则：机器学习工程的最佳实践经验</a></li><li><a href="http://mp.weixin.qq.com/s?biz=MzA3MzI4MjgzMw==&amp;mid=2650747821&amp;idx=3&amp;sn=0a74be78e4137a597d8ae62c2b3dd78a&amp;chksm=871af7d3b06d7ec56b3094b4771fb4031aa64f79c713c77373d9debcd1e7ffe338988fabd549&amp;scene=21#wechat_redirect">教程 从超参数到架构，一文简述模型优化策略</a></li><li><a href="http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html">http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html</a></li><li><a href="https://flashgene.com/archives/9846.html">https://flashgene.com/archives/9846.html</a></li><li><a href="https://wulc.me/2018/05/01/梯度裁剪及其作用/">https://wulc.me/2018/05/01/梯度裁剪及其作用/</a></li><li><a href="https://www.zhihu.com/question/57828011">https://www.zhihu.com/question/57828011</a></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/342465172">2021年NLP入门书籍推荐-李rumor</a></p><p><a href="https://zhuanlan.zhihu.com/p/58874484">NLP入门推荐书目（2019版）-刘知远</a></p><p><a href="https://cloud.tencent.com/developer/article/1814273">NLP爱好者学习资源推荐汇总</a></p><p><a href="https://zhuanlan.zhihu.com/p/59184256">初入NLP领域的一些小建议-香农科技·李级为</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>NLP教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keras优化器介绍</title>
    <link href="/2018/12/04/2018-12-04-Keras%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/"/>
    <url>/2018/12/04/2018-12-04-Keras%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="Keras优化器介绍"><a href="#Keras优化器介绍" class="headerlink" title="Keras优化器介绍"></a>Keras优化器介绍</h2><p>在 Sebastian Ruder 的<a href="https://arxiv.org/pdf/1609.04747.pdf">这篇论文</a>中给出了常用优化器的比较，包括： </p><ul><li>SGD (Stochastic gradient descent)</li><li>Mini-batch gradient descent</li><li>Adagrad</li><li>Adadelta</li><li>RMSprop</li><li>Adam</li></ul><p>最后给出如何选择优化器的Tips!!</p><span id="more"></span><h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><p>SGD 每次更新时对每个样本进行梯度更新。<br>缺点: SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs lasso">for i <span class="hljs-keyword">in</span> range(nb_epochs):  <br>    np.random.shuffle(<span class="hljs-built_in">data</span>)  <br>    for example <span class="hljs-keyword">in</span> <span class="hljs-built_in">data</span>:<br>        params_grad = evaluate_gradient(loss_function, example, <span class="hljs-keyword">params</span>)<br>        <span class="hljs-keyword">params</span> = <span class="hljs-keyword">params</span> - learning_rate * params_grad<br></code></pre></td></tr></table></figure><h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><p>MBGD 每一次利用一小批样本，即 n 个样本进行计算。<br>这样它可以降低参数更新时的方差，收敛更稳定。<br>另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(nb_epochs):<br>  np.random.shuffle(data)  <br>  <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> get<span class="hljs-constructor">_batches(<span class="hljs-params">data</span>, <span class="hljs-params">batch_size</span>=50)</span>:    <br>      params_grad = evaluate<span class="hljs-constructor">_gradient(<span class="hljs-params">loss_function</span>, <span class="hljs-params">batch</span>, <span class="hljs-params">params</span>)</span><br>      params = params - learning_rate<span class="hljs-operator"> * </span>params_grad<br></code></pre></td></tr></table></figure><p>缺点: </p><ol><li>learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。<br>有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。</li><li>此外，这种方法是对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。</li><li>另外，对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error 是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。</li></ol><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，<br>一般 η 就取 0.01。<br>缺点:它的缺点是分母会不断积累，这样学习率就会收缩并最终会变得非常小</p><h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p>这个算法是对 Adagrad 的改进</p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><ul><li>RMSprop 是 Geoff Hinton 提出的一种自适应学习率方法。</li><li>RMSprop 和 Adadelta 都是为了解决 Adagrad 学习率急剧下降问题的</li></ul><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><ul><li>这个算法是另一种计算每个参数的自适应学习率的方法。实践表明，Adam 比其他适应性学习方法效果要好。</li><li>除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的指数衰减平均值：</li></ul><p>超参数设定值:</p><ul><li>建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8</li></ul><h3 id="如何选择？"><a href="#如何选择？" class="headerlink" title="如何选择？"></a>如何选择？</h3><ul><li>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。</li><li>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。</li><li>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，</li><li>随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Adam 是最受欢迎和被广泛使用的优化算法之一，通常都会作为NLP领域研究人员的选择。<br>通常认为，Adam要明显优于传统的随机梯度下降(SGD)算法。<br>然而，虽然训练过程中Adam比SGD收敛快得多，但是SGD的在学习率上的退火速率要略胜于Adam。</p><p>最近的研究工作进一步表明，对SGD进行适当的动量调整将优于Adam算法。<br>SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点</p>]]></content>
    
    
    
    <tags>
      
      <tag>技术</tag>
      
      <tag>Keras</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mac优雅装机</title>
    <link href="/2018/11/13/2018-11-13-Mac%E4%BC%98%E9%9B%85%E8%A3%85%E6%9C%BA/"/>
    <url>/2018/11/13/2018-11-13-Mac%E4%BC%98%E9%9B%85%E8%A3%85%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="Mac优雅装机"><a href="#Mac优雅装机" class="headerlink" title="Mac优雅装机"></a>Mac优雅装机</h1><ul><li>安装工具 <code>Homebrew</code> 和 <code>MAS</code></li><li>日常使用的软件一键安装脚本</li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>利用 <code>Homebrew</code> 和 <code>MAS</code> 这两个工具来完成所有软件的自动安装；</p><ul><li>前者适用于非 MAS 的几乎所有软件</li><li>后者能够操纵 Mac App Store 接口来安装其中的应用</li></ul><p>这里给出我自己Mac的所有安装的软件，包括大部分<strong>日常使用的软件和计算机专业的软件安装</strong>；并写成如下一串「装机」脚本，妥善保管好以上代码，需要用的时候只需要粘贴到终端，而不是去各个网站下载安装包。</p><span id="more"></span><h2 id="开始优雅装机"><a href="#开始优雅装机" class="headerlink" title="开始优雅装机"></a>开始优雅装机</h2><ol><li><a href="https://brew.sh/index_zh-cn">下载 Homebrew</a></li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/bin/</span>bash -c <span class="hljs-string">&quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;</span><br></code></pre></td></tr></table></figure><p>上面的命令可能出现curl什么的访问不到，可以改用<a href="https://www.cnblogs.com/liyihua/p/12753163.html">自动脚本(全部国内地址)</a>（在Mac os终端中复制粘贴回车下面这句话)</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/bin/</span>zsh -c <span class="hljs-string">&quot;$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;</span><br></code></pre></td></tr></table></figure><ol><li>安装python</li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">echo</span> <span class="hljs-string">&quot;installing python&quot;</span><br><span class="hljs-attribute">brew</span> install python@<span class="hljs-number">3</span>.<span class="hljs-number">7</span><br><span class="hljs-attribute">echo</span> <span class="hljs-string">&quot;finish python installation&quot;</span><br></code></pre></td></tr></table></figure><ol><li>安装 wget</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;installing wget&quot;</span><br>brew install wget<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;finish wget installation&quot;</span><br></code></pre></td></tr></table></figure><ol><li>安装git</li></ol><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>git<br>echo <span class="hljs-string">&quot;finish git installation&quot;</span><br>echo <span class="hljs-string">&quot;empower git with hub&quot;</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>hub<span class="hljs-comment"># 将GitHub接口和git命令进行包装</span><br>alias git=hub<br></code></pre></td></tr></table></figure><ol><li>安装iterm2</li></ol><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>iterm2<br>echo <span class="hljs-string">&quot;finish iterm2 installation&quot;</span><br></code></pre></td></tr></table></figure><ol><li>安装zsh</li></ol><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>zsh<br>echo <span class="hljs-string">&quot;finish zsh installation&quot;</span><br></code></pre></td></tr></table></figure><ol><li>安装oh-my-zsh</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">/bin/sh -c <span class="hljs-string">&quot;<span class="hljs-subst">$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;finish oh my zsh installation&quot;</span><br></code></pre></td></tr></table></figure><ol><li>安装Homebrew Cask（Error: caskroom/cask was moved.）</li></ol><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs clean">brew install caskroom/cask/brew-cask<br><br>########################################################<br># 除了在终端使用的软件，其他的软件都用 Cask 来管理;<br><br># brew cask search 对于不是很确定要找的App的精确的名字，可以通过这个命令来检查<br># brew cask uninstall xx 卸载软件 <br># brew update 更新所有软件<br># 更新具体软件：brew upgrade 软件名 ，例：brew upgrade git<br># brew cask list 列出通过 Homebrew Cask 安装的包<br>########################################################<br></code></pre></td></tr></table></figure><p>​    利用Cask下载第三方应用</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask google-chrome <span class="hljs-comment"># 谷歌浏览器</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask iina<br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask alfred <span class="hljs-comment"># 效率神器</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask thunder <span class="hljs-comment"># 迅雷</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask <span class="hljs-keyword">baidunetdisk </span><span class="hljs-comment"># 百度云</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask microsoft-office <span class="hljs-comment"># office 2011 套件</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask <span class="hljs-keyword">insomniax</span><span class="hljs-comment"># MacBook防睡眠软件</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask parallels# <br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask steam<br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask sogouinput <span class="hljs-comment"># 搜狗输入法</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask caffeine <span class="hljs-comment"># 防休眠工具 Caffeine</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask mathtype <span class="hljs-comment"># Mathtype</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask qqmacmgr <span class="hljs-comment"># QQ电脑管家</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask qqmusic <span class="hljs-comment"># QQ音乐</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask neteasemusic <span class="hljs-comment"># 网易云音乐</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask youku <span class="hljs-comment"># 优酷</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask appzapper <span class="hljs-comment"># 软件卸载工具</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask appcleaner <span class="hljs-comment"># 软件卸载工具</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask <span class="hljs-keyword">betterzip </span><span class="hljs-comment"># 压缩解压缩工具</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask mounty <span class="hljs-comment"># NTFS 分区读写组件</span><br><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask <span class="hljs-keyword">sublime</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask typora <span class="hljs-comment"># markdown 工具</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask cajviewer <span class="hljs-comment"># CAJ文件</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask dash <br><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask visual-studio-code <span class="hljs-comment"># VScode</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask pycharm-ce <span class="hljs-comment"># pycharm </span><br><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask cheatsheet<br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask go2shell <span class="hljs-comment"># 快速在当前目录打开Shell的工具</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask glances <span class="hljs-comment"># 在命令行中查看你系统运行状态的工具</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask eudic <span class="hljs-comment"># 欧陆词典</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask nutstore <span class="hljs-comment"># 坚果云</span><br><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask Tunnelblick <span class="hljs-comment"># vpn</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask <span class="hljs-keyword">shadowsocksx-ng </span><span class="hljs-comment"># ShadowsocksX客户端软件</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask lantern <span class="hljs-comment"># 蓝灯</span><br><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask teamviewer-host <span class="hljs-comment"># 远程协助 Teamviewer</span><br><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>--cask vnc-viewer <span class="hljs-comment"># 远程协助 Teamviewer</span><br></code></pre></td></tr></table></figure><ol><li>下载 MAS</li></ol><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>mas<br></code></pre></td></tr></table></figure><ol><li>不想再忍受 Mac App Store 缓慢的打开速度；</li><li>批量安装或更新 Mac App Store 应用 <code>mas upgrade</code>；</li><li>快速切换 Mac App Store 账号。</li></ol><p>mas 是根据 Product Identifier 安装与更新应用，也提供了查询应用 ID 的命令:</p><ol><li>用命令 <code>mas search 关键词</code> 查询应用。比如在终端中执行 <code>mas search xcode</code>，大概 1 秒就显示了结果；</li><li>用命令 <code>mas list</code> 查询已安装应用及其识别码。</li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">mas</span> install <span class="hljs-number">1037126344</span> # apple configurator <span class="hljs-number">2</span><br><span class="hljs-attribute">mas</span> install <span class="hljs-number">425424353</span> # the unarchiver<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">441258766</span> # magnet<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">483820530</span> # qr journal<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">836500024</span> # wechat<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">1254743014</span> # lyricsx<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">451108668</span> # qq<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">523620159</span> # cleanmydrive <span class="hljs-number">2</span><br><br><span class="hljs-attribute">mas</span> install <span class="hljs-number">944848654</span> # NeteaseMusic (<span class="hljs-number">1</span>.<span class="hljs-number">5</span>.<span class="hljs-number">9</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">491854842</span> # 网易有道词典 (<span class="hljs-number">2</span>.<span class="hljs-number">3</span>.<span class="hljs-number">1</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">406056744</span> # Evernote (<span class="hljs-number">7</span>.<span class="hljs-number">0</span>.<span class="hljs-number">2</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">921458519</span> # DrCleaner (<span class="hljs-number">3</span>.<span class="hljs-number">3</span>.<span class="hljs-number">4</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">1091189122</span> # Bear (<span class="hljs-number">1</span>.<span class="hljs-number">4</span>.<span class="hljs-number">3</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">1071663619</span> # unrar (<span class="hljs-number">1</span>.<span class="hljs-number">1</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">981420053</span> # Media Player (<span class="hljs-number">2</span>.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">1012930195</span> # HandShaker (<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">966085870</span> # TickTick (<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">00</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">1032155965</span> # Foxit Reader (<span class="hljs-number">2</span>.<span class="hljs-number">4</span>.<span class="hljs-number">1</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">409203825</span> # Numbers (<span class="hljs-number">4</span>.<span class="hljs-number">3</span>.<span class="hljs-number">1</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">1184150999</span> # 护眼宝 (<span class="hljs-number">1</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span>)<br><span class="hljs-attribute">mas</span> install <span class="hljs-number">789066512</span> # Maipo (<span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p><em>PS：对于使用 MAS 安装应用需要 app ID，可以使用 mas list 来快速获取现安装的所有应用的 app ID。</em><br><em>把上面的代码保存为 s.command，然后给予执行权限<code>chmod u+x s.command</code>，以后只要双击即可使用</em></p><ol><li>重置启动台图标顺序</li></ol><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">defaults <span class="hljs-keyword">write</span> com.apple.dock ResetLaunchPad -<span class="hljs-type">bool</span> <span class="hljs-keyword">TRUE</span> &amp;&amp; killall Dock<br></code></pre></td></tr></table></figure><ol><li>安装python3.7</li></ol><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mel">xcode-<span class="hljs-keyword">select</span> --install <br>brew install <span class="hljs-keyword">python</span>@3<span class="hljs-number">.7</span><br></code></pre></td></tr></table></figure><p>Xcode12.5 依赖 Command Line Tools，安装 Xcode 的时候会弹出 Command Line Tools 的安装请求从而解决这个依赖。也可以去<a href="https://developer.apple.com/download/more/">Developer Apple</a>上手动下载对应的<code>Command Line Tools</code> 安装即可。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://sspai.com/post/43239">给 Mac 优雅地一键「装机」</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Mac</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从零开始用Python搭建神经网络</title>
    <link href="/2018/11/12/2018-11-12-%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%94%A8Python%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2018/11/12/2018-11-12-%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%94%A8Python%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="从零开始用Python构建神经网络"><a href="#从零开始用Python构建神经网络" class="headerlink" title="从零开始用Python构建神经网络"></a>从零开始用Python构建神经网络</h1><p>这是一份用于理解深度学习内部运作方式的初学者指南，内容涵盖</p><ul><li>神经网络定义</li><li>损失函数</li><li>前向传播</li><li>反向传播</li><li>梯度下降算法</li></ul><span id="more"></span><h2 id="什么是神经网络？"><a href="#什么是神经网络？" class="headerlink" title="什么是神经网络？"></a>什么是神经网络？</h2><p>可以理解为一个从输入映射到输出的数学函数，即对复杂函数的拟合。</p><p>神经网络由以下部分组成：</p><ul><li>一个输入层，x</li><li>任意数量的隐藏层</li><li>一个输出层，ŷ</li><li>每两层之间都有一组权重和偏置，W 和 b</li><li>每个隐藏层都要选择一个激活函数 σ</li></ul><p>写成矩阵形式，即:</p><script type="math/tex; mode=display">y=σ(W*X+b)</script><p><em>PS: 为什么需要非线性激活函数？</em></p><p>增强模型对复杂函数的拟合能力。</p><h2 id="神经网络的基本训练过程"><a href="#神经网络的基本训练过程" class="headerlink" title="神经网络的基本训练过程"></a>神经网络的基本训练过程</h2><p>训练过程的每一次迭代包含以下步骤：</p><ul><li>计算预测的输出 ŷ，称为前向传播；</li><li>更新权重和偏置，称为反向传播；</li></ul><p>简单的计算流程图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9aGy5KibRbtI61b29vJbh8ibzTYyzxOjLffu2Pm09XBLuiar4fsAeaibITgCdwI8fiaqmPkic9SCNd9ibCQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h3 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h3><ol><li>首先是我们已经有了训练数据</li><li>我们已经根据数据的规模、领域，建立了神经网络的基本结构，比如有几层，每一层有几个神经元</li><li>定义好损失函数来合理地计算误差</li></ol><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>一个简单 2 层神经网络的输出 ŷ 可以表示为：</p><script type="math/tex; mode=display">\hat y=σ(W_2σ(W_1*X+b_1)+b_2)</script><p>也可以写成：</p><script type="math/tex; mode=display">layer1=\sigma(W_1X+b_1)</script><script type="math/tex; mode=display">output=\sigma(W_2layer1+b_2)</script><p>其中，权重 <script type="math/tex">W</script> 和偏置 <script type="math/tex">b</script> 是影响输出 ŷ 的参数。</p><p>我们可以在 Python 代码中添加一个前向传播函数来做到这一点。（把b的值永远设置为1）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        self.<span class="hljs-built_in">input</span>      = x<br>        self.weights1   = np.random.rand(self.<span class="hljs-built_in">input</span>.shape[<span class="hljs-number">1</span>],<span class="hljs-number">4</span>) <br>        self.weights2   = np.random.rand(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>)                 <br>        self.y          = y<br>        self.output     = np.zeros(self.y.shape)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">feedforward</span>(<span class="hljs-params">self</span>):<br>        self.layer1 = sigmoid(np.dot(self.<span class="hljs-built_in">input</span>, self.weights1))<br>        self.output = sigmoid(np.dot(self.layer1, self.weights2))<br></code></pre></td></tr></table></figure><p>得到输出 ŷ 之后，如何判断预测结果的好坏呢？这就需要用到损失函数了。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>其目标是根据数据自动学习网络的权重，以便让所有输入 𝑥的预测输出 ŷ 接近目标 𝑦</p><p>为了衡量与该目标的差距，我们使用了一个误差平方和损失函数：</p><script type="math/tex; mode=display">Loss(y,\hat y)=\Sigma_i^n(y-\hat y)^2</script><p>误差平方和，即每个预测值和真实值之间差值的平均值。这个差值是取了平方项的，所以我们测量的是差值的绝对值。在训练过程中，我们的目标是找到一组最佳的权重和偏置，使损失函数最小化。</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>现在，我们已经找到了预测误差的方法（损失函数），那么我们需要一种方法将错误「传播」回去，从而更新权重和偏置。为了确定权重和偏置调整的适当值，我们需要知道损失函数对权重和偏置的偏导数。如果我们知道了偏导数，我们可以通过简单增加或减少偏导数的方式来更新权重和偏置。这就是所谓的梯度下降 $w=w-\alpha \frac{dL}{dw}$。</p><p>下面，我们开始反向传播误差导数。  根据我们的误差函数 <script type="math/tex">Loss(y,\hat y)=\frac{1}{2}(y-\hat y)^2</script>，我们通过链式法则得出可以得出：</p><script type="math/tex; mode=display">layer1=\sigma(W_1X+b_1)</script><script type="math/tex; mode=display">output=\sigma(W_2layer1+b_2)</script><script type="math/tex; mode=display">\hat y=σ(W_2σ(W_1*X+b_1)+b_2)</script><script type="math/tex; mode=display">z=W_2σ(W_1*X+b_1)+b_2=W_2layer1+b_2</script><script type="math/tex; mode=display">\frac{dL}{d\hat y}=y-\hat y</script><script type="math/tex; mode=display">\frac{dL}{dx}=\frac{dL}{d\hat y}\frac{d\hat y}{dx}=(y-\hat y)(1-\hat y)</script><script type="math/tex; mode=display">\frac{dL}{dw_2}=\frac{dL}{d\hat y}\frac{d\hat y}{dz}\frac{dz}{dw_2}=(y-\hat y)*\hat y(1-\hat y)*layer1</script><script type="math/tex; mode=display">\frac{dL}{dw_1}=\frac{dL}{d\hat y}\frac{d\hat y}{dz}\frac{dz}{dw_1}\\=(y-\hat y)*\hat y(1-\hat y)*\frac{dz}{dw_1}\\=(y-\hat y)*\hat y(1-\hat y)*W_2layer1(1-layer1)X</script><p>反向传播函数的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_derivative</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span>-x)<span class="hljs-comment"># sigmoid函数的导数</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        self.<span class="hljs-built_in">input</span>      = x<br>        self.weights1   = np.random.rand(self.<span class="hljs-built_in">input</span>.shape[<span class="hljs-number">1</span>],<span class="hljs-number">4</span>) <br>        self.weights2   = np.random.rand(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>)                 <br>        self.y          = y<br>        self.output     = np.zeros(self.y.shape)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">feedforward</span>(<span class="hljs-params">self</span>):<br>        self.layer1 = sigmoid(np.dot(self.<span class="hljs-built_in">input</span>, self.weights1))<br>        self.output = sigmoid(np.dot(self.layer1, self.weights2))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backprop</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># application of the chain rule to find derivative of the loss function with respect to weights2 and weights1</span><br>        d_weights2 = np.dot(self.layer1.T, (<span class="hljs-number">2</span>*(self.y - self.output) * sigmoid_derivative(self.output)))<br>        d_weights1 = np.dot(self.<span class="hljs-built_in">input</span>.T,  (np.dot(<span class="hljs-number">2</span>*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))<br><br>        <span class="hljs-comment"># update the weights with the derivative (slope) of the loss function</span><br>        self.weights1 += d_weights1<br>        self.weights2 += d_weights2<br></code></pre></td></tr></table></figure><p>为了更深入地理解微积分和链式法则在反向传播中的应用，我强烈推荐 3Blue1Brown 的视频教程。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>既然我们已经有了做前向传播和反向传播的完整 Python 代码，我们可以将神经网络应用到一个示例中，看看它的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">x = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]])<span class="hljs-comment"># data</span><br>y = np.array([[<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>]])<span class="hljs-comment"># label</span><br>nn = NeuralNetwork(x,y)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<span class="hljs-comment"># 迭代1000次</span><br>    nn.feedforward()<br>    nn.backprop()<br><span class="hljs-built_in">print</span>(nn.output)<br></code></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><div class="table-container"><table><thead><tr><th>prediction</th><th>Y</th></tr></thead><tbody><tr><td>0.02191928</td><td>0</td></tr><tr><td>0.97279658</td><td>1</td></tr><tr><td>0.95721863</td><td>1</td></tr><tr><td>0.04292296</td><td>0</td></tr></tbody></table></div><p>通过前向传播和反向传播算法成功训练了神经网络，预测值收敛到了真实值。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw%3D%3D&amp;mid=2650742819&amp;idx=1&amp;sn=e75e6fdef42ac027c5fe10a4c0615e70#wechat_redirect">无需深度学习框架，如何从零开始用Python构建神经网络</a></p><p><a href="https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/">Google 反向传播演示</a></p><p><a href="https://juejin.im/post/5a6721bd518825733201c4a2">MarkDown 插入数学公式实验大集合</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python可视化之matplotlib</title>
    <link href="/2018/11/11/2018-11-11-Python%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Bmatplotlib/"/>
    <url>/2018/11/11/2018-11-11-Python%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Bmatplotlib/</url>
    
    <content type="html"><![CDATA[<h2 id="利用matplotlib对数据进行可视化"><a href="#利用matplotlib对数据进行可视化" class="headerlink" title="利用matplotlib对数据进行可视化"></a>利用matplotlib对数据进行可视化</h2><ul><li>简单曲线图</li><li>复杂曲线图</li><li>子图</li><li>柱状图</li><li>横向柱状图</li><li>高级柱状图</li><li>层次柱状图</li><li>直方图</li><li>饼图</li><li>散点图</li><li>雷达图</li></ul><span id="more"></span><h4 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h4><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib<br><span class="hljs-keyword">import</span> matplotlib.mlab <span class="hljs-keyword">as</span> mlab<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib.font_manager <span class="hljs-keyword">as</span> fm<br></code></pre></td></tr></table></figure><h4 id="1、简单曲线图"><a href="#1、简单曲线图" class="headerlink" title="1、简单曲线图"></a>1、简单曲线图</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def simple_plot():<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    simple plot</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # 生成测试数据<br>    x = np.linspace(-np.pi, np.pi, 256, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>)<br>    y_cos, y_sin = np.cos(x), np.sin(x)<br><br>    # 生成画布，并设定标题<br>    plt.figure(figsize=(8, 6), <span class="hljs-attribute">dpi</span>=80)<br>    plt.title(<span class="hljs-string">&quot;简单曲线图&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    plt.grid(<span class="hljs-literal">True</span>)<br><br>    # 设置X轴<br>    plt.xlabel(<span class="hljs-string">&quot;X轴&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    plt.xlim(-4.0, 4.0)<br>    plt.xticks(np.linspace(-4, 4, 9, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>))<br><br>    # 设置Y轴<br>    plt.ylabel(<span class="hljs-string">&quot;Y轴&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    plt.ylim(-1.0, 1.0)<br>    plt.yticks(np.linspace(-1, 1, 9, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>))<br><br>    # 画两条曲线<br>    plt.plot(x, y_cos, <span class="hljs-string">&quot;b--&quot;</span>, <span class="hljs-attribute">linewidth</span>=2.0, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;cos示例&quot;</span>)<br>    plt.plot(x, y_sin, <span class="hljs-string">&quot;g-&quot;</span>, <span class="hljs-attribute">linewidth</span>=2.0, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;sin示例&quot;</span>)<br><br>    # 设置图例位置,loc可以为[upper, lower, left, right, center]<br>    plt.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&quot;upper left&quot;</span>, <span class="hljs-attribute">prop</span>=myfont, <span class="hljs-attribute">shadow</span>=<span class="hljs-literal">True</span>)<br><br>    # 图形显示<br>    plt.show()<br>    return<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/简单曲线图.png" alt=""></p><h4 id="2、复杂曲线图"><a href="#2、复杂曲线图" class="headerlink" title="2、复杂曲线图"></a>2、复杂曲线图</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def simple_advanced_plot():<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    simple advanced plot</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # 生成测试数据<br>    x = np.linspace(-np.pi, np.pi, 256, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>)<br>    y_cos, y_sin = np.cos(x), np.sin(x)<br><br>    # 生成画布, 并设定标题<br>    plt.figure(figsize=(8, 6), <span class="hljs-attribute">dpi</span>=80)<br>    plt.title(<span class="hljs-string">&quot;复杂曲线图&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    plt.grid(<span class="hljs-literal">True</span>)<br><br>    # 画图的另外一种方式<br>    ax_1 = plt.subplot(111)<br>    ax_1.plot(x, y_cos, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;blue&quot;</span>, <span class="hljs-attribute">linewidth</span>=2.0, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&quot;--&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;左cos&quot;</span>)<br>    ax_1.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&quot;upper left&quot;</span>, <span class="hljs-attribute">prop</span>=myfont, <span class="hljs-attribute">shadow</span>=<span class="hljs-literal">True</span>)<br><br>    # 设置Y轴(左边)<br>    ax_1.set_ylabel(<span class="hljs-string">&quot;左cos的y轴&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    ax_1.set_ylim(-1.0, 1.0)<br>    ax_1.set_yticks(np.linspace(-1, 1, 9, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>))<br><br>    # 画图的另外一种方式<br>    ax_2 = ax_1.twinx()<br>    ax_2.plot(x, y_sin, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;green&quot;</span>, <span class="hljs-attribute">linewidth</span>=2.0, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&quot;-&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;右sin&quot;</span>)<br>    ax_2.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&quot;upper right&quot;</span>, <span class="hljs-attribute">prop</span>=myfont, <span class="hljs-attribute">shadow</span>=<span class="hljs-literal">True</span>)<br><br>    # 设置Y轴(右边)<br>    ax_2.set_ylabel(<span class="hljs-string">&quot;右sin的y轴&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    ax_2.set_ylim(-2.0, 2.0)<br>    ax_2.set_yticks(np.linspace(-2, 2, 9, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>))<br><br>    # 设置X轴(共同)<br>    ax_1.set_xlabel(<span class="hljs-string">&quot;x轴&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    ax_1.set_xlim(-4.0, 4.0)<br>    ax_1.set_xticks(np.linspace(-4, 4, 9, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>))<br><br>    # 图形显示<br>    plt.show()<br>    return<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/复杂曲线图.png" alt=""></p><h4 id="3、子图"><a href="#3、子图" class="headerlink" title="3、子图"></a>3、子图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">subplot_plot</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    subplot plot</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 子图的style列表</span><br>    style_list = [<span class="hljs-string">&quot;g+-&quot;</span>, <span class="hljs-string">&quot;r*-&quot;</span>, <span class="hljs-string">&quot;b.-&quot;</span>, <span class="hljs-string">&quot;yo-&quot;</span>]<br><br>    <span class="hljs-comment"># 依次画图</span><br>    <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>        <span class="hljs-comment"># 生成测试数据</span><br>        x = np.linspace(<span class="hljs-number">0.0</span>, <span class="hljs-number">2</span>+num, num=<span class="hljs-number">10</span>*(num+<span class="hljs-number">1</span>))<br>        y = np.sin((<span class="hljs-number">5</span>-num) * np.pi * x)<br><br>        <span class="hljs-comment"># 子图的生成方式</span><br>        plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, num+<span class="hljs-number">1</span>)<br>        plt.title(<span class="hljs-string">&quot;子图 %d&quot;</span> % (num+<span class="hljs-number">1</span>), fontproperties=myfont)<br>        plt.plot(x, y, style_list[num])<br><br>    <span class="hljs-comment"># 图形显示</span><br>    plt.show()<br>    <span class="hljs-keyword">return</span><br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/子图.png" alt=""></p><h4 id="4、柱状图"><a href="#4、柱状图" class="headerlink" title="4、柱状图"></a>4、柱状图</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def bar_plot():<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    bar plot</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # 生成测试数据<br>    means_men = (20, 35, 30, 35, 27)<br>    means_women = (25, 32, 34, 20, 25)<br><br>    # 设置标题<br>    plt.title(<span class="hljs-string">&quot;柱状图&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br><br>    # 设置相关参数<br>    index = np.arange(len(means_men))<br>    bar_width = 0.35<br><br>    # 画柱状图<br>    plt.bar(index, means_men, <span class="hljs-attribute">width</span>=bar_width, <span class="hljs-attribute">alpha</span>=0.2, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;男生&quot;</span>)<br>    plt.bar(index+bar_width, means_women, <span class="hljs-attribute">width</span>=bar_width, <span class="hljs-attribute">alpha</span>=0.8, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;r&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;女生&quot;</span>)<br>    plt.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&quot;upper right&quot;</span>, <span class="hljs-attribute">prop</span>=myfont, <span class="hljs-attribute">shadow</span>=<span class="hljs-literal">True</span>)<br><br>    # 设置柱状图标示<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(index, means_men):<br>        plt.text(x, y+0.3, y, <span class="hljs-attribute">ha</span>=<span class="hljs-string">&quot;center&quot;</span>, <span class="hljs-attribute">va</span>=<span class="hljs-string">&quot;bottom&quot;</span>)<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(index, means_women):<br>        plt.text(x+bar_width, y+0.3, y, <span class="hljs-attribute">ha</span>=<span class="hljs-string">&quot;center&quot;</span>, <span class="hljs-attribute">va</span>=<span class="hljs-string">&quot;bottom&quot;</span>)<br><br>    # 设置刻度范围/坐标轴名称等<br>    plt.ylim(0, 45)<br>    plt.xlabel(<span class="hljs-string">&quot;分组Group&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    plt.ylabel(<span class="hljs-string">&quot;得分Scores&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    plt.xticks(index+(bar_width/2), (<span class="hljs-string">&quot;A组&quot;</span>, <span class="hljs-string">&quot;B组&quot;</span>, <span class="hljs-string">&quot;C组&quot;</span>, <span class="hljs-string">&quot;D组&quot;</span>, <span class="hljs-string">&quot;E组&quot;</span>), <span class="hljs-attribute">fontproperties</span>=myfont)<br><br>    # 图形显示<br>    plt.show()<br>    return<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/柱状图.png" alt=""></p><h4 id="5、横向柱状图"><a href="#5、横向柱状图" class="headerlink" title="5、横向柱状图"></a>5、横向柱状图</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def barh_plot():<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    barh plot</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # 生成测试数据<br>    means_men = (20, 35, 30, 35, 27)<br>    means_women = (25, 32, 34, 20, 25)<br><br>    # 设置标题<br>    plt.title(<span class="hljs-string">&quot;横向柱状图&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br><br>    # 设置相关参数<br>    index = np.arange(len(means_men))<br>    bar_height = 0.35<br><br>    # 画柱状图(水平方向)<br>    plt.barh(index, means_men, <span class="hljs-attribute">height</span>=bar_height, <span class="hljs-attribute">alpha</span>=0.2, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;Men&quot;</span>)<br>    plt.barh(index+bar_height, means_women, <span class="hljs-attribute">height</span>=bar_height, <span class="hljs-attribute">alpha</span>=0.8, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;r&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;Women&quot;</span>)<br>    plt.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&quot;upper right&quot;</span>, <span class="hljs-attribute">shadow</span>=<span class="hljs-literal">True</span>)<br><br>    # 设置柱状图标示<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(index, means_men):<br>        plt.text(y+0.3, x, y, <span class="hljs-attribute">ha</span>=<span class="hljs-string">&quot;left&quot;</span>, <span class="hljs-attribute">va</span>=<span class="hljs-string">&quot;center&quot;</span>)<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(index, means_women):<br>        plt.text(y+0.3, x+bar_height, y, <span class="hljs-attribute">ha</span>=<span class="hljs-string">&quot;left&quot;</span>, <span class="hljs-attribute">va</span>=<span class="hljs-string">&quot;center&quot;</span>)<br><br>    # 设置刻度范围/坐标轴名称等<br>    plt.xlim(0, 45)<br>    plt.xlabel(<span class="hljs-string">&quot;Scores&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;Group&quot;</span>)<br>    plt.yticks(index+(bar_height/2), (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>, <span class="hljs-string">&quot;E&quot;</span>))<br><br>    # 图形显示<br>    plt.show()<br>    return<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/横向柱状图.png" alt=""></p><h4 id="6、高级柱状图"><a href="#6、高级柱状图" class="headerlink" title="6、高级柱状图"></a>6、高级柱状图</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def bar_advanced_plot():<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    bar advanced plot</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # 生成测试数据<br>    means_men = np.array((20, 35, 30, 35, 27, 25, 32, 34, 20, 25))<br>    means_women = np.array((25, 32, 34, 20, 25, 20, 35, 30, 35, 27))<br><br>    # 设置标题<br>    plt.title(<span class="hljs-string">&quot;高级柱状图&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br><br>    # 设置相关参数<br>    index = np.arange(len(means_men))<br>    bar_width = 0.8<br><br>    # 画柱状图(两种:X轴以上/X轴以下)<br>    plt.bar(index, means_men, <span class="hljs-attribute">width</span>=bar_width, <span class="hljs-attribute">alpha</span>=0.4, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;Men&quot;</span>)<br>    plt.bar(index, -means_women, <span class="hljs-attribute">width</span>=bar_width, <span class="hljs-attribute">alpha</span>=0.4, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;r&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;Women&quot;</span>)<br><br>    # 画折线图(两种,和柱状图对应)<br>    plt.plot(index, means_men, <span class="hljs-attribute">marker</span>=<span class="hljs-string">&quot;o&quot;</span>, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&quot;-&quot;</span>, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;r&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;Men line&quot;</span>)<br>    plt.plot(index, -means_women, <span class="hljs-attribute">marker</span>=<span class="hljs-string">&quot;.&quot;</span>, <span class="hljs-attribute">linestyle</span>=<span class="hljs-string">&quot;--&quot;</span>, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;Women line&quot;</span>)<br><br>    # 设置图形标示(两种,和柱状图对应)<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(index, means_men):<br>        plt.text(x, y+1, y, <span class="hljs-attribute">ha</span>=<span class="hljs-string">&quot;center&quot;</span>, <span class="hljs-attribute">va</span>=<span class="hljs-string">&quot;bottom&quot;</span>)<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(index, means_women):<br>        plt.text(x, -y-1, y, <span class="hljs-attribute">ha</span>=<span class="hljs-string">&quot;center&quot;</span>, <span class="hljs-attribute">va</span>=<span class="hljs-string">&quot;top&quot;</span>)<br><br>    # 设置Y轴和图例位置<br>    plt.ylim(-45, 80)<br>    plt.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&quot;upper left&quot;</span>, <span class="hljs-attribute">shadow</span>=<span class="hljs-literal">True</span>)<br><br>    # 图形显示<br>    plt.show()<br>    return<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/高级柱状图.png" alt=""></p><h4 id="7、层次柱状图"><a href="#7、层次柱状图" class="headerlink" title="7、层次柱状图"></a>7、层次柱状图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">table_plot</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    table plot</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 生成测试数据</span><br>    data = np.array([<br>        [<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>],<br>        [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>],<br>        [<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>]<br>    ])<br><br>    <span class="hljs-comment"># 设置标题</span><br>    plt.title(<span class="hljs-string">&quot;层次柱状图&quot;</span>, fontproperties=myfont)<br><br>    <span class="hljs-comment"># 设置相关参数</span><br>    index = np.arange(<span class="hljs-built_in">len</span>(data[<span class="hljs-number">0</span>]))<br>    color_index = [<span class="hljs-string">&quot;r&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>]<br><br>    <span class="hljs-comment"># 声明底部位置</span><br>    bottom = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>])<br><br>    <span class="hljs-comment"># 依次画图,并更新底部位置</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data)):<br>        plt.bar(index, data[i], width=<span class="hljs-number">0.5</span>, color=color_index[i], bottom=bottom, alpha=<span class="hljs-number">0.7</span>, label=<span class="hljs-string">&quot;标签 %d&quot;</span> % i)<br>        bottom += data[i]<br><br>    <span class="hljs-comment"># 设置图例位置</span><br>    plt.legend(loc=<span class="hljs-string">&quot;upper left&quot;</span>, prop=myfont, shadow=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 图形显示</span><br>    plt.show()<br>    <span class="hljs-keyword">return</span><br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/层次柱状图.png" alt=""></p><h4 id="8、直方图"><a href="#8、直方图" class="headerlink" title="8、直方图"></a>8、直方图</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def histograms_plot():<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    histograms plot</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # 生成测试数据<br>    mu, sigma = 100, 15<br>    x = mu + sigma * np.random.randn(10000)<br><br>    # 设置标题<br>    plt.title(<span class="hljs-string">&quot;直方图&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br><br>    # 画直方图, 并返回相关结果<br>    n, bins, patches = plt.hist(x, <span class="hljs-attribute">bins</span>=50, <span class="hljs-attribute">normed</span>=1, <span class="hljs-attribute">cumulative</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;green&quot;</span>, <span class="hljs-attribute">alpha</span>=0.6, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;直方图&quot;</span>)<br><br>    # 根据直方图返回的结果, 画折线图<br>    y = mlab.normpdf(bins, mu, sigma)<br>    plt.plot(bins, y, <span class="hljs-string">&quot;r--&quot;</span>, <span class="hljs-attribute">label</span>=<span class="hljs-string">&quot;线条&quot;</span>)<br><br>    # 设置图例位置<br>    plt.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&quot;upper left&quot;</span>, <span class="hljs-attribute">prop</span>=myfont, <span class="hljs-attribute">shadow</span>=<span class="hljs-literal">True</span>)<br><br>    # 图形显示<br>    plt.show()<br>    return<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/直方图.png" alt=""></p><h4 id="9、饼图"><a href="#9、饼图" class="headerlink" title="9、饼图"></a>9、饼图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pie_plot</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    pie plot</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 生成测试数据</span><br>    sizes = [<span class="hljs-number">15</span>, <span class="hljs-number">30</span>, <span class="hljs-number">45</span>, <span class="hljs-number">10</span>]<br>    labels = [<span class="hljs-string">&quot;Frogs&quot;</span>, <span class="hljs-string">&quot;中文&quot;</span>, <span class="hljs-string">&quot;Dogs&quot;</span>, <span class="hljs-string">&quot;Logs&quot;</span>]<br>    colors = [<span class="hljs-string">&quot;yellowgreen&quot;</span>, <span class="hljs-string">&quot;gold&quot;</span>, <span class="hljs-string">&quot;lightskyblue&quot;</span>, <span class="hljs-string">&quot;lightcoral&quot;</span>]<br><br>    <span class="hljs-comment"># 设置标题</span><br>    plt.title(<span class="hljs-string">&quot;饼图&quot;</span>, fontproperties=myfont)<br><br>    <span class="hljs-comment"># 设置突出参数</span><br>    explode = [<span class="hljs-number">0</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br><br>    <span class="hljs-comment"># 画饼状图</span><br>    patches, l_text, p_text = plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=<span class="hljs-string">&quot;%1.1f%%&quot;</span>, shadow=<span class="hljs-literal">True</span>, startangle=<span class="hljs-number">90</span>)<br>    <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> l_text:<br>        text.set_fontproperties(myfont)<br>    plt.axis(<span class="hljs-string">&quot;equal&quot;</span>)<br><br>    <span class="hljs-comment"># 图形显示</span><br>    plt.show()<br>    <span class="hljs-keyword">return</span><br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/饼图.png" alt=""></p><h4 id="10、散点图"><a href="#10、散点图" class="headerlink" title="10、散点图"></a>10、散点图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">scatter_plot</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    scatter plot</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 生成测试数据</span><br>    point_count = <span class="hljs-number">1000</span><br>    x_index = np.random.random(point_count)<br>    y_index = np.random.random(point_count)<br><br>    <span class="hljs-comment"># 设置标题</span><br>    plt.title(<span class="hljs-string">&quot;散点图&quot;</span>, fontproperties=myfont)<br><br>    <span class="hljs-comment"># 设置相关参数</span><br>    color_list = np.random.random(point_count)<br>    scale_list = np.random.random(point_count) * <span class="hljs-number">100</span><br><br>    <span class="hljs-comment"># 画散点图</span><br>    plt.scatter(x_index, y_index, s=scale_list, c=color_list, marker=<span class="hljs-string">&quot;o&quot;</span>)<br><br>    <span class="hljs-comment"># 图形显示</span><br>    plt.show()<br>    <span class="hljs-keyword">return</span><br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/散点图.png" alt=""></p><h4 id="11、雷达图"><a href="#11、雷达图" class="headerlink" title="11、雷达图"></a>11、雷达图</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def radar_plot():<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    radar plot</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>    # 生成测试数据<br>    labels = np.array([<span class="hljs-string">&quot;A组&quot;</span>, <span class="hljs-string">&quot;B组&quot;</span>, <span class="hljs-string">&quot;C组&quot;</span>, <span class="hljs-string">&quot;D组&quot;</span>, <span class="hljs-string">&quot;E组&quot;</span>, <span class="hljs-string">&quot;F组&quot;</span>])<br>    data = np.array([68, 83, 90, 77, 89, 73])<br>    theta = np.linspace(0, 2*np.pi, len(data), <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">False</span>)<br><br>    # 数据预处理<br>    data = np.concatenate((data, [data[0]]))<br>    theta = np.concatenate((theta, [theta[0]]))<br><br>    # 画图方式<br>    plt.subplot(111, <span class="hljs-attribute">polar</span>=<span class="hljs-literal">True</span>)<br>    plt.title(<span class="hljs-string">&quot;雷达图&quot;</span>, <span class="hljs-attribute">fontproperties</span>=myfont)<br><br>    # 设置<span class="hljs-string">&quot;theta grid&quot;</span>/<span class="hljs-string">&quot;radar grid&quot;</span><br>    plt.thetagrids(theta*(180/np.pi), <span class="hljs-attribute">labels</span>=labels, <span class="hljs-attribute">fontproperties</span>=myfont)<br>    plt.rgrids(np.arange(20, 100, 20), <span class="hljs-attribute">labels</span>=np.arange(20, 100, 20), <span class="hljs-attribute">angle</span>=0)<br>    plt.ylim(0, 100)<br><br>    # 画雷达图,并填充雷达图内部区域<br>    plt.plot(theta, data, <span class="hljs-string">&quot;bo-&quot;</span>, <span class="hljs-attribute">linewidth</span>=2)<br>    plt.fill(theta, data, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;red&quot;</span>, <span class="hljs-attribute">alpha</span>=0.25)<br><br>    # 图形显示<br>    plt.show()<br>    return<br></code></pre></td></tr></table></figure><p><img src="https://ningshixian.github.io/resources/images/雷达图.png" alt=""></p><h2 id="数据可视化的库推荐"><a href="#数据可视化的库推荐" class="headerlink" title="数据可视化的库推荐"></a>数据可视化的库推荐</h2><p>这里推荐几个比较常用的：</p><ul><li>Matplotlib</li><li>Plotly</li><li><a href="http://seaborn.pydata.org/tutorial.html">Seaborn 教程</a></li><li>Ggplot</li><li>Bokeh</li><li>Pyechart</li><li>Pygal</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AllenNLP安装</title>
    <link href="/2018/11/05/2018-11-05-allennlp%E5%AE%89%E8%A3%85/"/>
    <url>/2018/11/05/2018-11-05-allennlp%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h2 id="Linux安装python3-6"><a href="#Linux安装python3-6" class="headerlink" title="Linux安装python3.6"></a>Linux安装python3.6</h2><p>AllenNLP requires Python 3.6.1 or later. and CUDA 9 only (or no GPU)<br><a href="https://www.cnblogs.com/kimyeee/p/7250560.html">https://www.cnblogs.com/kimyeee/p/7250560.html</a></p><span id="more"></span><ol><li>安装依赖环境</li></ol><p>　　<code># yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel</code></p><ol><li>下载Python3</li></ol><p>　　<a href="https://www.python.org/downloads/">https://www.python.org/downloads/</a></p><p>　　<code># wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz</code></p><p>3.安装python3</p><p>　　我个人习惯安装在/usr/local/python3（具体安装位置看个人喜好）</p><p>　　创建目录：</p><p>　　<code># mkdir -p /usr/local/python3</code></p><p>　　进入python3目录，解压下载好的Python-3.x.x.tgz包(具体包名因你下载的Python具体版本不不同⽽而不不同，如：我下载的是Python3.6.5.那我这里就是Python-3.6.5.tgz)</p><p>　　<code># tar -zxvf Python-3.6.5.tgz</code></p><p>4.进入解压后的目录，编译安装。</p><blockquote><p>在安装python时 ./configure指定引入ssl, 否则使用pip３６ install xxx命令时报ssl缺失的错误<br><a href="https://blog.csdn.net/youcijibi/article/details/78306961">https://blog.csdn.net/youcijibi/article/details/78306961</a><br><a href="https://blog.csdn.net/ChangerJJLee/article/details/83112006">https://blog.csdn.net/ChangerJJLee/article/details/83112006</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">cd</span> Python-3.6.5</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">./configure --with-ssl --prefix=/usr/local/python3</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">make &amp;&amp; make install</span><br></code></pre></td></tr></table></figure><p>5.建立python3的软链</p><p>　　<code># ln -s /usr/local/python3/bin/python3 /usr/bin/python36</code></p><p>6.并将/usr/local/python3/bin加入PATH</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># vim ~/.bash_profile</span><br><span class="hljs-comment"># .bash_profile</span><br><span class="hljs-comment"># Get the aliases and functions</span><br><span class="hljs-keyword">if</span> [ -f ~/.bashrc ]; <span class="hljs-keyword">then</span><br>. ~/.bashrc<br><span class="hljs-keyword">fi</span><br><span class="hljs-comment"># User specific environment and startup programs</span><br>PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$HOME</span>/bin:/usr/local/python3/bin<br><span class="hljs-built_in">export</span> PATH<br><br>按ESC，输入:wq回车退出。<br></code></pre></td></tr></table></figure><p>修改完记得执行行下面的命令，让上一步的修改生效：</p><p>　　<code># source ~/.bash_profile</code></p><p>检查Python3及pip3是否正常可用：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># python36 -V</span><br><span class="hljs-attribute">Python</span> <span class="hljs-number">3</span>.<span class="hljs-number">6</span>.<span class="hljs-number">5</span><br><span class="hljs-comment"># pip36 -V</span><br><span class="hljs-attribute">pip</span> <span class="hljs-number">9</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span> from /usr/local/python3/lib/python3.<span class="hljs-number">6</span>/site-packages (python <span class="hljs-number">3</span>.<span class="hljs-number">6</span>)<br></code></pre></td></tr></table></figure><p>7.不行的话在创建一下pip3的软链接</p><p>　　<code># ln -s /usr/local/python3/bin/pip3 /usr/bin/pip36</code></p><h2 id="安装allennlp包"><a href="#安装allennlp包" class="headerlink" title="安装allennlp包"></a>安装allennlp包</h2><blockquote><p><a href="https://github.com/allenai/allennlp">https://github.com/allenai/allennlp</a></p></blockquote><ol><li>Installing the library and dependencies is simple using pip.</li></ol><p>　　<code># pip36 install allennlp</code></p><ol><li>You can now test your installation with <code>allennlp test-install .</code></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
