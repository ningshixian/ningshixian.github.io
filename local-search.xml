<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>My New Post</title>
    <link href="/2023/04/24/My-New-Post/"/>
    <url>/2023/04/24/My-New-Post/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/04/24/hello-world/"/>
    <url>/2023/04/24/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LoRa 学习笔记</title>
    <link href="/2023/04/23/LoRa%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2023/04/23/LoRa%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="LoRa是什么？"><a href="#LoRa是什么？" class="headerlink" title="LoRa是什么？"></a>LoRa是什么？</h2><p>LoRA，出自论文<a href="https://arxiv.org/abs/2106.09685">《LoRA: Low-Rank Adaptation of Large Language Models》</a>，是Microsoft 于 2021 年推出的一项新技术，用于微调大型语言模型 (LLM)。<br>比如，GPT-3有1750亿参数，为了让它能干特定领域的活儿，需要做微调，但是如果直接对GPT-3做微调，成本太高太麻烦了。<br>LoRA的做法是，冻结预训练好的模型权重参数，然后在每个Transformer（Transforme就是GPT的那个T）块里注入可训练的层，由于不需要对模型的权重参数重新计算梯度，所以，大大减少了需要训练的计算量。<br>研究发现，LoRA的微调质量与全模型微调相当，我愿称之为神器。<br>要做个比喻的话，就好比是大模型的一个小模型，或者说是一个插件。<br>LoRA本来是给大语言模型准备的，但把它用在cross-attention layers（交叉关注层）也能影响用文字生成图片的效果。</p><h2 id="LoRa方法简介"><a href="#LoRa方法简介" class="headerlink" title="LoRa方法简介"></a>LoRa方法简介</h2><blockquote><p><a href="https://kexue.fm/archives/9590">梯度视角下的LoRA：简介、分析、猜测及推广</a></p></blockquote><p>LoRA借鉴了上述结果，提出对于预训练的参数矩阵$W_0∈ℝ^{m×n}$，我们不去直接微调$W_0$，而是对增量做低秩分解假设：</p><img src="/2023/04/23/LoRa%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png" class="" title="image.png"><p>其中$U,V$之一用全零初始化，$W_0$固定不变，优化器只优化$U,V$。由于本征维度很小的结论，所以$r$我们可以取得很小，很多时候我们甚至可以直接取$1$。所以说，LoRA是一种参数高效的微调方法，至少被优化的参数量大大降低了。</p><img src="/2023/04/23/LoRa%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.png" class="" title="image.png"><h2 id="PEFT对LORA的实现"><a href="#PEFT对LORA的实现" class="headerlink" title="PEFT对LORA的实现"></a>PEFT对LORA的实现</h2><p><a href="https://blog.csdn.net/weixin_44826203/article/details/129733930">https://blog.csdn.net/weixin_44826203/article/details/129733930</a></p><p><a href="https://mp.weixin.qq.com/s/bWl9Ke9dHmHPQEad-XZsyQ">https://mp.weixin.qq.com/s/bWl9Ke9dHmHPQEad-XZsyQ</a></p>]]></content>
    
    
    <categories>
      
      <category>ChatGPT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LoRa</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PPO算法</title>
    <link href="/2023/03/14/PPO%E7%AE%97%E6%B3%95/"/>
    <url>/2023/03/14/PPO%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><ul><li>根据 OpenAI 的<a href="https://blog.openai.com/openai-baselines-ppo/">官方博客</a>, PPO 已经成为他们在强化学习上的默认算法. <strong>如果一句话概括 PPO: OpenAI 提出的一种解决 Policy Gradient 不好确定 Learning rate (或者 Step size) 的问题. 因为如果 step size 过大, 学出来的 Policy 会一直乱动, 不会收敛, 但如果 Step Size 太小, 对于完成训练, 我们会等到绝望. PPO 利用 New Policy 和 Old Policy 的比例, 限制了 New Policy 的更新幅度, 让 Policy Gradient 对稍微大点的 Step size 不那么敏感.</strong></li><li>总的来说 PPO 是一套 Actor-Critic 结构, Actor 想<strong>最大化</strong> J_PPO, Critic 想<strong>最小化</strong> L_BL.</li></ul><h2 id="PG-Add-Constraint-→-PPO"><a href="#PG-Add-Constraint-→-PPO" class="headerlink" title="PG Add Constraint → PPO"></a>PG Add Constraint → PPO</h2><p>简单来说，PPO就是Policy Gradient的”off-policy”版本。为了满足<strong>Importance Sampling</strong>的使用条件，即防止$p_{\theta}$和$p_{\theta_{old}}$两个概率分布相差太多，PPO提供了两个解决方案：</p><ol><li>TRPO（Trust Region Policy Optimization）在目标函数外使用KL Penalty (惩罚项）来限制策略更新，希望在训练的过程中，new Policy 和 old Policy 的输出不要相差太大（因为输出的 action 是概率分布，也即计算两个概率分布之间的差别）。但是这种方法实现起来很复杂，需要更多的计算时间。</li></ol><p>$L^{PPO}(\theta)=E_{t}\left[r_t(\theta) * A_{t}\right]-\beta·KL[\pi_{\theta_{init}}|\pi_{\theta}]$</p><ol><li>PPO-Clip 在目标函数中使用 <strong>Clipped surrogate objective function </strong>来直接裁剪概率比率。所要做的事情本质上和TRPO是一样的，都是为了让两个分布（$θ$和$θ’$）之间的差距不致过大</li></ol><p>$L^{C L I P}(\theta)=\hat{\mathbb{E}}_{t}\left[\min \left(r_{t}(\theta) \hat{A}_{t}, \operatorname{clip}\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right]$</p><p>其中，$\beta$是可以动态调整的，称之为自适应KL惩罚（adaptive KL penalty）；$r_t(\theta)$表示Ratio Function，指产生同样的 token，在 Policy Model 和 Alignment Model 下的概率比值（It’s the probability of taking action a_t at state s_t in the current policy divided by the previous one. ）</p><p>$r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$</p><p>正如我们所看到的，$r_t(\theta)$表示当前策略和旧策略之间的概率比率，<strong>是估计旧策略和当前策略之间差异的一种简单方法</strong>。</p><ul><li>如果$r_t(\theta)&gt;1$，则在状态$s_t$下，动作$a_t$在当前策略中比旧策略更有可能执行。</li><li>如果$0&lt;r_t(\theta)&lt;1$，则在当前策略下执行该动作的可能性比旧策略下低。</li></ul><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1677118748202.png" class="" title="image.png"><h3 id="PPO-Clip-算法直观理解"><a href="#PPO-Clip-算法直观理解" class="headerlink" title="PPO-Clip 算法直观理解"></a>PPO-Clip 算法直观理解</h3><p>$L^{C L I P}(\theta)=\hat{\mathbb{E}}_{t}\left[\min \left(r_{t}(\theta) \hat{A}_{t}, \operatorname{clip}\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right]$<br>$r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$</p><p>整个目标函数在$min$这个大括号里有两部分，最终对比两部分哪部分更小，就取哪部分的值。<br>在括号的第二部分中，</p><ul><li>首先是裁剪函数$clip$：如果$p_{\theta}(a_t|s_t)$和$p_{\theta^k}(a_t|s_t)$之间的概率比落在范围$(1-ε)$和$(1+ε)$之外，$\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}$将被剪裁，使得其值最小不小于$(1-ε)$，最大不大于$(1+ε)$</li></ul><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1677049573025.jpeg" class="" title="image.png"><ul><li>然后是$clip$括号外乘以$A^{\theta’}(s_t,a_t)$：当$A&gt;0$，则说明这是好动作，那么希望增大这个action的几率$p_{\theta}(a_t|s_t)$，但是又不希望两者差异，即比值$\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}$太悬殊，所以增大到比值为$1+ε$就不要再增加了；当$A&lt;0$，则说明该动作不是好动作，那么希望这个action出现的几率$p_{\theta}(a_t|s_t)$越小越好，但$\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}$最小不能小过$(1-ε)$</li></ul><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1677049702486-4c83f34e-d965-4fd1-8337-135872b51c60.png" class="" title="image.png"><p>换言之，这个裁剪算法和KL散度约束所要做的事情本质上是一样的，都是为了让两个分布之间的差距不致过大，但裁剪算法相对好实现，别看看起来复杂，其实代码很好写，示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">// ratios即为重要性权重<br>// 括号里的environment_log_probs代表用于与环境交互的策略<br>ratios = torch.exp(log_probs - environment_log_probs)<br> <br>// 分别用sur_1、sur_2来计算公式的两部分<br><br>// 第一部分是重要性权重乘以优势函数<br>sur_1 = ratios * advs<br> <br>// 第二部分是具体的裁剪过程<br>sur_2 = torch.clamp(ratios, <span class="hljs-number">1</span> - clip_eps, <span class="hljs-number">1</span> + clip_eps) * advs<br> <br>// 最终看谁更小则取谁<br>clip_loss = -torch.<span class="hljs-built_in">min</span>(sur_1,sur_2).mean()<br></code></pre></td></tr></table></figure><h2 id="简单-PPO-的代码解读"><a href="#简单-PPO-的代码解读" class="headerlink" title="简单 PPO 的代码解读"></a>简单 PPO 的代码解读</h2><blockquote><p><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DPPO">https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DPPO</a></p></blockquote><p>我们用 Tensorflow 搭建神经网络, tensorboard 中可以看清晰的看到我们是如果搭建的:</p><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1679394322067.png" class="" title="image.png"><p>图中的 pi 就是我们的 Actor 了. 每次要进行 PPO 更新 Actor 和 Critic 的时候, 我们有需要将 pi 的参数复制给 oldpi. 这就是 update_oldpi 这个 operation 在做的事. Critic 和 Actor 的内部结构, 我们不会打开细说了. 因为就是一堆的神经网络而已. 这里的 Actor 使用了 normal distribution 正态分布输出动作.</p><p>这个 PPO 我们可以用一个 Python 的 class 代替:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 建 Actor Critic 网络</span><br>        <span class="hljs-comment"># 搭计算图纸 graph</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s, a, r</span>):<br>        <span class="hljs-comment"># 更新 PPO</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-comment"># 选动作</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_v</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-comment"># 算 state value</span><br></code></pre></td></tr></table></figure><p>而这个 PPO 和 env 环境的互动可以简化成这样.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">ppo = PPO()<br><span class="hljs-keyword">for</span> ep <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EP_MAX):<br>    s = env.reset()<br>    buffer_s, buffer_a, buffer_r = [], [], []<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EP_LEN):<br>        env.render()<br>        a = ppo.choose_action(s)<br>        s_, r, done, _ = env.step(a)<br>        buffer_s.append(s)<br>        buffer_a.append(a)<br>        buffer_r.append((r+<span class="hljs-number">8</span>)/<span class="hljs-number">8</span>)    <span class="hljs-comment"># normalize reward, 发现有帮助</span><br>        s = s_<br><br>        <span class="hljs-comment"># 如果 buffer 收集一个 batch 了或者 episode 完了</span><br>        <span class="hljs-keyword">if</span> (t+<span class="hljs-number">1</span>) % BATCH == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> t == EP_LEN-<span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># 计算 discounted reward</span><br>            v_s_ = ppo.get_v(s_)<br>            discounted_r = []<br>            <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> buffer_r[::-<span class="hljs-number">1</span>]:<br>                v_s_ = r + GAMMA * v_s_<br>                discounted_r.append(v_s_)<br>            discounted_r.reverse()<br><br>            bs, ba, br = batch(buffer_s, buffer_a, discounted_r)<br>            <span class="hljs-comment"># 清空 buffer</span><br>            buffer_s, buffer_a, buffer_r = [], [], []<br>            ppo.update(bs, ba, br)  <span class="hljs-comment"># 更新 PPO</span><br></code></pre></td></tr></table></figure><p>了解了这些更新步骤, 我们就来看看如何更新我们的 PPO. 我们更新 Critic 的时候是根据 刚刚计算的 discounted_r 和自己分析出来的 state value 这两者的差 (advantage). 然后最小化这个差值:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.advantage = self.tfdc_r - self.v   <span class="hljs-comment"># discounted reward - Critic 出来的 state value</span><br>        self.closs = tf.reduce_mean(tf.square(self.advantage))<br>        self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)<br><br></code></pre></td></tr></table></figure><p>两种更新 Actor 的方式 KL penalty 和 clipped surrogate objective</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.tfa = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, A_DIM], <span class="hljs-string">&#x27;action&#x27;</span>)<br>        self.tfadv = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;advantage&#x27;</span>)<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;loss&#x27;</span>):<br>            <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;surrogate&#x27;</span>):<br>                ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)<br>                surr = ratio * self.tfadv   <span class="hljs-comment"># surrogate objective</span><br>            <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span>:      <span class="hljs-comment"># 如果用 KL penatily</span><br>                self.tflam = tf.placeholder(tf.float32, <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;lambda&#x27;</span>)<br>                kl = kl_divergence(oldpi, pi)<br>                self.kl_mean = tf.reduce_mean(kl)<br>                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))<br>            <span class="hljs-keyword">else</span>:                               <span class="hljs-comment"># 如果用 clipping 的方式</span><br>                self.aloss = -tf.reduce_mean(tf.minimum(<br>                    surr,<br>                    tf.clip_by_value(ratio, <span class="hljs-number">1.</span>-METHOD[<span class="hljs-string">&#x27;epsilon&#x27;</span>], <span class="hljs-number">1.</span>+METHOD[<span class="hljs-string">&#x27;epsilon&#x27;</span>])*self.tfadv))<br><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;atrain&#x27;</span>):<br>            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)<br><br></code></pre></td></tr></table></figure><p>好了, 接下来就是最重要的更新 PPO 时间了, 同样, 如果觉得我这些代码省略的很严重, 请直接前往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/simply_PPO.py">Github 看全套代码</a>. 注意的是, 这个 update 的步骤里, 我们用 for loop 更新了很多遍 Actor 和 Critic, 在 loop 之前, pi 和 old pi 是一样的, 每次 loop 的之后, pi 会变动, 而 old pi 不变, 这样这个 surrogate 就会开始变动了. 这就是 PPO 的精辟.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s, a, r</span>):<br>        <span class="hljs-comment"># 先要将 oldpi 里的参数更新 pi 中的</span><br>        self.sess.run(self.update_oldpi_op)<br><br>        <span class="hljs-comment"># 更新 Actor 时, kl penalty 和 clipping 方式是不同的</span><br>        <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span>:  <span class="hljs-comment"># 如果用 KL penalty</span><br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A_UPDATE_STEPS):<br>                _, kl = self.sess.run(<br>                        [self.atrain_op, self.kl_mean],<br>                        &#123;self.tfs: s, self.tfa: a, self.tfadv: adv, self.tflam: METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>]&#125;)<br>                <span class="hljs-comment"># 之后根据 kl 的值, 调整 METHOD[&#x27;lam&#x27;] 这个参数</span><br>        <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># 如果用 clipping 的方法</span><br>            [self.sess.run(self.atrain_op, &#123;self.tfs: s, self.tfa: a, self.tfadv: adv&#125;) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A_UPDATE_STEPS)]<br><br>        <span class="hljs-comment"># 更新 Critic 的时候, 他们是一样的</span><br>        [self.sess.run(self.ctrain_op, &#123;self.tfs: s, self.tfdc_r: r&#125;) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(C_UPDATE_STEPS)]<br><br></code></pre></td></tr></table></figure><p>最后我们看一张学习的效果图:</p><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/1679394584080.png" class="" title="image.png"><p>好了这就是整个 PPO 的主要流程了, 其他的步骤都没那么重要了, 可以直接在我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/simply_PPO.py">Github 看全套代码</a> 中轻松弄懂. </p><h2 id="PPO-Actor-Critic-Loss"><a href="#PPO-Actor-Critic-Loss" class="headerlink" title="PPO Actor-Critic Loss"></a>PPO Actor-Critic Loss</h2><p>PPO Actor-Critic 风格的最终 Clipped Surrogate Objective Loss 看起来像这样，它是 Clipped Surrogate Objective 函数、Value Loss Function 和 Entropy bonus 的组合：</p><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/image.png" class="" title="image.png"><p>DeepMind 总结 OpenAI conference 上的 PPO 的伪代码</p><img src="/2023/03/14/PPO%E7%AE%97%E6%B3%95/image2.png" class="" title="image.png"><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.51cto.com/u_15721703/5575736">李宏毅老师的深度强化学习课程</a><br><a href="https://huggingface.co/deep-rl-course/unit8/introduction-sf?fw=pt">Unit 8. Introduction to PPO - Hugging Face</a><br><a href="https://huggingface.co/blog">图解人工反馈强化学习(RLHF) - Hugging Face</a></p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PPO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PPO要点摘录</title>
    <link href="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/"/>
    <url>/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>PPO（Proximal Policy Optimization）算法中，状态、动作、价值函数、奖励和策略模型的含义如下：</p><ol><li>状态（state）：状态是指环境当前的状态，如：对话历史与之前生成的序列。在强化学习中，智能体需要根据当前状态来做出决策。</li><li>动作（action）：动作是指智能体在当前状态下选择的行动，如：生成每一个token，从词表中采样即是一个动作。在强化学习中，智能体需要根据当前状态选择一个最优的动作。</li><li>价值函数（Value Function）：价值函数则是智能体对自己行为好坏的评估，它可以帮助智能体更好地指导自己的决策。价值函数可以根据当前状态或状态-动作对，预测智能体能够获得的期望回报。比如，在一个赛车游戏中，如果智能体在某个状态下选择了某个动作，那么价值函数可以预测出这个动作会获得多少奖励分数。通过不断地优化价值函数，智能体可以学习到在环境中做出最优的决策。</li><li>奖励（Reward）：奖励函数就像是老师对学生的评分一样，通过环境的反馈，对智能体的行为进行评价，给出一个分数用来衡量智能体做出的动作是否正确。比如，在一个赛车游戏中，如果智能体成功完成一次绕过障碍的动作，那么就可以获得一定的奖励分数。通过不断地累积奖励分数，智能体可以学习到在环境中做出最优的动作。</li><li>策略模型（Policy Model）：策略模型是指智能体根据当前状态选择动作的概率分布。在PPO算法中，策略模型通常使用神经网络来进行建模，输出每个动作的概率。</li></ol><p>综上所述，PPO算法需要使用神经网络来表示价值函数和策略模型，根据当前状态选择动作，并通过环境的反馈来获得奖励。通过不断优化策略和价值函数，PPO算法可以使智能体在环境中获得更高的奖励，从而实现智能体的学习和决策能力的提高。</p><blockquote><p>参考：<a href="https://spinningup.openai.com/en/latest/algorithms/vpg.html">https://spinningup.openai.com/en/latest/algorithms/vpg.html</a></p></blockquote><p>actor在进行训练之前，会先与环境进行交互，然后得到一组训练数据（由多条状态序列构成）。</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/46d23d87a5c5abf0ca1c48a7d816d46e.svg" class=""><p>每回合Trajectory<img src="https://cdn.nlark.com/yuque/__latex/ec1cc44b87fcbbced12dabd7375d36d3.svg#from=url&amp;id=SwJ8M&amp;originHeight=18&amp;originWidth=16&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;title=" alt="">的奖励</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/299758ac4817013ed2822cdfc136f77a.svg" class=""><p>最终目标就是要使<strong>期望奖励最大</strong>！</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/56897ebb6501e4a47704b198d7bcf5b9.svg" class=""><p>根据链式法则公式$\nabla \log f(x) = \frac{1}{f(x)} \nabla f(x)$，计算式(2)的梯度</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678462247057-e3c72c54-7cbc-4ab6-a0da-f50a57ffe335.png" class="" title="image.png"><blockquote><p> <img src="https://cdn.nlark.com/yuque/0/2023/svg/8420697/1678463145871-d64a91f7-2168-4694-8f90-ad3b099a879a.svg#clientId=u6f13600c-cac6-4&amp;from=paste&amp;id=u2bca4fe6&amp;originHeight=13&amp;originWidth=26&amp;originalType=url&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u3fe21edd-a2d2-441f-9acc-87d0a71c77c&amp;title=" alt="">是当前策略的优势函数<br>$\pi_{\theta}$表示带有参数$θ$的策略，$J(\pi_{\theta})$表示该策略的 期望回报</p></blockquote><p>策略梯度算法的工作原理是通过策略性能的随机梯度上升来更新策略参数：</p><p>$\theta ← \theta+\alpha\nabla{R}$</p><p>策略梯度（Policy Gradient）方法伪代码</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678463814708-26dacbd5-6edf-4056-bdec-5cb2cea3e278.png" class="" title="image.png"><hr><p>为了使得训练资料可以反复使用，使用两个actor</p><p>其中，一个固定参数，另一个可训练，通过<strong>Importance Sampling</strong>来联系两个分布：</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678285382558-74f3aa01-8cff-4c62-9528-a4b64c099e7c.png" class="" title="image.png"><blockquote><p>代入式4，梯度计算为：</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678285679173-c8804740-750d-4df1-8344-982403686226.png" class="" title="image.png"><p><img src="https://cdn.nlark.com/yuque/0/2023/png/8420697/1678285693021-0331d447-0af9-4991-8da0-54016205c657.png#averageHue=%23f2f5f4&amp;clientId=uef89bebf-095c-4&amp;from=paste&amp;height=55&amp;id=u1633ed96&amp;name=image.png&amp;originHeight=110&amp;originWidth=804&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=61955&amp;status=done&amp;style=none&amp;taskId=u234f4b37-ec52-4e28-a067-40e220a7611&amp;title=&amp;width=402" alt="image.png"></p></blockquote><p>代入式2，新的目标函数为：</p><p>$L=E_{\tau\in p_{\theta}(\tau)} [A(\tau)]=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[r_{\theta}(t)A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]$</p><p>$r_{\theta}(t)=\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} \mid s_{t}\right)}$</p><p>Advantages are computed using Generalized Advantage Estimation (GAE): $A_t$叫做策略优势估计，是PPO算法的核心!</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678443935731-6516a589-895f-43e6-bcb3-59c661d81ad8.png" class="" title="image.png"><p>PPO 进一步给目标函数添加约束项，防止$p_{\theta}$和$p_{\theta’}$两个概率分布相差太多！PPO 是一种 on-policy 算法，有两种主要变体：PPO-Penalty 和 PPO-Clip。</p><ul><li>PPO-Penalty：</li></ul><p>$L(s,a,θ_k,θ) = r_{\theta}(t)·A^{\pi_{\theta k}}(s,a)-\beta·KL(θ,θ_k)$</p><p>$r_{\theta}(t)=\frac{\pi_{\theta}\left(a \mid s\right)}{\pi_{\theta^{\prime}}\left(a \mid s\right)}$</p><ul><li>PPO-Clip：</li></ul><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678464277268-b4d8e192-dcc1-4e7f-9544-add909d41690.png" class="" title="image.png"><p>PPO方法伪代码</p><img src="/2023/03/14/PPO%20%E8%A6%81%E7%82%B9%E6%91%98%E5%BD%95/1678464873211-15bbc7ba-8930-4e13-abaf-a106d6df8151.png" class="" title="image.png"><p>符号解释：</p><ul><li>$p_{\theta}$表示可训练策略模型</li><li>$p_{\theta’}$表示参数固定的策略模型</li><li>$\frac{p_{\theta}(·)}{p_{\theta{‘}}(·)}$表示 Importance Sampling 系数，指产生同样的 token，在 Policy Model 和 Alignment Model 下的概率比值</li><li>$A$ 或$R$表示奖励，其给出一种利用长期奖励$r_t$与短期奖励$V({s_t})$计算当前步的好坏，在第t步的$A_t$分数越高，则代表该位置生成的质量越高。<ul><li>Reward Model，是一个固定参数的模型，其输出值$r_t$</li><li>Value Function Model，是一个可训练的价值函数，其输出值$V({s_t})$代表生成的每一个token的质量，是一个短期的奖励</li></ul></li><li>$KL()$表示KL散度，用作一个惩罚项，约束着每一次梯度更新以后不要产生与原模型相距甚远的回复。保证了模型训练的稳定性。</li></ul>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PPO</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
